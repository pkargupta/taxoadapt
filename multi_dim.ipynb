{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_definitions import initializeLLM, promptLLM, constructPrompt\n",
    "import json\n",
    "from utils import clean_json_string\n",
    "from collections import deque\n",
    "from taxonomy import Node\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.topic = \"natural language processing\"\n",
    "        self.dimensions = [\"tasks\", \"datasets\", \"methodologies\", \"evaluation_methods\", \"real_world_domains\"]\n",
    "        # self.dimensions = [\"evaluation_methods\"]\n",
    "        self.llm = 'gpt'\n",
    "        self.init_levels = 2\n",
    "\n",
    "        self.dataset = \"Reasoning\"\n",
    "        self.data_dir = f\"datasets/multi_dim/{self.dataset.lower().replace(' ', '_')}/\"\n",
    "        self.internal = f\"{self.dataset}.txt\"\n",
    "        self.external = f\"{self.dataset}_external.txt\"\n",
    "        self.groundtruth = \"groundtruth.txt\"\n",
    "        \n",
    "        self.length = 512\n",
    "        self.dim = 768\n",
    "\n",
    "        self.iters = 4\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-04 07:36:42 config.py:729] Defaulting to use mp for distributed inference\n",
      "WARNING 12-04 07:36:42 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-04 07:36:42 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-04 07:36:42 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 12-04 07:36:43 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-04 07:36:43 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:43 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-04 07:36:43 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 12-04 07:36:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:43 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:43 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 12-04 07:36:43 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_4,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:43 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_4,7.json\n",
      "INFO 12-04 07:36:44 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f4a9f5d2400>, local_subscribe_port=42023, remote_subscribe_port=None)\n",
      "INFO 12-04 07:36:44 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:44 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:44 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "INFO 12-04 07:36:44 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb129e66ab14f2fbf1e701dc8ab0c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:47 model_runner.py:732] Loading model weights took 7.5122 GB\n",
      "INFO 12-04 07:36:47 model_runner.py:732] Loading model weights took 7.5122 GB\n",
      "INFO 12-04 07:36:49 distributed_gpu_executor.py:56] # GPU blocks: 15716, # CPU blocks: 4096\n",
      "INFO 12-04 07:36:51 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-04 07:36:51 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:51 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:51 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-04 07:36:58 custom_all_reduce.py:219] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:58 model_runner.py:1225] Graph capturing finished in 7 secs.\n",
      "INFO 12-04 07:36:58 custom_all_reduce.py:219] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3166293)\u001b[0;0m INFO 12-04 07:36:58 model_runner.py:1225] Graph capturing finished in 7 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = initializeLLM(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct a 2-Level Multi-Dimensional Taxonomy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import multi_dim_prompt, NodeListSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to make this a directed acyclic graph (DAG) so maintain a list of the nodes\n",
    "roots = {}\n",
    "id2node = {}\n",
    "label2node = {}\n",
    "idx = 0\n",
    "\n",
    "for dim in args.dimensions:\n",
    "    mod_topic = args.topic.replace(' ', '_').lower() + f\"_{dim}\"\n",
    "    root = Node(\n",
    "            id=idx,\n",
    "            label=mod_topic,\n",
    "            dimension=dim\n",
    "        )\n",
    "    roots[dim] = root\n",
    "    id2node[idx] = root\n",
    "    label2node[mod_topic] = root\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.34s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  2.00s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "queue = deque([node for id, node in id2node.items()])\n",
    "\n",
    "# if taking long, you can probably parallelize this between the different taxonomies (expand by level)\n",
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    label = curr_node.label\n",
    "    # expand\n",
    "    system_instruction, main_prompt, json_output_format = multi_dim_prompt(curr_node)\n",
    "    prompts = [constructPrompt(args, system_instruction, main_prompt + \"\\n\\n\" + json_output_format)]\n",
    "    outputs = promptLLM(args=args, prompts=prompts, schema=NodeListSchema, max_new_tokens=3000, json_mode=True, temperature=0.1, top_p=0.99)[0]\n",
    "    outputs = json.loads(clean_json_string(outputs)) if \"```\" in outputs else json.loads(outputs.strip())\n",
    "    outputs = outputs['root_topic'] if 'root_topic' in outputs else outputs[label]\n",
    "\n",
    "    # add all children\n",
    "    for key, value in outputs.items():\n",
    "        key = key.replace(' ', '_').lower()\n",
    "        if (key not in label2node) or ((key in label2node) and (label2node[key].dimension != curr_node.dimension)):\n",
    "            child_node = Node(\n",
    "                    id=len(id2node),\n",
    "                    label=key,\n",
    "                    dimension=curr_node.dimension,\n",
    "                    description=value['description'],\n",
    "                    parents=[curr_node]\n",
    "                )\n",
    "            curr_node.add_child(key, child_node)\n",
    "            id2node[child_node.id] = child_node\n",
    "            label2node[key] = child_node\n",
    "            if child_node.level < args.init_levels:\n",
    "                queue.append(child_node)\n",
    "        \n",
    "        else:\n",
    "            child_node = label2node[key]\n",
    "            child_node.add_parent(curr_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': Node(label=natural_language_processing_tasks, dim=tasks, description=None, level=0),\n",
       " 'datasets': Node(label=natural_language_processing_datasets, dim=datasets, description=None, level=0),\n",
       " 'methodologies': Node(label=natural_language_processing_methodologies, dim=methodologies, description=None, level=0),\n",
       " 'evaluation_methods': Node(label=natural_language_processing_evaluation_methods, dim=evaluation_methods, description=None, level=0),\n",
       " 'real_world_domains': Node(label=natural_language_processing_real_world_domains, dim=real_world_domains, description=None, level=0)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: natural_language_processing_evaluation_methods\n",
      "Dimension: evaluation_methods\n",
      "Description: None\n",
      "Level: 0\n",
      "----------------------------------------\n",
      "Children:\n",
      "     Label: intrinsic_evaluation_methods\n",
      "     Dimension: evaluation_methods\n",
      "     Description: Intrinsic evaluation methods assess the performance of specific components or algorithms in natural language processing tasks, such as part-of-speech tagging or named entity recognition.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: annotation_based_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Annotation-based evaluation methods involve manual annotation of data for tasks such as sentiment analysis or named entity recognition.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: correlation_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Correlation evaluation methods assess the correlation between predicted outputs and ground truth labels to measure performance.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: perplexity_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Perplexity evaluation measures the uncertainty of a language model by calculating the average branching factor of the model.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: similarity_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Similarity evaluation methods compare the similarity between generated text and reference text using metrics like cosine similarity or BLEU score.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: coverage_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Coverage evaluation assesses the extent to which a language model can generate responses that cover a diverse range of possible outputs.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "     Label: extrinsic_evaluation_methods\n",
      "     Dimension: evaluation_methods\n",
      "     Description: Extrinsic evaluation methods measure the performance of natural language processing systems in real-world applications or tasks, such as sentiment analysis or machine translation.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: task-based_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Task-based evaluation methods assess the performance of a natural language processing system by measuring its effectiveness in completing specific tasks or applications.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: user_feedback_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: User feedback evaluation methods involve gathering input from end-users to evaluate the performance and usability of a natural language processing system in real-world scenarios.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: impact_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Impact evaluation methods focus on assessing the broader effects and consequences of implementing a natural language processing system on various stakeholders or domains.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: domain-specific_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Domain-specific evaluation methods tailor the assessment criteria to the specific domain or industry in which a natural language processing system is applied, ensuring relevance and accuracy.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "     Label: human_evaluation_methods\n",
      "     Dimension: evaluation_methods\n",
      "     Description: Human evaluation methods involve human annotators or judges assessing the quality of natural language processing outputs based on criteria like fluency, coherence, and relevance.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: annotation_methods\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Annotation methods involve human annotators labeling data for evaluation purposes in natural language processing tasks.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: crowdsourcing_techniques\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Crowdsourcing techniques utilize a large group of individuals to evaluate and provide feedback on natural language processing systems.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: user_studies\n",
      "          Dimension: evaluation_methods\n",
      "          Description: User studies involve observing and collecting feedback from users interacting with natural language processing systems to assess their performance and usability.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: expert_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Expert evaluation involves domain experts assessing the quality and effectiveness of natural language processing systems based on their expertise.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "     Label: automatic_evaluation_methods\n",
      "     Dimension: evaluation_methods\n",
      "     Description: Automatic evaluation methods use metrics and algorithms to quantitatively measure the performance of natural language processing systems, such as BLEU score for machine translation or F1 score for named entity recognition.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: automatic_scoring_methods\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Techniques that automatically assign scores or grades to the performance of natural language processing systems.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: automatic_metric_computation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Methods that automatically calculate evaluation metrics such as precision, recall, and F1 score for assessing the performance of NLP models.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: automated_annotation_generation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Processes that automatically generate annotations or labels for NLP tasks, aiding in the evaluation of model outputs.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: automated_baseline_comparison\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Automated comparison of NLP model performance against baseline systems to measure improvements or deviations in evaluation metrics.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: automated_error_analysis\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Automated techniques for analyzing errors made by NLP models during evaluation, providing insights for model refinement.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "     Label: corpus-based_evaluation_methods\n",
      "     Dimension: evaluation_methods\n",
      "     Description: Corpus-based evaluation methods involve using annotated datasets or corpora to evaluate the performance of natural language processing systems, providing a standardized way to compare different approaches.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: annotation_agreement\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Annotation agreement methods assess the consistency and agreement among annotators when labeling data in a corpus for evaluation purposes.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: automatic_evaluation_metrics\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Automatic evaluation metrics involve the use of computational algorithms to assess the quality and performance of natural language processing systems based on predefined criteria.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: human_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Human evaluation methods rely on human judges to assess the quality and effectiveness of natural language processing systems by directly interacting with the output.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: cross-lingual_evaluation\n",
      "          Dimension: evaluation_methods\n",
      "          Description: Cross-lingual evaluation methods focus on assessing the performance and generalization capabilities of natural language processing systems across different languages and language pairs.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "roots['evaluation_methods'].display(0, indent_multiplier=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from paper import Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.data_dir):\n",
    "    os.makedirs(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"EMNLP/EMNLP2024-papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2954/2954 [00:00<00:00, 5019.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal: 2954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "internal_collection = {}\n",
    "\n",
    "with open(os.path.join(args.data_dir, 'internal.txt'), 'w') as i:\n",
    "    internal_count = 0\n",
    "    id = 0\n",
    "    for p in tqdm(ds['train']):\n",
    "        temp_dict = {\"Title\": p['title'], \"Abstract\": p['abstract']}\n",
    "        formatted_dict = json.dumps(temp_dict)\n",
    "        i.write(f'{formatted_dict}\\n')\n",
    "        internal_collection[id] = Paper(id, p['title'], p['abstract'], label_opts=args.dimensions, internal=True)\n",
    "        internal_count += 1\n",
    "        id += 1\n",
    "print(f'Internal: {internal_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_ds = load_dataset(\"TimSchopf/nlp_taxonomy_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178521/178521 [00:15<00:00, 11656.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External Count: 178521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "external_collection = {}\n",
    "\n",
    "with open(os.path.join(args.data_dir, 'external.txt'), 'w') as e:\n",
    "    external_count = 0\n",
    "    id = len(internal_collection)\n",
    "    for p in tqdm(external_ds['train']):\n",
    "        temp_dict = {\"Title\": p['title'], \"Abstract\": p['abstract']}\n",
    "        formatted_dict = json.dumps(temp_dict)\n",
    "        e.write(f'{formatted_dict}\\n')\n",
    "        external_collection[id] = Paper(id, p['title'], p['abstract'], label_opts=args.dimensions, internal=False)\n",
    "        external_count += 1\n",
    "        id += 1\n",
    "print(f'External Count: {external_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enrich each node with a set of terms and sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxonomy import DAG\n",
    "args.llm = 'vllm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dags = {dim:DAG(root=root, dim=dim) for dim, root in roots.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 1012/1012 [01:03<00:00, 15.91it/s]\n",
      "Processed prompts: 100%|██████████| 30/30 [00:13<00:00,  2.29it/s, est. speed input: 1097.06 toks/s, output: 820.64 toks/s]\n",
      "Processed prompts: 100%|██████████| 30/30 [00:10<00:00,  2.76it/s, est. speed input: 1328.58 toks/s, output: 941.38 toks/s]\n",
      "Processed prompts: 100%|██████████| 30/30 [00:11<00:00,  2.70it/s, est. speed input: 1367.89 toks/s, output: 946.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 28/28 [00:10<00:00,  2.61it/s, est. speed input: 1243.65 toks/s, output: 898.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 31/31 [00:11<00:00,  2.67it/s, est. speed input: 1268.72 toks/s, output: 881.56 toks/s]\n"
     ]
    }
   ],
   "source": [
    "enriched_phrases = {dim:[] for dim in args.dimensions}\n",
    "enriched_sentences = {dim:[] for dim in args.dimensions}\n",
    "\n",
    "for dim, dag in dags.items():\n",
    "    all_phrases, all_sentences = dag.enrich_dag(args, id2node)\n",
    "    enriched_phrases[dim].extend(all_phrases)\n",
    "    enriched_sentences[dim].extend(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_classification': Node(label=text_classification, dim=tasks, description=Text classification involves categorizing text documents into predefined classes or categories based on their content., level=1),\n",
       " 'named_entity_recognition': Node(label=named_entity_recognition, dim=tasks, description=Named Entity Recognition (NER) is the task of identifying and classifying named entities in text into predefined categories such as names of persons, organizations, locations, etc., level=1),\n",
       " 'sentiment_analysis': Node(label=sentiment_analysis, dim=tasks, description=Sentiment analysis aims to determine the sentiment expressed in a piece of text, whether it is positive, negative, or neutral., level=1),\n",
       " 'machine_translation': Node(label=machine_translation, dim=tasks, description=Machine translation involves automatically translating text from one language to another, preserving the meaning of the original text., level=1),\n",
       " 'question_answering': Node(label=question_answering, dim=tasks, description=Question answering focuses on developing systems that can automatically answer questions posed in natural language, based on a given context or knowledge base., level=1)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots['tasks'].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question_answering_models',\n",
       " 'specific_facts',\n",
       " 'multiple_passage',\n",
       " 'knowledge_graph',\n",
       " 'inference_chain',\n",
       " 'questioning_technique',\n",
       " 'multi-hop_inference',\n",
       " 'entity_disambiguation',\n",
       " 'questioning_strategy',\n",
       " 'comprehension_assessment',\n",
       " 'questioning_methodology',\n",
       " 'question_formulation',\n",
       " 'pronoun_resolution',\n",
       " 'entity_resolution',\n",
       " 'text_to_question',\n",
       " 'text_understanding',\n",
       " 'questioning_framework',\n",
       " 'questioning_tool',\n",
       " 'anaphora_resolution',\n",
       " 'contextual_understanding',\n",
       " 'entity_linking',\n",
       " 'factual_information',\n",
       " 'cross_document',\n",
       " 'answer_selection',\n",
       " 'questioning_engine',\n",
       " 'co-reference',\n",
       " 'information_retrieval',\n",
       " 'question_chaining',\n",
       " 'factoid_questions',\n",
       " 'knowledge_base',\n",
       " 'question_answering_techniques',\n",
       " 'passage_retrieval',\n",
       " 'question_classification',\n",
       " 'answer_type',\n",
       " 'natural_language_inference',\n",
       " 'information_fusion',\n",
       " 'factual_data',\n",
       " 'question_generation_system',\n",
       " 'question_creation',\n",
       " 'contextual_reasoning',\n",
       " 'question_answering',\n",
       " 'inference_network',\n",
       " 'pronoun_identification',\n",
       " 'questioning_model',\n",
       " 'multiple_hops',\n",
       " 'entity_coreference_resolution',\n",
       " 'text_search',\n",
       " 'query_formulation',\n",
       " 'inference_path',\n",
       " 'questioning_approach',\n",
       " 'question_type',\n",
       " 'factual_knowledge',\n",
       " 'resolution_task',\n",
       " 'semantic_search',\n",
       " 'document_retrieval',\n",
       " 'question_type_identification',\n",
       " 'factual_knowledge_retrieval',\n",
       " 'answer_generation',\n",
       " 'coreference_system',\n",
       " 'question_answering_tasks',\n",
       " 'cross_passage_retrieval',\n",
       " 'referential_resolution',\n",
       " 'question_formulation_system',\n",
       " 'question_answering_systems',\n",
       " 'inference_graph',\n",
       " 'referential_resolution_task',\n",
       " 'coreference_analysis',\n",
       " 'factoid_extraction',\n",
       " 'inference_engine',\n",
       " 'entity_coreference',\n",
       " 'referential_linking',\n",
       " 'answer_extraction',\n",
       " 'entity_disambiguation_task',\n",
       " 'passage_linking',\n",
       " 'answer_retrieval',\n",
       " 'question_parsing',\n",
       " 'text_retrieval',\n",
       " 'multi-step_reasoning',\n",
       " 'cross_passage',\n",
       " 'reference_resolution',\n",
       " 'answer_ranking',\n",
       " 'coreference_chain',\n",
       " 'question_answering_methods',\n",
       " 'questioning_tactic',\n",
       " 'coreference_identification',\n",
       " 'reasoning_across',\n",
       " 'text_analysis',\n",
       " 'qa_systems',\n",
       " 'coreference_resolution_task',\n",
       " 'document_linking',\n",
       " 'questioning_method']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots['tasks'].children['question_answering'].get_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Pseudo-labels for Dimension/Type Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import type_cls_system_instruction, type_cls_main_prompt, TypeClsSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 139/139 [00:04<00:00, 29.89it/s]\n",
      "Processed prompts: 100%|██████████| 2954/2954 [04:08<00:00, 11.87it/s, est. speed input: 7570.80 toks/s, output: 367.93 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# do for internal collection\n",
    "\n",
    "prompts = [constructPrompt(args, type_cls_system_instruction, type_cls_main_prompt(paper)) for paper in internal_collection.values()]\n",
    "outputs = promptLLM(args=args, prompts=prompts, schema=TypeClsSchema, max_new_tokens=500, json_mode=True, temperature=0.1, top_p=0.99)\n",
    "outputs = [json.loads(clean_json_string(c)) if \"```\" in c else json.loads(c.strip()) for c in outputs]\n",
    "\n",
    "# do for external collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in roots:\n",
    "    roots[r].papers = {}\n",
    "type_dist = {dim:[] for dim in args.dimensions}\n",
    "for p_id, out in enumerate(outputs):\n",
    "    internal_collection[p_id].labels = {}\n",
    "    for key, val in out.items():\n",
    "        if val:\n",
    "            type_dist[key].append(internal_collection[p_id])\n",
    "            internal_collection[p_id].labels[key] = []\n",
    "            roots[key].papers[p_id] = internal_collection[p_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasks 2954\n",
      "datasets 696\n",
      "methodologies 2241\n",
      "evaluation_methods 1946\n",
      "real_world_domains 1408\n"
     ]
    }
   ],
   "source": [
    "for key, p in type_dist.items():\n",
    "    print(key, len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document Through the advent of pre-trained language models, there have been notable advancements in abstractive summarization systems. Simultaneously, a considerable number of novel methods for evaluating factual consistency in abstractive summarization systems has been developed. But these evaluation approaches incorporate substantial limitations, especially on refinement and interpretability. In this work, we propose highly effective and interpretable factual inconsistency detection method FIZZ (Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document) for abstractive summarization systems that is based on fine-grained atomic facts decomposition. Moreover, we align atomic facts decomposed from the summary with the source document through adaptive granularity expansion. These atomic facts represent a more fine-grained unit of information, facilitating detailed understanding and interpretability of the summary{'}s factual inconsistency. Experimental results demonstrate that our proposed factual consistency checking system significantly outperforms existing systems. We release the code at https://github.com/plm3332/FIZZ.\n",
      "Prompts have evil twins We discover that many natural-language prompts can be replaced by corresponding prompts that are unintelligible to humans but that provably elicit similar behavior in language models. We call these prompts {``}evil twins{''} because they are obfuscated and uninterpretable (evil), but at the same time mimic the functionality of the original natural-language prompts (twins). Remarkably, evil twins transfer between models. We find these prompts by solving a maximum-likelihood problem which has applications of independent interest.\n",
      "Table Question Answering for Low-resourced Indic Languages TableQA is the task of answering questions over tables of structured information, returning individual cells or tables as output. TableQA research has focused primarily on high-resource languages, leaving medium- and low-resource languages with little progress due to scarcity of annotated data and neural models. We address this gap by introducing a fully automatic large-scale tableQA data generation process for low-resource languages with limited budget. We incorporate our data generation method on two Indic languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA models trained on our large-scale datasets outperform state-of-the-art LLMs. We further study the trained models on different aspects, including mathematical reasoning capabilities and zero-shot cross-lingual transfer. Our work is the first on low-resource tableQA focusing on scalable data generation and evaluation procedures. Our proposed data generation method can be applied to any low-resource language with a web presence. We release datasets, models, and code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).\n",
      "ImageInWords: Unlocking Hyper-Detailed Image Descriptions Despite the longstanding adage {''}an image is worth a thousand words,{''} generating accurate hyper-detailed image descriptions remains unsolved. Trained on short web-scraped image-text, vision-language models often generate incomplete descriptions with visual inconsistencies. We address this via a novel data-centric approach with ImageInWords (IIW), a carefully designed human-in-the-loop framework for curating hyper-detailed image descriptions. Human evaluations on IIW data show major gains compared to recent datasets (+66{\\%}) and GPT-4V (+48{\\%}) across comprehensiveness, specificity, hallucinations, and more. We also show that fine-tuning with IIW data improves these metrics by +31{\\%} against models trained with prior work, even with only 9k samples. Lastly, we evaluate IIW models with text-to-image generation and vision-language reasoning tasks. Our generated descriptions result in the highest fidelity images, and boost compositional reasoning by up to 6{\\%} on ARO, SVO-Probes, and Winoground datasets. We release the IIW-Eval benchmark with human judgement labels, object and image-level annotations from our framework, and existing image caption datasets enriched via IIW-model.\n",
      "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents{'} social behaviors. Results affirm the framework{'}s effectiveness in creating adaptive agents and suggest LLM-based agents{'} potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field{'}s research and applications.\n",
      "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in healthcare applications. However, the application of LLMs in the identification and analysis of depressive states remains relatively unexplored, presenting an intriguing avenue for future research. In this paper, we present an innovative approach to employ an LLM in the realm of depression detection, integrating acoustic speech information into the LLM framework for this specific application. We investigate an efficient method for automatic depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. This approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of LLMs to comprehend and process speech signals. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. By encoding acoustic landmarks information into LLMs, evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines.\n",
      "Hateful Word in Context Classification Hate speech detection is a prevalent research field, yet it remains underexplored at the level of word meaning. This is significant, as terms used to convey hate often involve non-standard or novel usages which might be overlooked by commonly leveraged LMs trained on general language use. In this paper, we introduce the Hateful Word in Context Classification (\\textbf{HateWiC}) task and present a dataset of {\\textasciitilde}4000 WiC-instances, each labeled by three annotators. Our analyses and computational exploration focus on the interplay between the subjective nature (context-dependent connotations) and the descriptive nature (as described in dictionary definitions) of hateful word senses. HateWiC annotations confirm that hatefulness of a word in context does not always derive from the sense definition alone. We explore the prediction of both majority and individual annotator labels, and we experiment with modeling context- and sense-based inputs. Our findings indicate that including definitions proves effective overall, yet not in cases where hateful connotations vary. Conversely, including annotator demographics becomes more important for mitigating performance drop in subjective hate prediction.\n",
      "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze Hate speech is a complex and subjective phenomenon. In this paper, we present a dataset (GAZE4HATE) that provides gaze data collected in a hate speech annotation experiment. We study whether the gaze of an annotator provides predictors of their subjective hatefulness rating, and how gaze features can improve Hate Speech Detection (HSD). We conduct experiments on statistical modeling of subjective hate ratings and gaze and analyze to what extent rationales derived from hate speech models correspond to human gaze and explanations in our data. Finally, we introduce MEANION, a first gaze-integrated HSD model. Our experiments show that particular gaze features like dwell time or fixation counts systematically correlate with annotators{'} subjective hate ratings and improve predictions of text-only hate speech models.\n",
      "“Thinking” Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models Existing debiasing techniques are typically training-based or require access to the model{'}s internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine whether structured prompting techniques can offer opportunities for fair text generation. We evaluate a comprehensive end-user-focused iterative framework of debiasing that applies System 2 thinking processes for prompts to induce logical, reflective, and critical text generation, with single, multi-step, instruction, and role-based variants. By systematically evaluating many LLMs across many datasets and different prompting strategies, we show that the more complex System 2-based Implicative Prompts significantly improve over other techniques demonstrating lower mean bias in the outputs with competitive performance on the downstream tasks. Our work offers research directions for the design and the potential of end-user-focused evaluative frameworks for LLM use.\n",
      "A Usage-centric Take on Intent Understanding in E-Commerce Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its essential role in product recommendation and business user profiling analysis, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as {``}how a customer uses a product{''}, and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph: category-rigidity and property-ambiguity. They limit its ability to strongly align user intents with products having the most desirable property, and to recommend useful products across diverse categories. Following these observations, we introduce a Product Recovery Benchmark featuring a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark. Our code and dataset are available at https://github.com/stayones/Usgae-Centric-Intent-Understanding.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "dim_type = 'evaluation_methods'\n",
    "for paper_id in roots[dim_type].papers:\n",
    "    if count < 10:\n",
    "        print(roots[dim_type].papers[paper_id].title, roots[dim_type].papers[paper_id].abstract)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference. Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference. However, their applicability to various domains is limited because they tend to generate domain-specific datasets. In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain. This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm. Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs.\n",
      "Table Question Answering for Low-resourced Indic Languages TableQA is the task of answering questions over tables of structured information, returning individual cells or tables as output. TableQA research has focused primarily on high-resource languages, leaving medium- and low-resource languages with little progress due to scarcity of annotated data and neural models. We address this gap by introducing a fully automatic large-scale tableQA data generation process for low-resource languages with limited budget. We incorporate our data generation method on two Indic languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA models trained on our large-scale datasets outperform state-of-the-art LLMs. We further study the trained models on different aspects, including mathematical reasoning capabilities and zero-shot cross-lingual transfer. Our work is the first on low-resource tableQA focusing on scalable data generation and evaluation procedures. Our proposed data generation method can be applied to any low-resource language with a web presence. We release datasets, models, and code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).\n",
      "ImageInWords: Unlocking Hyper-Detailed Image Descriptions Despite the longstanding adage {''}an image is worth a thousand words,{''} generating accurate hyper-detailed image descriptions remains unsolved. Trained on short web-scraped image-text, vision-language models often generate incomplete descriptions with visual inconsistencies. We address this via a novel data-centric approach with ImageInWords (IIW), a carefully designed human-in-the-loop framework for curating hyper-detailed image descriptions. Human evaluations on IIW data show major gains compared to recent datasets (+66{\\%}) and GPT-4V (+48{\\%}) across comprehensiveness, specificity, hallucinations, and more. We also show that fine-tuning with IIW data improves these metrics by +31{\\%} against models trained with prior work, even with only 9k samples. Lastly, we evaluate IIW models with text-to-image generation and vision-language reasoning tasks. Our generated descriptions result in the highest fidelity images, and boost compositional reasoning by up to 6{\\%} on ARO, SVO-Probes, and Winoground datasets. We release the IIW-Eval benchmark with human judgement labels, object and image-level annotations from our framework, and existing image caption datasets enriched via IIW-model.\n",
      "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents{'} social behaviors. Results affirm the framework{'}s effectiveness in creating adaptive agents and suggest LLM-based agents{'} potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field{'}s research and applications.\n",
      "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in healthcare applications. However, the application of LLMs in the identification and analysis of depressive states remains relatively unexplored, presenting an intriguing avenue for future research. In this paper, we present an innovative approach to employ an LLM in the realm of depression detection, integrating acoustic speech information into the LLM framework for this specific application. We investigate an efficient method for automatic depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. This approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of LLMs to comprehend and process speech signals. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. By encoding acoustic landmarks information into LLMs, evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines.\n",
      "Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their prolonged training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training{---}a key factor in the costs associated with adding or customizing voices{---}often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement.\n",
      "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze Hate speech is a complex and subjective phenomenon. In this paper, we present a dataset (GAZE4HATE) that provides gaze data collected in a hate speech annotation experiment. We study whether the gaze of an annotator provides predictors of their subjective hatefulness rating, and how gaze features can improve Hate Speech Detection (HSD). We conduct experiments on statistical modeling of subjective hate ratings and gaze and analyze to what extent rationales derived from hate speech models correspond to human gaze and explanations in our data. Finally, we introduce MEANION, a first gaze-integrated HSD model. Our experiments show that particular gaze features like dwell time or fixation counts systematically correlate with annotators{'} subjective hate ratings and improve predictions of text-only hate speech models.\n",
      "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of {``}42{''}, we suggest using {``}2:42{''} as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstrate NumeroLogic applicability to general natural language modeling, improving language understanding performance in the MMLU benchmark.\n",
      "Studying and Mitigating Biases in Sign Language Understanding Models Ensuring that the benefits of sign language technologies are distributed equitably among all community members is crucial. Thus, it is important to address potential biases and inequities that may arise from the design or use of these resources. Crowd-sourced sign language datasets, such as the ASL Citizen dataset, are great resources for improving accessibility and preserving linguistic diversity, but they must be used thoughtfully to avoid reinforcing existing biases.In this work, we utilize the rich information about participant demographics and lexical features present in the ASL Citizen dataset to study and document the biases that may result from models trained on crowd-sourced sign datasets. Further, we apply several bias mitigation techniques during model training, and find that these techniques reduce performance disparities without decreasing accuracy. With the publication of this work, we release the demographic information about the participants in the ASL Citizen dataset to encourage future bias mitigation work in this space.\n",
      "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs{'} capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce *RoTBench*, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model{'}s resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "dim_type = 'real_world_domains'\n",
    "for paper_id in roots[dim_type].papers:\n",
    "    if count < 10:\n",
    "        print(roots[dim_type].papers[paper_id].title, roots[dim_type].papers[paper_id].abstract)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': [], 'methodologies': [], 'real_world_domains': []}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_collection[0].labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_any_match(patterns, input_string):\n",
    "    \"\"\"\n",
    "    Check if any pattern in the list matches the input string.\n",
    "\n",
    "    :param patterns: List of regex patterns (as strings)\n",
    "    :param input_string: The string to search within\n",
    "    :return: True if any pattern matches, otherwise False\n",
    "    \"\"\"\n",
    "    # Compile all the patterns to make matching more efficient\n",
    "    compiled_patterns = [re.compile(pattern) for pattern in patterns]\n",
    "    \n",
    "    # Check if any compiled pattern matches the input string\n",
    "    for compiled_pattern in compiled_patterns:\n",
    "        if compiled_pattern.search(input_string):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tasks',\n",
       " 'datasets',\n",
       " 'methodologies',\n",
       " 'evaluation_methods',\n",
       " 'real_world_applications']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2954/2954 [00:00<00:00, 6785.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': 0, 'datasets': 233, 'methodologies': 487, 'evaluation_methods': 23, 'real_world_applications': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178521/178521 [00:26<00:00, 6771.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': 0, 'datasets': 3377, 'methodologies': 24568, 'evaluation_methods': 416, 'real_world_applications': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pseudo_labels = {d:[] for d in args.dimensions}\n",
    "paper_dims = {}\n",
    "\n",
    "patterns = {\"datasets\": [r'introduce [\\s\\w]* benchmark', r'introduce [\\s\\w]* dataset', r'construct [\\s\\w]* benchmark', r'construct [\\s\\w]* dataset', r'propose [\\s\\w]* dataset', r'propose [\\s\\w]* benchmark', r'present [\\s\\w]* benchmark', r'present [\\s\\w]* dataset', r'develop [\\s\\w]* benchmark', r'develop [\\s\\w]* dataset', r'create [\\s\\w]* benchmark', r'create [\\s\\w]* dataset', r'provide [\\s\\w]* benchmark', r'provide [\\s\\w]* dataset', r'describe [\\s\\w]* benchmark', r'describe [\\s\\w]* dataset', r'propose a new benchmark', r'propose a new dataset', r'introduce a new benchmark', r'introduce a new dataset', r'we release [\\s\\w]* dataset', r'we release [\\s\\w]* benchmark', r'a new dataset for [\\s\\w]*', r'a new benchmark for [\\s\\w]*', r'dataset for [\\s\\w]* task', r'benchmark for [\\s\\w]* task', r'we present [\\s\\w]* dataset', r'we present [\\s\\w]* benchmark', r'dataset designed for [\\s\\w]*', r'benchmark designed for [\\s\\w]*', r'introducing [\\s\\w]* dataset', r'introducing [\\s\\w]* benchmark'],\n",
    "            \"methodologies\": [r'introduce [\\s\\w]* method', r'propose [\\s\\w]* method', r'design [\\s\\w]* method', r'present [\\s\\w]* method', r'develop [\\s\\w]* method', r'introduce [\\s\\w]* approach', r'propose [\\s\\w]* approach', r'design [\\s\\w]* approach', r'present [\\s\\w]* approach', r'develop [\\s\\w]* approach', r'we propose [\\s\\w]* method', r'we propose [\\s\\w]* approach', r'we introduce [\\s\\w]* method', r'we introduce [\\s\\w]* approach', r'we present [\\s\\w]* method', r'we present [\\s\\w]* approach', r'propose a novel method', r'propose a novel approach', r'introduce a novel method', r'introduce a novel approach', r'present a novel method', r'present a novel approach', r'propose [\\s\\w]* framework', r'introduce [\\s\\w]* framework', r'present [\\s\\w]* framework', r'design [\\s\\w]* framework', r'we propose [\\s\\w]* framework', r'we introduce [\\s\\w]* framework', r'we present [\\s\\w]* framework', r'our proposed method [\\s\\w]*', r'our proposed approach [\\s\\w]*', r'our proposed framework [\\s\\w]*', r'this paper proposes [\\s\\w]* method', r'this paper introduces [\\s\\w]* method', r'this paper presents [\\s\\w]* method', r'this paper develops [\\s\\w]* method', r'this paper proposes [\\s\\w]* approach', r'this paper introduces [\\s\\w]* approach', r'this paper presents [\\s\\w]* approach', r'this paper develops [\\s\\w]* approach', r'this paper proposes [\\s\\w]* framework', r'this paper introduces [\\s\\w]* framework', r'this paper presents [\\s\\w]* framework', r'this paper develops [\\s\\w]* framework'],\n",
    "            \"evaluation_methods\": [r'construct a [\\s\\w]* evaluate', r'design a [\\s\\w]* evaluate', r'propose a [\\s\\w]* evaluate', r'introduce [\\s\\w]* evaluation method', r'propose [\\s\\w]* evaluation method', r'design [\\s\\w]* evaluation method', r'develop [\\s\\w]* evaluation method', r'introduce [\\s\\w]* evaluation metric', r'propose [\\s\\w]* evaluation metric', r'design [\\s\\w]* evaluation metric', r'develop [\\s\\w]* evaluation metric', r'propose a novel evaluation method', r'propose a novel evaluation metric', r'present a novel evaluation framework', r'introduce a framework for evaluation', r'this paper proposes [\\s\\w]* evaluation', r'this paper introduces [\\s\\w]* evaluation', r'introduce [\\s\\w]* automatic evaluation', r'propose [\\s\\w]* automatic evaluation', r'develop [\\s\\w]* automatic evaluation', r'design [\\s\\w]* automatic evaluation', r'propose a novel automatic evaluation method', r'automatic evaluation of [\\s\\w]* task', r'develop a method for automatic evaluation', r'introduce [\\s\\w]* human evaluation', r'propose [\\s\\w]* human evaluation', r'develop [\\s\\w]* human evaluation', r'design [\\s\\w]* human evaluation', r'propose a framework for human evaluation', r'introduce a novel human evaluation method', r'conduct human evaluation of [\\s\\w]*', r'compare human and automatic evaluation', r'comparison of human evaluation and automatic evaluation', r'human evaluation versus automatic evaluation', r'evaluate using both human and automatic methods', r'analyze results from human and automatic evaluation']}\n",
    "\n",
    "for id, paper in tqdm(internal_collection.items(), total=len(internal_collection)):\n",
    "    for dim, dim_patterns in patterns.items():\n",
    "        if find_any_match(dim_patterns, f'{paper.title}: {paper.abstract}'.lower()):\n",
    "            pseudo_labels[dim].append(paper)\n",
    "            if id in paper_dims:\n",
    "                paper_dims[id].append(dim)\n",
    "            else:\n",
    "                paper_dims[id] = [dim]\n",
    "print({dim: len(papers) for dim, papers in pseudo_labels.items()})\n",
    "\n",
    "for id, paper in tqdm(external_collection.items(), total=len(external_collection)):\n",
    "    for dim, dim_patterns in patterns.items():\n",
    "        if find_any_match(dim_patterns, f'{paper.title}: {paper.abstract}'.lower()):\n",
    "            pseudo_labels[dim].append(paper)\n",
    "            if id in paper_dims:\n",
    "                paper_dims[id].append(dim)\n",
    "            else:\n",
    "                paper_dims[id] = [dim]\n",
    "\n",
    "print({dim: len(papers) for dim, papers in pseudo_labels.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27488,\n",
       " {0: ['datasets', 'methodologies'],\n",
       "  2: ['methodologies'],\n",
       "  6: ['methodologies'],\n",
       "  7: ['methodologies'],\n",
       "  9: ['datasets'],\n",
       "  10: ['datasets'],\n",
       "  13: ['datasets', 'methodologies'],\n",
       "  16: ['datasets'],\n",
       "  19: ['methodologies'],\n",
       "  21: ['datasets'],\n",
       "  31: ['methodologies'],\n",
       "  36: ['methodologies'],\n",
       "  40: ['methodologies'],\n",
       "  49: ['methodologies'],\n",
       "  54: ['methodologies'],\n",
       "  56: ['methodologies'],\n",
       "  59: ['methodologies'],\n",
       "  64: ['methodologies'],\n",
       "  85: ['methodologies'],\n",
       "  89: ['methodologies'],\n",
       "  90: ['methodologies'],\n",
       "  91: ['datasets'],\n",
       "  94: ['methodologies'],\n",
       "  97: ['datasets', 'methodologies'],\n",
       "  106: ['evaluation_methods'],\n",
       "  109: ['methodologies'],\n",
       "  112: ['datasets', 'methodologies'],\n",
       "  122: ['methodologies'],\n",
       "  123: ['methodologies'],\n",
       "  131: ['datasets'],\n",
       "  132: ['methodologies'],\n",
       "  133: ['methodologies'],\n",
       "  135: ['methodologies'],\n",
       "  148: ['methodologies'],\n",
       "  149: ['methodologies'],\n",
       "  150: ['methodologies'],\n",
       "  151: ['methodologies'],\n",
       "  156: ['methodologies'],\n",
       "  164: ['methodologies'],\n",
       "  165: ['methodologies', 'evaluation_methods'],\n",
       "  174: ['methodologies'],\n",
       "  176: ['datasets'],\n",
       "  184: ['methodologies'],\n",
       "  188: ['methodologies'],\n",
       "  190: ['methodologies'],\n",
       "  191: ['methodologies'],\n",
       "  194: ['methodologies'],\n",
       "  195: ['methodologies'],\n",
       "  199: ['methodologies'],\n",
       "  204: ['methodologies'],\n",
       "  207: ['methodologies'],\n",
       "  211: ['methodologies'],\n",
       "  217: ['methodologies'],\n",
       "  218: ['methodologies'],\n",
       "  224: ['methodologies'],\n",
       "  228: ['methodologies'],\n",
       "  230: ['methodologies'],\n",
       "  233: ['datasets'],\n",
       "  237: ['datasets'],\n",
       "  246: ['datasets'],\n",
       "  253: ['datasets', 'methodologies'],\n",
       "  254: ['methodologies'],\n",
       "  258: ['methodologies'],\n",
       "  263: ['methodologies'],\n",
       "  268: ['methodologies'],\n",
       "  271: ['datasets'],\n",
       "  272: ['methodologies'],\n",
       "  273: ['datasets'],\n",
       "  275: ['methodologies'],\n",
       "  279: ['methodologies'],\n",
       "  281: ['methodologies'],\n",
       "  283: ['methodologies'],\n",
       "  284: ['methodologies'],\n",
       "  286: ['methodologies'],\n",
       "  287: ['datasets', 'methodologies'],\n",
       "  299: ['methodologies'],\n",
       "  300: ['methodologies'],\n",
       "  302: ['methodologies'],\n",
       "  320: ['methodologies'],\n",
       "  332: ['methodologies'],\n",
       "  337: ['methodologies'],\n",
       "  338: ['methodologies'],\n",
       "  343: ['methodologies'],\n",
       "  346: ['methodologies'],\n",
       "  347: ['datasets'],\n",
       "  361: ['datasets'],\n",
       "  364: ['methodologies'],\n",
       "  366: ['methodologies'],\n",
       "  367: ['methodologies'],\n",
       "  379: ['datasets'],\n",
       "  383: ['methodologies'],\n",
       "  384: ['datasets'],\n",
       "  390: ['datasets'],\n",
       "  391: ['datasets'],\n",
       "  399: ['datasets'],\n",
       "  401: ['methodologies'],\n",
       "  402: ['methodologies'],\n",
       "  404: ['methodologies'],\n",
       "  407: ['methodologies'],\n",
       "  408: ['methodologies'],\n",
       "  410: ['datasets'],\n",
       "  420: ['methodologies'],\n",
       "  424: ['methodologies'],\n",
       "  425: ['datasets'],\n",
       "  428: ['datasets'],\n",
       "  436: ['methodologies'],\n",
       "  454: ['methodologies'],\n",
       "  455: ['datasets'],\n",
       "  465: ['methodologies', 'evaluation_methods'],\n",
       "  472: ['methodologies'],\n",
       "  473: ['methodologies'],\n",
       "  480: ['methodologies'],\n",
       "  483: ['methodologies'],\n",
       "  484: ['methodologies'],\n",
       "  486: ['methodologies'],\n",
       "  493: ['datasets'],\n",
       "  508: ['datasets', 'methodologies'],\n",
       "  509: ['methodologies'],\n",
       "  516: ['methodologies'],\n",
       "  519: ['datasets'],\n",
       "  523: ['methodologies'],\n",
       "  533: ['datasets'],\n",
       "  536: ['datasets'],\n",
       "  537: ['methodologies'],\n",
       "  542: ['methodologies'],\n",
       "  545: ['methodologies'],\n",
       "  553: ['datasets'],\n",
       "  556: ['datasets'],\n",
       "  557: ['methodologies'],\n",
       "  563: ['methodologies'],\n",
       "  565: ['methodologies'],\n",
       "  574: ['methodologies'],\n",
       "  576: ['methodologies'],\n",
       "  588: ['methodologies'],\n",
       "  597: ['methodologies'],\n",
       "  600: ['datasets'],\n",
       "  602: ['methodologies'],\n",
       "  615: ['methodologies'],\n",
       "  627: ['methodologies'],\n",
       "  630: ['methodologies'],\n",
       "  633: ['evaluation_methods'],\n",
       "  636: ['datasets'],\n",
       "  639: ['datasets'],\n",
       "  641: ['methodologies'],\n",
       "  642: ['datasets'],\n",
       "  647: ['datasets', 'methodologies'],\n",
       "  648: ['methodologies'],\n",
       "  656: ['methodologies', 'evaluation_methods'],\n",
       "  660: ['datasets'],\n",
       "  662: ['methodologies'],\n",
       "  665: ['methodologies'],\n",
       "  667: ['datasets'],\n",
       "  672: ['datasets'],\n",
       "  680: ['methodologies'],\n",
       "  685: ['methodologies'],\n",
       "  691: ['methodologies'],\n",
       "  693: ['datasets'],\n",
       "  694: ['methodologies'],\n",
       "  695: ['methodologies'],\n",
       "  700: ['methodologies'],\n",
       "  702: ['methodologies'],\n",
       "  725: ['datasets'],\n",
       "  726: ['methodologies'],\n",
       "  737: ['methodologies'],\n",
       "  742: ['methodologies'],\n",
       "  747: ['datasets'],\n",
       "  752: ['methodologies'],\n",
       "  758: ['datasets'],\n",
       "  760: ['methodologies'],\n",
       "  761: ['datasets', 'methodologies'],\n",
       "  762: ['methodologies'],\n",
       "  773: ['datasets', 'methodologies'],\n",
       "  778: ['methodologies'],\n",
       "  781: ['methodologies'],\n",
       "  782: ['methodologies'],\n",
       "  787: ['methodologies'],\n",
       "  789: ['methodologies'],\n",
       "  790: ['methodologies'],\n",
       "  797: ['datasets'],\n",
       "  802: ['methodologies'],\n",
       "  805: ['methodologies'],\n",
       "  811: ['methodologies'],\n",
       "  820: ['methodologies'],\n",
       "  826: ['datasets'],\n",
       "  829: ['methodologies'],\n",
       "  830: ['methodologies'],\n",
       "  832: ['datasets'],\n",
       "  833: ['methodologies'],\n",
       "  838: ['methodologies'],\n",
       "  845: ['methodologies'],\n",
       "  848: ['methodologies'],\n",
       "  850: ['methodologies'],\n",
       "  852: ['methodologies'],\n",
       "  858: ['datasets'],\n",
       "  862: ['methodologies'],\n",
       "  864: ['methodologies'],\n",
       "  875: ['methodologies'],\n",
       "  877: ['methodologies'],\n",
       "  887: ['methodologies'],\n",
       "  888: ['datasets'],\n",
       "  900: ['datasets'],\n",
       "  901: ['methodologies'],\n",
       "  914: ['datasets'],\n",
       "  928: ['methodologies'],\n",
       "  929: ['methodologies'],\n",
       "  935: ['datasets'],\n",
       "  936: ['datasets'],\n",
       "  945: ['datasets'],\n",
       "  946: ['datasets'],\n",
       "  954: ['methodologies'],\n",
       "  956: ['datasets'],\n",
       "  957: ['datasets'],\n",
       "  958: ['datasets', 'methodologies'],\n",
       "  960: ['datasets', 'methodologies'],\n",
       "  962: ['methodologies'],\n",
       "  968: ['datasets'],\n",
       "  973: ['methodologies'],\n",
       "  977: ['methodologies'],\n",
       "  984: ['methodologies'],\n",
       "  987: ['methodologies'],\n",
       "  993: ['methodologies'],\n",
       "  995: ['methodologies'],\n",
       "  1015: ['methodologies'],\n",
       "  1020: ['methodologies', 'evaluation_methods'],\n",
       "  1022: ['datasets'],\n",
       "  1025: ['datasets'],\n",
       "  1028: ['datasets'],\n",
       "  1029: ['methodologies'],\n",
       "  1032: ['methodologies'],\n",
       "  1035: ['methodologies'],\n",
       "  1036: ['methodologies'],\n",
       "  1045: ['methodologies'],\n",
       "  1047: ['datasets', 'methodologies'],\n",
       "  1057: ['methodologies'],\n",
       "  1059: ['methodologies'],\n",
       "  1061: ['methodologies'],\n",
       "  1070: ['datasets'],\n",
       "  1072: ['datasets'],\n",
       "  1081: ['methodologies'],\n",
       "  1082: ['datasets', 'methodologies'],\n",
       "  1088: ['methodologies'],\n",
       "  1099: ['datasets'],\n",
       "  1102: ['methodologies'],\n",
       "  1104: ['datasets'],\n",
       "  1108: ['datasets', 'methodologies'],\n",
       "  1111: ['methodologies', 'evaluation_methods'],\n",
       "  1114: ['datasets', 'methodologies'],\n",
       "  1116: ['methodologies'],\n",
       "  1121: ['methodologies'],\n",
       "  1125: ['datasets'],\n",
       "  1148: ['methodologies'],\n",
       "  1150: ['methodologies'],\n",
       "  1151: ['methodologies'],\n",
       "  1152: ['methodologies'],\n",
       "  1153: ['methodologies'],\n",
       "  1162: ['datasets'],\n",
       "  1167: ['methodologies'],\n",
       "  1174: ['methodologies'],\n",
       "  1175: ['methodologies'],\n",
       "  1178: ['methodologies'],\n",
       "  1186: ['methodologies'],\n",
       "  1194: ['datasets'],\n",
       "  1195: ['methodologies'],\n",
       "  1203: ['methodologies'],\n",
       "  1205: ['methodologies'],\n",
       "  1217: ['datasets'],\n",
       "  1218: ['datasets'],\n",
       "  1220: ['methodologies'],\n",
       "  1221: ['methodologies'],\n",
       "  1223: ['datasets'],\n",
       "  1224: ['datasets', 'methodologies'],\n",
       "  1235: ['methodologies'],\n",
       "  1236: ['methodologies'],\n",
       "  1239: ['datasets'],\n",
       "  1242: ['datasets', 'methodologies'],\n",
       "  1245: ['methodologies'],\n",
       "  1246: ['datasets'],\n",
       "  1247: ['methodologies'],\n",
       "  1249: ['datasets'],\n",
       "  1255: ['methodologies'],\n",
       "  1256: ['methodologies'],\n",
       "  1260: ['methodologies'],\n",
       "  1262: ['methodologies'],\n",
       "  1268: ['methodologies', 'evaluation_methods'],\n",
       "  1276: ['methodologies'],\n",
       "  1288: ['datasets'],\n",
       "  1291: ['methodologies'],\n",
       "  1299: ['methodologies'],\n",
       "  1310: ['evaluation_methods'],\n",
       "  1320: ['methodologies'],\n",
       "  1321: ['datasets'],\n",
       "  1332: ['datasets', 'methodologies'],\n",
       "  1333: ['methodologies'],\n",
       "  1334: ['methodologies'],\n",
       "  1335: ['datasets'],\n",
       "  1342: ['methodologies'],\n",
       "  1346: ['methodologies'],\n",
       "  1348: ['methodologies'],\n",
       "  1359: ['datasets'],\n",
       "  1361: ['methodologies'],\n",
       "  1364: ['methodologies'],\n",
       "  1365: ['methodologies'],\n",
       "  1366: ['methodologies'],\n",
       "  1373: ['methodologies'],\n",
       "  1375: ['methodologies'],\n",
       "  1377: ['methodologies'],\n",
       "  1378: ['methodologies'],\n",
       "  1384: ['datasets'],\n",
       "  1389: ['methodologies'],\n",
       "  1391: ['datasets'],\n",
       "  1393: ['methodologies'],\n",
       "  1400: ['methodologies'],\n",
       "  1402: ['datasets'],\n",
       "  1403: ['datasets', 'methodologies'],\n",
       "  1406: ['methodologies'],\n",
       "  1411: ['methodologies'],\n",
       "  1414: ['methodologies'],\n",
       "  1415: ['methodologies'],\n",
       "  1420: ['methodologies'],\n",
       "  1425: ['methodologies'],\n",
       "  1428: ['methodologies'],\n",
       "  1432: ['datasets', 'methodologies'],\n",
       "  1433: ['methodologies'],\n",
       "  1440: ['methodologies'],\n",
       "  1460: ['datasets'],\n",
       "  1461: ['methodologies'],\n",
       "  1462: ['datasets'],\n",
       "  1463: ['methodologies'],\n",
       "  1465: ['datasets', 'methodologies'],\n",
       "  1466: ['datasets'],\n",
       "  1472: ['datasets'],\n",
       "  1474: ['methodologies'],\n",
       "  1477: ['methodologies'],\n",
       "  1485: ['methodologies'],\n",
       "  1493: ['methodologies'],\n",
       "  1494: ['datasets'],\n",
       "  1497: ['methodologies'],\n",
       "  1499: ['methodologies'],\n",
       "  1500: ['methodologies'],\n",
       "  1503: ['datasets'],\n",
       "  1507: ['methodologies'],\n",
       "  1508: ['methodologies'],\n",
       "  1512: ['methodologies'],\n",
       "  1515: ['methodologies'],\n",
       "  1516: ['methodologies'],\n",
       "  1517: ['methodologies'],\n",
       "  1520: ['methodologies'],\n",
       "  1528: ['methodologies'],\n",
       "  1531: ['datasets', 'methodologies'],\n",
       "  1534: ['datasets'],\n",
       "  1538: ['datasets'],\n",
       "  1539: ['methodologies'],\n",
       "  1543: ['methodologies'],\n",
       "  1547: ['methodologies'],\n",
       "  1553: ['datasets', 'methodologies'],\n",
       "  1555: ['methodologies'],\n",
       "  1557: ['methodologies'],\n",
       "  1558: ['methodologies'],\n",
       "  1563: ['methodologies'],\n",
       "  1567: ['datasets', 'methodologies'],\n",
       "  1570: ['methodologies'],\n",
       "  1571: ['datasets'],\n",
       "  1573: ['datasets'],\n",
       "  1574: ['datasets', 'methodologies'],\n",
       "  1575: ['methodologies'],\n",
       "  1579: ['datasets'],\n",
       "  1586: ['methodologies'],\n",
       "  1588: ['methodologies'],\n",
       "  1592: ['datasets'],\n",
       "  1593: ['datasets', 'methodologies'],\n",
       "  1596: ['datasets'],\n",
       "  1602: ['methodologies'],\n",
       "  1606: ['methodologies'],\n",
       "  1607: ['methodologies'],\n",
       "  1615: ['datasets'],\n",
       "  1616: ['datasets'],\n",
       "  1625: ['methodologies'],\n",
       "  1630: ['methodologies'],\n",
       "  1639: ['methodologies'],\n",
       "  1642: ['datasets'],\n",
       "  1645: ['datasets', 'methodologies'],\n",
       "  1650: ['datasets'],\n",
       "  1651: ['methodologies', 'evaluation_methods'],\n",
       "  1652: ['methodologies'],\n",
       "  1655: ['methodologies'],\n",
       "  1661: ['methodologies'],\n",
       "  1667: ['methodologies'],\n",
       "  1668: ['methodologies'],\n",
       "  1673: ['datasets'],\n",
       "  1674: ['methodologies'],\n",
       "  1678: ['methodologies'],\n",
       "  1679: ['methodologies'],\n",
       "  1680: ['methodologies'],\n",
       "  1682: ['methodologies'],\n",
       "  1693: ['methodologies'],\n",
       "  1696: ['datasets'],\n",
       "  1703: ['methodologies'],\n",
       "  1704: ['datasets'],\n",
       "  1705: ['methodologies'],\n",
       "  1707: ['datasets'],\n",
       "  1722: ['methodologies'],\n",
       "  1726: ['datasets'],\n",
       "  1729: ['datasets'],\n",
       "  1733: ['methodologies'],\n",
       "  1734: ['datasets'],\n",
       "  1737: ['methodologies'],\n",
       "  1739: ['methodologies'],\n",
       "  1741: ['methodologies'],\n",
       "  1746: ['datasets'],\n",
       "  1747: ['methodologies'],\n",
       "  1754: ['methodologies'],\n",
       "  1755: ['methodologies'],\n",
       "  1759: ['methodologies'],\n",
       "  1761: ['datasets'],\n",
       "  1766: ['datasets', 'methodologies'],\n",
       "  1776: ['methodologies'],\n",
       "  1783: ['methodologies'],\n",
       "  1792: ['datasets'],\n",
       "  1798: ['evaluation_methods'],\n",
       "  1806: ['methodologies'],\n",
       "  1812: ['datasets'],\n",
       "  1816: ['datasets'],\n",
       "  1817: ['methodologies'],\n",
       "  1820: ['datasets'],\n",
       "  1830: ['methodologies'],\n",
       "  1831: ['methodologies'],\n",
       "  1843: ['methodologies'],\n",
       "  1848: ['methodologies'],\n",
       "  1854: ['datasets'],\n",
       "  1856: ['methodologies'],\n",
       "  1857: ['datasets'],\n",
       "  1861: ['methodologies'],\n",
       "  1864: ['methodologies'],\n",
       "  1871: ['methodologies'],\n",
       "  1874: ['methodologies'],\n",
       "  1877: ['datasets'],\n",
       "  1881: ['methodologies'],\n",
       "  1889: ['methodologies'],\n",
       "  1891: ['methodologies'],\n",
       "  1903: ['methodologies'],\n",
       "  1905: ['methodologies'],\n",
       "  1906: ['methodologies'],\n",
       "  1907: ['datasets'],\n",
       "  1908: ['methodologies'],\n",
       "  1912: ['datasets', 'methodologies'],\n",
       "  1913: ['methodologies', 'evaluation_methods'],\n",
       "  1917: ['methodologies'],\n",
       "  1923: ['methodologies'],\n",
       "  1927: ['methodologies'],\n",
       "  1932: ['methodologies'],\n",
       "  1933: ['methodologies'],\n",
       "  1934: ['datasets'],\n",
       "  1935: ['methodologies'],\n",
       "  1939: ['datasets'],\n",
       "  1940: ['methodologies'],\n",
       "  1944: ['methodologies'],\n",
       "  1948: ['methodologies'],\n",
       "  1951: ['methodologies'],\n",
       "  1952: ['datasets'],\n",
       "  1958: ['methodologies'],\n",
       "  1960: ['methodologies'],\n",
       "  1965: ['methodologies'],\n",
       "  1972: ['datasets'],\n",
       "  1973: ['methodologies'],\n",
       "  1980: ['methodologies'],\n",
       "  1986: ['methodologies'],\n",
       "  1992: ['methodologies'],\n",
       "  2005: ['methodologies'],\n",
       "  2007: ['methodologies'],\n",
       "  2008: ['methodologies'],\n",
       "  2018: ['datasets'],\n",
       "  2025: ['methodologies'],\n",
       "  2027: ['datasets'],\n",
       "  2029: ['methodologies'],\n",
       "  2035: ['methodologies'],\n",
       "  2036: ['methodologies'],\n",
       "  2040: ['methodologies'],\n",
       "  2041: ['methodologies'],\n",
       "  2042: ['methodologies'],\n",
       "  2046: ['datasets'],\n",
       "  2048: ['methodologies'],\n",
       "  2049: ['methodologies'],\n",
       "  2051: ['methodologies'],\n",
       "  2054: ['methodologies'],\n",
       "  2055: ['methodologies'],\n",
       "  2061: ['methodologies'],\n",
       "  2063: ['methodologies'],\n",
       "  2067: ['methodologies'],\n",
       "  2068: ['methodologies'],\n",
       "  2071: ['datasets'],\n",
       "  2073: ['methodologies'],\n",
       "  2075: ['methodologies'],\n",
       "  2076: ['methodologies'],\n",
       "  2085: ['methodologies'],\n",
       "  2091: ['methodologies'],\n",
       "  2092: ['methodologies'],\n",
       "  2095: ['datasets'],\n",
       "  2106: ['datasets'],\n",
       "  2110: ['datasets'],\n",
       "  2111: ['methodologies'],\n",
       "  2112: ['methodologies'],\n",
       "  2113: ['datasets'],\n",
       "  2116: ['methodologies'],\n",
       "  2125: ['methodologies'],\n",
       "  2129: ['methodologies'],\n",
       "  2134: ['datasets'],\n",
       "  2135: ['datasets'],\n",
       "  2136: ['methodologies'],\n",
       "  2137: ['datasets'],\n",
       "  2141: ['methodologies'],\n",
       "  2145: ['methodologies'],\n",
       "  2148: ['datasets'],\n",
       "  2149: ['methodologies'],\n",
       "  2157: ['methodologies'],\n",
       "  2158: ['datasets'],\n",
       "  2159: ['datasets'],\n",
       "  2164: ['datasets'],\n",
       "  2165: ['methodologies'],\n",
       "  2169: ['methodologies'],\n",
       "  2171: ['datasets'],\n",
       "  2172: ['datasets'],\n",
       "  2177: ['datasets'],\n",
       "  2181: ['methodologies'],\n",
       "  2185: ['datasets'],\n",
       "  2189: ['datasets'],\n",
       "  2204: ['methodologies'],\n",
       "  2207: ['methodologies'],\n",
       "  2209: ['datasets'],\n",
       "  2211: ['evaluation_methods'],\n",
       "  2214: ['methodologies'],\n",
       "  2217: ['datasets'],\n",
       "  2221: ['datasets'],\n",
       "  2223: ['methodologies'],\n",
       "  2228: ['evaluation_methods'],\n",
       "  2231: ['methodologies'],\n",
       "  2232: ['methodologies'],\n",
       "  2236: ['methodologies'],\n",
       "  2238: ['methodologies'],\n",
       "  2243: ['methodologies'],\n",
       "  2246: ['methodologies'],\n",
       "  2248: ['methodologies'],\n",
       "  2251: ['datasets'],\n",
       "  2255: ['evaluation_methods'],\n",
       "  2256: ['methodologies'],\n",
       "  2259: ['methodologies'],\n",
       "  2263: ['methodologies'],\n",
       "  2268: ['datasets'],\n",
       "  2275: ['methodologies'],\n",
       "  2277: ['methodologies'],\n",
       "  2278: ['datasets'],\n",
       "  2283: ['methodologies'],\n",
       "  2292: ['datasets'],\n",
       "  2294: ['methodologies'],\n",
       "  2296: ['datasets'],\n",
       "  2310: ['methodologies'],\n",
       "  2318: ['methodologies'],\n",
       "  2320: ['methodologies'],\n",
       "  2321: ['datasets'],\n",
       "  2323: ['datasets'],\n",
       "  2336: ['datasets'],\n",
       "  2343: ['methodologies'],\n",
       "  2345: ['methodologies'],\n",
       "  2348: ['datasets'],\n",
       "  2350: ['datasets'],\n",
       "  2352: ['methodologies'],\n",
       "  2355: ['datasets'],\n",
       "  2360: ['datasets'],\n",
       "  2363: ['methodologies'],\n",
       "  2374: ['datasets'],\n",
       "  2375: ['methodologies'],\n",
       "  2378: ['methodologies'],\n",
       "  2380: ['datasets'],\n",
       "  2384: ['datasets'],\n",
       "  2390: ['datasets'],\n",
       "  2392: ['datasets'],\n",
       "  2396: ['methodologies'],\n",
       "  2398: ['methodologies'],\n",
       "  2405: ['datasets'],\n",
       "  2406: ['methodologies'],\n",
       "  2413: ['datasets'],\n",
       "  2414: ['methodologies'],\n",
       "  2415: ['methodologies'],\n",
       "  2420: ['methodologies'],\n",
       "  2423: ['methodologies'],\n",
       "  2428: ['methodologies'],\n",
       "  2429: ['datasets', 'methodologies'],\n",
       "  2430: ['datasets'],\n",
       "  2434: ['methodologies'],\n",
       "  2435: ['datasets'],\n",
       "  2441: ['datasets'],\n",
       "  2442: ['datasets'],\n",
       "  2448: ['methodologies'],\n",
       "  2450: ['methodologies'],\n",
       "  2458: ['methodologies'],\n",
       "  2461: ['methodologies'],\n",
       "  2475: ['datasets'],\n",
       "  2477: ['methodologies', 'evaluation_methods'],\n",
       "  2478: ['methodologies'],\n",
       "  2480: ['evaluation_methods'],\n",
       "  2484: ['methodologies'],\n",
       "  2485: ['methodologies'],\n",
       "  2494: ['datasets', 'methodologies', 'evaluation_methods'],\n",
       "  2502: ['datasets', 'methodologies'],\n",
       "  2504: ['datasets', 'methodologies'],\n",
       "  2511: ['datasets'],\n",
       "  2517: ['methodologies'],\n",
       "  2521: ['datasets'],\n",
       "  2522: ['methodologies'],\n",
       "  2524: ['datasets'],\n",
       "  2537: ['datasets'],\n",
       "  2544: ['methodologies'],\n",
       "  2545: ['methodologies'],\n",
       "  2547: ['methodologies'],\n",
       "  2555: ['methodologies'],\n",
       "  2559: ['methodologies'],\n",
       "  2568: ['methodologies'],\n",
       "  2576: ['methodologies'],\n",
       "  2577: ['datasets'],\n",
       "  2578: ['methodologies'],\n",
       "  2587: ['datasets', 'methodologies'],\n",
       "  2596: ['methodologies'],\n",
       "  2603: ['methodologies'],\n",
       "  2607: ['datasets'],\n",
       "  2612: ['methodologies'],\n",
       "  2613: ['datasets'],\n",
       "  2618: ['methodologies'],\n",
       "  2625: ['datasets'],\n",
       "  2629: ['methodologies'],\n",
       "  2633: ['methodologies'],\n",
       "  2637: ['datasets'],\n",
       "  2641: ['methodologies'],\n",
       "  2643: ['evaluation_methods'],\n",
       "  2647: ['methodologies', 'evaluation_methods'],\n",
       "  2648: ['datasets'],\n",
       "  2649: ['datasets'],\n",
       "  2650: ['methodologies'],\n",
       "  2652: ['methodologies'],\n",
       "  2661: ['methodologies'],\n",
       "  2669: ['datasets'],\n",
       "  2672: ['methodologies'],\n",
       "  2675: ['methodologies'],\n",
       "  2679: ['datasets'],\n",
       "  2694: ['methodologies', 'evaluation_methods'],\n",
       "  2695: ['datasets'],\n",
       "  2698: ['datasets'],\n",
       "  2702: ['datasets'],\n",
       "  2715: ['methodologies'],\n",
       "  2718: ['methodologies'],\n",
       "  2720: ['datasets'],\n",
       "  2722: ['methodologies'],\n",
       "  2725: ['datasets'],\n",
       "  2732: ['datasets'],\n",
       "  2733: ['methodologies'],\n",
       "  2737: ['methodologies'],\n",
       "  2739: ['datasets', 'methodologies'],\n",
       "  2743: ['datasets'],\n",
       "  2754: ['methodologies'],\n",
       "  2755: ['methodologies'],\n",
       "  2760: ['methodologies'],\n",
       "  2774: ['methodologies'],\n",
       "  2775: ['methodologies'],\n",
       "  2780: ['datasets'],\n",
       "  2784: ['datasets'],\n",
       "  2792: ['datasets'],\n",
       "  2799: ['methodologies'],\n",
       "  2800: ['datasets'],\n",
       "  2813: ['datasets'],\n",
       "  2828: ['methodologies'],\n",
       "  2832: ['evaluation_methods'],\n",
       "  2843: ['datasets'],\n",
       "  2855: ['datasets'],\n",
       "  2857: ['datasets'],\n",
       "  2860: ['datasets'],\n",
       "  2861: ['datasets'],\n",
       "  2862: ['methodologies'],\n",
       "  2864: ['methodologies'],\n",
       "  2873: ['methodologies'],\n",
       "  2890: ['methodologies'],\n",
       "  2892: ['methodologies'],\n",
       "  2908: ['methodologies'],\n",
       "  2911: ['methodologies'],\n",
       "  2917: ['methodologies'],\n",
       "  2921: ['datasets'],\n",
       "  2926: ['datasets'],\n",
       "  2930: ['datasets', 'methodologies', 'evaluation_methods'],\n",
       "  2931: ['datasets', 'methodologies'],\n",
       "  2934: ['methodologies'],\n",
       "  2937: ['datasets'],\n",
       "  2943: ['methodologies'],\n",
       "  2949: ['methodologies'],\n",
       "  2951: ['methodologies'],\n",
       "  2952: ['methodologies'],\n",
       "  2954: ['datasets'],\n",
       "  2976: ['datasets'],\n",
       "  3006: ['datasets'],\n",
       "  3011: ['datasets', 'methodologies'],\n",
       "  3012: ['methodologies'],\n",
       "  3029: ['methodologies'],\n",
       "  3047: ['methodologies'],\n",
       "  3074: ['datasets'],\n",
       "  3078: ['methodologies'],\n",
       "  3079: ['methodologies'],\n",
       "  3082: ['datasets'],\n",
       "  3083: ['methodologies'],\n",
       "  3108: ['methodologies'],\n",
       "  3146: ['methodologies'],\n",
       "  3191: ['methodologies'],\n",
       "  3195: ['methodologies'],\n",
       "  3226: ['methodologies'],\n",
       "  3227: ['methodologies'],\n",
       "  3234: ['methodologies'],\n",
       "  3246: ['methodologies'],\n",
       "  3256: ['methodologies'],\n",
       "  3259: ['datasets'],\n",
       "  3266: ['methodologies'],\n",
       "  3319: ['methodologies'],\n",
       "  3326: ['methodologies'],\n",
       "  3338: ['methodologies'],\n",
       "  3339: ['methodologies'],\n",
       "  3343: ['evaluation_methods'],\n",
       "  3344: ['methodologies'],\n",
       "  3352: ['methodologies'],\n",
       "  3361: ['methodologies'],\n",
       "  3372: ['datasets'],\n",
       "  3409: ['methodologies'],\n",
       "  3432: ['methodologies'],\n",
       "  3454: ['methodologies'],\n",
       "  3484: ['methodologies'],\n",
       "  3608: ['methodologies'],\n",
       "  3611: ['methodologies'],\n",
       "  3614: ['methodologies'],\n",
       "  3631: ['methodologies'],\n",
       "  3710: ['methodologies'],\n",
       "  3723: ['methodologies'],\n",
       "  3776: ['methodologies'],\n",
       "  3779: ['methodologies'],\n",
       "  3801: ['methodologies'],\n",
       "  3814: ['methodologies'],\n",
       "  3826: ['methodologies'],\n",
       "  3830: ['methodologies'],\n",
       "  3831: ['methodologies'],\n",
       "  3833: ['methodologies'],\n",
       "  3834: ['methodologies'],\n",
       "  3837: ['methodologies'],\n",
       "  3839: ['methodologies'],\n",
       "  3859: ['methodologies'],\n",
       "  3884: ['methodologies'],\n",
       "  3932: ['methodologies'],\n",
       "  3938: ['methodologies'],\n",
       "  3943: ['methodologies'],\n",
       "  3957: ['methodologies'],\n",
       "  3960: ['methodologies', 'evaluation_methods'],\n",
       "  3961: ['methodologies'],\n",
       "  3967: ['methodologies'],\n",
       "  3974: ['methodologies'],\n",
       "  3976: ['methodologies'],\n",
       "  3977: ['methodologies'],\n",
       "  3982: ['methodologies'],\n",
       "  3991: ['methodologies'],\n",
       "  3992: ['methodologies'],\n",
       "  4003: ['methodologies'],\n",
       "  4006: ['methodologies'],\n",
       "  4007: ['methodologies'],\n",
       "  4009: ['methodologies'],\n",
       "  4016: ['methodologies'],\n",
       "  4017: ['methodologies'],\n",
       "  4026: ['methodologies'],\n",
       "  4034: ['methodologies'],\n",
       "  4036: ['methodologies'],\n",
       "  4037: ['methodologies'],\n",
       "  4044: ['methodologies'],\n",
       "  4048: ['datasets'],\n",
       "  4049: ['datasets'],\n",
       "  4051: ['datasets'],\n",
       "  4053: ['datasets'],\n",
       "  4056: ['datasets'],\n",
       "  4057: ['datasets'],\n",
       "  4059: ['datasets'],\n",
       "  4062: ['datasets'],\n",
       "  4063: ['datasets', 'methodologies'],\n",
       "  4065: ['datasets'],\n",
       "  4075: ['methodologies'],\n",
       "  4077: ['methodologies'],\n",
       "  4086: ['methodologies'],\n",
       "  4101: ['methodologies'],\n",
       "  4105: ['methodologies'],\n",
       "  4112: ['methodologies'],\n",
       "  4114: ['methodologies'],\n",
       "  4120: ['methodologies'],\n",
       "  4123: ['methodologies'],\n",
       "  4137: ['methodologies'],\n",
       "  4141: ['methodologies'],\n",
       "  4146: ['methodologies'],\n",
       "  4155: ['methodologies'],\n",
       "  4165: ['datasets'],\n",
       "  4176: ['methodologies'],\n",
       "  4178: ['methodologies'],\n",
       "  4180: ['datasets'],\n",
       "  4194: ['methodologies'],\n",
       "  4200: ['methodologies'],\n",
       "  4213: ['methodologies'],\n",
       "  4233: ['methodologies'],\n",
       "  4236: ['methodologies'],\n",
       "  4251: ['methodologies'],\n",
       "  4274: ['methodologies'],\n",
       "  4275: ['methodologies'],\n",
       "  4276: ['methodologies'],\n",
       "  4277: ['datasets'],\n",
       "  4287: ['datasets', 'evaluation_methods'],\n",
       "  4290: ['methodologies'],\n",
       "  4293: ['methodologies'],\n",
       "  4302: ['methodologies'],\n",
       "  4303: ['methodologies'],\n",
       "  4305: ['methodologies'],\n",
       "  4313: ['methodologies'],\n",
       "  4317: ['methodologies'],\n",
       "  4323: ['methodologies'],\n",
       "  4324: ['methodologies'],\n",
       "  4327: ['datasets', 'methodologies'],\n",
       "  4337: ['methodologies'],\n",
       "  4354: ['methodologies'],\n",
       "  4358: ['methodologies'],\n",
       "  4360: ['methodologies'],\n",
       "  4368: ['methodologies'],\n",
       "  4373: ['methodologies'],\n",
       "  4376: ['methodologies'],\n",
       "  4391: ['methodologies'],\n",
       "  4395: ['methodologies'],\n",
       "  4405: ['methodologies'],\n",
       "  4413: ['methodologies'],\n",
       "  4414: ['methodologies'],\n",
       "  4417: ['datasets', 'methodologies'],\n",
       "  4418: ['methodologies', 'evaluation_methods'],\n",
       "  4428: ['methodologies'],\n",
       "  4431: ['datasets'],\n",
       "  4435: ['methodologies'],\n",
       "  4442: ['datasets'],\n",
       "  4448: ['methodologies'],\n",
       "  4449: ['methodologies'],\n",
       "  4464: ['methodologies'],\n",
       "  4465: ['datasets'],\n",
       "  4466: ['datasets'],\n",
       "  4468: ['methodologies'],\n",
       "  4470: ['methodologies'],\n",
       "  4474: ['methodologies'],\n",
       "  4481: ['methodologies'],\n",
       "  4482: ['methodologies'],\n",
       "  4497: ['datasets', 'methodologies'],\n",
       "  4500: ['datasets'],\n",
       "  4501: ['methodologies'],\n",
       "  4508: ['methodologies'],\n",
       "  4517: ['datasets'],\n",
       "  4538: ['methodologies'],\n",
       "  4551: ['methodologies'],\n",
       "  4554: ['datasets'],\n",
       "  4569: ['methodologies'],\n",
       "  4578: ['methodologies'],\n",
       "  4588: ['methodologies'],\n",
       "  4591: ['datasets'],\n",
       "  4597: ['methodologies'],\n",
       "  4611: ['methodologies'],\n",
       "  4613: ['datasets'],\n",
       "  4616: ['datasets'],\n",
       "  4631: ['methodologies'],\n",
       "  4633: ['datasets', 'methodologies'],\n",
       "  4656: ['methodologies'],\n",
       "  4657: ['methodologies'],\n",
       "  4660: ['methodologies'],\n",
       "  4666: ['methodologies'],\n",
       "  4667: ['methodologies'],\n",
       "  4669: ['datasets'],\n",
       "  4692: ['methodologies'],\n",
       "  4704: ['methodologies'],\n",
       "  4722: ['datasets'],\n",
       "  4738: ['methodologies'],\n",
       "  4746: ['methodologies'],\n",
       "  4752: ['methodologies'],\n",
       "  4753: ['methodologies'],\n",
       "  4754: ['datasets'],\n",
       "  4773: ['datasets'],\n",
       "  4780: ['datasets'],\n",
       "  4785: ['datasets'],\n",
       "  4820: ['methodologies'],\n",
       "  4832: ['methodologies'],\n",
       "  4833: ['methodologies'],\n",
       "  4835: ['methodologies'],\n",
       "  4837: ['methodologies'],\n",
       "  4842: ['methodologies'],\n",
       "  4855: ['methodologies'],\n",
       "  4858: ['methodologies'],\n",
       "  4864: ['methodologies'],\n",
       "  4875: ['datasets'],\n",
       "  4877: ['methodologies'],\n",
       "  4885: ['methodologies'],\n",
       "  4888: ['methodologies'],\n",
       "  4893: ['methodologies'],\n",
       "  4898: ['methodologies'],\n",
       "  4899: ['methodologies'],\n",
       "  4900: ['methodologies'],\n",
       "  4901: ['methodologies'],\n",
       "  4902: ['methodologies'],\n",
       "  4904: ['methodologies'],\n",
       "  4905: ['methodologies'],\n",
       "  4911: ['methodologies'],\n",
       "  4919: ['methodologies'],\n",
       "  4922: ['methodologies'],\n",
       "  4923: ['methodologies'],\n",
       "  4932: ['datasets'],\n",
       "  4942: ['methodologies'],\n",
       "  4954: ['datasets'],\n",
       "  4957: ['datasets'],\n",
       "  4958: ['datasets'],\n",
       "  5009: ['methodologies'],\n",
       "  5011: ['methodologies'],\n",
       "  5013: ['methodologies'],\n",
       "  5017: ['methodologies'],\n",
       "  5019: ['methodologies'],\n",
       "  5031: ['methodologies'],\n",
       "  5046: ['methodologies'],\n",
       "  5052: ['methodologies'],\n",
       "  5058: ['datasets'],\n",
       "  5069: ['methodologies'],\n",
       "  5070: ['datasets'],\n",
       "  5077: ['methodologies'],\n",
       "  5081: ['methodologies'],\n",
       "  5084: ['methodologies'],\n",
       "  5089: ['methodologies'],\n",
       "  5090: ['methodologies'],\n",
       "  5095: ['datasets'],\n",
       "  5103: ['datasets'],\n",
       "  5105: ['datasets'],\n",
       "  5106: ['datasets'],\n",
       "  5108: ['datasets'],\n",
       "  5110: ['datasets'],\n",
       "  5111: ['datasets'],\n",
       "  5113: ['datasets'],\n",
       "  5125: ['methodologies'],\n",
       "  5134: ['methodologies'],\n",
       "  5137: ['methodologies'],\n",
       "  5143: ['methodologies'],\n",
       "  5145: ['methodologies'],\n",
       "  5147: ['methodologies'],\n",
       "  5155: ['methodologies'],\n",
       "  5168: ['methodologies'],\n",
       "  5170: ['methodologies'],\n",
       "  5172: ['methodologies'],\n",
       "  5175: ['methodologies'],\n",
       "  5179: ['datasets'],\n",
       "  5187: ['methodologies'],\n",
       "  5200: ['methodologies'],\n",
       "  5214: ['methodologies'],\n",
       "  5230: ['methodologies'],\n",
       "  5238: ['methodologies'],\n",
       "  5243: ['methodologies'],\n",
       "  5246: ['methodologies'],\n",
       "  5247: ['methodologies'],\n",
       "  5249: ['methodologies'],\n",
       "  5250: ['methodologies'],\n",
       "  5262: ['methodologies'],\n",
       "  5265: ['methodologies'],\n",
       "  5274: ['methodologies'],\n",
       "  5275: ['methodologies'],\n",
       "  5283: ['methodologies'],\n",
       "  5336: ['methodologies'],\n",
       "  5337: ['methodologies'],\n",
       "  5353: ['datasets'],\n",
       "  5356: ['methodologies'],\n",
       "  5358: ['methodologies'],\n",
       "  5369: ['datasets'],\n",
       "  5380: ['methodologies'],\n",
       "  5381: ['methodologies'],\n",
       "  5382: ['methodologies'],\n",
       "  5394: ['methodologies'],\n",
       "  5403: ['methodologies'],\n",
       "  5424: ['methodologies'],\n",
       "  5430: ['methodologies'],\n",
       "  5435: ['methodologies'],\n",
       "  5439: ['methodologies'],\n",
       "  5452: ['methodologies'],\n",
       "  5469: ['datasets'],\n",
       "  5471: ['methodologies'],\n",
       "  5478: ['methodologies'],\n",
       "  5479: ['methodologies'],\n",
       "  5498: ['methodologies'],\n",
       "  5499: ['methodologies'],\n",
       "  5504: ['datasets', 'methodologies'],\n",
       "  5539: ['methodologies'],\n",
       "  5552: ['methodologies'],\n",
       "  5555: ['methodologies'],\n",
       "  5563: ['methodologies'],\n",
       "  5564: ['methodologies'],\n",
       "  5565: ['methodologies'],\n",
       "  5566: ['methodologies'],\n",
       "  5567: ['methodologies'],\n",
       "  5568: ['methodologies'],\n",
       "  5572: ['methodologies'],\n",
       "  5574: ['methodologies'],\n",
       "  5575: ['methodologies'],\n",
       "  5577: ['methodologies'],\n",
       "  5578: ['methodologies'],\n",
       "  ...})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_dims), paper_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large Language Models (LLMs) struggle with providing current information due to the outdated pre-training data. Existing methods for updating LLMs, such as knowledge editing and continual fine-tuning, have significant drawbacks in generalizability of new information and the requirements on structured updating corpus. We identify the core challenge behind these drawbacks: the LM-logical discrepancy featuring the difference between language modeling probabilities and logical probabilities. To evaluate and address the core challenge, we propose a new task formulation of the information updating task that only requires the provision of an unstructured updating corpus and evaluates the performance of information updating on the generalizability to question-answer pairs pertaining to the updating information.We further propose a novel and effective pipeline approach for the task, highlighting a self-prompting-based question-answer generation process and a associative distillation methods to bridge the LM-logical discrepancy.We develop two datasets for evaluation, one sourced from news articles published in March and April 2023, and the other from the Natural Questions benchmark.Experimental results demonstrate the superiority of our approach, significantly increasing the factual consistency score (on a scale from 0 to 1) by up to 0.16. Furthermore, our method effectively mitigates forgetting utilizing a compact replay buffer with only 2.3{\\\\%} of the training tokens.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_collection[2494].abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loose Classification of Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.llm = 'vllm'\n",
    "# initializeLLM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2954"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(internal_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': <taxonomy.DAG at 0x7f4a46135550>,\n",
       " 'datasets': <taxonomy.DAG at 0x7f4a46135430>,\n",
       " 'methodologies': <taxonomy.DAG at 0x7f4a46135bb0>,\n",
       " 'evaluation_methods': <taxonomy.DAG at 0x7f4a461353d0>,\n",
       " 'real_world_domains': <taxonomy.DAG at 0x7f4a46135d90>}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  natural_language_processing_methodologies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 1056/1056 [01:08<00:00, 15.50it/s]\n",
      "Processed prompts: 100%|██████████| 2241/2241 [11:00<00:00,  3.39it/s, est. speed input: 7712.11 toks/s, output: 168.19 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  topic_modeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 0it [00:00, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  machine_translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1313/1313 [06:24<00:00,  3.42it/s, est. speed input: 7576.98 toks/s, output: 194.43 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  sentiment_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 0it [00:00, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  named_entity_recognition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 0it [00:00, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  text_classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1103/1103 [05:48<00:00,  3.16it/s, est. speed input: 7535.86 toks/s, output: 189.02 toks/s]\n"
     ]
    }
   ],
   "source": [
    "dags['methodologies'].classify_dag(args, collection=dags['methodologies'].root.papers, label2node=label2node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1103"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roots['methodologies'].children['text_classification'].papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': Node(label=natural_language_processing_tasks, dim=tasks, description=None, level=0),\n",
       " 'datasets': Node(label=natural_language_processing_datasets, dim=datasets, description=None, level=0),\n",
       " 'methodologies': Node(label=natural_language_processing_methodologies, dim=methodologies, description=None, level=0),\n",
       " 'evaluation_methods': Node(label=natural_language_processing_evaluation_methods, dim=evaluation_methods, description=None, level=0),\n",
       " 'real_world_domains': Node(label=natural_language_processing_real_world_domains, dim=real_world_domains, description=None, level=0)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_classification_datasets': Node(label=text_classification_datasets, dim=datasets, description=Datasets specifically curated for training and evaluating text classification models in natural language processing tasks., level=1),\n",
       " 'named_entity_recognition_datasets': Node(label=named_entity_recognition_datasets, dim=datasets, description=Datasets containing annotated entities such as names, locations, and organizations for training and evaluating named entity recognition models., level=1),\n",
       " 'sentiment_analysis_datasets': Node(label=sentiment_analysis_datasets, dim=datasets, description=Datasets designed for sentiment analysis tasks, providing labeled data for sentiment polarity classification., level=1),\n",
       " 'question_answering_datasets': Node(label=question_answering_datasets, dim=datasets, description=Datasets structured to support question answering systems by providing question-answer pairs for training and evaluation., level=1),\n",
       " 'machine_translation_datasets': Node(label=machine_translation_datasets, dim=datasets, description=Datasets comprising parallel corpora in multiple languages to facilitate training and evaluation of machine translation models., level=1)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots['datasets'].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_classification': Node(label=text_classification, description=Text classification involves categorizing text data into predefined classes or categories., level=1),\n",
       " 'named_entity_recognition': Node(label=named_entity_recognition, description=Named entity recognition is the task of identifying and classifying named entities in text., level=1),\n",
       " 'machine_translation': Node(label=machine_translation, description=Machine translation involves translating text from one language to another., level=1),\n",
       " 'text_generation': Node(label=text_generation, description=Text generation focuses on generating coherent and contextually relevant text., level=1)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662/662 [00:00<00:00, 426911.02it/s]\n"
     ]
    }
   ],
   "source": [
    "unlabeled = []\n",
    "\n",
    "for paper_id, paper in tqdm(root.papers.items()):\n",
    "    add = True\n",
    "    for c in root.children.values():\n",
    "        if paper_id in c.papers:\n",
    "            add = False\n",
    "    if add:\n",
    "        unlabeled.append(paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
