{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_definitions import initializeLLM, promptLLM, constructPrompt\n",
    "import json\n",
    "from utils import clean_json_string\n",
    "from collections import deque\n",
    "from taxonomy import Node\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.topic = \"natural language processing\"\n",
    "        self.dimensions = [\"tasks\", \"datasets\", \"methodologies\", \"evaluation_methods\", \"real_world_domains\"]\n",
    "        self.llm = 'vllm'\n",
    "        self.init_levels = 2\n",
    "\n",
    "        self.dataset = \"Reasoning\"\n",
    "        self.data_dir = f\"datasets/multi_dim/{self.dataset.lower().replace(' ', '_')}/\"\n",
    "        self.internal = f\"{self.dataset}.txt\"\n",
    "        self.external = f\"{self.dataset}_external.txt\"\n",
    "        self.groundtruth = \"groundtruth.txt\"\n",
    "        \n",
    "        self.length = 512\n",
    "        self.dim = 768\n",
    "\n",
    "        self.iters = 4\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-25 07:32:56 config.py:729] Defaulting to use mp for distributed inference\n",
      "WARNING 11-25 07:32:56 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-25 07:32:56 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-25 07:32:56 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 11-25 07:32:57 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-25 07:32:57 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:32:57 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-25 07:32:57 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 11-25 07:32:57 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:32:57 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:32:57 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-25 07:32:58 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_5,6.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:32:58 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_5,6.json\n",
      "INFO 11-25 07:32:58 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fceb99d8190>, local_subscribe_port=41507, remote_subscribe_port=None)\n",
      "INFO 11-25 07:32:58 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:32:58 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:32:58 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "INFO 11-25 07:32:58 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acdf42a219a478782c118d0eabed800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:33:02 model_runner.py:732] Loading model weights took 7.5122 GB\n",
      "INFO 11-25 07:33:02 model_runner.py:732] Loading model weights took 7.5122 GB\n",
      "INFO 11-25 07:33:03 distributed_gpu_executor.py:56] # GPU blocks: 15716, # CPU blocks: 4096\n",
      "INFO 11-25 07:33:05 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-25 07:33:05 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:33:05 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:33:05 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-25 07:33:13 custom_all_reduce.py:219] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:33:13 custom_all_reduce.py:219] Registering 1040 cuda graph addresses\n",
      "INFO 11-25 07:33:13 model_runner.py:1225] Graph capturing finished in 8 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3229229)\u001b[0;0m INFO 11-25 07:33:13 model_runner.py:1225] Graph capturing finished in 8 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = initializeLLM(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct a 2-Level Multi-Dimensional Taxonomy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import multi_dim_prompt, NodeListSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to make this a directed acyclic graph (DAG) so maintain a list of the nodes\n",
    "roots = {}\n",
    "id2node = {}\n",
    "label2node = {}\n",
    "idx = 0\n",
    "\n",
    "for dim in args.dimensions:\n",
    "    mod_topic = args.topic.replace(' ', '_').lower() + f\"_{dim}\"\n",
    "    root = Node(\n",
    "            id=idx,\n",
    "            label=mod_topic,\n",
    "            dimension=dim\n",
    "        )\n",
    "    roots[dim] = root\n",
    "    id2node[idx] = root\n",
    "    label2node[mod_topic] = root\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it, est. speed input: 80.14 toks/s, output: 64.57 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.61s/it, est. speed input: 80.81 toks/s, output: 65.49 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it, est. speed input: 75.79 toks/s, output: 64.86 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 74.81 toks/s, output: 64.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it, est. speed input: 107.96 toks/s, output: 64.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it, est. speed input: 74.01 toks/s, output: 64.67 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it, est. speed input: 81.27 toks/s, output: 64.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it, est. speed input: 73.88 toks/s, output: 64.80 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.60s/it, est. speed input: 82.51 toks/s, output: 65.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.11s/it, est. speed input: 71.37 toks/s, output: 65.26 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it, est. speed input: 78.50 toks/s, output: 65.90 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.91s/it, est. speed input: 80.45 toks/s, output: 64.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it, est. speed input: 91.17 toks/s, output: 65.75 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.16s/it, est. speed input: 74.10 toks/s, output: 64.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.88s/it, est. speed input: 75.76 toks/s, output: 65.68 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it, est. speed input: 101.87 toks/s, output: 65.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it, est. speed input: 82.23 toks/s, output: 65.34 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.22s/it, est. speed input: 67.09 toks/s, output: 64.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it, est. speed input: 77.40 toks/s, output: 64.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it, est. speed input: 77.94 toks/s, output: 65.30 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it, est. speed input: 72.92 toks/s, output: 64.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it, est. speed input: 70.38 toks/s, output: 64.86 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.21s/it, est. speed input: 68.84 toks/s, output: 65.42 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it, est. speed input: 72.77 toks/s, output: 65.86 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it, est. speed input: 42.39 toks/s, output: 66.03 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it, est. speed input: 174.32 toks/s, output: 64.51 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it, est. speed input: 84.97 toks/s, output: 65.57 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it, est. speed input: 87.45 toks/s, output: 65.08 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.38s/it, est. speed input: 108.66 toks/s, output: 65.28 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it, est. speed input: 103.03 toks/s, output: 64.89 toks/s]\n"
     ]
    }
   ],
   "source": [
    "queue = deque([node for id, node in id2node.items()])\n",
    "\n",
    "# if taking long, you can probably parallelize this between the different taxonomies (expand by level)\n",
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    label = curr_node.label\n",
    "    # expand\n",
    "    system_instruction, main_prompt, json_output_format = multi_dim_prompt(curr_node)\n",
    "    prompts = [constructPrompt(args, system_instruction, main_prompt + \"\\n\\n\" + json_output_format)]\n",
    "    outputs = promptLLM(args=args, prompts=prompts, schema=NodeListSchema, max_new_tokens=3000, json_mode=True, temperature=0.1, top_p=0.99)[0]\n",
    "    outputs = json.loads(clean_json_string(outputs)) if \"```\" in outputs else json.loads(outputs.strip())\n",
    "    outputs = outputs['root_topic']\n",
    "\n",
    "    # add all children\n",
    "    for key, value in outputs.items():\n",
    "        key = key.replace(' ', '_').lower()\n",
    "        if (key not in label2node) or ((key in label2node) and (label2node[key].dimension != curr_node.dimension)):\n",
    "            child_node = Node(\n",
    "                    id=len(id2node),\n",
    "                    label=key,\n",
    "                    dimension=curr_node.dimension,\n",
    "                    description=value['description'],\n",
    "                    parents=[curr_node]\n",
    "                )\n",
    "            curr_node.add_child(key, child_node)\n",
    "            id2node[child_node.id] = child_node\n",
    "            label2node[key] = child_node\n",
    "            if child_node.level < args.init_levels:\n",
    "                queue.append(child_node)\n",
    "        \n",
    "        else:\n",
    "            child_node = label2node[key]\n",
    "            child_node.add_parent(curr_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': Node(label=natural_language_processing_tasks, dim=tasks, description=None, level=0),\n",
       " 'datasets': Node(label=natural_language_processing_datasets, dim=datasets, description=None, level=0),\n",
       " 'methodologies': Node(label=natural_language_processing_methodologies, dim=methodologies, description=None, level=0),\n",
       " 'evaluation_methods': Node(label=natural_language_processing_evaluation_methods, dim=evaluation_methods, description=None, level=0),\n",
       " 'real_world_domains': Node(label=natural_language_processing_real_world_domains, dim=real_world_domains, description=None, level=0)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: natural_language_processing_methodologies\n",
      "Dimension: methodologies\n",
      "Description: None\n",
      "Level: 0\n",
      "----------------------------------------\n",
      "Children:\n",
      "     Label: supervised_learning\n",
      "     Dimension: methodologies\n",
      "     Description: Supervised learning is a type of machine learning where the algorithm is trained on labeled data to learn the mapping between input and output.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: regression\n",
      "          Dimension: methodologies\n",
      "          Description: A type of supervised learning where the goal is to predict a continuous output value.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: classification\n",
      "          Dimension: methodologies\n",
      "          Description: A type of supervised learning where the goal is to predict a categorical output value.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: clustering\n",
      "          Dimension: methodologies\n",
      "          Description: A type of unsupervised learning where the goal is to group similar data points together.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: decision_trees\n",
      "          Dimension: methodologies\n",
      "          Description: A type of supervised learning where the goal is to create a tree-like model to make predictions.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: neural_networks\n",
      "          Dimension: methodologies\n",
      "          Description: A type of supervised learning where the goal is to create a complex model to make predictions.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "     Label: unsupervised_learning\n",
      "     Dimension: methodologies\n",
      "     Description: Unsupervised learning is a type of machine learning where the algorithm is trained on unlabeled data to discover patterns or relationships.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: dimensionality_reduction\n",
      "          Dimension: methodologies\n",
      "          Description: Dimensionality reduction is a type of unsupervised learning that reduces the number of features or dimensions in a dataset while retaining most of the information.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: feature_extraction\n",
      "          Dimension: methodologies\n",
      "          Description: Feature extraction is a type of unsupervised learning that automatically selects a subset of the most relevant features from a dataset.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: manifold_learning\n",
      "          Dimension: methodologies\n",
      "          Description: Manifold learning is a type of unsupervised learning that aims to identify the underlying structure of a dataset by learning a lower-dimensional representation.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: density_estimation\n",
      "          Dimension: methodologies\n",
      "          Description: Density estimation is a type of unsupervised learning that estimates the underlying probability distribution of a dataset.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "     Label: reinforcement_learning\n",
      "     Dimension: methodologies\n",
      "     Description: Reinforcement learning is a type of machine learning where the algorithm learns by interacting with an environment and receiving rewards or penalties for its actions.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: q-learning\n",
      "          Dimension: methodologies\n",
      "          Description: Q-learning is a model-free reinforcement learning algorithm that learns an action-value function, Q(s, a), which estimates the expected return when taking action a in state s.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: deep_q-networks\n",
      "          Dimension: methodologies\n",
      "          Description: Deep Q-Networks (DQN) is a type of reinforcement learning algorithm that uses a deep neural network to approximate the action-value function, Q(s, a).\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: policy_gradient_methods\n",
      "          Dimension: methodologies\n",
      "          Description: Policy gradient methods are a type of reinforcement learning algorithm that learn a policy, π(a|s), which maps states to actions.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: actor-critic_methods\n",
      "          Dimension: methodologies\n",
      "          Description: Actor-critic methods are a type of reinforcement learning algorithm that combine policy gradient methods and value function estimation to learn both the policy and the value function.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: model-based_methods\n",
      "          Dimension: methodologies\n",
      "          Description: Model-based methods are a type of reinforcement learning algorithm that learn a model of the environment and use it to plan and make decisions.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "     Label: deep_learning\n",
      "     Dimension: methodologies\n",
      "     Description: Deep learning is a subset of machine learning that uses neural networks with multiple layers to learn complex patterns in data.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: meta_learning\n",
      "          Dimension: methodologies\n",
      "          Description: A type of deep learning where the model learns to learn from other models or tasks, often to improve its ability to adapt to new tasks.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "     Label: transfer_learning\n",
      "     Dimension: methodologies\n",
      "     Description: Transfer learning is a technique where a pre-trained model is fine-tuned on a new task to leverage knowledge from a related task.\n",
      "     Level: 1\n",
      "     ----------------------------------------\n",
      "     Children:\n",
      "          Label: domain_adaptation\n",
      "          Dimension: methodologies\n",
      "          Description: Domain adaptation is a type of transfer learning that involves adapting a model trained on one domain to another domain.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: knowledge_distillation\n",
      "          Dimension: methodologies\n",
      "          Description: Knowledge distillation is a transfer learning method that involves training a smaller model to mimic the behavior of a larger, pre-trained model.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: meta-learning\n",
      "          Dimension: methodologies\n",
      "          Description: Meta-learning is a type of transfer learning that involves training a model to learn how to learn from a few examples.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: multi-task_learning\n",
      "          Dimension: methodologies\n",
      "          Description: Multi-task learning is a transfer learning method that involves training a model on multiple tasks simultaneously, with the goal of improving performance on each task.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "          Label: self-supervised_learning\n",
      "          Dimension: methodologies\n",
      "          Description: Self-supervised learning is a type of transfer learning that involves training a model on unlabeled data, with the goal of learning useful representations.\n",
      "          Level: 2\n",
      "          ----------------------------------------\n",
      "     ----------------------------------------\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "roots['evaluation_methods'].display(0, indent_multiplier=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from paper import Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.data_dir):\n",
    "    os.makedirs(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"EMNLP/EMNLP2024-papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2954 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2954/2954 [00:00<00:00, 5190.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal: 2954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "internal_collection = {}\n",
    "\n",
    "with open(os.path.join(args.data_dir, 'internal.txt'), 'w') as i:\n",
    "    internal_count = 0\n",
    "    id = 0\n",
    "    for p in tqdm(ds['train']):\n",
    "        temp_dict = {\"Title\": p['title'], \"Abstract\": p['abstract']}\n",
    "        formatted_dict = json.dumps(temp_dict)\n",
    "        i.write(f'{formatted_dict}\\n')\n",
    "        internal_collection[id] = Paper(id, p['title'], p['abstract'], label_opts=args.dimensions, internal=True)\n",
    "        internal_count += 1\n",
    "        id += 1\n",
    "print(f'Internal: {internal_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_ds = load_dataset(\"TimSchopf/nlp_taxonomy_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178521/178521 [00:14<00:00, 12194.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External Count: 178521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "external_collection = {}\n",
    "\n",
    "with open(os.path.join(args.data_dir, 'external.txt'), 'w') as e:\n",
    "    external_count = 0\n",
    "    id = len(internal_collection)\n",
    "    for p in tqdm(external_ds['train']):\n",
    "        temp_dict = {\"Title\": p['title'], \"Abstract\": p['abstract']}\n",
    "        formatted_dict = json.dumps(temp_dict)\n",
    "        e.write(f'{formatted_dict}\\n')\n",
    "        external_collection[id] = Paper(id, p['title'], p['abstract'], label_opts=args.dimensions, internal=False)\n",
    "        external_count += 1\n",
    "        id += 1\n",
    "print(f'External Count: {external_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enrich each node with a set of terms and sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxonomy import DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dags = {dim:DAG(root=root, dim=dim) for dim, root in roots.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 1012/1012 [01:03<00:00, 15.82it/s]\n",
      "Processed prompts: 100%|██████████| 26/26 [00:10<00:00,  2.40it/s, est. speed input: 1147.85 toks/s, output: 892.82 toks/s]\n",
      "Processed prompts: 100%|██████████| 27/27 [00:12<00:00,  2.24it/s, est. speed input: 1060.84 toks/s, output: 758.86 toks/s]\n",
      "Processed prompts: 100%|██████████| 26/26 [00:10<00:00,  2.47it/s, est. speed input: 1142.85 toks/s, output: 860.17 toks/s]\n",
      "Processed prompts: 100%|██████████| 29/29 [00:10<00:00,  2.65it/s, est. speed input: 1288.17 toks/s, output: 919.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 26/26 [00:08<00:00,  3.04it/s, est. speed input: 1346.50 toks/s, output: 888.25 toks/s]\n"
     ]
    }
   ],
   "source": [
    "enriched_phrases = {dim:[] for dim in args.dimensions}\n",
    "enriched_sentences = {dim:[] for dim in args.dimensions}\n",
    "\n",
    "for dim, dag in dags.items():\n",
    "    all_phrases, all_sentences = dag.enrich_dag(args, id2node)\n",
    "    enriched_phrases[dim].extend(all_phrases)\n",
    "    enriched_sentences[dim].extend(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_classification': Node(label=text_classification, dim=tasks, description=The process of assigning a label to a piece of text based on its content, such as spam vs. non-spam emails or positive vs. negative reviews., level=1),\n",
       " 'sentiment_analysis': Node(label=sentiment_analysis, dim=tasks, description=The process of determining the emotional tone or sentiment of a piece of text, such as whether a review is positive or negative., level=1),\n",
       " 'named_entity_recognition': Node(label=named_entity_recognition, dim=tasks, description=The process of identifying and categorizing named entities in unstructured text, such as people, places, and organizations., level=1),\n",
       " 'language_translation': Node(label=language_translation, dim=tasks, description=The process of converting text from one language to another, such as translating English to Spanish., level=1),\n",
       " 'question_answering': Node(label=question_answering, dim=tasks, description=The process of automatically answering questions based on a given text or knowledge base., level=1)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots['tasks'].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_localization',\n",
       " 'language_modeling_approaches',\n",
       " 'language_generation_models',\n",
       " 'language_alignment',\n",
       " 'text_decomposition',\n",
       " 'text_segmentation_software',\n",
       " 'text_equivalence',\n",
       " 'language_compression',\n",
       " 'text_expansion',\n",
       " 'sentence_splitting',\n",
       " 'translation_accuracy',\n",
       " 'phrase_extraction',\n",
       " 'text_modification',\n",
       " 'language_model',\n",
       " 'text_forecasting',\n",
       " 'text_parsing',\n",
       " 'language_prediction',\n",
       " 'statistical_models',\n",
       " 'text_segmentation_algorithms',\n",
       " 'text_rewriting',\n",
       " 'sequence_prediction',\n",
       " 'language_pair',\n",
       " 'machine_translation',\n",
       " 'text_division',\n",
       " 'target_language',\n",
       " 'next_word_prediction',\n",
       " 'predictive_language',\n",
       " 'tokenization',\n",
       " 'machine_learning',\n",
       " 'language_understanding',\n",
       " 'translation_error',\n",
       " 'language_generation',\n",
       " 'sentence_boundary_detection',\n",
       " 'translation_speed',\n",
       " 'text_forecasting_models',\n",
       " 'translation',\n",
       " 'word_identification',\n",
       " 'algorithmic_approach',\n",
       " 'language_transfer',\n",
       " 'text_chunking',\n",
       " 'text_segmentation_techniques',\n",
       " 'segmentation_techniques',\n",
       " 'text_generation',\n",
       " 'text_segmentation_tools',\n",
       " 'source_language',\n",
       " 'language_transformation',\n",
       " 'text_splitting',\n",
       " 'language_fusion',\n",
       " 'text_segmentation_methods',\n",
       " 'language_decomposition',\n",
       " 'text_completion',\n",
       " 'language_processing',\n",
       " 'text_transposition',\n",
       " 'text_transformation',\n",
       " 'translation_quality',\n",
       " 'text_modeling',\n",
       " 'translation_cost',\n",
       " 'word_prediction',\n",
       " 'text_synthesis',\n",
       " 'language_modeling_techniques',\n",
       " 'text_transliteration',\n",
       " 'text_segmentation_approaches',\n",
       " 'language_fidelity',\n",
       " 'translation_system',\n",
       " 'language_modeling_algorithms',\n",
       " 'text_processing',\n",
       " 'language_forecasting',\n",
       " 'predictive_text',\n",
       " 'text_analysis',\n",
       " 'language_reconstruction',\n",
       " 'contextual_language']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots['tasks'].children['language_translation'].get_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Pseudo-labels for Dimension/Type Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_any_match(patterns, input_string):\n",
    "    \"\"\"\n",
    "    Check if any pattern in the list matches the input string.\n",
    "\n",
    "    :param patterns: List of regex patterns (as strings)\n",
    "    :param input_string: The string to search within\n",
    "    :return: True if any pattern matches, otherwise False\n",
    "    \"\"\"\n",
    "    # Compile all the patterns to make matching more efficient\n",
    "    compiled_patterns = [re.compile(pattern) for pattern in patterns]\n",
    "    \n",
    "    # Check if any compiled pattern matches the input string\n",
    "    for compiled_pattern in compiled_patterns:\n",
    "        if compiled_pattern.search(input_string):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tasks',\n",
       " 'datasets',\n",
       " 'methodologies',\n",
       " 'evaluation_methods',\n",
       " 'real_world_applications']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2954/2954 [00:00<00:00, 6785.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': 0, 'datasets': 233, 'methodologies': 487, 'evaluation_methods': 23, 'real_world_applications': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178521/178521 [00:26<00:00, 6771.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tasks': 0, 'datasets': 3377, 'methodologies': 24568, 'evaluation_methods': 416, 'real_world_applications': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pseudo_labels = {d:[] for d in args.dimensions}\n",
    "paper_dims = {}\n",
    "\n",
    "patterns = {\"datasets\": [r'introduce [\\s\\w]* benchmark', r'introduce [\\s\\w]* dataset', r'construct [\\s\\w]* benchmark', r'construct [\\s\\w]* dataset', r'propose [\\s\\w]* dataset', r'propose [\\s\\w]* benchmark', r'present [\\s\\w]* benchmark', r'present [\\s\\w]* dataset', r'develop [\\s\\w]* benchmark', r'develop [\\s\\w]* dataset', r'create [\\s\\w]* benchmark', r'create [\\s\\w]* dataset', r'provide [\\s\\w]* benchmark', r'provide [\\s\\w]* dataset', r'describe [\\s\\w]* benchmark', r'describe [\\s\\w]* dataset', r'propose a new benchmark', r'propose a new dataset', r'introduce a new benchmark', r'introduce a new dataset', r'we release [\\s\\w]* dataset', r'we release [\\s\\w]* benchmark', r'a new dataset for [\\s\\w]*', r'a new benchmark for [\\s\\w]*', r'dataset for [\\s\\w]* task', r'benchmark for [\\s\\w]* task', r'we present [\\s\\w]* dataset', r'we present [\\s\\w]* benchmark', r'dataset designed for [\\s\\w]*', r'benchmark designed for [\\s\\w]*', r'introducing [\\s\\w]* dataset', r'introducing [\\s\\w]* benchmark'],\n",
    "            \"methodologies\": [r'introduce [\\s\\w]* method', r'propose [\\s\\w]* method', r'design [\\s\\w]* method', r'present [\\s\\w]* method', r'develop [\\s\\w]* method', r'introduce [\\s\\w]* approach', r'propose [\\s\\w]* approach', r'design [\\s\\w]* approach', r'present [\\s\\w]* approach', r'develop [\\s\\w]* approach', r'we propose [\\s\\w]* method', r'we propose [\\s\\w]* approach', r'we introduce [\\s\\w]* method', r'we introduce [\\s\\w]* approach', r'we present [\\s\\w]* method', r'we present [\\s\\w]* approach', r'propose a novel method', r'propose a novel approach', r'introduce a novel method', r'introduce a novel approach', r'present a novel method', r'present a novel approach', r'propose [\\s\\w]* framework', r'introduce [\\s\\w]* framework', r'present [\\s\\w]* framework', r'design [\\s\\w]* framework', r'we propose [\\s\\w]* framework', r'we introduce [\\s\\w]* framework', r'we present [\\s\\w]* framework', r'our proposed method [\\s\\w]*', r'our proposed approach [\\s\\w]*', r'our proposed framework [\\s\\w]*', r'this paper proposes [\\s\\w]* method', r'this paper introduces [\\s\\w]* method', r'this paper presents [\\s\\w]* method', r'this paper develops [\\s\\w]* method', r'this paper proposes [\\s\\w]* approach', r'this paper introduces [\\s\\w]* approach', r'this paper presents [\\s\\w]* approach', r'this paper develops [\\s\\w]* approach', r'this paper proposes [\\s\\w]* framework', r'this paper introduces [\\s\\w]* framework', r'this paper presents [\\s\\w]* framework', r'this paper develops [\\s\\w]* framework'],\n",
    "            \"evaluation_methods\": [r'construct a [\\s\\w]* evaluate', r'design a [\\s\\w]* evaluate', r'propose a [\\s\\w]* evaluate', r'introduce [\\s\\w]* evaluation method', r'propose [\\s\\w]* evaluation method', r'design [\\s\\w]* evaluation method', r'develop [\\s\\w]* evaluation method', r'introduce [\\s\\w]* evaluation metric', r'propose [\\s\\w]* evaluation metric', r'design [\\s\\w]* evaluation metric', r'develop [\\s\\w]* evaluation metric', r'propose a novel evaluation method', r'propose a novel evaluation metric', r'present a novel evaluation framework', r'introduce a framework for evaluation', r'this paper proposes [\\s\\w]* evaluation', r'this paper introduces [\\s\\w]* evaluation', r'introduce [\\s\\w]* automatic evaluation', r'propose [\\s\\w]* automatic evaluation', r'develop [\\s\\w]* automatic evaluation', r'design [\\s\\w]* automatic evaluation', r'propose a novel automatic evaluation method', r'automatic evaluation of [\\s\\w]* task', r'develop a method for automatic evaluation', r'introduce [\\s\\w]* human evaluation', r'propose [\\s\\w]* human evaluation', r'develop [\\s\\w]* human evaluation', r'design [\\s\\w]* human evaluation', r'propose a framework for human evaluation', r'introduce a novel human evaluation method', r'conduct human evaluation of [\\s\\w]*', r'compare human and automatic evaluation', r'comparison of human evaluation and automatic evaluation', r'human evaluation versus automatic evaluation', r'evaluate using both human and automatic methods', r'analyze results from human and automatic evaluation']}\n",
    "\n",
    "for id, paper in tqdm(internal_collection.items(), total=len(internal_collection)):\n",
    "    for dim, dim_patterns in patterns.items():\n",
    "        if find_any_match(dim_patterns, f'{paper.title}: {paper.abstract}'.lower()):\n",
    "            pseudo_labels[dim].append(paper)\n",
    "            if id in paper_dims:\n",
    "                paper_dims[id].append(dim)\n",
    "            else:\n",
    "                paper_dims[id] = [dim]\n",
    "print({dim: len(papers) for dim, papers in pseudo_labels.items()})\n",
    "\n",
    "for id, paper in tqdm(external_collection.items(), total=len(external_collection)):\n",
    "    for dim, dim_patterns in patterns.items():\n",
    "        if find_any_match(dim_patterns, f'{paper.title}: {paper.abstract}'.lower()):\n",
    "            pseudo_labels[dim].append(paper)\n",
    "            if id in paper_dims:\n",
    "                paper_dims[id].append(dim)\n",
    "            else:\n",
    "                paper_dims[id] = [dim]\n",
    "\n",
    "print({dim: len(papers) for dim, papers in pseudo_labels.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27488,\n",
       " {0: ['datasets', 'methodologies'],\n",
       "  2: ['methodologies'],\n",
       "  6: ['methodologies'],\n",
       "  7: ['methodologies'],\n",
       "  9: ['datasets'],\n",
       "  10: ['datasets'],\n",
       "  13: ['datasets', 'methodologies'],\n",
       "  16: ['datasets'],\n",
       "  19: ['methodologies'],\n",
       "  21: ['datasets'],\n",
       "  31: ['methodologies'],\n",
       "  36: ['methodologies'],\n",
       "  40: ['methodologies'],\n",
       "  49: ['methodologies'],\n",
       "  54: ['methodologies'],\n",
       "  56: ['methodologies'],\n",
       "  59: ['methodologies'],\n",
       "  64: ['methodologies'],\n",
       "  85: ['methodologies'],\n",
       "  89: ['methodologies'],\n",
       "  90: ['methodologies'],\n",
       "  91: ['datasets'],\n",
       "  94: ['methodologies'],\n",
       "  97: ['datasets', 'methodologies'],\n",
       "  106: ['evaluation_methods'],\n",
       "  109: ['methodologies'],\n",
       "  112: ['datasets', 'methodologies'],\n",
       "  122: ['methodologies'],\n",
       "  123: ['methodologies'],\n",
       "  131: ['datasets'],\n",
       "  132: ['methodologies'],\n",
       "  133: ['methodologies'],\n",
       "  135: ['methodologies'],\n",
       "  148: ['methodologies'],\n",
       "  149: ['methodologies'],\n",
       "  150: ['methodologies'],\n",
       "  151: ['methodologies'],\n",
       "  156: ['methodologies'],\n",
       "  164: ['methodologies'],\n",
       "  165: ['methodologies', 'evaluation_methods'],\n",
       "  174: ['methodologies'],\n",
       "  176: ['datasets'],\n",
       "  184: ['methodologies'],\n",
       "  188: ['methodologies'],\n",
       "  190: ['methodologies'],\n",
       "  191: ['methodologies'],\n",
       "  194: ['methodologies'],\n",
       "  195: ['methodologies'],\n",
       "  199: ['methodologies'],\n",
       "  204: ['methodologies'],\n",
       "  207: ['methodologies'],\n",
       "  211: ['methodologies'],\n",
       "  217: ['methodologies'],\n",
       "  218: ['methodologies'],\n",
       "  224: ['methodologies'],\n",
       "  228: ['methodologies'],\n",
       "  230: ['methodologies'],\n",
       "  233: ['datasets'],\n",
       "  237: ['datasets'],\n",
       "  246: ['datasets'],\n",
       "  253: ['datasets', 'methodologies'],\n",
       "  254: ['methodologies'],\n",
       "  258: ['methodologies'],\n",
       "  263: ['methodologies'],\n",
       "  268: ['methodologies'],\n",
       "  271: ['datasets'],\n",
       "  272: ['methodologies'],\n",
       "  273: ['datasets'],\n",
       "  275: ['methodologies'],\n",
       "  279: ['methodologies'],\n",
       "  281: ['methodologies'],\n",
       "  283: ['methodologies'],\n",
       "  284: ['methodologies'],\n",
       "  286: ['methodologies'],\n",
       "  287: ['datasets', 'methodologies'],\n",
       "  299: ['methodologies'],\n",
       "  300: ['methodologies'],\n",
       "  302: ['methodologies'],\n",
       "  320: ['methodologies'],\n",
       "  332: ['methodologies'],\n",
       "  337: ['methodologies'],\n",
       "  338: ['methodologies'],\n",
       "  343: ['methodologies'],\n",
       "  346: ['methodologies'],\n",
       "  347: ['datasets'],\n",
       "  361: ['datasets'],\n",
       "  364: ['methodologies'],\n",
       "  366: ['methodologies'],\n",
       "  367: ['methodologies'],\n",
       "  379: ['datasets'],\n",
       "  383: ['methodologies'],\n",
       "  384: ['datasets'],\n",
       "  390: ['datasets'],\n",
       "  391: ['datasets'],\n",
       "  399: ['datasets'],\n",
       "  401: ['methodologies'],\n",
       "  402: ['methodologies'],\n",
       "  404: ['methodologies'],\n",
       "  407: ['methodologies'],\n",
       "  408: ['methodologies'],\n",
       "  410: ['datasets'],\n",
       "  420: ['methodologies'],\n",
       "  424: ['methodologies'],\n",
       "  425: ['datasets'],\n",
       "  428: ['datasets'],\n",
       "  436: ['methodologies'],\n",
       "  454: ['methodologies'],\n",
       "  455: ['datasets'],\n",
       "  465: ['methodologies', 'evaluation_methods'],\n",
       "  472: ['methodologies'],\n",
       "  473: ['methodologies'],\n",
       "  480: ['methodologies'],\n",
       "  483: ['methodologies'],\n",
       "  484: ['methodologies'],\n",
       "  486: ['methodologies'],\n",
       "  493: ['datasets'],\n",
       "  508: ['datasets', 'methodologies'],\n",
       "  509: ['methodologies'],\n",
       "  516: ['methodologies'],\n",
       "  519: ['datasets'],\n",
       "  523: ['methodologies'],\n",
       "  533: ['datasets'],\n",
       "  536: ['datasets'],\n",
       "  537: ['methodologies'],\n",
       "  542: ['methodologies'],\n",
       "  545: ['methodologies'],\n",
       "  553: ['datasets'],\n",
       "  556: ['datasets'],\n",
       "  557: ['methodologies'],\n",
       "  563: ['methodologies'],\n",
       "  565: ['methodologies'],\n",
       "  574: ['methodologies'],\n",
       "  576: ['methodologies'],\n",
       "  588: ['methodologies'],\n",
       "  597: ['methodologies'],\n",
       "  600: ['datasets'],\n",
       "  602: ['methodologies'],\n",
       "  615: ['methodologies'],\n",
       "  627: ['methodologies'],\n",
       "  630: ['methodologies'],\n",
       "  633: ['evaluation_methods'],\n",
       "  636: ['datasets'],\n",
       "  639: ['datasets'],\n",
       "  641: ['methodologies'],\n",
       "  642: ['datasets'],\n",
       "  647: ['datasets', 'methodologies'],\n",
       "  648: ['methodologies'],\n",
       "  656: ['methodologies', 'evaluation_methods'],\n",
       "  660: ['datasets'],\n",
       "  662: ['methodologies'],\n",
       "  665: ['methodologies'],\n",
       "  667: ['datasets'],\n",
       "  672: ['datasets'],\n",
       "  680: ['methodologies'],\n",
       "  685: ['methodologies'],\n",
       "  691: ['methodologies'],\n",
       "  693: ['datasets'],\n",
       "  694: ['methodologies'],\n",
       "  695: ['methodologies'],\n",
       "  700: ['methodologies'],\n",
       "  702: ['methodologies'],\n",
       "  725: ['datasets'],\n",
       "  726: ['methodologies'],\n",
       "  737: ['methodologies'],\n",
       "  742: ['methodologies'],\n",
       "  747: ['datasets'],\n",
       "  752: ['methodologies'],\n",
       "  758: ['datasets'],\n",
       "  760: ['methodologies'],\n",
       "  761: ['datasets', 'methodologies'],\n",
       "  762: ['methodologies'],\n",
       "  773: ['datasets', 'methodologies'],\n",
       "  778: ['methodologies'],\n",
       "  781: ['methodologies'],\n",
       "  782: ['methodologies'],\n",
       "  787: ['methodologies'],\n",
       "  789: ['methodologies'],\n",
       "  790: ['methodologies'],\n",
       "  797: ['datasets'],\n",
       "  802: ['methodologies'],\n",
       "  805: ['methodologies'],\n",
       "  811: ['methodologies'],\n",
       "  820: ['methodologies'],\n",
       "  826: ['datasets'],\n",
       "  829: ['methodologies'],\n",
       "  830: ['methodologies'],\n",
       "  832: ['datasets'],\n",
       "  833: ['methodologies'],\n",
       "  838: ['methodologies'],\n",
       "  845: ['methodologies'],\n",
       "  848: ['methodologies'],\n",
       "  850: ['methodologies'],\n",
       "  852: ['methodologies'],\n",
       "  858: ['datasets'],\n",
       "  862: ['methodologies'],\n",
       "  864: ['methodologies'],\n",
       "  875: ['methodologies'],\n",
       "  877: ['methodologies'],\n",
       "  887: ['methodologies'],\n",
       "  888: ['datasets'],\n",
       "  900: ['datasets'],\n",
       "  901: ['methodologies'],\n",
       "  914: ['datasets'],\n",
       "  928: ['methodologies'],\n",
       "  929: ['methodologies'],\n",
       "  935: ['datasets'],\n",
       "  936: ['datasets'],\n",
       "  945: ['datasets'],\n",
       "  946: ['datasets'],\n",
       "  954: ['methodologies'],\n",
       "  956: ['datasets'],\n",
       "  957: ['datasets'],\n",
       "  958: ['datasets', 'methodologies'],\n",
       "  960: ['datasets', 'methodologies'],\n",
       "  962: ['methodologies'],\n",
       "  968: ['datasets'],\n",
       "  973: ['methodologies'],\n",
       "  977: ['methodologies'],\n",
       "  984: ['methodologies'],\n",
       "  987: ['methodologies'],\n",
       "  993: ['methodologies'],\n",
       "  995: ['methodologies'],\n",
       "  1015: ['methodologies'],\n",
       "  1020: ['methodologies', 'evaluation_methods'],\n",
       "  1022: ['datasets'],\n",
       "  1025: ['datasets'],\n",
       "  1028: ['datasets'],\n",
       "  1029: ['methodologies'],\n",
       "  1032: ['methodologies'],\n",
       "  1035: ['methodologies'],\n",
       "  1036: ['methodologies'],\n",
       "  1045: ['methodologies'],\n",
       "  1047: ['datasets', 'methodologies'],\n",
       "  1057: ['methodologies'],\n",
       "  1059: ['methodologies'],\n",
       "  1061: ['methodologies'],\n",
       "  1070: ['datasets'],\n",
       "  1072: ['datasets'],\n",
       "  1081: ['methodologies'],\n",
       "  1082: ['datasets', 'methodologies'],\n",
       "  1088: ['methodologies'],\n",
       "  1099: ['datasets'],\n",
       "  1102: ['methodologies'],\n",
       "  1104: ['datasets'],\n",
       "  1108: ['datasets', 'methodologies'],\n",
       "  1111: ['methodologies', 'evaluation_methods'],\n",
       "  1114: ['datasets', 'methodologies'],\n",
       "  1116: ['methodologies'],\n",
       "  1121: ['methodologies'],\n",
       "  1125: ['datasets'],\n",
       "  1148: ['methodologies'],\n",
       "  1150: ['methodologies'],\n",
       "  1151: ['methodologies'],\n",
       "  1152: ['methodologies'],\n",
       "  1153: ['methodologies'],\n",
       "  1162: ['datasets'],\n",
       "  1167: ['methodologies'],\n",
       "  1174: ['methodologies'],\n",
       "  1175: ['methodologies'],\n",
       "  1178: ['methodologies'],\n",
       "  1186: ['methodologies'],\n",
       "  1194: ['datasets'],\n",
       "  1195: ['methodologies'],\n",
       "  1203: ['methodologies'],\n",
       "  1205: ['methodologies'],\n",
       "  1217: ['datasets'],\n",
       "  1218: ['datasets'],\n",
       "  1220: ['methodologies'],\n",
       "  1221: ['methodologies'],\n",
       "  1223: ['datasets'],\n",
       "  1224: ['datasets', 'methodologies'],\n",
       "  1235: ['methodologies'],\n",
       "  1236: ['methodologies'],\n",
       "  1239: ['datasets'],\n",
       "  1242: ['datasets', 'methodologies'],\n",
       "  1245: ['methodologies'],\n",
       "  1246: ['datasets'],\n",
       "  1247: ['methodologies'],\n",
       "  1249: ['datasets'],\n",
       "  1255: ['methodologies'],\n",
       "  1256: ['methodologies'],\n",
       "  1260: ['methodologies'],\n",
       "  1262: ['methodologies'],\n",
       "  1268: ['methodologies', 'evaluation_methods'],\n",
       "  1276: ['methodologies'],\n",
       "  1288: ['datasets'],\n",
       "  1291: ['methodologies'],\n",
       "  1299: ['methodologies'],\n",
       "  1310: ['evaluation_methods'],\n",
       "  1320: ['methodologies'],\n",
       "  1321: ['datasets'],\n",
       "  1332: ['datasets', 'methodologies'],\n",
       "  1333: ['methodologies'],\n",
       "  1334: ['methodologies'],\n",
       "  1335: ['datasets'],\n",
       "  1342: ['methodologies'],\n",
       "  1346: ['methodologies'],\n",
       "  1348: ['methodologies'],\n",
       "  1359: ['datasets'],\n",
       "  1361: ['methodologies'],\n",
       "  1364: ['methodologies'],\n",
       "  1365: ['methodologies'],\n",
       "  1366: ['methodologies'],\n",
       "  1373: ['methodologies'],\n",
       "  1375: ['methodologies'],\n",
       "  1377: ['methodologies'],\n",
       "  1378: ['methodologies'],\n",
       "  1384: ['datasets'],\n",
       "  1389: ['methodologies'],\n",
       "  1391: ['datasets'],\n",
       "  1393: ['methodologies'],\n",
       "  1400: ['methodologies'],\n",
       "  1402: ['datasets'],\n",
       "  1403: ['datasets', 'methodologies'],\n",
       "  1406: ['methodologies'],\n",
       "  1411: ['methodologies'],\n",
       "  1414: ['methodologies'],\n",
       "  1415: ['methodologies'],\n",
       "  1420: ['methodologies'],\n",
       "  1425: ['methodologies'],\n",
       "  1428: ['methodologies'],\n",
       "  1432: ['datasets', 'methodologies'],\n",
       "  1433: ['methodologies'],\n",
       "  1440: ['methodologies'],\n",
       "  1460: ['datasets'],\n",
       "  1461: ['methodologies'],\n",
       "  1462: ['datasets'],\n",
       "  1463: ['methodologies'],\n",
       "  1465: ['datasets', 'methodologies'],\n",
       "  1466: ['datasets'],\n",
       "  1472: ['datasets'],\n",
       "  1474: ['methodologies'],\n",
       "  1477: ['methodologies'],\n",
       "  1485: ['methodologies'],\n",
       "  1493: ['methodologies'],\n",
       "  1494: ['datasets'],\n",
       "  1497: ['methodologies'],\n",
       "  1499: ['methodologies'],\n",
       "  1500: ['methodologies'],\n",
       "  1503: ['datasets'],\n",
       "  1507: ['methodologies'],\n",
       "  1508: ['methodologies'],\n",
       "  1512: ['methodologies'],\n",
       "  1515: ['methodologies'],\n",
       "  1516: ['methodologies'],\n",
       "  1517: ['methodologies'],\n",
       "  1520: ['methodologies'],\n",
       "  1528: ['methodologies'],\n",
       "  1531: ['datasets', 'methodologies'],\n",
       "  1534: ['datasets'],\n",
       "  1538: ['datasets'],\n",
       "  1539: ['methodologies'],\n",
       "  1543: ['methodologies'],\n",
       "  1547: ['methodologies'],\n",
       "  1553: ['datasets', 'methodologies'],\n",
       "  1555: ['methodologies'],\n",
       "  1557: ['methodologies'],\n",
       "  1558: ['methodologies'],\n",
       "  1563: ['methodologies'],\n",
       "  1567: ['datasets', 'methodologies'],\n",
       "  1570: ['methodologies'],\n",
       "  1571: ['datasets'],\n",
       "  1573: ['datasets'],\n",
       "  1574: ['datasets', 'methodologies'],\n",
       "  1575: ['methodologies'],\n",
       "  1579: ['datasets'],\n",
       "  1586: ['methodologies'],\n",
       "  1588: ['methodologies'],\n",
       "  1592: ['datasets'],\n",
       "  1593: ['datasets', 'methodologies'],\n",
       "  1596: ['datasets'],\n",
       "  1602: ['methodologies'],\n",
       "  1606: ['methodologies'],\n",
       "  1607: ['methodologies'],\n",
       "  1615: ['datasets'],\n",
       "  1616: ['datasets'],\n",
       "  1625: ['methodologies'],\n",
       "  1630: ['methodologies'],\n",
       "  1639: ['methodologies'],\n",
       "  1642: ['datasets'],\n",
       "  1645: ['datasets', 'methodologies'],\n",
       "  1650: ['datasets'],\n",
       "  1651: ['methodologies', 'evaluation_methods'],\n",
       "  1652: ['methodologies'],\n",
       "  1655: ['methodologies'],\n",
       "  1661: ['methodologies'],\n",
       "  1667: ['methodologies'],\n",
       "  1668: ['methodologies'],\n",
       "  1673: ['datasets'],\n",
       "  1674: ['methodologies'],\n",
       "  1678: ['methodologies'],\n",
       "  1679: ['methodologies'],\n",
       "  1680: ['methodologies'],\n",
       "  1682: ['methodologies'],\n",
       "  1693: ['methodologies'],\n",
       "  1696: ['datasets'],\n",
       "  1703: ['methodologies'],\n",
       "  1704: ['datasets'],\n",
       "  1705: ['methodologies'],\n",
       "  1707: ['datasets'],\n",
       "  1722: ['methodologies'],\n",
       "  1726: ['datasets'],\n",
       "  1729: ['datasets'],\n",
       "  1733: ['methodologies'],\n",
       "  1734: ['datasets'],\n",
       "  1737: ['methodologies'],\n",
       "  1739: ['methodologies'],\n",
       "  1741: ['methodologies'],\n",
       "  1746: ['datasets'],\n",
       "  1747: ['methodologies'],\n",
       "  1754: ['methodologies'],\n",
       "  1755: ['methodologies'],\n",
       "  1759: ['methodologies'],\n",
       "  1761: ['datasets'],\n",
       "  1766: ['datasets', 'methodologies'],\n",
       "  1776: ['methodologies'],\n",
       "  1783: ['methodologies'],\n",
       "  1792: ['datasets'],\n",
       "  1798: ['evaluation_methods'],\n",
       "  1806: ['methodologies'],\n",
       "  1812: ['datasets'],\n",
       "  1816: ['datasets'],\n",
       "  1817: ['methodologies'],\n",
       "  1820: ['datasets'],\n",
       "  1830: ['methodologies'],\n",
       "  1831: ['methodologies'],\n",
       "  1843: ['methodologies'],\n",
       "  1848: ['methodologies'],\n",
       "  1854: ['datasets'],\n",
       "  1856: ['methodologies'],\n",
       "  1857: ['datasets'],\n",
       "  1861: ['methodologies'],\n",
       "  1864: ['methodologies'],\n",
       "  1871: ['methodologies'],\n",
       "  1874: ['methodologies'],\n",
       "  1877: ['datasets'],\n",
       "  1881: ['methodologies'],\n",
       "  1889: ['methodologies'],\n",
       "  1891: ['methodologies'],\n",
       "  1903: ['methodologies'],\n",
       "  1905: ['methodologies'],\n",
       "  1906: ['methodologies'],\n",
       "  1907: ['datasets'],\n",
       "  1908: ['methodologies'],\n",
       "  1912: ['datasets', 'methodologies'],\n",
       "  1913: ['methodologies', 'evaluation_methods'],\n",
       "  1917: ['methodologies'],\n",
       "  1923: ['methodologies'],\n",
       "  1927: ['methodologies'],\n",
       "  1932: ['methodologies'],\n",
       "  1933: ['methodologies'],\n",
       "  1934: ['datasets'],\n",
       "  1935: ['methodologies'],\n",
       "  1939: ['datasets'],\n",
       "  1940: ['methodologies'],\n",
       "  1944: ['methodologies'],\n",
       "  1948: ['methodologies'],\n",
       "  1951: ['methodologies'],\n",
       "  1952: ['datasets'],\n",
       "  1958: ['methodologies'],\n",
       "  1960: ['methodologies'],\n",
       "  1965: ['methodologies'],\n",
       "  1972: ['datasets'],\n",
       "  1973: ['methodologies'],\n",
       "  1980: ['methodologies'],\n",
       "  1986: ['methodologies'],\n",
       "  1992: ['methodologies'],\n",
       "  2005: ['methodologies'],\n",
       "  2007: ['methodologies'],\n",
       "  2008: ['methodologies'],\n",
       "  2018: ['datasets'],\n",
       "  2025: ['methodologies'],\n",
       "  2027: ['datasets'],\n",
       "  2029: ['methodologies'],\n",
       "  2035: ['methodologies'],\n",
       "  2036: ['methodologies'],\n",
       "  2040: ['methodologies'],\n",
       "  2041: ['methodologies'],\n",
       "  2042: ['methodologies'],\n",
       "  2046: ['datasets'],\n",
       "  2048: ['methodologies'],\n",
       "  2049: ['methodologies'],\n",
       "  2051: ['methodologies'],\n",
       "  2054: ['methodologies'],\n",
       "  2055: ['methodologies'],\n",
       "  2061: ['methodologies'],\n",
       "  2063: ['methodologies'],\n",
       "  2067: ['methodologies'],\n",
       "  2068: ['methodologies'],\n",
       "  2071: ['datasets'],\n",
       "  2073: ['methodologies'],\n",
       "  2075: ['methodologies'],\n",
       "  2076: ['methodologies'],\n",
       "  2085: ['methodologies'],\n",
       "  2091: ['methodologies'],\n",
       "  2092: ['methodologies'],\n",
       "  2095: ['datasets'],\n",
       "  2106: ['datasets'],\n",
       "  2110: ['datasets'],\n",
       "  2111: ['methodologies'],\n",
       "  2112: ['methodologies'],\n",
       "  2113: ['datasets'],\n",
       "  2116: ['methodologies'],\n",
       "  2125: ['methodologies'],\n",
       "  2129: ['methodologies'],\n",
       "  2134: ['datasets'],\n",
       "  2135: ['datasets'],\n",
       "  2136: ['methodologies'],\n",
       "  2137: ['datasets'],\n",
       "  2141: ['methodologies'],\n",
       "  2145: ['methodologies'],\n",
       "  2148: ['datasets'],\n",
       "  2149: ['methodologies'],\n",
       "  2157: ['methodologies'],\n",
       "  2158: ['datasets'],\n",
       "  2159: ['datasets'],\n",
       "  2164: ['datasets'],\n",
       "  2165: ['methodologies'],\n",
       "  2169: ['methodologies'],\n",
       "  2171: ['datasets'],\n",
       "  2172: ['datasets'],\n",
       "  2177: ['datasets'],\n",
       "  2181: ['methodologies'],\n",
       "  2185: ['datasets'],\n",
       "  2189: ['datasets'],\n",
       "  2204: ['methodologies'],\n",
       "  2207: ['methodologies'],\n",
       "  2209: ['datasets'],\n",
       "  2211: ['evaluation_methods'],\n",
       "  2214: ['methodologies'],\n",
       "  2217: ['datasets'],\n",
       "  2221: ['datasets'],\n",
       "  2223: ['methodologies'],\n",
       "  2228: ['evaluation_methods'],\n",
       "  2231: ['methodologies'],\n",
       "  2232: ['methodologies'],\n",
       "  2236: ['methodologies'],\n",
       "  2238: ['methodologies'],\n",
       "  2243: ['methodologies'],\n",
       "  2246: ['methodologies'],\n",
       "  2248: ['methodologies'],\n",
       "  2251: ['datasets'],\n",
       "  2255: ['evaluation_methods'],\n",
       "  2256: ['methodologies'],\n",
       "  2259: ['methodologies'],\n",
       "  2263: ['methodologies'],\n",
       "  2268: ['datasets'],\n",
       "  2275: ['methodologies'],\n",
       "  2277: ['methodologies'],\n",
       "  2278: ['datasets'],\n",
       "  2283: ['methodologies'],\n",
       "  2292: ['datasets'],\n",
       "  2294: ['methodologies'],\n",
       "  2296: ['datasets'],\n",
       "  2310: ['methodologies'],\n",
       "  2318: ['methodologies'],\n",
       "  2320: ['methodologies'],\n",
       "  2321: ['datasets'],\n",
       "  2323: ['datasets'],\n",
       "  2336: ['datasets'],\n",
       "  2343: ['methodologies'],\n",
       "  2345: ['methodologies'],\n",
       "  2348: ['datasets'],\n",
       "  2350: ['datasets'],\n",
       "  2352: ['methodologies'],\n",
       "  2355: ['datasets'],\n",
       "  2360: ['datasets'],\n",
       "  2363: ['methodologies'],\n",
       "  2374: ['datasets'],\n",
       "  2375: ['methodologies'],\n",
       "  2378: ['methodologies'],\n",
       "  2380: ['datasets'],\n",
       "  2384: ['datasets'],\n",
       "  2390: ['datasets'],\n",
       "  2392: ['datasets'],\n",
       "  2396: ['methodologies'],\n",
       "  2398: ['methodologies'],\n",
       "  2405: ['datasets'],\n",
       "  2406: ['methodologies'],\n",
       "  2413: ['datasets'],\n",
       "  2414: ['methodologies'],\n",
       "  2415: ['methodologies'],\n",
       "  2420: ['methodologies'],\n",
       "  2423: ['methodologies'],\n",
       "  2428: ['methodologies'],\n",
       "  2429: ['datasets', 'methodologies'],\n",
       "  2430: ['datasets'],\n",
       "  2434: ['methodologies'],\n",
       "  2435: ['datasets'],\n",
       "  2441: ['datasets'],\n",
       "  2442: ['datasets'],\n",
       "  2448: ['methodologies'],\n",
       "  2450: ['methodologies'],\n",
       "  2458: ['methodologies'],\n",
       "  2461: ['methodologies'],\n",
       "  2475: ['datasets'],\n",
       "  2477: ['methodologies', 'evaluation_methods'],\n",
       "  2478: ['methodologies'],\n",
       "  2480: ['evaluation_methods'],\n",
       "  2484: ['methodologies'],\n",
       "  2485: ['methodologies'],\n",
       "  2494: ['datasets', 'methodologies', 'evaluation_methods'],\n",
       "  2502: ['datasets', 'methodologies'],\n",
       "  2504: ['datasets', 'methodologies'],\n",
       "  2511: ['datasets'],\n",
       "  2517: ['methodologies'],\n",
       "  2521: ['datasets'],\n",
       "  2522: ['methodologies'],\n",
       "  2524: ['datasets'],\n",
       "  2537: ['datasets'],\n",
       "  2544: ['methodologies'],\n",
       "  2545: ['methodologies'],\n",
       "  2547: ['methodologies'],\n",
       "  2555: ['methodologies'],\n",
       "  2559: ['methodologies'],\n",
       "  2568: ['methodologies'],\n",
       "  2576: ['methodologies'],\n",
       "  2577: ['datasets'],\n",
       "  2578: ['methodologies'],\n",
       "  2587: ['datasets', 'methodologies'],\n",
       "  2596: ['methodologies'],\n",
       "  2603: ['methodologies'],\n",
       "  2607: ['datasets'],\n",
       "  2612: ['methodologies'],\n",
       "  2613: ['datasets'],\n",
       "  2618: ['methodologies'],\n",
       "  2625: ['datasets'],\n",
       "  2629: ['methodologies'],\n",
       "  2633: ['methodologies'],\n",
       "  2637: ['datasets'],\n",
       "  2641: ['methodologies'],\n",
       "  2643: ['evaluation_methods'],\n",
       "  2647: ['methodologies', 'evaluation_methods'],\n",
       "  2648: ['datasets'],\n",
       "  2649: ['datasets'],\n",
       "  2650: ['methodologies'],\n",
       "  2652: ['methodologies'],\n",
       "  2661: ['methodologies'],\n",
       "  2669: ['datasets'],\n",
       "  2672: ['methodologies'],\n",
       "  2675: ['methodologies'],\n",
       "  2679: ['datasets'],\n",
       "  2694: ['methodologies', 'evaluation_methods'],\n",
       "  2695: ['datasets'],\n",
       "  2698: ['datasets'],\n",
       "  2702: ['datasets'],\n",
       "  2715: ['methodologies'],\n",
       "  2718: ['methodologies'],\n",
       "  2720: ['datasets'],\n",
       "  2722: ['methodologies'],\n",
       "  2725: ['datasets'],\n",
       "  2732: ['datasets'],\n",
       "  2733: ['methodologies'],\n",
       "  2737: ['methodologies'],\n",
       "  2739: ['datasets', 'methodologies'],\n",
       "  2743: ['datasets'],\n",
       "  2754: ['methodologies'],\n",
       "  2755: ['methodologies'],\n",
       "  2760: ['methodologies'],\n",
       "  2774: ['methodologies'],\n",
       "  2775: ['methodologies'],\n",
       "  2780: ['datasets'],\n",
       "  2784: ['datasets'],\n",
       "  2792: ['datasets'],\n",
       "  2799: ['methodologies'],\n",
       "  2800: ['datasets'],\n",
       "  2813: ['datasets'],\n",
       "  2828: ['methodologies'],\n",
       "  2832: ['evaluation_methods'],\n",
       "  2843: ['datasets'],\n",
       "  2855: ['datasets'],\n",
       "  2857: ['datasets'],\n",
       "  2860: ['datasets'],\n",
       "  2861: ['datasets'],\n",
       "  2862: ['methodologies'],\n",
       "  2864: ['methodologies'],\n",
       "  2873: ['methodologies'],\n",
       "  2890: ['methodologies'],\n",
       "  2892: ['methodologies'],\n",
       "  2908: ['methodologies'],\n",
       "  2911: ['methodologies'],\n",
       "  2917: ['methodologies'],\n",
       "  2921: ['datasets'],\n",
       "  2926: ['datasets'],\n",
       "  2930: ['datasets', 'methodologies', 'evaluation_methods'],\n",
       "  2931: ['datasets', 'methodologies'],\n",
       "  2934: ['methodologies'],\n",
       "  2937: ['datasets'],\n",
       "  2943: ['methodologies'],\n",
       "  2949: ['methodologies'],\n",
       "  2951: ['methodologies'],\n",
       "  2952: ['methodologies'],\n",
       "  2954: ['datasets'],\n",
       "  2976: ['datasets'],\n",
       "  3006: ['datasets'],\n",
       "  3011: ['datasets', 'methodologies'],\n",
       "  3012: ['methodologies'],\n",
       "  3029: ['methodologies'],\n",
       "  3047: ['methodologies'],\n",
       "  3074: ['datasets'],\n",
       "  3078: ['methodologies'],\n",
       "  3079: ['methodologies'],\n",
       "  3082: ['datasets'],\n",
       "  3083: ['methodologies'],\n",
       "  3108: ['methodologies'],\n",
       "  3146: ['methodologies'],\n",
       "  3191: ['methodologies'],\n",
       "  3195: ['methodologies'],\n",
       "  3226: ['methodologies'],\n",
       "  3227: ['methodologies'],\n",
       "  3234: ['methodologies'],\n",
       "  3246: ['methodologies'],\n",
       "  3256: ['methodologies'],\n",
       "  3259: ['datasets'],\n",
       "  3266: ['methodologies'],\n",
       "  3319: ['methodologies'],\n",
       "  3326: ['methodologies'],\n",
       "  3338: ['methodologies'],\n",
       "  3339: ['methodologies'],\n",
       "  3343: ['evaluation_methods'],\n",
       "  3344: ['methodologies'],\n",
       "  3352: ['methodologies'],\n",
       "  3361: ['methodologies'],\n",
       "  3372: ['datasets'],\n",
       "  3409: ['methodologies'],\n",
       "  3432: ['methodologies'],\n",
       "  3454: ['methodologies'],\n",
       "  3484: ['methodologies'],\n",
       "  3608: ['methodologies'],\n",
       "  3611: ['methodologies'],\n",
       "  3614: ['methodologies'],\n",
       "  3631: ['methodologies'],\n",
       "  3710: ['methodologies'],\n",
       "  3723: ['methodologies'],\n",
       "  3776: ['methodologies'],\n",
       "  3779: ['methodologies'],\n",
       "  3801: ['methodologies'],\n",
       "  3814: ['methodologies'],\n",
       "  3826: ['methodologies'],\n",
       "  3830: ['methodologies'],\n",
       "  3831: ['methodologies'],\n",
       "  3833: ['methodologies'],\n",
       "  3834: ['methodologies'],\n",
       "  3837: ['methodologies'],\n",
       "  3839: ['methodologies'],\n",
       "  3859: ['methodologies'],\n",
       "  3884: ['methodologies'],\n",
       "  3932: ['methodologies'],\n",
       "  3938: ['methodologies'],\n",
       "  3943: ['methodologies'],\n",
       "  3957: ['methodologies'],\n",
       "  3960: ['methodologies', 'evaluation_methods'],\n",
       "  3961: ['methodologies'],\n",
       "  3967: ['methodologies'],\n",
       "  3974: ['methodologies'],\n",
       "  3976: ['methodologies'],\n",
       "  3977: ['methodologies'],\n",
       "  3982: ['methodologies'],\n",
       "  3991: ['methodologies'],\n",
       "  3992: ['methodologies'],\n",
       "  4003: ['methodologies'],\n",
       "  4006: ['methodologies'],\n",
       "  4007: ['methodologies'],\n",
       "  4009: ['methodologies'],\n",
       "  4016: ['methodologies'],\n",
       "  4017: ['methodologies'],\n",
       "  4026: ['methodologies'],\n",
       "  4034: ['methodologies'],\n",
       "  4036: ['methodologies'],\n",
       "  4037: ['methodologies'],\n",
       "  4044: ['methodologies'],\n",
       "  4048: ['datasets'],\n",
       "  4049: ['datasets'],\n",
       "  4051: ['datasets'],\n",
       "  4053: ['datasets'],\n",
       "  4056: ['datasets'],\n",
       "  4057: ['datasets'],\n",
       "  4059: ['datasets'],\n",
       "  4062: ['datasets'],\n",
       "  4063: ['datasets', 'methodologies'],\n",
       "  4065: ['datasets'],\n",
       "  4075: ['methodologies'],\n",
       "  4077: ['methodologies'],\n",
       "  4086: ['methodologies'],\n",
       "  4101: ['methodologies'],\n",
       "  4105: ['methodologies'],\n",
       "  4112: ['methodologies'],\n",
       "  4114: ['methodologies'],\n",
       "  4120: ['methodologies'],\n",
       "  4123: ['methodologies'],\n",
       "  4137: ['methodologies'],\n",
       "  4141: ['methodologies'],\n",
       "  4146: ['methodologies'],\n",
       "  4155: ['methodologies'],\n",
       "  4165: ['datasets'],\n",
       "  4176: ['methodologies'],\n",
       "  4178: ['methodologies'],\n",
       "  4180: ['datasets'],\n",
       "  4194: ['methodologies'],\n",
       "  4200: ['methodologies'],\n",
       "  4213: ['methodologies'],\n",
       "  4233: ['methodologies'],\n",
       "  4236: ['methodologies'],\n",
       "  4251: ['methodologies'],\n",
       "  4274: ['methodologies'],\n",
       "  4275: ['methodologies'],\n",
       "  4276: ['methodologies'],\n",
       "  4277: ['datasets'],\n",
       "  4287: ['datasets', 'evaluation_methods'],\n",
       "  4290: ['methodologies'],\n",
       "  4293: ['methodologies'],\n",
       "  4302: ['methodologies'],\n",
       "  4303: ['methodologies'],\n",
       "  4305: ['methodologies'],\n",
       "  4313: ['methodologies'],\n",
       "  4317: ['methodologies'],\n",
       "  4323: ['methodologies'],\n",
       "  4324: ['methodologies'],\n",
       "  4327: ['datasets', 'methodologies'],\n",
       "  4337: ['methodologies'],\n",
       "  4354: ['methodologies'],\n",
       "  4358: ['methodologies'],\n",
       "  4360: ['methodologies'],\n",
       "  4368: ['methodologies'],\n",
       "  4373: ['methodologies'],\n",
       "  4376: ['methodologies'],\n",
       "  4391: ['methodologies'],\n",
       "  4395: ['methodologies'],\n",
       "  4405: ['methodologies'],\n",
       "  4413: ['methodologies'],\n",
       "  4414: ['methodologies'],\n",
       "  4417: ['datasets', 'methodologies'],\n",
       "  4418: ['methodologies', 'evaluation_methods'],\n",
       "  4428: ['methodologies'],\n",
       "  4431: ['datasets'],\n",
       "  4435: ['methodologies'],\n",
       "  4442: ['datasets'],\n",
       "  4448: ['methodologies'],\n",
       "  4449: ['methodologies'],\n",
       "  4464: ['methodologies'],\n",
       "  4465: ['datasets'],\n",
       "  4466: ['datasets'],\n",
       "  4468: ['methodologies'],\n",
       "  4470: ['methodologies'],\n",
       "  4474: ['methodologies'],\n",
       "  4481: ['methodologies'],\n",
       "  4482: ['methodologies'],\n",
       "  4497: ['datasets', 'methodologies'],\n",
       "  4500: ['datasets'],\n",
       "  4501: ['methodologies'],\n",
       "  4508: ['methodologies'],\n",
       "  4517: ['datasets'],\n",
       "  4538: ['methodologies'],\n",
       "  4551: ['methodologies'],\n",
       "  4554: ['datasets'],\n",
       "  4569: ['methodologies'],\n",
       "  4578: ['methodologies'],\n",
       "  4588: ['methodologies'],\n",
       "  4591: ['datasets'],\n",
       "  4597: ['methodologies'],\n",
       "  4611: ['methodologies'],\n",
       "  4613: ['datasets'],\n",
       "  4616: ['datasets'],\n",
       "  4631: ['methodologies'],\n",
       "  4633: ['datasets', 'methodologies'],\n",
       "  4656: ['methodologies'],\n",
       "  4657: ['methodologies'],\n",
       "  4660: ['methodologies'],\n",
       "  4666: ['methodologies'],\n",
       "  4667: ['methodologies'],\n",
       "  4669: ['datasets'],\n",
       "  4692: ['methodologies'],\n",
       "  4704: ['methodologies'],\n",
       "  4722: ['datasets'],\n",
       "  4738: ['methodologies'],\n",
       "  4746: ['methodologies'],\n",
       "  4752: ['methodologies'],\n",
       "  4753: ['methodologies'],\n",
       "  4754: ['datasets'],\n",
       "  4773: ['datasets'],\n",
       "  4780: ['datasets'],\n",
       "  4785: ['datasets'],\n",
       "  4820: ['methodologies'],\n",
       "  4832: ['methodologies'],\n",
       "  4833: ['methodologies'],\n",
       "  4835: ['methodologies'],\n",
       "  4837: ['methodologies'],\n",
       "  4842: ['methodologies'],\n",
       "  4855: ['methodologies'],\n",
       "  4858: ['methodologies'],\n",
       "  4864: ['methodologies'],\n",
       "  4875: ['datasets'],\n",
       "  4877: ['methodologies'],\n",
       "  4885: ['methodologies'],\n",
       "  4888: ['methodologies'],\n",
       "  4893: ['methodologies'],\n",
       "  4898: ['methodologies'],\n",
       "  4899: ['methodologies'],\n",
       "  4900: ['methodologies'],\n",
       "  4901: ['methodologies'],\n",
       "  4902: ['methodologies'],\n",
       "  4904: ['methodologies'],\n",
       "  4905: ['methodologies'],\n",
       "  4911: ['methodologies'],\n",
       "  4919: ['methodologies'],\n",
       "  4922: ['methodologies'],\n",
       "  4923: ['methodologies'],\n",
       "  4932: ['datasets'],\n",
       "  4942: ['methodologies'],\n",
       "  4954: ['datasets'],\n",
       "  4957: ['datasets'],\n",
       "  4958: ['datasets'],\n",
       "  5009: ['methodologies'],\n",
       "  5011: ['methodologies'],\n",
       "  5013: ['methodologies'],\n",
       "  5017: ['methodologies'],\n",
       "  5019: ['methodologies'],\n",
       "  5031: ['methodologies'],\n",
       "  5046: ['methodologies'],\n",
       "  5052: ['methodologies'],\n",
       "  5058: ['datasets'],\n",
       "  5069: ['methodologies'],\n",
       "  5070: ['datasets'],\n",
       "  5077: ['methodologies'],\n",
       "  5081: ['methodologies'],\n",
       "  5084: ['methodologies'],\n",
       "  5089: ['methodologies'],\n",
       "  5090: ['methodologies'],\n",
       "  5095: ['datasets'],\n",
       "  5103: ['datasets'],\n",
       "  5105: ['datasets'],\n",
       "  5106: ['datasets'],\n",
       "  5108: ['datasets'],\n",
       "  5110: ['datasets'],\n",
       "  5111: ['datasets'],\n",
       "  5113: ['datasets'],\n",
       "  5125: ['methodologies'],\n",
       "  5134: ['methodologies'],\n",
       "  5137: ['methodologies'],\n",
       "  5143: ['methodologies'],\n",
       "  5145: ['methodologies'],\n",
       "  5147: ['methodologies'],\n",
       "  5155: ['methodologies'],\n",
       "  5168: ['methodologies'],\n",
       "  5170: ['methodologies'],\n",
       "  5172: ['methodologies'],\n",
       "  5175: ['methodologies'],\n",
       "  5179: ['datasets'],\n",
       "  5187: ['methodologies'],\n",
       "  5200: ['methodologies'],\n",
       "  5214: ['methodologies'],\n",
       "  5230: ['methodologies'],\n",
       "  5238: ['methodologies'],\n",
       "  5243: ['methodologies'],\n",
       "  5246: ['methodologies'],\n",
       "  5247: ['methodologies'],\n",
       "  5249: ['methodologies'],\n",
       "  5250: ['methodologies'],\n",
       "  5262: ['methodologies'],\n",
       "  5265: ['methodologies'],\n",
       "  5274: ['methodologies'],\n",
       "  5275: ['methodologies'],\n",
       "  5283: ['methodologies'],\n",
       "  5336: ['methodologies'],\n",
       "  5337: ['methodologies'],\n",
       "  5353: ['datasets'],\n",
       "  5356: ['methodologies'],\n",
       "  5358: ['methodologies'],\n",
       "  5369: ['datasets'],\n",
       "  5380: ['methodologies'],\n",
       "  5381: ['methodologies'],\n",
       "  5382: ['methodologies'],\n",
       "  5394: ['methodologies'],\n",
       "  5403: ['methodologies'],\n",
       "  5424: ['methodologies'],\n",
       "  5430: ['methodologies'],\n",
       "  5435: ['methodologies'],\n",
       "  5439: ['methodologies'],\n",
       "  5452: ['methodologies'],\n",
       "  5469: ['datasets'],\n",
       "  5471: ['methodologies'],\n",
       "  5478: ['methodologies'],\n",
       "  5479: ['methodologies'],\n",
       "  5498: ['methodologies'],\n",
       "  5499: ['methodologies'],\n",
       "  5504: ['datasets', 'methodologies'],\n",
       "  5539: ['methodologies'],\n",
       "  5552: ['methodologies'],\n",
       "  5555: ['methodologies'],\n",
       "  5563: ['methodologies'],\n",
       "  5564: ['methodologies'],\n",
       "  5565: ['methodologies'],\n",
       "  5566: ['methodologies'],\n",
       "  5567: ['methodologies'],\n",
       "  5568: ['methodologies'],\n",
       "  5572: ['methodologies'],\n",
       "  5574: ['methodologies'],\n",
       "  5575: ['methodologies'],\n",
       "  5577: ['methodologies'],\n",
       "  5578: ['methodologies'],\n",
       "  ...})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_dims), paper_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large Language Models (LLMs) struggle with providing current information due to the outdated pre-training data. Existing methods for updating LLMs, such as knowledge editing and continual fine-tuning, have significant drawbacks in generalizability of new information and the requirements on structured updating corpus. We identify the core challenge behind these drawbacks: the LM-logical discrepancy featuring the difference between language modeling probabilities and logical probabilities. To evaluate and address the core challenge, we propose a new task formulation of the information updating task that only requires the provision of an unstructured updating corpus and evaluates the performance of information updating on the generalizability to question-answer pairs pertaining to the updating information.We further propose a novel and effective pipeline approach for the task, highlighting a self-prompting-based question-answer generation process and a associative distillation methods to bridge the LM-logical discrepancy.We develop two datasets for evaluation, one sourced from news articles published in March and April 2023, and the other from the Natural Questions benchmark.Experimental results demonstrate the superiority of our approach, significantly increasing the factual consistency score (on a scale from 0 to 1) by up to 0.16. Furthermore, our method effectively mitigates forgetting utilizing a compact replay buffer with only 2.3{\\\\%} of the training tokens.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "internal_collection[2494].abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loose Classification of Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.llm = 'vllm'\n",
    "# initializeLLM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(internal_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  natural_language_processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 662/662 [02:50<00:00,  3.89it/s, est. speed input: 7856.39 toks/s, output: 163.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  text_generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 36/36 [00:08<00:00,  4.37it/s, est. speed input: 7234.70 toks/s, output: 169.95 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  style_transfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 8/8 [00:08<00:00,  1.07s/it, est. speed input: 416.70 toks/s, output: 145.10 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  conditional_text_generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 25/25 [00:10<00:00,  2.42it/s, est. speed input: 1047.19 toks/s, output: 272.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  language_modeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 8/8 [00:10<00:00,  1.31s/it, est. speed input: 310.58 toks/s, output: 138.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  machine_translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 142/142 [00:31<00:00,  4.47it/s, est. speed input: 7600.24 toks/s, output: 196.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  multimodal_machine_translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 3/3 [00:02<00:00,  1.39it/s, est. speed input: 574.18 toks/s, output: 144.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  unsupervised_machine_translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 42/42 [00:21<00:00,  1.92it/s, est. speed input: 864.95 toks/s, output: 469.65 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting:  neural_machine_translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 63/63 [00:29<00:00,  2.11it/s, est. speed input: 882.75 toks/s, output: 399.36 toks/s] \n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 1 column 2513 (char 2512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_dag\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minternal_collection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2node\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/taxonomy.py:275\u001b[0m, in \u001b[0;36mDAG.classify_dag\u001b[0;34m(self, args, collection, label2node)\u001b[0m\n\u001b[1;32m    272\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(classify_prompt(current_node, paper))\n\u001b[1;32m    274\u001b[0m output \u001b[38;5;241m=\u001b[39m promptLLM(args, prompts, schema\u001b[38;5;241m=\u001b[39mClassifySchema, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m)\n\u001b[0;32m--> 275\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(clean_json_string(c)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m c \u001b[38;5;28;01melse\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(c\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m output]\n\u001b[1;32m    276\u001b[0m class_options \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m current_node\u001b[38;5;241m.\u001b[39mget_children()]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (paper_id, paper), out_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(papers\u001b[38;5;241m.\u001b[39mitems(), output_dict):\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/taxonomy.py:275\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(classify_prompt(current_node, paper))\n\u001b[1;32m    274\u001b[0m output \u001b[38;5;241m=\u001b[39m promptLLM(args, prompts, schema\u001b[38;5;241m=\u001b[39mClassifySchema, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m)\n\u001b[0;32m--> 275\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(clean_json_string(c)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m c \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m output]\n\u001b[1;32m    276\u001b[0m class_options \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m current_node\u001b[38;5;241m.\u001b[39mget_children()]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (paper_id, paper), out_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(papers\u001b[38;5;241m.\u001b[39mitems(), output_dict):\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 2513 (char 2512)"
     ]
    }
   ],
   "source": [
    "dag.classify_dag(args, collection=internal_collection, label2node=label2node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_classification': Node(label=text_classification, description=Text classification involves categorizing text data into predefined classes or categories., level=1),\n",
       " 'named_entity_recognition': Node(label=named_entity_recognition, description=Named entity recognition is the task of identifying and classifying named entities in text., level=1),\n",
       " 'machine_translation': Node(label=machine_translation, description=Machine translation involves translating text from one language to another., level=1),\n",
       " 'text_generation': Node(label=text_generation, description=Text generation focuses on generating coherent and contextually relevant text., level=1)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662/662 [00:00<00:00, 426911.02it/s]\n"
     ]
    }
   ],
   "source": [
    "unlabeled = []\n",
    "\n",
    "for paper_id, paper in tqdm(root.papers.items()):\n",
    "    add = True\n",
    "    for c in root.children.values():\n",
    "        if paper_id in c.papers:\n",
    "            add = False\n",
    "    if add:\n",
    "        unlabeled.append(paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
