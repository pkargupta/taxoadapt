{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import deque\n",
    "\n",
    "def bfs_json(data):\n",
    "    out_json = {}\n",
    "    queue = deque([(data, out_json)])\n",
    "\n",
    "    while queue:\n",
    "        current, json_pointer = queue.popleft()\n",
    "        json_pointer[\"label\"] = current[\"label\"]\n",
    "        json_pointer[\"description\"] = current[\"description\"]\n",
    "        if \"paper_ids\" not in current:\n",
    "            print(current[\"label\"], \": NO PAPERS\")\n",
    "        else:\n",
    "            json_pointer[\"papers\"] = len(current[\"paper_ids\"])\n",
    "\n",
    "        if (\"children\" in current) and (len(current[\"children\"]) > 0):\n",
    "            json_pointer[\"children\"] = []\n",
    "            for c in range(len(current[\"children\"])):\n",
    "                json_pointer[\"children\"].append({})\n",
    "                queue.append((current[\"children\"][c], json_pointer[\"children\"][c]))\n",
    "\n",
    "    return out_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hate_speech_detection : NO PAPERS\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/emnlp_2024/final_taxo_tasks.json', 'r') as f:\n",
    "    output = json.load(f)\n",
    "\n",
    "filtered_output = bfs_json(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'natural_language_processing',\n",
       " 'description': None,\n",
       " 'papers': 2954,\n",
       " 'children': [{'label': 'text_classification',\n",
       "   'description': 'The task of assigning predefined categories to text documents based on their content, often used in spam detection, sentiment analysis, and topic labeling.',\n",
       "   'papers': 161,\n",
       "   'children': [{'label': 'sentiment_analysis',\n",
       "     'description': 'The task of determining the emotional tone behind a series of words, used to understand the attitudes, opinions, and emotions expressed in text.',\n",
       "     'papers': 18,\n",
       "     'children': [{'label': 'aspect_based_sentiment_analysis',\n",
       "       'description': 'Aspect-based sentiment analysis focuses on determining sentiment towards specific aspects or features of a product or service.',\n",
       "       'papers': 2},\n",
       "      {'label': 'emotion_classification',\n",
       "       'description': 'Emotion classification involves categorizing text based on the emotional states expressed within it.',\n",
       "       'papers': 5},\n",
       "      {'label': 'hate_speech_detection',\n",
       "       'description': 'Hate speech detection aims to identify and classify text that expresses hate or discrimination against individuals or groups.'},\n",
       "      {'label': 'multimodal_sentiment_analysis',\n",
       "       'description': 'Multimodal sentiment analysis integrates information from multiple modalities, such as text, audio, and visual data, to assess sentiment.',\n",
       "       'papers': 1},\n",
       "      {'label': 'binary_sentiment_classification',\n",
       "       'description': 'Binary sentiment classification involves categorizing text into two sentiment categories, typically positive or negative.',\n",
       "       'papers': 4}]},\n",
       "    {'label': 'document_classification',\n",
       "     'description': 'Document classification involves categorizing entire documents into predefined classes based on their content, useful for organizing large datasets.',\n",
       "     'papers': 70},\n",
       "    {'label': 'multi_label_classification',\n",
       "     'description': 'Multi-label classification is the task of assigning multiple labels to a single instance, allowing for more complex categorization.',\n",
       "     'papers': 69},\n",
       "    {'label': 'abuse_detection',\n",
       "     'description': 'Abuse detection focuses on identifying harmful or abusive content in text, crucial for maintaining safe online environments.',\n",
       "     'papers': 38},\n",
       "    {'label': 'topic_classification',\n",
       "     'description': 'Topic classification involves categorizing text into specific topics, enabling better organization and retrieval of information.',\n",
       "     'papers': 40}]},\n",
       "  {'label': 'named_entity_recognition',\n",
       "   'description': 'The process of identifying and classifying key entities in text, such as names of people, organizations, locations, and other specific terms.',\n",
       "   'papers': 50,\n",
       "   'children': [{'label': 'cross_domain_named_entity_recognition',\n",
       "     'description': 'This cluster focuses on recognizing named entities across different domains and contexts.',\n",
       "     'papers': 20},\n",
       "    {'label': 'entity_disambiguation',\n",
       "     'description': 'This cluster encompasses tasks related to resolving ambiguities in entity identification and classification.',\n",
       "     'papers': 4},\n",
       "    {'label': 'medical_entity_recognition',\n",
       "     'description': 'This cluster is dedicated to the identification and classification of medical-related entities in text.',\n",
       "     'papers': 5},\n",
       "    {'label': 'legal_named_entity_recognition',\n",
       "     'description': 'This cluster focuses on the identification of named entities specifically within legal texts and contexts.',\n",
       "     'papers': 7},\n",
       "    {'label': 'nested_named_entity_recognition',\n",
       "     'description': 'This cluster deals with the recognition of entities that are nested within other entities in text.',\n",
       "     'papers': 7}]},\n",
       "  {'label': 'machine_translation',\n",
       "   'description': 'The task of automatically translating text from one language to another while preserving its meaning and context.',\n",
       "   'papers': 242,\n",
       "   'children': [{'label': 'neural_machine_translation',\n",
       "     'description': 'This cluster focuses on the use of neural network architectures to perform machine translation tasks.',\n",
       "     'papers': 193},\n",
       "    {'label': 'evaluation',\n",
       "     'description': 'This cluster encompasses various methods and metrics for evaluating the quality and effectiveness of machine translation systems.',\n",
       "     'papers': 79},\n",
       "    {'label': 'domain_adaptation',\n",
       "     'description': 'This cluster includes techniques aimed at adapting machine translation systems to specific domains or contexts.',\n",
       "     'papers': 44},\n",
       "    {'label': 'multilingual_translation',\n",
       "     'description': 'This cluster focuses on translating text across multiple languages, often leveraging shared resources and techniques.',\n",
       "     'papers': 87},\n",
       "    {'label': 'creative_translation',\n",
       "     'description': 'This cluster explores innovative approaches to translation that emphasize creativity and cultural nuances.',\n",
       "     'papers': 7}]},\n",
       "  {'label': 'text_summarization',\n",
       "   'description': 'The process of creating a concise and coherent summary of a longer text document, capturing the main ideas and essential information.',\n",
       "   'papers': 79,\n",
       "   'children': [{'label': 'abstractive_summarization',\n",
       "     'description': 'Abstractive summarization involves generating new sentences that capture the essence of the original text, rather than merely extracting portions of it.',\n",
       "     'papers': 34},\n",
       "    {'label': 'extractive_summarization',\n",
       "     'description': 'Extractive summarization focuses on identifying and extracting key sentences or phrases from the original text to create a summary.',\n",
       "     'papers': 7},\n",
       "    {'label': 'multi_document_summarization',\n",
       "     'description': 'Multi-document summarization aims to create a coherent summary from multiple documents on the same topic.',\n",
       "     'papers': 13},\n",
       "    {'label': 'text_simplification',\n",
       "     'description': 'Text simplification involves modifying the original text to make it easier to read and understand while retaining the main ideas.',\n",
       "     'papers': 22},\n",
       "    {'label': 'query-focused_summarization',\n",
       "     'description': 'Query-focused summarization generates summaries that are tailored to specific queries or information needs.',\n",
       "     'papers': 13}]},\n",
       "  {'label': 'question_answering',\n",
       "   'description': 'The task of automatically answering questions posed by humans in natural language, often leveraging large datasets and advanced algorithms.',\n",
       "   'papers': 285,\n",
       "   'children': [{'label': 'open_domain_question_answering',\n",
       "     'description': 'This cluster focuses on question answering systems that can handle a wide range of topics and domains without being restricted to a specific context.',\n",
       "     'papers': 124},\n",
       "    {'label': 'visual_question_answering',\n",
       "     'description': 'This cluster encompasses question answering tasks that involve visual inputs, requiring the model to interpret and reason about images or videos.',\n",
       "     'papers': 51},\n",
       "    {'label': 'conversational_question_answering',\n",
       "     'description': 'This cluster includes question answering systems designed for interactive dialogue, focusing on maintaining context and coherence in conversations.',\n",
       "     'papers': 19},\n",
       "    {'label': 'reasoning_in_question_answering',\n",
       "     'description': 'This cluster focuses on question answering tasks that require complex reasoning, including logical and causal reasoning.',\n",
       "     'papers': 90},\n",
       "    {'label': 'extractive_question_answering',\n",
       "     'description': 'This cluster pertains to question answering tasks that involve extracting answers directly from a given text or document.',\n",
       "     'papers': 66}]},\n",
       "  {'label': 'text_generation',\n",
       "   'description': 'The process of generating coherent and contextually relevant text based on a given input or prompt.',\n",
       "   'papers': 504,\n",
       "   'children': [{'label': 'code_generation',\n",
       "     'description': 'The process of generating code snippets or entire programs based on given specifications or prompts.',\n",
       "     'papers': 29},\n",
       "    {'label': 'dialogue_generation',\n",
       "     'description': 'The creation of conversational responses or dialogues based on user inputs or prompts.',\n",
       "     'papers': 58},\n",
       "    {'label': 'content_generation',\n",
       "     'description': 'The generation of various types of content, including articles, stories, and other written materials.',\n",
       "     'papers': 339},\n",
       "    {'label': 'text_synthesis',\n",
       "     'description': 'The process of combining information from various sources to create coherent text.',\n",
       "     'papers': 86},\n",
       "    {'label': 'text_style_transfer',\n",
       "     'description': 'The transformation of text from one style to another while preserving its content.',\n",
       "     'papers': 13}]},\n",
       "  {'label': 'information_extraction',\n",
       "   'description': 'The task of automatically extracting structured information from unstructured text, such as entities, relationships, and events.',\n",
       "   'papers': 558,\n",
       "   'children': [{'label': 'entity_extraction',\n",
       "     'description': 'The task of identifying and classifying key entities from unstructured text into predefined categories.',\n",
       "     'papers': 35},\n",
       "    {'label': 'event_extraction',\n",
       "     'description': 'The process of identifying and extracting events and their attributes from text data.',\n",
       "     'papers': 32},\n",
       "    {'label': 'relation_extraction',\n",
       "     'description': 'The task of identifying and extracting relationships between entities in text.',\n",
       "     'papers': 34},\n",
       "    {'label': 'fact_extraction',\n",
       "     'description': 'The process of extracting factual information from text, often for verification or knowledge representation.',\n",
       "     'papers': 75},\n",
       "    {'label': 'document_information_extraction',\n",
       "     'description': 'The extraction of structured information from entire documents rather than isolated text segments.',\n",
       "     'papers': 420},\n",
       "    {'label': 'memory_extraction',\n",
       "     'description': 'The task of retrieving and structuring memory-related information from unstructured text sources.',\n",
       "     'papers': 2},\n",
       "    {'label': 'impact_analysis',\n",
       "     'description': 'The process of assessing the effects or implications of certain information extracted from text.',\n",
       "     'papers': 21},\n",
       "    {'label': 'bias_detection',\n",
       "     'description': 'The task of identifying and analyzing biases present in textual data.',\n",
       "     'papers': 16},\n",
       "    {'label': 'data_narration',\n",
       "     'description': 'The process of generating narratives or summaries from extracted data points.',\n",
       "     'papers': 1},\n",
       "    {'label': 'clinical_decision_making',\n",
       "     'description': 'The extraction of relevant information to support decision-making in clinical settings.',\n",
       "     'papers': 3}]},\n",
       "  {'label': 'language_modeling',\n",
       "   'description': 'The task of developing models that can understand and generate human language, often used for various NLP applications.',\n",
       "   'papers': 2148,\n",
       "   'children': [{'label': 'multimodal_language_modeling',\n",
       "     'description': 'This cluster focuses on the development and application of language models that integrate multiple modalities, such as text, audio, and visual data.',\n",
       "     'papers': 143},\n",
       "    {'label': 'knowledge_injection',\n",
       "     'description': 'This cluster encompasses techniques and methods for enhancing language models with external knowledge sources to improve their performance and accuracy.',\n",
       "     'papers': 66},\n",
       "    {'label': 'instruction_tuning',\n",
       "     'description': 'This cluster focuses on the methods and strategies for fine-tuning language models to follow specific instructions and improve their task performance.',\n",
       "     'papers': 91},\n",
       "    {'label': 'language_model_evaluation',\n",
       "     'description': 'This cluster includes various methodologies and metrics for assessing the performance and effectiveness of language models.',\n",
       "     'papers': 822},\n",
       "    {'label': 'language_modeling_for_nlp_applications',\n",
       "     'description': 'This cluster encompasses the application of language modeling techniques specifically tailored for various natural language processing tasks.',\n",
       "     'papers': 1668},\n",
       "    {'label': 'language_model_interpretability',\n",
       "     'description': 'This cluster focuses on understanding and explaining the decision-making processes of language models to ensure transparency and trustworthiness.',\n",
       "     'papers': 276},\n",
       "    {'label': 'language_model_safety',\n",
       "     'description': 'This cluster encompasses techniques and strategies aimed at ensuring the safety and robustness of language models against various attacks and biases.',\n",
       "     'papers': 348},\n",
       "    {'label': 'language_model_evaluation_and_benchmarking',\n",
       "     'description': 'This cluster includes methodologies and practices for evaluating and benchmarking the performance of language models across different tasks.',\n",
       "     'papers': 224},\n",
       "    {'label': 'language_model_application_in_social_sciences',\n",
       "     'description': 'This cluster focuses on the application of language modeling techniques in the field of social sciences, including analysis of social media and computational studies.',\n",
       "     'papers': 56},\n",
       "    {'label': 'language_modeling_for_reasoning_and_argumentation',\n",
       "     'description': 'This cluster encompasses tasks related to reasoning, argumentation, and commonsense understanding within language models.',\n",
       "     'papers': 369}]}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/multi_dim/emnlp_2024/final_taxo_tasks.json', 'r') as f:\n",
    "    output = json.load(f)\n",
    "\n",
    "filtered_output = bfs_json(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/multi_dim/emnlp_2024/final_taxo_tasks_filtered.json', 'w') as f:\n",
    "    json.dump(filtered_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_definitions import initializeLLM, promptLLM, constructPrompt\n",
    "import json\n",
    "from utils import clean_json_string\n",
    "from collections import deque\n",
    "from taxonomy import Node\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.topic = \"natural language processing\"\n",
    "        self.dimensions = [\"tasks\", \"datasets\", \"methodologies\", \"evaluation_methods\", \"real_world_domains\"]\n",
    "        # self.dimensions = [\"evaluation_methods\"]\n",
    "        self.llm = 'gpt'\n",
    "        self.init_levels = 2\n",
    "\n",
    "        self.dataset = \"Reasoning\"\n",
    "        self.data_dir = f\"datasets/multi_dim/{self.dataset.lower().replace(' ', '_')}/\"\n",
    "        self.internal = f\"{self.dataset}.txt\"\n",
    "        self.external = f\"{self.dataset}_external.txt\"\n",
    "        self.groundtruth = \"groundtruth.txt\"\n",
    "        \n",
    "        self.length = 512\n",
    "        self.dim = 768\n",
    "\n",
    "        self.iters = 4\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-18 06:34:48 config.py:729] Defaulting to use mp for distributed inference\n",
      "WARNING 12-18 06:34:48 arg_utils.py:776] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 12-18 06:34:48 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=True)\n",
      "WARNING 12-18 06:34:49 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-18 06:34:49 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m INFO 12-18 06:34:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-18 06:34:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-18 06:34:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-18 06:34:49 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 12-18 06:34:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m INFO 12-18 06:34:49 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 12-18 06:34:49 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 12-18 06:34:49 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m INFO 12-18 06:34:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 12-18 06:34:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 12-18 06:34:49 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "WARNING 12-18 06:34:50 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m WARNING 12-18 06:34:50 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-18 06:34:50 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-18 06:34:50 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-18 06:34:50 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f4647fa2be0>, local_subscribe_port=52245, remote_subscribe_port=None)\n",
      "INFO 12-18 06:34:50 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m INFO 12-18 06:34:50 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m INFO 12-18 06:34:50 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 12-18 06:34:50 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m INFO 12-18 06:34:50 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m INFO 12-18 06:34:50 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m INFO 12-18 06:34:50 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "INFO 12-18 06:34:50 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72d2051021d419e980f47ce42ecfa34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m INFO 12-18 06:34:52 model_runner.py:732] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m INFO 12-18 06:34:52 model_runner.py:732] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m INFO 12-18 06:34:52 model_runner.py:732] Loading model weights took 3.7710 GB\n",
      "INFO 12-18 06:34:53 model_runner.py:732] Loading model weights took 3.7710 GB\n",
      "INFO 12-18 06:35:06 distributed_gpu_executor.py:56] # GPU blocks: 48911, # CPU blocks: 8192\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m INFO 12-18 06:35:08 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m INFO 12-18 06:35:08 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-18 06:35:08 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-18 06:35:08 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m INFO 12-18 06:35:08 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-18 06:35:08 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m INFO 12-18 06:35:08 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-18 06:35:08 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098262)\u001b[0;0m INFO 12-18 06:35:25 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2098263)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2098261)\u001b[0;0m INFO 12-18 06:35:25 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "INFO 12-18 06:35:25 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "INFO 12-18 06:35:25 model_runner.py:1225] Graph capturing finished in 17 secs.\n",
      "INFO 12-18 06:35:25 block_manager_v1.py:247] Automatic prefix caching is enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = initializeLLM(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct a 2-Level Multi-Dimensional Taxonomy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import multi_dim_prompt, NodeListSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to make this a directed acyclic graph (DAG) so maintain a list of the nodes\n",
    "roots = {}\n",
    "id2node = {}\n",
    "label2node = {}\n",
    "idx = 0\n",
    "\n",
    "for dim in args.dimensions:\n",
    "    mod_topic = args.topic.replace(' ', '_').lower() + f\"_{dim}\"\n",
    "    root = Node(\n",
    "            id=idx,\n",
    "            label=mod_topic,\n",
    "            dimension=dim\n",
    "        )\n",
    "    roots[dim] = root\n",
    "    id2node[idx] = root\n",
    "    label2node[mod_topic] = root\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.55s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.28s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.29s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "queue = deque([node for id, node in id2node.items()])\n",
    "\n",
    "# if taking long, you can probably parallelize this between the different taxonomies (expand by level)\n",
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    label = curr_node.label\n",
    "    dim = curr_node.dimension\n",
    "    # expand\n",
    "    system_instruction, main_prompt, json_output_format = multi_dim_prompt(curr_node)\n",
    "    prompts = [constructPrompt(args, system_instruction, main_prompt + \"\\n\\n\" + json_output_format)]\n",
    "    outputs = promptLLM(args=args, prompts=prompts, schema=NodeListSchema, max_new_tokens=3000, json_mode=True, temperature=0.1, top_p=0.99)[0]\n",
    "    outputs = json.loads(clean_json_string(outputs)) if \"```\" in outputs else json.loads(outputs.strip())\n",
    "    outputs = outputs['root_topic'] if 'root_topic' in outputs else outputs[label]\n",
    "\n",
    "    # add all children\n",
    "    for key, value in outputs.items():\n",
    "        mod_key = key.replace(' ', '_').lower() + f\"_{dim}\"\n",
    "        if mod_key not in label2node:\n",
    "            child_node = Node(\n",
    "                    id=len(id2node),\n",
    "                    label=mod_key,\n",
    "                    dimension=dim,\n",
    "                    description=value['description'],\n",
    "                    parents=[curr_node]\n",
    "                )\n",
    "            curr_node.add_child(mod_key, child_node)\n",
    "            id2node[child_node.id] = child_node\n",
    "            label2node[mod_key] = child_node\n",
    "            if child_node.level < args.init_levels:\n",
    "                queue.append(child_node)\n",
    "        elif label2node[mod_key] in label2node[label].get_ancestors():\n",
    "            continue\n",
    "        else:\n",
    "            child_node = label2node[mod_key]\n",
    "            curr_node.add_child(mod_key, child_node)\n",
    "            child_node.add_parent(curr_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': Node(label=natural_language_processing_tasks, dim=tasks, description=None, level=0),\n",
       " 'datasets': Node(label=natural_language_processing_datasets, dim=datasets, description=None, level=0),\n",
       " 'methodologies': Node(label=natural_language_processing_methodologies, dim=methodologies, description=None, level=0),\n",
       " 'evaluation_methods': Node(label=natural_language_processing_evaluation_methods, dim=evaluation_methods, description=None, level=0),\n",
       " 'real_world_domains': Node(label=natural_language_processing_real_world_domains, dim=real_world_domains, description=None, level=0)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots['tasks'].children['text_classification_tasks'].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots['tasks'].display(0, indent_multiplier=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from paper import Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.data_dir):\n",
    "    os.makedirs(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"EMNLP/EMNLP2024-papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2954/2954 [00:00<00:00, 5226.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal: 2954\n"
     ]
    }
   ],
   "source": [
    "internal_collection = {}\n",
    "\n",
    "with open(os.path.join(args.data_dir, 'internal.txt'), 'w') as i:\n",
    "    internal_count = 0\n",
    "    id = 0\n",
    "    for p in tqdm(ds['train']):\n",
    "        temp_dict = {\"Title\": p['title'], \"Abstract\": p['abstract']}\n",
    "        formatted_dict = json.dumps(temp_dict)\n",
    "        i.write(f'{formatted_dict}\\n')\n",
    "        internal_collection[id] = Paper(id, p['title'], p['abstract'], label_opts=args.dimensions, internal=True)\n",
    "        internal_count += 1\n",
    "        id += 1\n",
    "print(f'Internal: {internal_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_ds = load_dataset(\"TimSchopf/nlp_taxonomy_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178521/178521 [00:14<00:00, 12126.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External Count: 178521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "external_collection = {}\n",
    "\n",
    "with open(os.path.join(args.data_dir, 'external.txt'), 'w') as e:\n",
    "    external_count = 0\n",
    "    id = len(internal_collection)\n",
    "    for p in tqdm(external_ds['train']):\n",
    "        temp_dict = {\"Title\": p['title'], \"Abstract\": p['abstract']}\n",
    "        formatted_dict = json.dumps(temp_dict)\n",
    "        e.write(f'{formatted_dict}\\n')\n",
    "        external_collection[id] = Paper(id, p['title'], p['abstract'], label_opts=args.dimensions, internal=False)\n",
    "        external_count += 1\n",
    "        id += 1\n",
    "print(f'External Count: {external_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enrich each node with a set of terms and sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxonomy import DAG\n",
    "args.llm = 'vllm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dags = {dim:DAG(root=root, dim=dim) for dim, root in roots.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 1012/1012 [01:02<00:00, 16.17it/s]\n",
      "Processed prompts: 100%|██████████| 29/29 [00:10<00:00,  2.65it/s, est. speed input: 1380.46 toks/s, output: 971.57 toks/s]\n",
      "Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.13it/s, est. speed input: 1575.81 toks/s, output: 1026.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 30/30 [00:10<00:00,  2.79it/s, est. speed input: 1522.69 toks/s, output: 1014.11 toks/s]\n",
      "Processed prompts: 100%|██████████| 29/29 [00:10<00:00,  2.82it/s, est. speed input: 1515.20 toks/s, output: 972.94 toks/s]\n",
      "Processed prompts: 100%|██████████| 30/30 [00:09<00:00,  3.02it/s, est. speed input: 1680.07 toks/s, output: 1001.69 toks/s]\n"
     ]
    }
   ],
   "source": [
    "enriched_phrases = {dim:[] for dim in args.dimensions}\n",
    "enriched_sentences = {dim:[] for dim in args.dimensions}\n",
    "\n",
    "for dim, dag in dags.items():\n",
    "    all_phrases, all_sentences = dag.enrich_dag(args, id2node)\n",
    "    enriched_phrases[dim].extend(all_phrases)\n",
    "    enriched_sentences[dim].extend(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots['tasks'].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots['tasks'].children['text_summarization_tasks'].get_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify Pseudo-labels for Dimension/Type Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import type_cls_system_instruction, type_cls_main_prompt, TypeClsSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 139/139 [00:04<00:00, 30.37it/s]\n",
      "Processed prompts: 100%|██████████| 2954/2954 [02:30<00:00, 19.67it/s, est. speed input: 12821.17 toks/s, output: 609.71 toks/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 1082/1082 [01:01<00:00, 18.26it/s]toks/s, output: 707.01 toks/s]"
     ]
    }
   ],
   "source": [
    "# do for internal collection\n",
    "\n",
    "prompts = [constructPrompt(args, type_cls_system_instruction, type_cls_main_prompt(paper)) for paper in internal_collection.values()]\n",
    "outputs = promptLLM(args=args, prompts=prompts, schema=TypeClsSchema, max_new_tokens=500, json_mode=True, temperature=0.1, top_p=0.99)\n",
    "outputs = [json.loads(clean_json_string(c)) if \"```\" in c else json.loads(c.strip()) for c in outputs]\n",
    "\n",
    "# do for external collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in roots:\n",
    "    roots[r].papers = {}\n",
    "type_dist = {dim:[] for dim in args.dimensions}\n",
    "for p_id, out in enumerate(outputs):\n",
    "    internal_collection[p_id].labels = {}\n",
    "    for key, val in out.items():\n",
    "        if val:\n",
    "            type_dist[key].append(internal_collection[p_id])\n",
    "            internal_collection[p_id].labels[key] = []\n",
    "            roots[key].papers[p_id] = internal_collection[p_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, p in type_dist.items():\n",
    "    print(key, len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "dim_type = 'datasets'\n",
    "for paper_id in roots[dim_type].papers:\n",
    "    if count < 10:\n",
    "        print(roots[dim_type].papers[paper_id].title, roots[dim_type].papers[paper_id].abstract)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zero-Shot Classification of Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': <taxonomy.DAG at 0x7f45e454db80>,\n",
       " 'datasets': <taxonomy.DAG at 0x7f45ee5e2e80>,\n",
       " 'methodologies': <taxonomy.DAG at 0x7f45e45f5640>,\n",
       " 'evaluation_methods': <taxonomy.DAG at 0x7f45e45f5eb0>,\n",
       " 'real_world_domains': <taxonomy.DAG at 0x7f45e45f5f70>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification for datasets; 675 papers\n",
      "visiting: natural_language_processing_datasets; # of papers: 675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 1082/1082 [01:08<00:00, 15.78it/s]\n",
      "Processed prompts:   0%|          | 0/675 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-18 06:45:33 scheduler.py:1099] Sequence group 3456 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 675/675 [03:55<00:00,  2.87it/s, est. speed input: 6674.48 toks/s, output: 281.13 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting: machine_translation_datasets_datasets; # of papers: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 54/54 [00:16<00:00,  3.30it/s, est. speed input: 7759.36 toks/s, output: 339.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting: question_answering_datasets_datasets; # of papers: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 90/90 [00:29<00:00,  3.05it/s, est. speed input: 7675.73 toks/s, output: 357.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting: sentiment_analysis_datasets_datasets; # of papers: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 22/22 [00:07<00:00,  3.09it/s, est. speed input: 6188.88 toks/s, output: 310.14 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting: named_entity_recognition_datasets_datasets; # of papers: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 29/29 [00:10<00:00,  2.66it/s, est. speed input: 6814.21 toks/s, output: 255.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting: text_classification_datasets_datasets; # of papers: 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 161/161 [00:46<00:00,  3.48it/s, est. speed input: 8331.31 toks/s, output: 336.15 toks/s]\n"
     ]
    }
   ],
   "source": [
    "dim_type = 'datasets'\n",
    "dag = dags[dim_type]\n",
    "print(f'classification for {dim_type}; {len(dag.root.papers)} papers')\n",
    "sample_out = dag.classify_dag(args, label2node=label2node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots['datasets'].display(0, indent_multiplier=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, p in roots['datasets'].children['question_answering_datasets_datasets'].children['reading_comprehension_datasets_datasets'].papers.items():\n",
    "    print(f'- {p.title}\\n\\t{p.abstract}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expansion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_papers = {}\n",
    "for idx, p in roots['datasets'].papers.items():\n",
    "    unlabeled = True\n",
    "    for c in roots['datasets'].children.values():\n",
    "        if idx in c.papers:\n",
    "            unlabeled = False\n",
    "    if unlabeled:\n",
    "        unlabeled_papers[idx] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import sibling_system_instruction, sibling_main_prompt, SiblingExpansionSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 340/340 [00:02<00:00, 137.69it/s, est. speed input: 80730.26 toks/s, output: 2157.00 toks/s]\n"
     ]
    }
   ],
   "source": [
    "exp_prompts = [constructPrompt(args, sibling_system_instruction, sibling_main_prompt(paper, roots['datasets'])) for paper in unlabeled_papers.values()]\n",
    "exp_outputs = promptLLM(args=args, prompts=exp_prompts, schema=SiblingExpansionSchema, max_new_tokens=300, json_mode=True, temperature=0.1, top_p=0.99)\n",
    "exp_outputs = [json.loads(clean_json_string(c)) if \"```\" in c else json.loads(c.strip()) for c in exp_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_class_label': 'vision_language_datasets_datasets'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0, 6, 10\n",
    "exp_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340, 251)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_exp_options), len(set(all_exp_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp_options = list(set([i['new_class_label'] for i in exp_outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text_classification_datasets_datasets', 'named_entity_recognition_datasets_datasets', 'sentiment_analysis_datasets_datasets', 'question_answering_datasets_datasets', 'machine_translation_datasets_datasets']\n"
     ]
    }
   ],
   "source": [
    "print(list(roots['datasets'].children.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_generation_datasets_datasets', 'sequential_sentence_classification_datasets_datasets', 'color_understanding_datasets_datasets', 'disinformation_detection_datasets_datasets', 'long-form_text_generation_datasets_datasets', 'document_understanding_datasets_datasets', 'multimodal_event_analysis_datasets_datasets', 'multimodal_model_evaluation_datasets_datasets', 'natural_language_processing_tasks', 'long_context_language_models_evaluation_datasets_datasets', 'legal_analysis_datasets_datasets', 'low_level_chart_question_answering_datasets_datasets', 'biomedical_translation_datasets_datasets', 'retrieval_augmentation_datasets_datasets', 'performance_prediction_datasets_datasets', 'zero-shot_commonsense_reasoning_datasets_datasets', 'multimodal_machine_translation_datasets_datasets', 'fact_checking_datasets_datasets', 'online_text_based_counseling_datasets_datasets', 'safety_risk_awareness_datasets_datasets', 'text-to-table_generation_datasets_datasets', 'multi-modal_language_models_datasets_datasets', 'topic_modeling_datasets_datasets', 'workflow_guided_planning_datasets_datasets', 'bias_assessment_datasets_datasets', 'data_quality_evaluation_datasets_datasets', 'function_calling_datasets_datasets', 'domain_adaptation_datasets_datasets', 'legal_reasoning_datasets_datasets', 'instructed_retrieval_datasets_datasets', 'conversational_support_systems', 'multimodal_modeling_datasets_datasets', 'cross-cultural_computational_social_science_datasets_datasets', 'dialog_flow_extraction_datasets_datasets', 'natural_language_inference_datasets_datasets', 'multilingual_nlp_datasets_datasets', 'travel_planning_datasets_datasets', 'robustness_evaluation_datasets_datasets', 'long_form_narrative_document_segmentation_datasets_datasets', 'knowledge_probing_datasets_datasets', 'natural_language_processing_datasets_datasets', 'text_classification_datasets_datasets', 'data_visualization_datasets_datasets', 'analogical_reasoning_datasets_datasets', 'language_model_safety_datasets_datasets', 'causal_reasoning_datasets_datasets', 'bias_verification_datasets_datasets', 'cultural_bias_in_language_models_datasets_datasets', 'event_extraction_datasets_datasets', 'long_context_generation_datasets_datasets', 'multimodal_learning_datasets_datasets', 'vision_language_datasets_datasets', 'suicide_detection_datasets_datasets', 'reinforcement_learning_datasets_datasets', 'video_moment_retrieval_datasets_datasets', 'parallelism_detection_datasets_datasets', 'model_compression_datasets_datasets', 'table_summarization_datasets_datasets', 'video_discourse_parsing_datasets_datasets', 'language_modeling_datasets_datasets', 'relation_extraction_datasets_datasets', 'temporal_fact_reasoning_datasets_datasets', 'clinical_triage_datasets_datasets', 'keyphrase_generation_datasets_datasets', 'emotion_analysis_datasets_datasets', 'vision-language_models', 'active_learning_datasets_datasets', 'logical_reasoning_evaluation_datasets_datasets', 'data_deduplication_datasets_datasets', 'generative_safety_evaluation_datasets_datasets', 'math_language_model_evaluation_datasets', 'language_model_evaluation_datasets_datasets', 'multimodal_math_evaluation_datasets_datasets', 'data_story_generation_datasets_datasets', 'visual_reasoning_datasets_datasets', 'data_de_identification_datasets_datasets', 'evaluation_benchmark_datasets_datasets', 'evaluation_mechanisms_for_agents', 'hallucination_detection_datasets_datasets', 'instruction_tuning_datasets_datasets', 'role-playing_datasets_datasets', 'vision-language_models_datasets_datasets', 'multilingual_factuality_evaluation_datasets', 'historical-psychological_text_analysis_datasets_datasets', 'task_oriented_dialogue_datasets_datasets', 'multimodal_datasets_datasets', 'vision_language_models', 'data_leakage_evaluation_datasets_datasets', 'word_sense_disambiguation_datasets_datasets', 'vision_language_models_datasets_datasets', 'grapheme_to_phoneme_conversion_datasets_datasets', 'multimodal_embodied_ai_datasets_datasets', 'multilingual_math_reasoning_datasets_datasets', 'data_narration_datasets_datasets', 'satire_comprehension_datasets_datasets', 'multi-document_summarization_datasets_datasets', 'private_language_models_datasets_datasets', 'argument_quality_assessment_datasets_datasets', 'moral_beliefs_evaluation_datasets_datasets', 'cognitive_distortion_detection_datasets_datasets', 'multimodal_evaluation_datasets_datasets', 'generalized_agent_capabilities', 'judicial_intelligence_datasets_datasets', 'vision-grounded_decision_making_datasets_datasets', 'reasoning_in_the_wild_datasets_datasets', 'copyright_compliance_datasets_datasets', 'dialogue_systems_datasets_datasets', 'time_series_analysis_datasets_datasets', 'text_style_transfer_datasets_datasets', 'retrieval_augmented_generation_datasets_datasets', 'confidence_calibration_datasets_datasets', 'fairness_benchmarking_datasets_datasets', 'optical_character_recognition_datasets_datasets', 'legal_summarization_datasets_datasets', 'human_robot_interaction_datasets_datasets', 'intent_detection_datasets_datasets', 'bias_detection_datasets_datasets', 'speech_relation_extraction_datasets_datasets', 'information_networks_datasets_datasets', 'legal_language_understanding_datasets_datasets', 'zero_resource_hallucination_prevention_datasets_datasets', 'zero-shot_reasoning_datasets_datasets', 'bias_mitigation_datasets_datasets', 'text_generation_datasets_datasets', 'vision-language_model_alignment_datasets_datasets', 'conversation_analysis_datasets_datasets', 'time_series_reasoning_datasets_datasets', 'text_summarization_datasets_datasets', 'buddhist_text_classification_datasets_datasets', 'multimodal_models_risks', 'computational_meme_understanding_datasets_datasets', 'financial_market_prediction_datasets_datasets', 'syntactic_analysis_datasets_datasets', 'cross-modal_interaction_datasets_datasets', 'leaderboard_generation_datasets_datasets', 'cultural_comparative_commonsense_datasets_datasets', 'counterfactual_reasoning_datasets_datasets', 'table_generation_datasets_datasets', 'document_intelligence_datasets_datasets', 'user_simulation_datasets_datasets', 'code_debugging_datasets_datasets', 'rhetorical_understanding_and_generation_datasets_datasets', 'nlp_datasets_datasets', 'bias_evaluation_datasets_datasets', 'aspect_based_summarization_datasets_datasets', 'theory_of_mind_datasets_datasets', 'code_generation_datasets_datasets', 'script_evaluation_datasets_datasets', 'scientific_reasoning_datasets_datasets', 'multilingual_summarization_datasets_datasets', 'conversational_ai_datasets_datasets', 'data_preprocessing_datasets_datasets', 'vision-language_datasets_datasets', 'information_extraction_datasets_datasets', 'multimodal_summarization_datasets_datasets', 'cross-lingual_knowledge_editing_datasets_datasets', 'reasoning_ability_evaluation_datasets_datasets', 'language_documentation_datasets_datasets', 'natural_language_generation_datasets_datasets', 'multi-modal_learning_datasets_datasets', 'evaluation_benchmarking_datasets_datasets', 'multimodal_speech_recognition_datasets_datasets', 'object_affordance_datasets_datasets', 'code_retrieval_datasets_datasets', 'information_retrieval_datasets_datasets', 'cross-cultural_recipe_retrieval_datasets_datasets', 'long_context_language_models', 'explanation_request_parsing_datasets_datasets', 'multimodal_procedural_planning_datasets_datasets', 'machine_translation_datasets_datasets', 'multimodal_dialogue_datasets_datasets', 'evaluation_metrics_datasets_datasets', 'reasoning_datasets_datasets', 'probabilistic_reasoning_datasets_datasets', 'mathematical_reasoning_datasets_datasets', 'bias_association_datasets_datasets', 'text_generation_evaluation_datasets_datasets', 'temporal_information_understanding_datasets_datasets', 'fact-checking_datasets_datasets', 'language_contact_datasets_datasets', 'noise_detection_datasets_datasets', 'knowledge_editing_datasets_datasets', 'safety_alignment_datasets_datasets', 'financial_text_classification_datasets_datasets', 'instructional_meta_datasets_datasets', 'explainable_personality_recognition_datasets_datasets', 'autonomous_agents_datasets_datasets', 'dialogue_level_hallucination_evaluation_benchmarks', 'fact_verification_datasets_datasets', 'narrative_analysis_datasets_datasets', 'temporal_video_language_tasks', 'leaderboard_construction_datasets_datasets', 'evaluation_paradigms', 'clinical_decision_making_datasets_datasets', 'long_context_language_model_benchmarking_datasets_datasets', 'multi_audio_processing_datasets_datasets', 'conversation_forecasting_datasets_datasets', 'audio_deepfake_detection_datasets_datasets', 'tool_use_alignment_datasets_datasets', 'interleaved_text_and_image_generation_datasets_datasets', 'natural_language_reasoning_datasets_datasets', 'temporal_claim_verification_datasets_datasets', 'prompting_large_language_models', 'online_community_moderation_datasets_datasets', 'document_to_image_generation_datasets_datasets', 'industrial_safety_datasets_datasets', 'story_generation_datasets_datasets', 'vision_and_language_datasets_datasets', 'reasoning_tasks_datasets_datasets', 'zero-shot_learning_datasets_datasets', 'temporally_consistent_factuality_evaluation_datasets_datasets', 'data_driven_science_datasets', 'document_retrieval_datasets_datasets', 'multimodal_language_models_datasets_datasets', 'pedagogical_alignment_datasets_datasets', 'open-domain_dialogue_evaluation_datasets_datasets', 'hierarchical_text_clustering_datasets_datasets', 'text-to-table_tasks', 'fine_grained_object_classification_datasets_datasets', 'multimodal_detection_datasets_datasets', 'multimodal_mathematical_reasoning_datasets_datasets', 'poetry_evaluation_datasets_datasets', 'legal_question_answering_datasets_datasets', 'speech_recognition_datasets_datasets', 'multimodal_nlp_datasets_datasets', 'formal_theorem_proving_datasets_datasets', 'summarization_evaluation_datasets_datasets', 'multimodal_stance_detection_datasets_datasets', 'multimodal_data_hub_datasets_datasets', 'event_factuality_detection_datasets_datasets', 'multilingual_long_context_models_datasets_datasets', 'tabular_question_answering_datasets_datasets', 'speech_representation_learning_datasets_datasets', 'cognitive_modeling_datasets_datasets', 'multilingual_datasets_datasets', 'image_text_retrieval_datasets_datasets', 'dialog_tutoring_datasets_datasets', 'dialogue_generation_datasets_datasets', 'command_line_embedding_datasets_datasets', 'multilingual_benchmark_datasets_datasets', 'image_captioning_datasets_datasets', 'privacy_control_datasets_datasets', 'document_level_event_extraction_datasets_datasets', 'multitask_learning_datasets_datasets', 'heart_sound_diagnosis_datasets_datasets', 'math_problem_solving_datasets_datasets', 'multimodal_analysis_datasets_datasets', 'document_hierarchy_parsing_datasets_datasets', 'healthcare_advice_recognition_datasets_datasets', 'cross-cultural_communication_datasets_datasets', 'logical_reasoning_datasets_datasets']\n"
     ]
    }
   ],
   "source": [
    "print(all_exp_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_any_match(patterns, input_string):\n",
    "    \"\"\"\n",
    "    Check if any pattern in the list matches the input string.\n",
    "\n",
    "    :param patterns: List of regex patterns (as strings)\n",
    "    :param input_string: The string to search within\n",
    "    :return: True if any pattern matches, otherwise False\n",
    "    \"\"\"\n",
    "    # Compile all the patterns to make matching more efficient\n",
    "    compiled_patterns = [re.compile(pattern) for pattern in patterns]\n",
    "    \n",
    "    # Check if any compiled pattern matches the input string\n",
    "    for compiled_pattern in compiled_patterns:\n",
    "        if compiled_pattern.search(input_string):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labels = {d:[] for d in args.dimensions}\n",
    "paper_dims = {}\n",
    "\n",
    "patterns = {\"datasets\": [r'introduce [\\s\\w]* benchmark', r'introduce [\\s\\w]* dataset', r'construct [\\s\\w]* benchmark', r'construct [\\s\\w]* dataset', r'propose [\\s\\w]* dataset', r'propose [\\s\\w]* benchmark', r'present [\\s\\w]* benchmark', r'present [\\s\\w]* dataset', r'develop [\\s\\w]* benchmark', r'develop [\\s\\w]* dataset', r'create [\\s\\w]* benchmark', r'create [\\s\\w]* dataset', r'provide [\\s\\w]* benchmark', r'provide [\\s\\w]* dataset', r'describe [\\s\\w]* benchmark', r'describe [\\s\\w]* dataset', r'propose a new benchmark', r'propose a new dataset', r'introduce a new benchmark', r'introduce a new dataset', r'we release [\\s\\w]* dataset', r'we release [\\s\\w]* benchmark', r'a new dataset for [\\s\\w]*', r'a new benchmark for [\\s\\w]*', r'dataset for [\\s\\w]* task', r'benchmark for [\\s\\w]* task', r'we present [\\s\\w]* dataset', r'we present [\\s\\w]* benchmark', r'dataset designed for [\\s\\w]*', r'benchmark designed for [\\s\\w]*', r'introducing [\\s\\w]* dataset', r'introducing [\\s\\w]* benchmark'],\n",
    "            \"methodologies\": [r'introduce [\\s\\w]* method', r'propose [\\s\\w]* method', r'design [\\s\\w]* method', r'present [\\s\\w]* method', r'develop [\\s\\w]* method', r'introduce [\\s\\w]* approach', r'propose [\\s\\w]* approach', r'design [\\s\\w]* approach', r'present [\\s\\w]* approach', r'develop [\\s\\w]* approach', r'we propose [\\s\\w]* method', r'we propose [\\s\\w]* approach', r'we introduce [\\s\\w]* method', r'we introduce [\\s\\w]* approach', r'we present [\\s\\w]* method', r'we present [\\s\\w]* approach', r'propose a novel method', r'propose a novel approach', r'introduce a novel method', r'introduce a novel approach', r'present a novel method', r'present a novel approach', r'propose [\\s\\w]* framework', r'introduce [\\s\\w]* framework', r'present [\\s\\w]* framework', r'design [\\s\\w]* framework', r'we propose [\\s\\w]* framework', r'we introduce [\\s\\w]* framework', r'we present [\\s\\w]* framework', r'our proposed method [\\s\\w]*', r'our proposed approach [\\s\\w]*', r'our proposed framework [\\s\\w]*', r'this paper proposes [\\s\\w]* method', r'this paper introduces [\\s\\w]* method', r'this paper presents [\\s\\w]* method', r'this paper develops [\\s\\w]* method', r'this paper proposes [\\s\\w]* approach', r'this paper introduces [\\s\\w]* approach', r'this paper presents [\\s\\w]* approach', r'this paper develops [\\s\\w]* approach', r'this paper proposes [\\s\\w]* framework', r'this paper introduces [\\s\\w]* framework', r'this paper presents [\\s\\w]* framework', r'this paper develops [\\s\\w]* framework'],\n",
    "            \"evaluation_methods\": [r'construct a [\\s\\w]* evaluate', r'design a [\\s\\w]* evaluate', r'propose a [\\s\\w]* evaluate', r'introduce [\\s\\w]* evaluation method', r'propose [\\s\\w]* evaluation method', r'design [\\s\\w]* evaluation method', r'develop [\\s\\w]* evaluation method', r'introduce [\\s\\w]* evaluation metric', r'propose [\\s\\w]* evaluation metric', r'design [\\s\\w]* evaluation metric', r'develop [\\s\\w]* evaluation metric', r'propose a novel evaluation method', r'propose a novel evaluation metric', r'present a novel evaluation framework', r'introduce a framework for evaluation', r'this paper proposes [\\s\\w]* evaluation', r'this paper introduces [\\s\\w]* evaluation', r'introduce [\\s\\w]* automatic evaluation', r'propose [\\s\\w]* automatic evaluation', r'develop [\\s\\w]* automatic evaluation', r'design [\\s\\w]* automatic evaluation', r'propose a novel automatic evaluation method', r'automatic evaluation of [\\s\\w]* task', r'develop a method for automatic evaluation', r'introduce [\\s\\w]* human evaluation', r'propose [\\s\\w]* human evaluation', r'develop [\\s\\w]* human evaluation', r'design [\\s\\w]* human evaluation', r'propose a framework for human evaluation', r'introduce a novel human evaluation method', r'conduct human evaluation of [\\s\\w]*', r'compare human and automatic evaluation', r'comparison of human evaluation and automatic evaluation', r'human evaluation versus automatic evaluation', r'evaluate using both human and automatic methods', r'analyze results from human and automatic evaluation']}\n",
    "\n",
    "for id, paper in tqdm(internal_collection.items(), total=len(internal_collection)):\n",
    "    for dim, dim_patterns in patterns.items():\n",
    "        if find_any_match(dim_patterns, f'{paper.title}: {paper.abstract}'.lower()):\n",
    "            pseudo_labels[dim].append(paper)\n",
    "            if id in paper_dims:\n",
    "                paper_dims[id].append(dim)\n",
    "            else:\n",
    "                paper_dims[id] = [dim]\n",
    "print({dim: len(papers) for dim, papers in pseudo_labels.items()})\n",
    "\n",
    "for id, paper in tqdm(external_collection.items(), total=len(external_collection)):\n",
    "    for dim, dim_patterns in patterns.items():\n",
    "        if find_any_match(dim_patterns, f'{paper.title}: {paper.abstract}'.lower()):\n",
    "            pseudo_labels[dim].append(paper)\n",
    "            if id in paper_dims:\n",
    "                paper_dims[id].append(dim)\n",
    "            else:\n",
    "                paper_dims[id] = [dim]\n",
    "\n",
    "print({dim: len(papers) for dim, papers in pseudo_labels.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paper_dims), paper_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_collection[2494].abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loose Classification of Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.llm = 'vllm'\n",
    "# initializeLLM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(internal_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dags['methodologies'].classify_dag(args, collection=dags['methodologies'].root.papers, label2node=label2node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(roots['methodologies'].children['text_classification'].papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots['datasets'].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled = []\n",
    "\n",
    "for paper_id, paper in tqdm(root.papers.items()):\n",
    "    add = True\n",
    "    for c in root.children.values():\n",
    "        if paper_id in c.papers:\n",
    "            add = False\n",
    "    if add:\n",
    "        unlabeled.append(paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/pk36/epimine/agreement/2019_legislative_hk_human.txt', \"r\") as f:\n",
    "    human = [line.strip().split('\\t') for line in f.readlines() if len(line.strip())]\n",
    "    human_dict = {text.strip(): int(ep) if ep.isdigit() else -1 for ep, text in human}\n",
    "\n",
    "with open('/home/pk36/epimine/agreement/2019_legislative_hk_claude.txt', \"r\") as f:\n",
    "    claude = [line.strip().split('\\t') for line in f.readlines() if len(line.strip())]\n",
    "    claude_dict = {text.strip(): int(ep) if ep.isdigit() else -1 for ep, text in claude}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(human)\n",
    "data = pd.DataFrame(data={'Segment': np.concatenate((np.arange(num_rows),np.arange(num_rows)), axis=0), \n",
    "                   'Annotator': np.array([0]*num_rows + [1]*num_rows),\n",
    "                   'Episode': [human_dict[i[1]] for i in human] + [claude_dict[i[1]] for i in human]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>ICC</th>\n",
       "      <th>F</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>pval</th>\n",
       "      <th>CI95%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ICC1</th>\n",
       "      <td>Single raters absolute</td>\n",
       "      <td>0.629</td>\n",
       "      <td>4.386</td>\n",
       "      <td>316</td>\n",
       "      <td>317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.56, 0.69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2</th>\n",
       "      <td>Single random raters</td>\n",
       "      <td>0.629</td>\n",
       "      <td>4.412</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.56, 0.69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3</th>\n",
       "      <td>Single fixed raters</td>\n",
       "      <td>0.630</td>\n",
       "      <td>4.412</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.56, 0.69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC1k</th>\n",
       "      <td>Average raters absolute</td>\n",
       "      <td>0.772</td>\n",
       "      <td>4.386</td>\n",
       "      <td>316</td>\n",
       "      <td>317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.72, 0.82]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2k</th>\n",
       "      <td>Average random raters</td>\n",
       "      <td>0.772</td>\n",
       "      <td>4.412</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.72, 0.82]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3k</th>\n",
       "      <td>Average fixed raters</td>\n",
       "      <td>0.773</td>\n",
       "      <td>4.412</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.72, 0.82]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Description    ICC      F  df1  df2  pval         CI95%\n",
       "Type                                                                      \n",
       "ICC1    Single raters absolute  0.629  4.386  316  317   0.0  [0.56, 0.69]\n",
       "ICC2      Single random raters  0.629  4.412  316  316   0.0  [0.56, 0.69]\n",
       "ICC3       Single fixed raters  0.630  4.412  316  316   0.0  [0.56, 0.69]\n",
       "ICC1k  Average raters absolute  0.772  4.386  316  317   0.0  [0.72, 0.82]\n",
       "ICC2k    Average random raters  0.772  4.412  316  316   0.0  [0.72, 0.82]\n",
       "ICC3k     Average fixed raters  0.773  4.412  316  316   0.0  [0.72, 0.82]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_icc = pg.intraclass_corr(data=data, targets='Segment', raters='Annotator',\n",
    "                         ratings='Episode').round(3)\n",
    "ep_icc.set_index(\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6136012364760433"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y1 = [human_dict[i] for i in human_dict]\n",
    "y2 = [claude_dict[i] for i in human_dict]\n",
    "cohen_kappa_score(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/shared/data3/yzhan238/episode/gt/2014_moco_shootings_gt.txt', \"r\") as f:\n",
    "    human = [line.strip().split('\\t') for line in f.readlines()]\n",
    "    human = {int(i[0]): int(i[1])-1 if i[1].isdigit() else -1 for i in human} # segment id: episode id\n",
    "with open('/home/pk36/epimine/groundtruth/2014_moco_shootings_groundtruth.txt', \"r\") as f:\n",
    "    auto = [line.strip().split('\\t')[0] for line in f.readlines()]\n",
    "    auto = {idx: int(i) if i.isdigit() else -1 for idx, i in enumerate(auto)}\n",
    "\n",
    "with open('/home/pk36/epimine/groundtruth/2014_moco_shootings_groundtruth.txt', \"r\") as f:\n",
    "    seg_text = {idx:line.strip().split('\\t')[1] for idx, line in enumerate(f.readlines()) if len(line.strip())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(human)\n",
    "data = pd.DataFrame(data={'Segment': np.concatenate((np.arange(num_rows),np.arange(num_rows)), axis=0), \n",
    "                   'Annotator': np.array([0]*num_rows + [1]*num_rows),\n",
    "                   'Episode': [human[i] for i in human] + [auto[i] for i in human]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Episode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annotator</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Segment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Episode   \n",
       "Annotator       0  1\n",
       "Segment             \n",
       "0               4 -1\n",
       "1               4 -1\n",
       "2              -1  0\n",
       "3               4 -1\n",
       "4               3  4\n",
       "5              -1 -1\n",
       "6              -1  0\n",
       "7               5 -1\n",
       "8               1  1\n",
       "9              -1  0\n",
       "10              0 -1\n",
       "11             -1 -1\n",
       "12              0  5\n",
       "13             -1 -1\n",
       "14              0  0\n",
       "15             -1  0\n",
       "16             -1 -1\n",
       "17             -1 -1\n",
       "18             -1 -1\n",
       "19              0 -1\n",
       "20             -1  0\n",
       "21             -1  1\n",
       "22              5  4\n",
       "23              5 -1\n",
       "24             -1 -1\n",
       "25              5 -1\n",
       "26             -1  4\n",
       "27             -1 -1\n",
       "28             -1 -1\n",
       "29             -1 -1\n",
       "30             -1  1\n",
       "31             -1  0\n",
       "32             -1  0\n",
       "33             -1 -1\n",
       "34             -1  2\n",
       "35             -1  0\n",
       "36              0  5\n",
       "37             -1 -1\n",
       "38             -1  2\n",
       "39              3  0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pivot(index='Segment', columns='Annotator').head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>ICC</th>\n",
       "      <th>F</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>pval</th>\n",
       "      <th>CI95%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ICC1</th>\n",
       "      <td>Single raters absolute</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>0.852</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.741</td>\n",
       "      <td>[-0.31, 0.16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2</th>\n",
       "      <td>Single random raters</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.841</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>0.757</td>\n",
       "      <td>[-0.33, 0.16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3</th>\n",
       "      <td>Single fixed raters</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.841</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>0.757</td>\n",
       "      <td>[-0.32, 0.16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC1k</th>\n",
       "      <td>Average raters absolute</td>\n",
       "      <td>-0.174</td>\n",
       "      <td>0.852</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.741</td>\n",
       "      <td>[-0.91, 0.28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2k</th>\n",
       "      <td>Average random raters</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>0.841</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>0.757</td>\n",
       "      <td>[-0.97, 0.27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3k</th>\n",
       "      <td>Average fixed raters</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.841</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>0.757</td>\n",
       "      <td>[-0.94, 0.27]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Description    ICC      F  df1  df2   pval          CI95%\n",
       "Type                                                                        \n",
       "ICC1    Single raters absolute -0.080  0.852   65   66  0.741  [-0.31, 0.16]\n",
       "ICC2      Single random raters -0.088  0.841   65   65  0.757  [-0.33, 0.16]\n",
       "ICC3       Single fixed raters -0.087  0.841   65   65  0.757  [-0.32, 0.16]\n",
       "ICC1k  Average raters absolute -0.174  0.852   65   66  0.741  [-0.91, 0.28]\n",
       "ICC2k    Average random raters -0.192  0.841   65   65  0.757  [-0.97, 0.27]\n",
       "ICC3k     Average fixed raters -0.189  0.841   65   65  0.757  [-0.94, 0.27]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep_icc = pg.intraclass_corr(data=data, targets='Segment', raters='Annotator',\n",
    "                         ratings='Episode').round(3)\n",
    "ep_icc.set_index(\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 23\n",
    "data = pd.DataFrame(data={'Episode': np.concatenate((np.arange(num_rows),np.arange(num_rows)), axis=0), \n",
    "                   'Annotator': np.array([0]*num_rows + [1]*num_rows),\n",
    "                   'precision': [0.2222,0.6667,0,1,0,1,0,0.25,0,0,0.5,1,0,0.5,0,0.5,1,0.6,0,0.29,0.2,0.5,0,0.5,0.5,0.2,0.5,0.5,0,0.3333,0,0,0,0,1,0.3333,0,0,0,0,0,0,0.2857,0,0,0],\n",
    "                   'recall': [0.5,0.333,0,0.25,0,0.2,0,0.111,0,0,0.222222,0.4,0,0.125,0,0.4,0.125,0.375,0,0.22,0.2,0.14,0,0.5,0.1667,0.25,0.25,0.2857,0,0.2,0,0,0,0,0.2857,0.125,0,0,0,0,0,0,0.2,0,0,0],\n",
    "                   'f1': [0.3077,0.444,0,0.4,0,0.333,0,0.153,0,0,0.30769,0.75,0,0.2,0,0.44,0.22,0.46,0,0.25,0.2,0.22,0,0.5,0.25,0.2222,0.3333,0.3636,0,0.25,0,0,0,0,0.4444,0.1818,0,0,0,0,0,0,0.2353,0,0,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>ICC</th>\n",
       "      <th>F</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>pval</th>\n",
       "      <th>CI95%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ICC1</th>\n",
       "      <td>Single raters absolute</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1.394</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.217</td>\n",
       "      <td>[-0.25, 0.53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2</th>\n",
       "      <td>Single random raters</td>\n",
       "      <td>0.207</td>\n",
       "      <td>1.601</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.139</td>\n",
       "      <td>[-0.16, 0.55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3</th>\n",
       "      <td>Single fixed raters</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1.601</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.139</td>\n",
       "      <td>[-0.19, 0.58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC1k</th>\n",
       "      <td>Average raters absolute</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1.394</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.217</td>\n",
       "      <td>[-0.67, 0.69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2k</th>\n",
       "      <td>Average random raters</td>\n",
       "      <td>0.343</td>\n",
       "      <td>1.601</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.139</td>\n",
       "      <td>[-0.38, 0.71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3k</th>\n",
       "      <td>Average fixed raters</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.601</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.139</td>\n",
       "      <td>[-0.47, 0.74]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Description    ICC      F  df1  df2   pval          CI95%\n",
       "Type                                                                        \n",
       "ICC1    Single raters absolute  0.165  1.394   22   23  0.217  [-0.25, 0.53]\n",
       "ICC2      Single random raters  0.207  1.601   22   22  0.139  [-0.16, 0.55]\n",
       "ICC3       Single fixed raters  0.231  1.601   22   22  0.139  [-0.19, 0.58]\n",
       "ICC1k  Average raters absolute  0.283  1.394   22   23  0.217  [-0.67, 0.69]\n",
       "ICC2k    Average random raters  0.343  1.601   22   22  0.139  [-0.38, 0.71]\n",
       "ICC3k     Average fixed raters  0.375  1.601   22   22  0.139  [-0.47, 0.74]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_icc = pg.intraclass_corr(data=data, targets='Episode', raters='Annotator',\n",
    "                         ratings='precision').round(3)\n",
    "prec_icc.set_index(\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>ICC</th>\n",
       "      <th>F</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>pval</th>\n",
       "      <th>CI95%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ICC1</th>\n",
       "      <td>Single raters absolute</td>\n",
       "      <td>0.314</td>\n",
       "      <td>1.914</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.065</td>\n",
       "      <td>[-0.1, 0.64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2</th>\n",
       "      <td>Single random raters</td>\n",
       "      <td>0.329</td>\n",
       "      <td>2.048</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.050</td>\n",
       "      <td>[-0.06, 0.64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3</th>\n",
       "      <td>Single fixed raters</td>\n",
       "      <td>0.344</td>\n",
       "      <td>2.048</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.050</td>\n",
       "      <td>[-0.07, 0.66]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC1k</th>\n",
       "      <td>Average raters absolute</td>\n",
       "      <td>0.478</td>\n",
       "      <td>1.914</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.065</td>\n",
       "      <td>[-0.21, 0.78]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2k</th>\n",
       "      <td>Average random raters</td>\n",
       "      <td>0.495</td>\n",
       "      <td>2.048</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.050</td>\n",
       "      <td>[-0.13, 0.78]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3k</th>\n",
       "      <td>Average fixed raters</td>\n",
       "      <td>0.512</td>\n",
       "      <td>2.048</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.050</td>\n",
       "      <td>[-0.15, 0.79]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Description    ICC      F  df1  df2   pval          CI95%\n",
       "Type                                                                        \n",
       "ICC1    Single raters absolute  0.314  1.914   22   23  0.065   [-0.1, 0.64]\n",
       "ICC2      Single random raters  0.329  2.048   22   22  0.050  [-0.06, 0.64]\n",
       "ICC3       Single fixed raters  0.344  2.048   22   22  0.050  [-0.07, 0.66]\n",
       "ICC1k  Average raters absolute  0.478  1.914   22   23  0.065  [-0.21, 0.78]\n",
       "ICC2k    Average random raters  0.495  2.048   22   22  0.050  [-0.13, 0.78]\n",
       "ICC3k     Average fixed raters  0.512  2.048   22   22  0.050  [-0.15, 0.79]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_icc = pg.intraclass_corr(data=data, targets='Episode', raters='Annotator',\n",
    "                         ratings='recall').round(3)\n",
    "recall_icc.set_index(\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>ICC</th>\n",
       "      <th>F</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>pval</th>\n",
       "      <th>CI95%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ICC1</th>\n",
       "      <td>Single raters absolute</td>\n",
       "      <td>0.247</td>\n",
       "      <td>1.657</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.118</td>\n",
       "      <td>[-0.17, 0.59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2</th>\n",
       "      <td>Single random raters</td>\n",
       "      <td>0.271</td>\n",
       "      <td>1.812</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.086</td>\n",
       "      <td>[-0.11, 0.6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3</th>\n",
       "      <td>Single fixed raters</td>\n",
       "      <td>0.289</td>\n",
       "      <td>1.812</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.086</td>\n",
       "      <td>[-0.13, 0.62]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC1k</th>\n",
       "      <td>Average raters absolute</td>\n",
       "      <td>0.397</td>\n",
       "      <td>1.657</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.118</td>\n",
       "      <td>[-0.4, 0.74]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC2k</th>\n",
       "      <td>Average random raters</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.812</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.086</td>\n",
       "      <td>[-0.25, 0.75]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICC3k</th>\n",
       "      <td>Average fixed raters</td>\n",
       "      <td>0.448</td>\n",
       "      <td>1.812</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.086</td>\n",
       "      <td>[-0.3, 0.77]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Description    ICC      F  df1  df2   pval          CI95%\n",
       "Type                                                                        \n",
       "ICC1    Single raters absolute  0.247  1.657   22   23  0.118  [-0.17, 0.59]\n",
       "ICC2      Single random raters  0.271  1.812   22   22  0.086   [-0.11, 0.6]\n",
       "ICC3       Single fixed raters  0.289  1.812   22   22  0.086  [-0.13, 0.62]\n",
       "ICC1k  Average raters absolute  0.397  1.657   22   23  0.118   [-0.4, 0.74]\n",
       "ICC2k    Average random raters  0.426  1.812   22   22  0.086  [-0.25, 0.75]\n",
       "ICC3k     Average fixed raters  0.448  1.812   22   22  0.086   [-0.3, 0.77]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_icc = pg.intraclass_corr(data=data, targets='Episode', raters='Annotator',\n",
    "                         ratings='f1').round(3)\n",
    "f1_icc.set_index(\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
