Label: natural_language_processing
Dimension: evaluation_methods
Description: None
Level: 0
# of Papers: 1962
Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (4, 'Table Question Answering for Low-resourced Indic Languages'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions')]
----------------------------------------
Children:
     Label: benchmarking
     Dimension: evaluation_methods
     Description: This method involves systematically comparing the performance of various natural language processing models on standardized datasets to establish performance baselines and identify strengths and weaknesses.
     Level: 1
     # of Papers: 512
     Example Papers: [(4, 'Table Question Answering for Low-resourced Indic Languages'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (13, 'A Usage-centric Take on Intent Understanding in E-Commerce')]
     ----------------------------------------
     Children:
          Label: evaluation_metrics
          Dimension: evaluation_methods
          Description: This cluster focuses on the specific metrics used to quantify the performance of natural language processing models during benchmarking, providing standardized measures for comparison.
          Level: 2
          # of Papers: 482
          Example Papers: [(4, 'Table Question Answering for Low-resourced Indic Languages'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (13, 'A Usage-centric Take on Intent Understanding in E-Commerce')]
          ----------------------------------------
          Children:
               Label: performance_metrics
               Dimension: evaluation_methods
               Description: This cluster focuses on metrics that quantify the overall performance of NLP models, providing a comprehensive view of their effectiveness across various tasks.
               Level: 3
               # of Papers: 90
               Example Papers: [(46, 'LongEmbed: Extending Embedding Models for Long Context Retrieval'), (70, 'ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval'), (72, 'Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation')]
               ----------------------------------------
               Label: robustness_metrics
               Dimension: evaluation_methods
               Description: This cluster encompasses metrics specifically designed to evaluate the robustness of NLP models against adversarial inputs and variations in data.
               Level: 3
               # of Papers: 14
               Example Papers: [(126, 'VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation'), (801, 'Can Automatic Metrics Assess High-Quality Translations?'), (948, 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation')]
               ----------------------------------------
               Label: ranking_metrics
               Dimension: evaluation_methods
               Description: This cluster includes metrics that assess the ranking capabilities of models, particularly in tasks where the order of results is crucial for user satisfaction.
               Level: 3
               # of Papers: 20
               Example Papers: [(24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (249, 'PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval'), (490, 'FIRST: Faster Improved Listwise Reranking with Single Token Decoding')]
               ----------------------------------------
               Label: translation_metrics
               Dimension: evaluation_methods
               Description: This cluster is dedicated to metrics that evaluate the quality of machine translation outputs, focusing on accuracy, fluency, and fidelity to the source text.
               Level: 3
               # of Papers: 42
               Example Papers: [(624, 'Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering'), (640, 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation'), (802, 'Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation')]
               ----------------------------------------
               Label: automatic_evaluation
               Dimension: evaluation_methods
               Description: This cluster covers metrics used for automatic evaluation of NLP models, allowing for quick assessments without human intervention, often used in benchmarking.
               Level: 3
               # of Papers: 349
               Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (29, 'EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models'), (38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds')]
               ----------------------------------------
          ----------------------------------------
     ----------------------------------------
     Label: comparative_studies
     Dimension: evaluation_methods
     Description: This approach focuses on analyzing and contrasting different NLP techniques or models to understand their relative effectiveness and applicability in specific tasks or domains.
     Level: 1
     # of Papers: 823
     Example Papers: [(4, 'Table Question Answering for Low-resourced Indic Languages'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (6, 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay')]
     ----------------------------------------
     Children:
          Label: bias_evaluation
          Dimension: evaluation_methods
          Description: This cluster examines the methods used to assess and compare biases present in NLP models, highlighting their impact on fairness and ethical considerations.
          Level: 2
          # of Papers: 116
          Example Papers: [(9, 'Hateful Word in Context Classification'), (12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (15, 'Systematic Biases in LLM Simulations of Debates')]
          ----------------------------------------
          Children:
               Label: name_based_bias
               Dimension: evaluation_methods
               Description: This subtopic specifically addresses biases that arise from the names used in datasets and models, examining how name representation can influence model outcomes and fairness.
               Level: 3
               # of Papers: 2
               Example Papers: [(33, 'A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers'), (412, '"You Gotta be a Doctor, Lin" : An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations')]
               ----------------------------------------
               Label: bias_in_data
               Dimension: evaluation_methods
               Description: This subtopic focuses on the biases present in datasets, exploring how data collection methods and sample selection can lead to skewed representations and outcomes.
               Level: 3
               # of Papers: 44
               Example Papers: [(15, 'Systematic Biases in LLM Simulations of Debates'), (28, 'On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models'), (33, 'A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers')]
               ----------------------------------------
               Label: bias_in_language
               Dimension: evaluation_methods
               Description: This subtopic examines the biases that emerge from language use in datasets and models, analyzing how linguistic choices can affect fairness and model performance.
               Level: 3
               # of Papers: 88
               Example Papers: [(9, 'Hateful Word in Context Classification'), (12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (15, 'Systematic Biases in LLM Simulations of Debates')]
               ----------------------------------------
               Label: dialect_discrimination
               Dimension: evaluation_methods
               Description: This subtopic investigates biases related to dialects, highlighting how variations in language can lead to discrimination in model predictions and evaluations.
               Level: 3
               # of Papers: 3
               Example Papers: [(749, 'Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination'), (2336, 'Modeling Gender and Dialect Bias in Automatic Speech Recognition'), (2746, 'AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark')]
               ----------------------------------------
               Label: bias_variation
               Dimension: evaluation_methods
               Description: This subtopic addresses the variations of bias across different contexts and datasets, focusing on how biases can shift and manifest differently depending on the scenario.
               Level: 3
               # of Papers: 15
               Example Papers: [(9, 'Hateful Word in Context Classification'), (28, 'On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models'), (69, 'AgentReview: Exploring Peer Review Dynamics with LLM Agents')]
               ----------------------------------------
          ----------------------------------------
          Label: ensemble_methods
          Dimension: evaluation_methods
          Description: This cluster investigates the comparative effectiveness of ensemble techniques in NLP, analyzing how combining multiple models can enhance performance.
          Level: 2
          # of Papers: 407
          Example Papers: [(6, 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay'), (14, 'Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs'), (24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing')]
          ----------------------------------------
          Children:
               Label: model_ensemble_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on the assessment of various model ensemble techniques, analyzing how the combination of different models can lead to improved performance in natural language processing tasks.
               Level: 3
               # of Papers: 48
               Example Papers: [(24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (65, 'AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation'), (79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment')]
               ----------------------------------------
               Label: performance_comparison_of_ensemble_methods
               Dimension: evaluation_methods
               Description: This subtopic investigates the comparative performance metrics of different ensemble methods, providing insights into their effectiveness across various NLP applications.
               Level: 3
               # of Papers: 91
               Example Papers: [(14, 'Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs'), (152, 'QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios'), (181, 'Autoregressive Pre-Training on Pixels and Texts')]
               ----------------------------------------
               Label: robustness_analysis_of_ensemble_techniques
               Dimension: evaluation_methods
               Description: This subtopic examines the robustness of ensemble techniques in NLP, evaluating how well these methods perform under varying conditions and datasets.
               Level: 3
               # of Papers: 28
               Example Papers: [(26, 'Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation'), (31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices'), (233, 'When Context Leads but Parametric Memory Follows in Large Language Models')]
               ----------------------------------------
               Label: ensemble_method_optimization
               Dimension: evaluation_methods
               Description: This subtopic explores strategies for optimizing ensemble methods, focusing on parameter tuning and selection processes to enhance model performance.
               Level: 3
               # of Papers: 183
               Example Papers: [(30, 'On Fake News Detection with LLM Enhanced Semantics Mining'), (34, 'Mitigating the Alignment Tax of RLHF'), (43, 'GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation')]
               ----------------------------------------
               Label: ensemble_method_variability_studies
               Dimension: evaluation_methods
               Description: This subtopic analyzes the variability in performance outcomes of ensemble methods, investigating how different configurations and combinations impact results in NLP tasks.
               Level: 3
               # of Papers: 105
               Example Papers: [(6, 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay'), (30, 'On Fake News Detection with LLM Enhanced Semantics Mining'), (34, 'Mitigating the Alignment Tax of RLHF')]
               ----------------------------------------
          ----------------------------------------
          Label: domain_adaptation
          Dimension: evaluation_methods
          Description: This cluster explores the evaluation of NLP models in the context of domain adaptation, focusing on how well models transfer knowledge across different domains.
          Level: 2
          # of Papers: 283
          Example Papers: [(13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices')]
          ----------------------------------------
          Children:
               Label: evaluation_benchmarks
               Dimension: evaluation_methods
               Description: This subtopic encompasses standardized benchmarks designed to evaluate domain adaptation in NLP, providing a consistent framework for comparing model performance across diverse datasets.
               Level: 3
               # of Papers: 106
               Example Papers: [(23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (34, 'Mitigating the Alignment Tax of RLHF'), (54, 'Chain-of-Dictionary Prompting Elicits Translation in Large Language Models')]
               ----------------------------------------
               Label: cross_domain_evaluation
               Dimension: evaluation_methods
               Description: This subtopic explores evaluation strategies specifically aimed at assessing how well NLP models perform when transferring knowledge from one domain to another, highlighting challenges and solutions in cross-domain scenarios.
               Level: 3
               # of Papers: 122
               Example Papers: [(43, 'GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation'), (94, 'Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective'), (139, 'In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search')]
               ----------------------------------------
          ----------------------------------------
          Label: single_step_inference_rule_classification
          Dimension: evaluation_methods
          Description: This cluster focuses on the evaluation of classification methods that utilize single-step inference rules, assessing their effectiveness and applicability in various NLP tasks.
          Level: 2
          # of Papers: 18
          Example Papers: [(152, 'QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios'), (208, 'Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method'), (219, 'Incubating Text Classifiers Following User Instruction with Nothing but LLM')]
          ----------------------------------------
     ----------------------------------------
     Label: user_studies
     Dimension: evaluation_methods
     Description: This evaluation method involves gathering feedback from end-users to assess the usability, effectiveness, and satisfaction of NLP applications in real-world scenarios.
     Level: 1
     # of Papers: 68
     Example Papers: [(10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (41, 'Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections')]
     ----------------------------------------
     Children:
          Label: subjective_annotation
          Dimension: evaluation_methods
          Description: This subtopic focuses on gathering qualitative feedback from users through subjective annotation methods to evaluate the usability and effectiveness of NLP applications.
          Level: 2
          # of Papers: 2
          Example Papers: [(10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (2676, 'Constructing a Sentiment-Annotated Corpus of Austrian Historical Newspapers: Challenges, Tools, and Annotator Experience')]
          ----------------------------------------
          Label: evaluation_of_nlp_effectiveness
          Dimension: evaluation_methods
          Description: This subtopic involves assessing the overall effectiveness of NLP systems based on user feedback and real-world performance metrics.
          Level: 2
          # of Papers: 42
          Example Papers: [(21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (145, 'Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course'), (208, 'Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method')]
          ----------------------------------------
          Children:
               Label: user_perceptions
               Dimension: evaluation_methods
               Description: This subtopic explores how users perceive and interact with NLP systems, emphasizing qualitative feedback and subjective assessments of effectiveness.
               Level: 3
               # of Papers: 13
               Example Papers: [(243, "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters"), (278, 'How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?'), (1050, 'Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting')]
               ----------------------------------------
               Label: user_model_based_evaluation
               Dimension: evaluation_methods
               Description: This subtopic involves evaluating NLP systems based on user models, which predict user behavior and preferences, allowing for tailored assessments of system effectiveness.
               Level: 3
               # of Papers: 28
               Example Papers: [(145, 'Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course'), (208, 'Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method'), (209, 'A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models')]
               ----------------------------------------
          ----------------------------------------
          Label: human_evaluation
          Dimension: evaluation_methods
          Description: This subtopic emphasizes the role of human evaluators in assessing the quality and performance of NLP applications through direct user studies.
          Level: 2
          # of Papers: 56
          Example Papers: [(21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (41, 'Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections'), (58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs')]
          ----------------------------------------
          Children:
               Label: user_study
               Dimension: evaluation_methods
               Description: This subtopic focuses on the design and implementation of user studies to evaluate the effectiveness and usability of NLP applications through direct feedback from users.
               Level: 3
               # of Papers: 21
               Example Papers: [(208, 'Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method'), (243, "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters"), (278, 'How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?')]
               ----------------------------------------
               Label: expert_evaluation
               Dimension: evaluation_methods
               Description: This subtopic emphasizes the assessment of NLP applications by domain experts who provide qualitative insights and judgments based on their specialized knowledge.
               Level: 3
               # of Papers: 14
               Example Papers: [(58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs'), (590, 'Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles'), (710, 'PATIENT-ps: Using Large Language Models to Simulate Patients for Training Mental Health Professionals')]
               ----------------------------------------
               Label: human_in_the_loop
               Dimension: evaluation_methods
               Description: This subtopic explores methodologies that incorporate human feedback into the evaluation process, allowing for iterative improvements and adjustments based on evaluator input.
               Level: 3
               # of Papers: 34
               Example Papers: [(21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (41, 'Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections'), (130, "I Need Help! Evaluating LLM's Ability to Ask for Users' Support: A Case Study on Text-to-SQL Generation")]
               ----------------------------------------
          ----------------------------------------
          Label: eliciting_expert_feedback
          Dimension: evaluation_methods
          Description: This subtopic pertains to the process of gathering insights and evaluations from domain experts to improve the usability and effectiveness of NLP systems.
          Level: 2
          # of Papers: 8
          Example Papers: [(590, 'Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles'), (960, "StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning"), (1112, 'The Empirical Variability of Narrative Perceptions of Social Media Texts')]
          ----------------------------------------
          Label: gender_bias
          Dimension: evaluation_methods
          Description: This subtopic examines user studies focused on identifying and evaluating gender bias in NLP applications through user feedback and analysis.
          Level: 2
          # of Papers: 8
          Example Papers: [(362, "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context"), (749, 'Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination'), (1001, 'What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study')]
          ----------------------------------------
     ----------------------------------------
     Label: error_analysis
     Dimension: evaluation_methods
     Description: This method entails a detailed examination of the errors made by NLP models to identify common failure modes and inform improvements in model design and training.
     Level: 1
     # of Papers: 661
     Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (9, 'Hateful Word in Context Classification'), (12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models')]
     ----------------------------------------
     Children:
          Label: bias_detection
          Dimension: evaluation_methods
          Description: This subtopic focuses on identifying and analyzing biases present in NLP models, aiming to understand their impact on model performance and fairness.
          Level: 2
          # of Papers: 56
          Example Papers: [(12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (15, 'Systematic Biases in LLM Simulations of Debates'), (28, 'On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models')]
          ----------------------------------------
          Children:
               Label: bias_attribution
               Dimension: evaluation_methods
               Description: This subtopic focuses on methods for identifying the sources of bias in NLP models, analyzing how specific features or training data contribute to biased outcomes.
               Level: 3
               # of Papers: 51
               Example Papers: [(12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (15, 'Systematic Biases in LLM Simulations of Debates'), (28, 'On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models')]
               ----------------------------------------
          ----------------------------------------
          Label: hallucination_detection
          Dimension: evaluation_methods
          Description: This subtopic involves the detection of hallucinations in NLP outputs, where models generate information that is not grounded in the input data.
          Level: 2
          # of Papers: 51
          Example Papers: [(66, 'EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models'), (83, 'Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps'), (104, 'HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding')]
          ----------------------------------------
          Children:
               Label: output_verification
               Dimension: evaluation_methods
               Description: This subtopic focuses on methods that verify the accuracy and reliability of generated outputs by comparing them against trusted sources or factual databases.
               Level: 3
               # of Papers: 7
               Example Papers: [(151, 'Knowledge Verification to Nip Hallucination in the Bud'), (431, 'XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs'), (469, 'Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification')]
               ----------------------------------------
               Label: consistency_checking
               Dimension: evaluation_methods
               Description: This subtopic involves evaluating the consistency of generated information across multiple outputs or instances to identify discrepancies indicative of hallucinations.
               Level: 3
               # of Papers: 36
               Example Papers: [(66, 'EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models'), (104, 'HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding'), (115, 'Embedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection')]
               ----------------------------------------
               Label: factual_accuracy_assessment
               Dimension: evaluation_methods
               Description: This subtopic emphasizes the assessment of factual accuracy in generated content, ensuring that the information aligns with known facts and data.
               Level: 3
               # of Papers: 46
               Example Papers: [(66, 'EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models'), (83, 'Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps'), (104, 'HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding')]
               ----------------------------------------
               Label: contextual_relevance_analysis
               Dimension: evaluation_methods
               Description: This subtopic examines the relevance of generated outputs in relation to the input context, identifying instances where the output strays from the expected topic or content.
               Level: 3
               # of Papers: 11
               Example Papers: [(83, 'Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps'), (431, 'XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs'), (726, 'Analysis of Plan-based Retrieval for Grounded Text Generation')]
               ----------------------------------------
               Label: user_feedback_integration
               Dimension: evaluation_methods
               Description: This subtopic explores the incorporation of user feedback to detect and evaluate hallucinations, leveraging real-world user interactions to improve output reliability.
               Level: 3
               # of Papers: 1
               Example Papers: [(2444, 'Dial BeInfo for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning')]
               ----------------------------------------
          ----------------------------------------
          Label: adversarial_analysis
          Dimension: evaluation_methods
          Description: This subtopic encompasses the examination of adversarial attacks and vulnerabilities in NLP models, assessing their robustness against malicious inputs.
          Level: 2
          # of Papers: 57
          Example Papers: [(26, 'Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation'), (32, 'Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection'), (40, 'FLIRT: Feedback Loop In-context Red Teaming')]
          ----------------------------------------
          Children:
               Label: backdoor_attacks
               Dimension: evaluation_methods
               Description: This subtopic focuses on the examination of backdoor attacks, where malicious inputs are embedded into the training data to manipulate the model's behavior during inference.
               Level: 3
               # of Papers: 7
               Example Papers: [(513, 'CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models'), (641, 'Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning'), (731, 'BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models')]
               ----------------------------------------
               Label: adversarial_attack
               Dimension: evaluation_methods
               Description: This subtopic encompasses the study of various adversarial attacks aimed at exploiting vulnerabilities in NLP models by generating inputs that lead to incorrect outputs.
               Level: 3
               # of Papers: 55
               Example Papers: [(26, 'Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation'), (32, 'Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection'), (40, 'FLIRT: Feedback Loop In-context Red Teaming')]
               ----------------------------------------
               Label: attack_detection
               Dimension: evaluation_methods
               Description: This subtopic involves the development and evaluation of methods for detecting adversarial attacks on NLP models, ensuring robustness against malicious inputs.
               Level: 3
               # of Papers: 13
               Example Papers: [(26, 'Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation'), (513, 'CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models'), (533, 'Ranking Manipulation for Conversational Search Engines')]
               ----------------------------------------
          ----------------------------------------
          Label: error_detection
          Dimension: evaluation_methods
          Description: This subtopic is dedicated to identifying and categorizing errors made by NLP models, providing insights into common failure modes.
          Level: 2
          # of Papers: 478
          Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (9, 'Hateful Word in Context Classification'), (17, 'Uncertainty in Language Models: Assessment through Rank-Calibration')]
          ----------------------------------------
          Children:
               Label: syntax_error_detection
               Dimension: evaluation_methods
               Description: This subtopic focuses on identifying and categorizing errors related to the grammatical structure of sentences, including issues with syntax and sentence formation.
               Level: 3
               # of Papers: 11
               Example Papers: [(52, 'Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs'), (175, 'Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection'), (343, 'Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing')]
               ----------------------------------------
               Label: semantic_error_detection
               Dimension: evaluation_methods
               Description: This subtopic is dedicated to detecting errors that arise from misunderstandings or misinterpretations of meaning within text, highlighting issues with semantics.
               Level: 3
               # of Papers: 66
               Example Papers: [(38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds'), (48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue'), (125, 'An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference')]
               ----------------------------------------
               Label: contextual_error_detection
               Dimension: evaluation_methods
               Description: This subtopic involves identifying errors that occur due to a lack of contextual understanding, where the model fails to grasp the broader context of the text.
               Level: 3
               # of Papers: 44
               Example Papers: [(9, 'Hateful Word in Context Classification'), (21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (32, 'Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection')]
               ----------------------------------------
               Label: factual_error_detection
               Dimension: evaluation_methods
               Description: This subtopic is centered on identifying inaccuracies in factual information presented by NLP models, ensuring that the content aligns with verified data.
               Level: 3
               # of Papers: 277
               Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices')]
               ----------------------------------------
               Label: pragmatic_error_detection
               Dimension: evaluation_methods
               Description: This subtopic addresses errors related to the practical use of language, including issues with tone, politeness, and appropriateness in various contexts.
               Level: 3
               # of Papers: 17
               Example Papers: [(32, 'Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection'), (118, 'Aligning Language Models to Explicitly Handle Ambiguity'), (258, 'Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset')]
               ----------------------------------------
               Label: modal_error_detection
               Dimension: evaluation_methods
               Description: This subtopic focuses on identifying and analyzing errors that occur in the context of modal expressions, which convey necessity, possibility, or permission in language.
               Level: 3
               # of Papers: 6
               Example Papers: [(221, 'Conditional and Modal Reasoning in Large Language Models'), (482, 'Perceptions of Linguistic Uncertainty by Language Models and Humans'), (689, 'Self-Powered LLM Modality Expansion for Large Speech-Text Models')]
               ----------------------------------------
          ----------------------------------------
          Label: component_analysis
          Dimension: evaluation_methods
          Description: This subtopic involves a detailed examination of specific components within NLP models to understand their contributions to overall performance and error rates.
          Level: 2
          # of Papers: 47
          Example Papers: [(18, 'RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning'), (190, 'Neuron-Level Knowledge Attribution in Large Language Models'), (233, 'When Context Leads but Parametric Memory Follows in Large Language Models')]
          ----------------------------------------
          Children:
               Label: component_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on assessing the individual components of NLP models to determine their effectiveness and contribution to the overall model performance.
               Level: 3
               # of Papers: 21
               Example Papers: [(246, 'An Analysis of Multilingual FActScore'), (573, 'When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models'), (733, 'FAC^2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition')]
               ----------------------------------------
               Label: expert_analysis
               Dimension: evaluation_methods
               Description: This subtopic involves the evaluation of NLP models through the insights and assessments provided by domain experts, highlighting qualitative aspects of model performance.
               Level: 3
               # of Papers: 5
               Example Papers: [(992, 'Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs'), (1079, 'Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks'), (1700, 'Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models')]
               ----------------------------------------
               Label: prompt_analysis
               Dimension: evaluation_methods
               Description: This subtopic examines the impact of different prompts on the performance of NLP models, analyzing how variations in input can influence outcomes.
               Level: 3
               # of Papers: 7
               Example Papers: [(1079, 'Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks'), (1226, 'The Death and Life of Great Prompts: Analyzing the Evolution of LLM Prompts from the Structural Perspective'), (1700, 'Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models')]
               ----------------------------------------
               Label: context_analysis
               Dimension: evaluation_methods
               Description: This subtopic investigates how contextual factors affect the performance of NLP models, focusing on the role of surrounding information in understanding and generating language.
               Level: 3
               # of Papers: 8
               Example Papers: [(233, 'When Context Leads but Parametric Memory Follows in Large Language Models'), (647, 'Red Teaming Language Models for Processing Contradictory Dialogues'), (929, 'Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions')]
               ----------------------------------------
               Label: layer_importance
               Dimension: evaluation_methods
               Description: This subtopic evaluates the significance of different layers within neural network architectures in NLP models, analyzing their contributions to the model's decision-making process.
               Level: 3
               # of Papers: 4
               Example Papers: [(1079, 'Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks'), (1700, 'Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models'), (2478, 'Investigating Layer Importance in Large Language Models')]
               ----------------------------------------
          ----------------------------------------
     ----------------------------------------
     Label: proposed_metrics
     Dimension: evaluation_methods
     Description: This involves the development of new evaluation metrics or frameworks specifically tailored for assessing the performance of NLP systems, aiming to provide more nuanced insights than traditional metrics.
     Level: 1
     # of Papers: 700
     Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (9, 'Hateful Word in Context Classification')]
     ----------------------------------------
     Children:
          Label: faithfulness_evaluation
          Dimension: evaluation_methods
          Description: This cluster focuses on metrics designed to assess the faithfulness of NLP outputs, ensuring that generated content accurately reflects the input data and maintains the intended meaning.
          Level: 2
          # of Papers: 351
          Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (18, 'RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning')]
          ----------------------------------------
          Children:
               Label: pun_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on evaluating the faithfulness of NLP outputs in relation to the use of puns, ensuring that the intended humor and meaning are preserved in generated content.
               Level: 3
               ----------------------------------------
               Label: repetition_suppression
               Dimension: evaluation_methods
               Description: This subtopic addresses the assessment of faithfulness by evaluating the suppression of repetitive content in generated outputs, ensuring that the information presented is diverse and relevant.
               Level: 3
               # of Papers: 1
               Example Papers: [(1631, 'Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation')]
               ----------------------------------------
               Label: temporal_information_evaluation
               Dimension: evaluation_methods
               Description: This subtopic examines the faithfulness of NLP outputs by evaluating how well they maintain and represent temporal information from the input data.
               Level: 3
               # of Papers: 4
               Example Papers: [(2085, 'Mental Disorder Classification via Temporal Representation of Text'), (2395, 'Gradient Localization Improves Lifelong Pretraining of Language Models'), (2399, 'Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models')]
               ----------------------------------------
               Label: claim_detection
               Dimension: evaluation_methods
               Description: This subtopic involves the evaluation of the faithfulness of generated content by detecting and verifying claims made in the output against the input data.
               Level: 3
               # of Papers: 1
               Example Papers: [(2577, 'Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis')]
               ----------------------------------------
               Label: non_monotonic_reasoning_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on evaluating the faithfulness of NLP outputs in scenarios involving non-monotonic reasoning, ensuring that the reasoning process aligns with the intended conclusions.
               Level: 3
               # of Papers: 1
               Example Papers: [(1159, 'Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models')]
               ----------------------------------------
               Label: hallucination_prevention_evaluation
               Dimension: evaluation_methods
               Description: This subtopic addresses the assessment of faithfulness by evaluating methods aimed at preventing hallucinations in generated content, ensuring that outputs remain grounded in reality.
               Level: 3
               # of Papers: 7
               Example Papers: [(254, 'F^2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation'), (462, 'Attribute or Abstain: Large Language Models as Long Document Assistants'), (468, 'Towards Verifiable Text Generation with Evolving Memory and Self-Reflection')]
               ----------------------------------------
               Label: adversarial_logits_pairing_evaluation
               Dimension: evaluation_methods
               Description: This subtopic examines the faithfulness of NLP outputs through the evaluation of adversarial logits pairing, ensuring that the model's responses are robust against adversarial attacks.
               Level: 3
               # of Papers: 2
               Example Papers: [(106, 'DA^3: A Distribution-Aware Adversarial Attack against Language Models'), (906, 'IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method')]
               ----------------------------------------
               Label: faithfulness_hallucination_evaluation
               Dimension: evaluation_methods
               Description: This subtopic involves the evaluation of the faithfulness of generated content by specifically assessing the presence and impact of hallucinations in the outputs.
               Level: 3
               # of Papers: 309
               Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (34, 'Mitigating the Alignment Tax of RLHF'), (52, 'Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs')]
               ----------------------------------------
               Label: helpfulness_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on evaluating the faithfulness of NLP outputs by measuring their helpfulness and relevance to user queries, ensuring that the information provided is both accurate and useful.
               Level: 3
               # of Papers: 9
               Example Papers: [(357, 'VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment'), (486, 'MisinfoEval: Generative AI in the Era of "Alternative Facts"'), (593, 'Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering')]
               ----------------------------------------
          ----------------------------------------
          Label: fairness_metrics
          Dimension: evaluation_methods
          Description: This cluster encompasses metrics aimed at evaluating the fairness of NLP systems, addressing biases and ensuring equitable treatment across different demographic groups.
          Level: 2
          # of Papers: 21
          Example Papers: [(59, 'Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence'), (424, 'Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning'), (475, "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias")]
          ----------------------------------------
          Label: toxicity_detection
          Dimension: evaluation_methods
          Description: This cluster includes metrics specifically developed for detecting toxic language in NLP outputs, providing insights into harmful or offensive content generation.
          Level: 2
          # of Papers: 19
          Example Papers: [(114, 'CMD: a framework for Context-aware Model self-Detoxification'), (242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (475, "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias")]
          ----------------------------------------
          Label: confidence_calibration
          Dimension: evaluation_methods
          Description: This cluster is centered on metrics that evaluate the calibration of confidence scores in NLP models, ensuring that predicted probabilities accurately reflect true likelihoods.
          Level: 2
          # of Papers: 57
          Example Papers: [(17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity'), (225, 'PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling')]
          ----------------------------------------
          Children:
               Label: probability_calibration
               Dimension: evaluation_methods
               Description: This subtopic focuses on methods that assess how well the predicted probabilities from NLP models align with actual outcomes, ensuring that high-confidence predictions correspond to high accuracy.
               Level: 3
               # of Papers: 9
               Example Papers: [(172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity'), (299, 'Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method'), (582, 'Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding')]
               ----------------------------------------
               Label: confidence_score_evaluation
               Dimension: evaluation_methods
               Description: This subtopic encompasses techniques for evaluating the reliability of confidence scores generated by NLP models, aiming to determine whether these scores are trustworthy indicators of model performance.
               Level: 3
               # of Papers: 41
               Example Papers: [(17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity'), (272, 'Bayesian Calibration of Win Rate Estimation with LLM Evaluators')]
               ----------------------------------------
               Label: calibration_curve_analysis
               Dimension: evaluation_methods
               Description: This subtopic involves the analysis of calibration curves, which visually represent the relationship between predicted probabilities and observed frequencies, providing insights into the calibration quality of NLP models.
               Level: 3
               # of Papers: 37
               Example Papers: [(17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity'), (272, 'Bayesian Calibration of Win Rate Estimation with LLM Evaluators')]
               ----------------------------------------
               Label: expected_calibration_error
               Dimension: evaluation_methods
               Description: This subtopic deals with the computation of expected calibration error (ECE), a metric that quantifies the average difference between predicted probabilities and actual outcomes across different confidence levels.
               Level: 3
               # of Papers: 38
               Example Papers: [(17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity'), (272, 'Bayesian Calibration of Win Rate Estimation with LLM Evaluators')]
               ----------------------------------------
               Label: reliability_diagrams
               Dimension: evaluation_methods
               Description: This subtopic focuses on the use of reliability diagrams to visually assess the calibration of confidence scores, allowing for a straightforward comparison between predicted probabilities and empirical frequencies.
               Level: 3
               # of Papers: 34
               Example Papers: [(17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity'), (272, 'Bayesian Calibration of Win Rate Estimation with LLM Evaluators')]
               ----------------------------------------
          ----------------------------------------
          Label: evaluation_bias
          Dimension: evaluation_methods
          Description: This cluster focuses on metrics that assess various forms of bias in evaluation processes, aiming to identify and mitigate biases that may affect the performance assessment of NLP systems.
          Level: 2
          # of Papers: 131
          Example Papers: [(21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices'), (48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue')]
          ----------------------------------------
          Children:
               Label: bias_in_evaluation
               Dimension: evaluation_methods
               Description: This cluster focuses on identifying and analyzing various biases that can influence the evaluation outcomes of NLP systems.
               Level: 3
               # of Papers: 45
               Example Papers: [(21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (71, 'Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments'), (174, 'How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics')]
               ----------------------------------------
               Label: bias_in_evaluation_methods
               Dimension: evaluation_methods
               Description: This cluster examines biases specifically arising from the methodologies employed in the evaluation processes of NLP systems.
               Level: 3
               # of Papers: 6
               Example Papers: [(781, 'Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration'), (890, 'Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability'), (989, 'Data Contamination Can Cross Language Barriers')]
               ----------------------------------------
               Label: bias_in_evaluation_metrics
               Dimension: evaluation_methods
               Description: This cluster investigates biases present in the metrics used to assess the performance of NLP systems, aiming to ensure fair evaluations.
               Level: 3
               # of Papers: 21
               Example Papers: [(518, 'APPLS: Evaluating Evaluation Metrics for Plain Language Summarization'), (640, 'PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation'), (657, 'QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation')]
               ----------------------------------------
               Label: bias_in_evaluation_process
               Dimension: evaluation_methods
               Description: This cluster explores biases that may occur throughout the entire evaluation process, from design to implementation.
               Level: 3
               # of Papers: 70
               Example Papers: [(31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices'), (48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue'), (69, 'AgentReview: Exploring Peer Review Dynamics with LLM Agents')]
               ----------------------------------------
               Label: bias_assessment
               Dimension: evaluation_methods
               Description: This cluster is dedicated to the assessment and measurement of biases in evaluation practices, providing insights into their impact on NLP system performance.
               Level: 3
               # of Papers: 13
               Example Papers: [(186, 'ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws'), (284, 'Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs'), (763, 'A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations')]
               ----------------------------------------
          ----------------------------------------
          Label: latency_evaluation
          Dimension: evaluation_methods
          Description: This cluster focuses on metrics designed to assess the latency of NLP systems, measuring the time taken for models to generate outputs and ensuring responsiveness in real-time applications.
          Level: 2
          # of Papers: 57
          Example Papers: [(57, 'BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering'), (67, 'Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization'), (99, 'Rethinking Token Reduction for State Space Models')]
          ----------------------------------------
          Children:
               Label: response_time_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on measuring the time taken for an NLP model to respond to a given input, ensuring that the system meets the required responsiveness standards for real-time applications.
               Level: 3
               # of Papers: 1
               Example Papers: [(1254, 'Improving Minimum Bayes Risk Decoding with Multi-Prompt')]
               ----------------------------------------
               Label: throughput_evaluation
               Dimension: evaluation_methods
               Description: This subtopic assesses the number of requests an NLP system can handle in a given time frame, providing insights into the system's efficiency and capacity under load.
               Level: 3
               ----------------------------------------
               Label: end_to_end_latency_evaluation
               Dimension: evaluation_methods
               Description: This subtopic evaluates the total time taken from input submission to output generation in an NLP system, encompassing all processing stages to ensure comprehensive latency assessment.
               Level: 3
               # of Papers: 44
               Example Papers: [(57, 'BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering'), (99, 'Rethinking Token Reduction for State Space Models'), (140, 'AutoScraper: A Progressive Understanding Web Agent for Web Scraper Generation')]
               ----------------------------------------
               Label: latency_variability_evaluation
               Dimension: evaluation_methods
               Description: This subtopic examines the fluctuations in latency over time, analyzing how consistent an NLP system's response times are under varying conditions and workloads.
               Level: 3
               # of Papers: 6
               Example Papers: [(392, 'Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion'), (1003, 'Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning'), (1268, 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models')]
               ----------------------------------------
               Label: real_time_performance_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on assessing the performance of NLP systems in real-time scenarios, ensuring that latency metrics align with the demands of immediate user interactions.
               Level: 3
               # of Papers: 14
               Example Papers: [(99, 'Rethinking Token Reduction for State Space Models'), (848, 'Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach'), (940, 'FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping')]
               ----------------------------------------
          ----------------------------------------
     ----------------------------------------
     Label: automated_evaluation
     Dimension: evaluation_methods
     Description: This method utilizes algorithms and computational techniques to assess the performance of NLP models, providing objective metrics without human intervention.
     Level: 1
     # of Papers: 919
     Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (7, 'When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection')]
     ----------------------------------------
     Children:
          Label: robustness_evaluation
          Dimension: evaluation_methods
          Description: This cluster focuses on methods that assess the robustness of NLP models against various forms of perturbations and adversarial inputs, ensuring their reliability in real-world applications.
          Level: 2
          # of Papers: 127
          Example Papers: [(31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices'), (34, 'Mitigating the Alignment Tax of RLHF'), (48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue')]
          ----------------------------------------
          Children:
               Label: adversarial_inputs
               Dimension: evaluation_methods
               Description: This subtopic focuses on evaluating the impact of adversarial inputs on NLP models, assessing how these inputs can affect model performance and reliability.
               Level: 3
               # of Papers: 3
               Example Papers: [(165, 'Evaluating Large Language Models via Linguistic Profiling'), (678, 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models'), (1758, 'Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts')]
               ----------------------------------------
               Label: adversarial_robustness
               Dimension: evaluation_methods
               Description: This subtopic encompasses methods that specifically measure the robustness of NLP models against adversarial attacks, ensuring they maintain performance under challenging conditions.
               Level: 3
               # of Papers: 49
               Example Papers: [(31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices'), (91, 'Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?'), (106, 'DA^3: A Distribution-Aware Adversarial Attack against Language Models')]
               ----------------------------------------
               Label: adversarial_attack_detection
               Dimension: evaluation_methods
               Description: This subtopic involves techniques for detecting adversarial attacks on NLP models, aiming to identify and mitigate potential threats to model integrity.
               Level: 3
               # of Papers: 13
               Example Papers: [(106, 'DA^3: A Distribution-Aware Adversarial Attack against Language Models'), (376, 'Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models'), (426, 'Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment')]
               ----------------------------------------
               Label: adversarial_defense
               Dimension: evaluation_methods
               Description: This subtopic covers strategies and methods designed to defend NLP models against adversarial attacks, enhancing their resilience and reliability.
               Level: 3
               # of Papers: 59
               Example Papers: [(48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue'), (106, 'DA^3: A Distribution-Aware Adversarial Attack against Language Models'), (151, 'Knowledge Verification to Nip Hallucination in the Bud')]
               ----------------------------------------
               Label: domain_shifts
               Dimension: evaluation_methods
               Description: This subtopic examines the effects of domain shifts on NLP model performance, evaluating how well models adapt to changes in data distribution and context.
               Level: 3
               # of Papers: 25
               Example Papers: [(380, 'Can Large Language Models Learn Independent Causal Mechanisms?'), (569, 'Towards Robust Speech Representation Learning for Thousands of Languages'), (678, 'Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models')]
               ----------------------------------------
          ----------------------------------------
          Label: uncertainty_quantification
          Dimension: evaluation_methods
          Description: This cluster encompasses techniques that quantify the uncertainty in NLP model predictions, providing insights into the confidence levels of the outputs generated by these models.
          Level: 2
          # of Papers: 42
          Example Papers: [(17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (118, 'Aligning Language Models to Explicitly Handle Ambiguity'), (172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity')]
          ----------------------------------------
          Children:
               Label: confidence_estimation
               Dimension: evaluation_methods
               Description: This subtopic focuses on techniques that estimate the confidence levels of predictions made by NLP models, helping to understand the reliability of the outputs.
               Level: 3
               # of Papers: 15
               Example Papers: [(17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity'), (210, "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison")]
               ----------------------------------------
               Label: evaluation_of_ambiguity_handling
               Dimension: evaluation_methods
               Description: This subtopic examines methods for assessing how well NLP models manage ambiguous inputs, providing insights into their robustness and decision-making processes.
               Level: 3
               # of Papers: 6
               Example Papers: [(118, 'Aligning Language Models to Explicitly Handle Ambiguity'), (756, 'Don\'t Just Say "I don\'t know"! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations'), (768, 'A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences')]
               ----------------------------------------
               Label: calibration
               Dimension: evaluation_methods
               Description: This subtopic involves techniques aimed at adjusting the output probabilities of NLP models to better reflect true likelihoods, enhancing the interpretability of model predictions.
               Level: 3
               # of Papers: 29
               Example Papers: [(17, 'Uncertainty in Language Models: Assessment through Rank-Calibration'), (172, 'Calibrating the Confidence of Large Language Models by Eliciting Fidelity'), (210, "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison")]
               ----------------------------------------
               Label: sampling_methods
               Dimension: evaluation_methods
               Description: This subtopic explores various sampling techniques used to quantify uncertainty in model predictions, allowing for a more comprehensive understanding of potential outcomes.
               Level: 3
               # of Papers: 8
               Example Papers: [(298, 'LUQ: Long-text Uncertainty Quantification for LLMs'), (653, 'What Are the Odds? Language Models Are Capable of Probabilistic Reasoning'), (922, 'Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?')]
               ----------------------------------------
               Label: conformal_prediction
               Dimension: evaluation_methods
               Description: This subtopic covers methods that provide valid prediction intervals for model outputs, ensuring that the uncertainty quantification adheres to statistical guarantees.
               Level: 3
               # of Papers: 14
               Example Papers: [(238, 'Teaching LLMs to Abstain across Languages via Multilingual Feedback'), (298, 'LUQ: Long-text Uncertainty Quantification for LLMs'), (526, 'Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation')]
               ----------------------------------------
          ----------------------------------------
          Label: evaluation_frameworks
          Dimension: evaluation_methods
          Description: This cluster includes structured methodologies and frameworks designed to systematically evaluate the performance of NLP models across different tasks and datasets.
          Level: 2
          # of Papers: 647
          Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze")]
          ----------------------------------------
          Children:
               Label: metric_evaluation
               Dimension: evaluation_methods
               Description: This cluster focuses on the evaluation of various metrics used to assess the performance of NLP models, ensuring that the metrics are reliable and valid for different tasks.
               Level: 3
               # of Papers: 62
               Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (35, 'Evaluating Readability and Faithfulness of Concept-based Explanations'), (126, 'VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation')]
               ----------------------------------------
               Label: model_evaluation
               Dimension: evaluation_methods
               Description: This cluster encompasses methodologies specifically designed to evaluate the overall performance and effectiveness of NLP models across various applications.
               Level: 3
               # of Papers: 499
               Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (30, 'On Fake News Detection with LLM Enhanced Semantics Mining')]
               ----------------------------------------
               Label: task_evaluation
               Dimension: evaluation_methods
               Description: This cluster includes frameworks that systematically evaluate the performance of NLP models on specific tasks, ensuring task-specific metrics and benchmarks are utilized.
               Level: 3
               # of Papers: 286
               Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze")]
               ----------------------------------------
               Label: prompt_engineering
               Dimension: evaluation_methods
               Description: This cluster focuses on the evaluation frameworks that assess the effectiveness of different prompting strategies in enhancing the performance of NLP models.
               Level: 3
               # of Papers: 39
               Example Papers: [(12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (71, 'Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments'), (120, 'GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models')]
               ----------------------------------------
               Label: multimodal_evaluation
               Dimension: evaluation_methods
               Description: This cluster includes evaluation methods that assess the performance of models that integrate multiple modalities, such as text, images, and audio, ensuring comprehensive evaluation across diverse data types.
               Level: 3
               # of Papers: 61
               Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (40, 'FLIRT: Feedback Loop In-context Red Teaming'), (61, 'RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models')]
               ----------------------------------------
          ----------------------------------------
          Label: evaluation_dataset
          Dimension: evaluation_methods
          Description: This cluster pertains to the creation and utilization of specialized datasets that are specifically designed for the evaluation of NLP models, ensuring comprehensive assessment metrics.
          Level: 2
          # of Papers: 289
          Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze")]
          ----------------------------------------
          Children:
               Label: dataset_creation
               Dimension: evaluation_methods
               Description: This subtopic focuses on the methodologies and processes involved in creating specialized datasets tailored for evaluating NLP models.
               Level: 3
               # of Papers: 98
               Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds')]
               ----------------------------------------
               Label: dataset_evaluation
               Dimension: evaluation_methods
               Description: This subtopic encompasses the techniques and practices used to assess the quality and effectiveness of datasets in the context of NLP model evaluation.
               Level: 3
               # of Papers: 221
               Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (30, 'On Fake News Detection with LLM Enhanced Semantics Mining')]
               ----------------------------------------
               Label: dataset_generation
               Dimension: evaluation_methods
               Description: This subtopic deals with the automated or semi-automated generation of datasets designed specifically for evaluating various NLP tasks.
               Level: 3
               # of Papers: 17
               Example Papers: [(139, 'In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search'), (283, 'From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis'), (674, 'Unknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data')]
               ----------------------------------------
               Label: dataset_distillation
               Dimension: evaluation_methods
               Description: This subtopic involves the process of refining and optimizing datasets to enhance their utility for evaluating NLP models.
               Level: 3
               # of Papers: 6
               Example Papers: [(776, 'How Do Your Code LLMs perform? Empowering Code Instruction Tuning with Really Good Data'), (1070, 'SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation'), (1940, 'MetaKP: On-Demand Keyphrase Generation')]
               ----------------------------------------
               Label: domain_specific_dataset
               Dimension: evaluation_methods
               Description: This subtopic pertains to the creation and use of datasets that are tailored to specific domains, ensuring relevant evaluation metrics for NLP models in those areas.
               Level: 3
               # of Papers: 25
               Example Papers: [(960, "StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning"), (1125, 'CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research'), (1127, 'VIEWS: Entity-Aware News Video Captioning')]
               ----------------------------------------
          ----------------------------------------
          Label: metric_bias
          Dimension: evaluation_methods
          Description: This cluster addresses the biases present in evaluation metrics used for NLP models, focusing on identifying and mitigating unfair advantages or disadvantages in model assessments.
          Level: 2
          # of Papers: 49
          Example Papers: [(59, 'Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence'), (126, 'VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation'), (145, 'Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course')]
          ----------------------------------------
          Children:
               Label: fairness_metrics
               Label: robustness_metrics
               Label: diversity_metrics
               Dimension: evaluation_methods
               Description: This subtopic explores evaluation metrics that quantify the diversity of outputs generated by NLP models, aiming to mitigate biases that arise from homogeneity in model responses.
               Level: 3
               # of Papers: 8
               Example Papers: [(186, 'ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws'), (518, 'APPLS: Evaluating Evaluation Metrics for Plain Language Summarization'), (895, 'Leveraging Large Language Models for NLG Evaluation: Advances and Challenges')]
               ----------------------------------------
               Label: sensitivity_analysis
               Dimension: evaluation_methods
               Description: This subtopic involves evaluation methods that analyze how sensitive NLP model performance is to changes in input data, helping to identify potential biases in model assessments.
               Level: 3
               # of Papers: 14
               Example Papers: [(242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (450, 'PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data'), (518, 'APPLS: Evaluating Evaluation Metrics for Plain Language Summarization')]
               ----------------------------------------
               Label: disparity_metrics
               Dimension: evaluation_methods
               Description: This subtopic examines metrics that highlight disparities in model performance across different subgroups, focusing on identifying and addressing unequal treatment in evaluations.
               Level: 3
               # of Papers: 17
               Example Papers: [(145, 'Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course'), (242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (450, 'PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data')]
               ----------------------------------------
          ----------------------------------------
     ----------------------------------------
     Label: task_specific_evaluation
     Dimension: evaluation_methods
     Description: This approach focuses on evaluating NLP models based on their performance in specific tasks, such as sentiment analysis or machine translation, to ensure relevance and applicability.
     Level: 1
     # of Papers: 1334
     Example Papers: [(4, 'Table Question Answering for Low-resourced Indic Languages'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (6, 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay')]
     ----------------------------------------
     Children:
          Label: machine_translation_evaluation
          Dimension: evaluation_methods
          Description: This subtopic focuses on evaluating the performance of machine translation systems, assessing their accuracy and fluency in translating text between different languages.
          Level: 2
          # of Papers: 173
          Example Papers: [(23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (34, 'Mitigating the Alignment Tax of RLHF'), (54, 'Chain-of-Dictionary Prompting Elicits Translation in Large Language Models')]
          ----------------------------------------
          Children:
               Label: automatic_evaluation_metrics
               Dimension: evaluation_methods
               Description: This subtopic focuses on quantitative metrics used to automatically assess the quality of machine translation outputs, such as BLEU, METEOR, and TER, which provide numerical scores based on various linguistic criteria.
               Level: 3
               # of Papers: 58
               Example Papers: [(23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (247, 'Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models'), (248, 'RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering')]
               ----------------------------------------
               Label: human_evaluation_methods
               Dimension: evaluation_methods
               Description: This subtopic encompasses qualitative approaches where human judges evaluate the translations for accuracy, fluency, and overall quality, often involving direct comparisons between machine-generated and human-generated translations.
               Level: 3
               # of Papers: 14
               Example Papers: [(572, 'An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance'), (633, 'MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language'), (1281, 'TransAgents: Build Your Translation Company with Language Agents')]
               ----------------------------------------
               Label: evaluation_benchmarks
               Label: error_analysis_techniques
               Dimension: evaluation_methods
               Description: This subtopic involves systematic methods for identifying and categorizing errors in machine translation outputs, helping to understand common pitfalls and areas for improvement in translation systems.
               Level: 3
               # of Papers: 43
               Example Papers: [(34, 'Mitigating the Alignment Tax of RLHF'), (187, 'Word Alignment as Preference for Machine Translation'), (213, 'What do Large Language Models Need for Machine Translation Evaluation?')]
               ----------------------------------------
               Label: user_studies_in_translation_evaluation
               Dimension: evaluation_methods
               Description: This subtopic examines the impact of user experience and preferences on the evaluation of machine translation systems, often involving surveys and usability testing to gather feedback from end-users.
               Level: 3
               # of Papers: 1
               Example Papers: [(2744, "Reference-Based Metrics Are Biased Against Blind and Low-Vision Users' Image Description Preferences")]
               ----------------------------------------
          ----------------------------------------
          Label: gender_bias_evaluation
          Dimension: evaluation_methods
          Description: This subtopic examines the presence and impact of gender bias in NLP models, evaluating how these models perform across different genders in various tasks.
          Level: 2
          # of Papers: 28
          Example Papers: [(12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (28, 'On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models'), (153, 'African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification')]
          ----------------------------------------
          Label: toxic_language_detection
          Dimension: evaluation_methods
          Description: This subtopic is dedicated to evaluating models designed to detect toxic language in text, ensuring their effectiveness in identifying harmful or offensive content.
          Level: 2
          # of Papers: 52
          Example Papers: [(9, 'Hateful Word in Context Classification'), (10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (114, 'CMD: a framework for Context-aware Model self-Detoxification')]
          ----------------------------------------
          Children:
               Label: automated_toxicity_scoring
               Dimension: evaluation_methods
               Description: This subtopic focuses on the development and evaluation of automated systems that assign toxicity scores to text, providing a quantitative measure of harmfulness.
               Level: 3
               # of Papers: 19
               Example Papers: [(9, 'Hateful Word in Context Classification'), (10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (135, 'Towards Low-Resource Harmful Meme Detection with LMM Agents')]
               ----------------------------------------
               Label: human_annotation_evaluation
               Dimension: evaluation_methods
               Description: This subtopic emphasizes the methods and practices involved in human annotation for toxic language detection, ensuring the reliability and accuracy of human judgments in identifying offensive content.
               Level: 3
               # of Papers: 6
               Example Papers: [(10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (621, 'Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation'), (1220, 'Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree')]
               ----------------------------------------
               Label: cross-linguistic_evaluation
               Dimension: evaluation_methods
               Description: This subtopic explores the evaluation of toxic language detection models across different languages, assessing their effectiveness and adaptability in diverse linguistic contexts.
               Level: 3
               # of Papers: 25
               Example Papers: [(135, 'Towards Low-Resource Harmful Meme Detection with LMM Agents'), (254, 'F^2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation'), (344, 'ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations')]
               ----------------------------------------
               Label: contextual_analysis_methods
               Dimension: evaluation_methods
               Description: This subtopic investigates evaluation methods that take into account the context in which language is used, enhancing the detection of toxicity by considering surrounding text and situational factors.
               Level: 3
               # of Papers: 20
               Example Papers: [(9, 'Hateful Word in Context Classification'), (114, 'CMD: a framework for Context-aware Model self-Detoxification'), (135, 'Towards Low-Resource Harmful Meme Detection with LMM Agents')]
               ----------------------------------------
               Label: real_time_detection_evaluation
               Dimension: evaluation_methods
               Description: This subtopic is dedicated to evaluating models designed for real-time toxic language detection, focusing on their performance and responsiveness in live environments.
               Level: 3
               # of Papers: 22
               Example Papers: [(135, 'Towards Low-Resource Harmful Meme Detection with LMM Agents'), (254, 'F^2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation'), (518, 'APPLS: Evaluating Evaluation Metrics for Plain Language Summarization')]
               ----------------------------------------
          ----------------------------------------
          Label: arabic_sentiment_analysis
          Dimension: evaluation_methods
          Description: This subtopic focuses on the evaluation of sentiment analysis models specifically tailored for Arabic text, assessing their ability to accurately interpret sentiments in this language.
          Level: 2
          # of Papers: 6
          Example Papers: [(153, 'African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification'), (2387, 'Gazelle: An Instruction Dataset for Arabic Writing Assistance'), (2435, 'BiMediX: Bilingual Medical Mixture of Experts LLM')]
          ----------------------------------------
          Label: clickbait_detection
          Dimension: evaluation_methods
          Description: This subtopic evaluates models that identify clickbait content, measuring their effectiveness in distinguishing misleading headlines from genuine ones.
          Level: 2
          # of Papers: 7
          Example Papers: [(30, 'On Fake News Detection with LLM Enhanced Semantics Mining'), (153, 'African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification'), (574, 'Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference')]
          ----------------------------------------
          Label: natural_language_inference_evaluation
          Dimension: evaluation_methods
          Description: This subtopic focuses on evaluating models that perform natural language inference, assessing their ability to determine the logical relationship between pairs of sentences.
          Level: 2
          # of Papers: 260
          Example Papers: [(35, 'Evaluating Readability and Faithfulness of Concept-based Explanations'), (43, 'GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation'), (52, 'Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs')]
          ----------------------------------------
          Children:
               Label: logical_relationship_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on evaluating models based on their ability to accurately identify and classify the logical relationships, such as entailment, contradiction, and neutrality, between pairs of sentences.
               Level: 3
               # of Papers: 28
               Example Papers: [(171, 'Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving'), (313, 'LogicST: A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations'), (498, 'MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents')]
               ----------------------------------------
               Label: robustness_testing
               Dimension: evaluation_methods
               Description: This subtopic examines the resilience of natural language inference models against adversarial examples and variations in input, assessing their performance under challenging conditions.
               Level: 3
               # of Papers: 31
               Example Papers: [(70, 'ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval'), (91, 'Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?'), (137, 'Direct Multi-Turn Preference Optimization for Language Agents')]
               ----------------------------------------
               Label: generalization_assessment
               Dimension: evaluation_methods
               Description: This subtopic evaluates how well natural language inference models generalize their learned knowledge to unseen data, ensuring that they can apply their reasoning skills beyond the training set.
               Level: 3
               # of Papers: 116
               Example Papers: [(52, 'Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs'), (70, 'ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval'), (91, 'Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?')]
               ----------------------------------------
               Label: explainability_evaluation
               Dimension: evaluation_methods
               Description: This subtopic investigates the interpretability of natural language inference models, focusing on how well they can provide understandable justifications for their predictions regarding sentence pairs.
               Level: 3
               # of Papers: 36
               Example Papers: [(35, 'Evaluating Readability and Faithfulness of Concept-based Explanations'), (171, 'Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving'), (251, 'ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback')]
               ----------------------------------------
               Label: cross_domain_evaluation
          ----------------------------------------
          Label: sentiment_analysis_evaluation
          Dimension: evaluation_methods
          Description: This subtopic is dedicated to evaluating sentiment analysis models, measuring their effectiveness in accurately interpreting and classifying sentiments expressed in text.
          Level: 2
          # of Papers: 144
          Example Papers: [(7, 'When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection'), (35, 'Evaluating Readability and Faithfulness of Concept-based Explanations'), (39, 'Tokenization Is More Than Compression')]
          ----------------------------------------
          Children:
               Label: accuracy_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on measuring the accuracy of sentiment analysis models by comparing their predictions against ground truth labels to determine the proportion of correct classifications.
               Level: 3
               # of Papers: 11
               Example Papers: [(882, 'ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models'), (948, 'Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation'), (1006, 'Calibrating Language Models with Adaptive Temperature Scaling')]
               ----------------------------------------
               Label: precision_recall_evaluation
               Dimension: evaluation_methods
               Description: This subtopic emphasizes the evaluation of sentiment analysis models through precision and recall metrics, assessing the balance between correctly identified positive sentiments and the total number of predicted positive sentiments.
               Level: 3
               # of Papers: 10
               Example Papers: [(305, 'How Far Can We Extract Diverse Perspectives from Large Language Models?'), (882, 'ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models'), (1238, 'Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text')]
               ----------------------------------------
               Label: f1_score_evaluation
               Dimension: evaluation_methods
               Description: This subtopic is dedicated to evaluating sentiment analysis models using the F1 score, which combines precision and recall into a single metric to provide a balanced measure of model performance.
               Level: 3
               # of Papers: 24
               Example Papers: [(35, 'Evaluating Readability and Faithfulness of Concept-based Explanations'), (48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue'), (161, 'Message Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification')]
               ----------------------------------------
               Label: confusion_matrix_analysis
               Dimension: evaluation_methods
               Description: This subtopic involves the use of confusion matrices to evaluate sentiment analysis models, allowing for a detailed breakdown of true positives, false positives, true negatives, and false negatives.
               Level: 3
               # of Papers: 8
               Example Papers: [(882, 'ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models'), (1238, 'Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text'), (1363, 'Refining App Reviews: Dataset, Methodology, and Evaluation')]
               ----------------------------------------
               Label: cross_validation_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on the application of cross-validation techniques to assess the robustness and generalizability of sentiment analysis models across different subsets of data.
               Level: 3
               # of Papers: 96
               Example Papers: [(43, 'GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation'), (48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue'), (77, 'Model Balancing Helps Low-data Training and Fine-tuning')]
               ----------------------------------------
          ----------------------------------------
          Label: patent_citation_evaluation
          Dimension: evaluation_methods
          Description: This subtopic examines the evaluation of models that analyze patent citations, assessing their accuracy in identifying and interpreting citation relationships.
          Level: 2
          # of Papers: 5
          Example Papers: [(22, 'An Experimental Analysis on Evaluating Patent Citations'), (153, 'African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification'), (747, 'DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models')]
          ----------------------------------------
          Label: logic_reasoning_evaluation
          Dimension: evaluation_methods
          Description: This subtopic focuses on evaluating models that perform logical reasoning tasks, measuring their effectiveness in drawing conclusions based on given premises.
          Level: 2
          # of Papers: 285
          Example Papers: [(12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (26, 'Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation'), (31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices')]
          ----------------------------------------
          Children:
               Label: evaluation_benchmark
               Dimension: evaluation_methods
               Description: This subtopic refers to standardized tests and benchmarks designed to evaluate the effectiveness of logical reasoning models, providing a reference point for comparison.
               Level: 3
               # of Papers: 242
               Example Papers: [(12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (26, 'Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation'), (31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices')]
               ----------------------------------------
               Label: conceptual_reasoning
               Dimension: evaluation_methods
               Description: This subtopic focuses on the evaluation of models that engage in reasoning about abstract concepts and relationships, measuring their capability to understand and manipulate conceptual information.
               Level: 3
               # of Papers: 51
               Example Papers: [(12, '"Thinking" Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models'), (127, 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models'), (221, 'Conditional and Modal Reasoning in Large Language Models')]
               ----------------------------------------
               Label: temporal_reasoning_evaluation
               Dimension: evaluation_methods
               Description: This subtopic is dedicated to assessing models that perform reasoning involving time-related information, evaluating their ability to understand and infer temporal relationships.
               Level: 3
               # of Papers: 14
               Example Papers: [(434, 'UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models'), (579, 'CARER - ClinicAl Reasoning-Enhanced Representation for Temporal Health Risk Prediction'), (642, 'Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models')]
               ----------------------------------------
          ----------------------------------------
          Label: hallucination_detection_evaluation
          Dimension: evaluation_methods
          Description: This subtopic is dedicated to evaluating models that detect hallucinations in generated text, ensuring their reliability in producing accurate and coherent outputs.
          Level: 2
          # of Papers: 194
          Example Papers: [(10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (61, 'RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models'), (104, 'HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding')]
          ----------------------------------------
          Children:
               Label: qualitative_analysis
               Dimension: evaluation_methods
               Description: This subtopic focuses on qualitative methods for evaluating hallucination detection, emphasizing human judgment and interpretative assessments of model outputs.
               Level: 3
               # of Papers: 1
               Example Papers: [(1637, 'Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning')]
               ----------------------------------------
               Label: quantitative_metrics
               Dimension: evaluation_methods
               Description: This subtopic encompasses quantitative evaluation methods that utilize statistical measures to assess the performance of hallucination detection models.
               Level: 3
               # of Papers: 14
               Example Papers: [(316, 'NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian'), (342, 'SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales'), (352, 'Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation')]
               ----------------------------------------
               Label: benchmarking_studies
               Dimension: evaluation_methods
               Description: This subtopic involves comparative studies that benchmark various hallucination detection models against established datasets and metrics.
               Level: 3
               # of Papers: 112
               Example Papers: [(10, "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze"), (104, 'HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding'), (151, 'Knowledge Verification to Nip Hallucination in the Bud')]
               ----------------------------------------
               Label: user_studies
               Label: error_analysis
          ----------------------------------------
     ----------------------------------------
     Label: cross_language_evaluation
     Dimension: evaluation_methods
     Description: This evaluation method examines the performance of NLP models across different languages, highlighting their adaptability and effectiveness in multilingual contexts.
     Level: 1
     # of Papers: 443
     Example Papers: [(4, 'Table Question Answering for Low-resourced Indic Languages'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts')]
     ----------------------------------------
     Children:
          Label: multilingual_evaluation
          Dimension: evaluation_methods
          Description: This cluster focuses on evaluating NLP models' performance across multiple languages, assessing their effectiveness and adaptability in multilingual contexts.
          Level: 2
          # of Papers: 102
          Example Papers: [(60, 'Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval'), (235, 'When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages'), (238, 'Teaching LLMs to Abstain across Languages via Multilingual Feedback')]
          ----------------------------------------
          Children:
               Label: cross_language_recipe_retrieval
               Dimension: evaluation_methods
               Description: This subtopic specifically addresses the evaluation of systems designed to retrieve recipes across different languages, analyzing their accuracy and relevance in multilingual settings.
               Level: 3
               # of Papers: 1
               Example Papers: [(60, 'Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval')]
               ----------------------------------------
               Label: multilingual_feedback
               Dimension: evaluation_methods
               Description: This subtopic focuses on the evaluation of feedback mechanisms in multilingual systems, assessing how effectively user input is gathered and utilized across different languages.
               Level: 3
               # of Papers: 5
               Example Papers: [(238, 'Teaching LLMs to Abstain across Languages via Multilingual Feedback'), (450, 'PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data'), (728, 'RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs')]
               ----------------------------------------
               Label: multilingual_translation
               Dimension: evaluation_methods
               Description: This subtopic examines the evaluation of translation quality in multilingual contexts, analyzing the accuracy and fluency of translations produced by various systems.
               Level: 3
               # of Papers: 83
               Example Papers: [(235, 'When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages'), (246, 'An Analysis of Multilingual FActScore'), (322, 'On Mitigating Performance Disparities in Multilingual Speech Recognition')]
               ----------------------------------------
          ----------------------------------------
          Label: zero_shot_cross_lingual_evaluation
          Dimension: evaluation_methods
          Description: This cluster examines the ability of NLP models to perform evaluations in a cross-lingual setting without prior training on the target language, highlighting their generalization capabilities.
          Level: 2
          # of Papers: 30
          Example Papers: [(78, 'Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment'), (175, 'Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection'), (395, 'Revealing the Parallel Multilingual Learning within Large Language Models')]
          ----------------------------------------
          Label: cross-lingual_transfer_learning
          Dimension: evaluation_methods
          Description: This cluster investigates the methods and effectiveness of transferring knowledge from one language to another in NLP models, emphasizing the evaluation of their performance in cross-lingual tasks.
          Level: 2
          # of Papers: 249
          Example Papers: [(4, 'Table Question Answering for Low-resourced Indic Languages'), (23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds')]
          ----------------------------------------
          Children:
               Label: evaluation_procedures
               Dimension: evaluation_methods
               Description: This subtopic outlines the specific procedures and protocols followed during the evaluation of cross-lingual transfer learning, detailing the steps taken to ensure rigorous and reproducible assessments.
               Level: 3
               # of Papers: 245
               Example Papers: [(4, 'Table Question Answering for Low-resourced Indic Languages'), (23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds')]
               ----------------------------------------
          ----------------------------------------
          Label: multilingual_model_analysis
          Dimension: evaluation_methods
          Description: This cluster encompasses the evaluation of various multilingual models, focusing on their strengths and weaknesses in handling tasks across different languages.
          Level: 2
          # of Papers: 111
          Example Papers: [(54, 'Chain-of-Dictionary Prompting Elicits Translation in Large Language Models'), (79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment'), (193, 'Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models')]
          ----------------------------------------
          Children:
               Label: cross_lingual_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on evaluating the performance of multilingual models across different languages, assessing their ability to transfer knowledge and skills from one language to another.
               Level: 3
               # of Papers: 71
               Example Papers: [(235, 'When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages'), (239, 'Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration'), (314, 'Concept Space Alignment in Multilingual LLMs')]
               ----------------------------------------
               Label: evaluation_of_translation_models
               Dimension: evaluation_methods
               Description: This subtopic encompasses the assessment of various translation models, analyzing their effectiveness and accuracy in translating text between multiple languages.
               Level: 3
               # of Papers: 23
               Example Papers: [(54, 'Chain-of-Dictionary Prompting Elicits Translation in Large Language Models'), (462, 'Attribute or Abstain: Large Language Models as Long Document Assistants'), (823, 'Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks')]
               ----------------------------------------
               Label: cross_cultural_evaluation
               Dimension: evaluation_methods
               Description: This subtopic examines how multilingual models perform in diverse cultural contexts, evaluating their sensitivity and adaptability to cultural nuances in language processing.
               Level: 3
               # of Papers: 15
               Example Papers: [(450, 'PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data'), (531, 'Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the US'), (661, 'Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications')]
               ----------------------------------------
               Label: cross_language_transfer
               Dimension: evaluation_methods
               Description: This subtopic investigates the ability of multilingual models to leverage knowledge from one language to improve performance in another, focusing on the mechanisms of transfer learning in multilingual settings.
               Level: 3
               # of Papers: 84
               Example Papers: [(54, 'Chain-of-Dictionary Prompting Elicits Translation in Large Language Models'), (79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment'), (193, 'Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models')]
               ----------------------------------------
          ----------------------------------------
          Label: cross_language_generalization
          Dimension: evaluation_methods
          Description: This cluster evaluates how well NLP models generalize their learning from one language to another, assessing their performance in diverse linguistic contexts.
          Level: 2
          # of Papers: 196
          Example Papers: [(23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds'), (58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs')]
          ----------------------------------------
          Children:
               Label: cross_language_model_transfer
               Dimension: evaluation_methods
               Description: This subtopic evaluates the effectiveness of transferring models trained in one language to perform tasks in another language, focusing on the adaptability and performance of the models across linguistic boundaries.
               Level: 3
               # of Papers: 107
               Example Papers: [(23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs'), (79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment')]
               ----------------------------------------
               Label: cross_language_performance_benchmarking
               Dimension: evaluation_methods
               Description: This subtopic involves the systematic assessment of NLP models' performance across multiple languages, providing a comparative analysis of their capabilities in diverse linguistic contexts.
               Level: 3
               # of Papers: 76
               Example Papers: [(235, 'When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages'), (238, 'Teaching LLMs to Abstain across Languages via Multilingual Feedback'), (334, 'Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval')]
               ----------------------------------------
               Label: cross_language_task_adaptation
               Dimension: evaluation_methods
               Description: This subtopic examines methods for adapting NLP tasks from one language to another, assessing how well models can adjust to different linguistic structures and semantics.
               Level: 3
               # of Papers: 96
               Example Papers: [(23, 'Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?'), (60, 'Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval'), (94, 'Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective')]
               ----------------------------------------
               Label: cross_language_error_analysis
               Dimension: evaluation_methods
               Description: This subtopic focuses on identifying and analyzing errors made by NLP models when processing languages different from their training language, aiming to uncover common pitfalls and areas for improvement.
               Level: 3
               # of Papers: 39
               Example Papers: [(112, 'Do We Need Language-Specific Fact-Checking Models? The Case of Chinese'), (238, 'Teaching LLMs to Abstain across Languages via Multilingual Feedback'), (246, 'An Analysis of Multilingual FActScore')]
               ----------------------------------------
               Label: cross_language_dataset_evaluation
               Dimension: evaluation_methods
               Description: This subtopic assesses the quality and effectiveness of datasets used for training and evaluating NLP models across languages, ensuring they are representative and suitable for cross-language tasks.
               Level: 3
               # of Papers: 85
               Example Papers: [(38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds'), (60, 'Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval'), (94, 'Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective')]
               ----------------------------------------
          ----------------------------------------
     ----------------------------------------
     Label: real_time_evaluation
     Dimension: evaluation_methods
     Description: This method assesses NLP models in real-time scenarios, measuring their performance and responsiveness during live interactions or applications.
     Level: 1
     # of Papers: 196
     Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (34, 'Mitigating the Alignment Tax of RLHF')]
     ----------------------------------------
     Children:
          Label: online_evaluation
          Dimension: evaluation_methods
          Description: This subtopic focuses on evaluating NLP models in real-time settings, specifically during live interactions, to assess their performance and responsiveness.
          Level: 2
          # of Papers: 104
          Example Papers: [(38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds'), (75, 'QUDSELECT: Selective Decoding for Questions Under Discussion Parsing'), (79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment')]
          ----------------------------------------
          Children:
               Label: human_in_the_loop_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on evaluation methods that incorporate human feedback and interaction during the evaluation process, enhancing the assessment of NLP models in real-time settings.
               Level: 3
               # of Papers: 5
               Example Papers: [(38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds'), (1358, 'ReportGPT: Human-in-the-loop Verifiable Table-to-Text Generation'), (1742, 'Rater Cohesion and Quality from a Vicarious Perspective')]
               ----------------------------------------
               Label: online_evaluation_metrics
               Dimension: evaluation_methods
               Description: This subtopic encompasses the various metrics and criteria specifically designed to evaluate the performance of NLP models in real-time interactions, ensuring accurate and relevant assessments.
               Level: 3
               # of Papers: 91
               Example Papers: [(75, 'QUDSELECT: Selective Decoding for Questions Under Discussion Parsing'), (79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment'), (91, 'Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?')]
               ----------------------------------------
               Label: copyright_evaluation
               Dimension: evaluation_methods
               Description: This subtopic addresses the evaluation of NLP models in terms of copyright compliance and intellectual property considerations during online interactions.
               Level: 3
               # of Papers: 1
               Example Papers: [(97, 'SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation')]
               ----------------------------------------
               Label: cross_document_evaluation
               Dimension: evaluation_methods
               Description: This subtopic involves evaluating NLP models based on their performance across multiple documents in real-time, assessing their ability to maintain context and coherence.
               Level: 3
               # of Papers: 1
               Example Papers: [(2113, 'MDCR: A Dataset for Multi-Document Conditional Reasoning')]
               ----------------------------------------
          ----------------------------------------
          Label: model_pruning
          Dimension: evaluation_methods
          Description: This subtopic involves techniques for reducing the size of NLP models in real-time applications, ensuring efficient performance without significant loss of accuracy.
          Level: 2
          # of Papers: 6
          Example Papers: [(774, 'Structured Optimal Brain Pruning for Large Language Models'), (1003, 'Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning'), (1100, 'Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval')]
          ----------------------------------------
          Label: online_learning
          Dimension: evaluation_methods
          Description: This subtopic pertains to the continuous learning of NLP models in real-time, allowing them to adapt and improve based on new data encountered during live interactions.
          Level: 2
          # of Papers: 19
          Example Papers: [(44, 'DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities'), (77, 'Model Balancing Helps Low-data Training and Fine-tuning'), (424, 'Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning')]
          ----------------------------------------
          Label: online_adaptation
          Dimension: evaluation_methods
          Description: This subtopic covers methods for adapting NLP models in real-time to changing conditions or user inputs, enhancing their relevance and effectiveness.
          Level: 2
          # of Papers: 28
          Example Papers: [(44, 'DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities'), (77, 'Model Balancing Helps Low-data Training and Fine-tuning'), (380, 'Can Large Language Models Learn Independent Causal Mechanisms?')]
          ----------------------------------------
          Label: online_misinformation_evaluation
          Dimension: evaluation_methods
          Description: This subtopic focuses on assessing the ability of NLP models to identify and respond to misinformation in real-time scenarios, ensuring accurate and reliable interactions.
          Level: 2
          # of Papers: 9
          Example Papers: [(486, 'MisinfoEval: Generative AI in the Era of "Alternative Facts"'), (845, 'Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach'), (893, 'FAME: Towards Factual Multi-Task Model Editing')]
          ----------------------------------------
     ----------------------------------------
     Label: longitudinal_studies
     Dimension: evaluation_methods
     Description: This approach involves evaluating NLP systems over extended periods to understand their performance trends and stability in dynamic environments.
     Level: 1
     # of Papers: 150
     Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (21, '"We Demand Justice!": Towards Social Context Grounding of Political Texts'), (34, 'Mitigating the Alignment Tax of RLHF')]
     ----------------------------------------
     Children:
          Label: evaluation_of_nlp_performance
          Dimension: evaluation_methods
          Description: This subtopic focuses on assessing the overall performance of NLP systems over time, analyzing metrics such as accuracy, precision, and recall in dynamic environments.
          Level: 2
          # of Papers: 3
          Example Papers: [(1615, 'Evaluating Automatic Metrics with Incremental Machine Translation Systems'), (1758, 'Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts'), (2733, 'An NLP Case Study on Predicting the Before and After of the Ukraine-Russia and Hamas-Israel Conflicts')]
          ----------------------------------------
          Label: evaluation_of_temporal_fluctuations
          Dimension: evaluation_methods
          Description: This subtopic examines the variations in NLP system performance due to temporal factors, exploring how changes in data or context affect outcomes over extended periods.
          Level: 2
          # of Papers: 4
          Example Papers: [(939, "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks"), (1097, 'Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models'), (1615, 'Evaluating Automatic Metrics with Incremental Machine Translation Systems')]
          ----------------------------------------
          Label: evaluation_of_relationships
          Dimension: evaluation_methods
          Description: This subtopic investigates the relationships between different NLP models and their performance trends over time, aiming to understand how these interactions influence overall effectiveness.
          Level: 2
          # of Papers: 1
          Example Papers: [(2001, 'Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy')]
          ----------------------------------------
          Label: evaluation_of_conversational_dynamics
          Dimension: evaluation_methods
          Description: This subtopic evaluates how NLP systems perform in conversational settings over time, focusing on aspects such as coherence, engagement, and user satisfaction in dynamic dialogues.
          Level: 2
          # of Papers: 1
          Example Papers: [(2773, 'Personality Differences Drive Conversational Dynamics: A High-Dimensional NLP Approach')]
          ----------------------------------------
          Label: evaluation_of_nlp_models
          Dimension: evaluation_methods
          Description: This subtopic is dedicated to the longitudinal assessment of specific NLP models, analyzing their stability and adaptability in response to evolving datasets and tasks.
          Level: 2
          # of Papers: 128
          Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds'), (58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs')]
          ----------------------------------------
          Children:
               Label: consistency_evaluation
               Dimension: evaluation_methods
               Description: This subtopic focuses on measuring the consistency of NLP model outputs over time and across varying input conditions to ensure reliability.
               Level: 3
               # of Papers: 22
               Example Papers: [(91, 'Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?'), (174, 'How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics'), (289, 'FOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation')]
               ----------------------------------------
               Label: benchmarking_nlp_models
               Dimension: evaluation_methods
               Description: This subtopic involves the systematic comparison of NLP models against established benchmarks to evaluate their performance and capabilities.
               Level: 3
               # of Papers: 92
               Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (38, 'CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds'), (58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs')]
               ----------------------------------------
               Label: continual_learning
               Dimension: evaluation_methods
               Description: This subtopic examines the ability of NLP models to learn and adapt continuously from new data without forgetting previously acquired knowledge.
               Level: 3
               # of Papers: 31
               Example Papers: [(79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment'), (276, 'KidLM: Advancing Language Models for Children - Early Insights and Future Directions'), (585, 'Belief Revision: The Adaptability of Large Language Models Reasoning')]
               ----------------------------------------
               Label: bias_reduction
               Dimension: evaluation_methods
               Description: This subtopic addresses methods and strategies aimed at identifying and mitigating biases present in NLP models to promote fairness and equity.
               Level: 3
               # of Papers: 21
               Example Papers: [(107, 'Evaluating Psychological Safety of Large Language Models'), (242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (326, 'The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse')]
               ----------------------------------------
          ----------------------------------------
     ----------------------------------------
----------------------------------------
