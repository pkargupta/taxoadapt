Label: natural_language_processing
Dimension: methodologies
Description: None
Level: 0
# of Papers: 2392
Example Papers: [(0, 'UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation'), (1, 'Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation'), (2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document')]
----------------------------------------
Children:
     Label: statistical_methods
     Dimension: methodologies
     Description: This methodology focuses on the use of statistical techniques to analyze and model linguistic data, enabling the extraction of patterns and insights from large corpora.
     Level: 1
     # of Papers: 41
     Example Papers: [(31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices'), (131, 'Oddballs and Misfits: Detecting Implicit Abuse in Which Identity Groups are Depicted as Deviating from the Norm'), (168, 'Understanding Higher-Order Correlations Among Semantic Components in Embeddings')]
     ----------------------------------------
     Children:
          Label: descriptive_statistics
          Dimension: methodologies
          Description: Descriptive statistics involves summarizing and organizing data to understand its main characteristics, often using measures such as mean, median, mode, and standard deviation.
          Level: 2
          # of Papers: 8
          Example Papers: [(865, 'CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models'), (1543, 'PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models'), (2153, 'Numbers Matter! Bringing Quantity-awareness to Retrieval Systems')]
          ----------------------------------------
          Label: inferential_statistics
          Dimension: methodologies
          Description: Inferential statistics allows researchers to make predictions or inferences about a population based on a sample of data, utilizing techniques such as hypothesis testing and confidence intervals.
          Level: 2
          # of Papers: 20
          Example Papers: [(31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices'), (268, 'Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-Context Models'), (509, 'Statistical Uncertainty in Word Embeddings: GloVe-V')]
          ----------------------------------------
          Label: regression_analysis
          Dimension: methodologies
          Description: Regression analysis is a statistical method used to examine the relationship between dependent and independent variables, helping to predict outcomes and identify trends.
          Level: 2
          # of Papers: 6
          Example Papers: [(915, 'Scaling Laws for Linear Complexity Language Models'), (2153, 'Numbers Matter! Bringing Quantity-awareness to Retrieval Systems'), (2488, 'Lossy Context Surprisal Predicts Task-Dependent Patterns in Relative Clause Processing')]
          ----------------------------------------
          Label: bayesian_statistics
          Dimension: methodologies
          Description: Bayesian statistics is an approach that incorporates prior knowledge or beliefs into the statistical analysis, updating the probability of a hypothesis as more evidence becomes available.
          Level: 2
          # of Papers: 10
          Example Papers: [(31, 'On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices'), (535, 'Precise Model Benchmarking with Only a Few Observations'), (653, 'What Are the Odds? Language Models Are Capable of Probabilistic Reasoning')]
          ----------------------------------------
          Label: multivariate_analysis
          Dimension: methodologies
          Description: Multivariate analysis involves examining multiple variables simultaneously to understand their relationships and effects, often used in complex data sets.
          Level: 2
          # of Papers: 6
          Example Papers: [(168, 'Understanding Higher-Order Correlations Among Semantic Components in Embeddings'), (509, 'Statistical Uncertainty in Word Embeddings: GloVe-V'), (1064, 'Exploring Intra and Inter-language Consistency in Embeddings with ICA')]
          ----------------------------------------
     ----------------------------------------
     Label: machine_learning_approaches
     Dimension: methodologies
     Description: This methodology encompasses various machine learning techniques, including supervised and unsupervised learning, to develop models that can understand and generate human language.
     Level: 1
     # of Papers: 1882
     Example Papers: [(0, 'UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation'), (1, 'Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation'), (2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document')]
     ----------------------------------------
     Children:
          Label: supervised_learning
          Dimension: methodologies
          Description: A method where the model is trained on labeled data, allowing it to learn the relationship between input features and the corresponding output labels.
          Level: 2
          # of Papers: 284
          Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (22, 'An Experimental Analysis on Evaluating Patent Citations')]
          ----------------------------------------
          Children:
               Label: linear_regression
               Dimension: methodologies
               Description: Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.
               Level: 3
               # of Papers: 1
               Example Papers: [(1171, 'BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability')]
               ----------------------------------------
               Label: logistic_regression
               Dimension: methodologies
               Description: Logistic regression is a predictive analysis algorithm that is used for binary classification problems, modeling the probability of a certain class or event existing.
               Level: 3
               # of Papers: 1
               Example Papers: [(1171, 'BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability')]
               ----------------------------------------
               Label: support_vector_machines
               Dimension: methodologies
               Description: Support Vector Machines (SVM) are supervised learning models that analyze data for classification and regression analysis, aiming to find the hyperplane that best separates different classes.
               Level: 3
               # of Papers: 3
               Example Papers: [(634, 'Revisiting Supertagging for faster HPSG parsing'), (1171, 'BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability'), (1786, 'Exploiting Careful Design of SVM Solution for Aspect-term Sentiment Analysis')]
               ----------------------------------------
               Label: decision_trees
               Dimension: methodologies
               Description: Decision trees are a rule-based methodology that uses a tree-like model of decisions and their possible consequences, providing a clear and interpretable structure for decision-making.
               Level: 2
               # of Papers: 3
               Example Papers: [(908, 'Exploring Space Efficiency in a Tree-based Linear Model for Extreme Multi-label Classification'), (1171, 'BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability'), (2589, 'An Incremental Clustering Baseline for Event Detection on Twitter')]
               ----------------------------------------
               Label: random_forests
               Dimension: methodologies
               Description: Random forests are an ensemble learning method that constructs multiple decision trees during training and outputs the mode of their classifications for improved accuracy and robustness.
               Level: 3
               # of Papers: 2
               Example Papers: [(1010, 'NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition'), (2271, 'Random Label Forests: An Ensemble Method with Label Subsampling For Extreme Multi-Label Problems')]
               ----------------------------------------
               Label: transfer_learning
               Dimension: methodologies
               Description: A technique where a model developed for a particular task is reused as the starting point for a model on a second task, leveraging previously learned knowledge.
               Level: 2
               # of Papers: 216
               Example Papers: [(175, 'Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection'), (428, 'More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs'), (675, 'TL-CL: Task And Language Incremental Continual Learning')]
               ----------------------------------------
               Children:
                    Label: domain_adaptation
                    Dimension: methodologies
                    Description: Domain adaptation is a technique in transfer learning that focuses on adapting a model trained on one domain to perform well on a different but related domain.
                    Level: 3
                    # of Papers: 2
                    Example Papers: [(428, 'More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs'), (675, 'TL-CL: Task And Language Incremental Continual Learning')]
                    ----------------------------------------
                    Label: fine_tuning
                    Dimension: methodologies
                    Description: Fine-tuning involves taking a pre-trained model and making small adjustments to its parameters on a new dataset to improve performance on a specific task.
                    Level: 3
                    # of Papers: 7
                    Example Papers: [(175, 'Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection'), (428, 'More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs'), (675, 'TL-CL: Task And Language Incremental Continual Learning')]
                    ----------------------------------------
                    Label: multi-task_learning
                    Dimension: methodologies
                    Description: Multi-task learning simultaneously trains a model on multiple related tasks, allowing it to generalize better and share representations across different natural language processing challenges.
                    Level: 2
                    # of Papers: 125
                    Example Papers: [(13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (27, 'Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation'), (30, 'On Fake News Detection with LLM Enhanced Semantics Mining')]
                    ----------------------------------------
                    Children:
                         Label: task_specific_loss_functions
                         Dimension: methodologies
                         Description: This approach involves designing unique loss functions for each task to optimize their performance simultaneously during training.
                         Level: 3
                         # of Papers: 1
                         Example Papers: [(2356, 'Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method')]
                         ----------------------------------------
                         Label: shared_representation_learning
                         Dimension: methodologies
                         Description: In this methodology, a common representation is learned across multiple tasks, allowing for shared knowledge and improved generalization.
                         Level: 3
                         # of Papers: 47
                         Example Papers: [(64, 'DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing'), (100, 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering'), (175, 'Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection')]
                         ----------------------------------------
                         Label: parameter_sharing
                         Dimension: methodologies
                         Description: This technique utilizes the same model parameters across different tasks, reducing the overall model size and improving efficiency.
                         Level: 3
                         # of Papers: 61
                         Example Papers: [(27, 'Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation'), (64, 'DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing'), (100, 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering')]
                         ----------------------------------------
                         Label: multi_task_regularization
                         Dimension: methodologies
                         Description: This method applies regularization techniques that encourage the model to learn from multiple tasks, preventing overfitting and enhancing robustness.
                         Level: 3
                         # of Papers: 117
                         Example Papers: [(27, 'Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation'), (49, "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification"), (60, 'Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval')]
                         ----------------------------------------
                         Label: task_relation_exploitation
                         Dimension: methodologies
                         Description: This approach focuses on identifying and leveraging the relationships between tasks to improve learning outcomes and task performance.
                         Level: 3
                         # of Papers: 62
                         Example Papers: [(49, "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification"), (64, 'DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing'), (100, 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering')]
                         ----------------------------------------
                    ----------------------------------------
                    Label: feature_extraction
                    Dimension: methodologies
                    Description: Feature extraction in transfer learning involves using a pre-trained model to extract relevant features from new data, which can then be used for training a simpler model.
                    Level: 3
                    # of Papers: 2
                    Example Papers: [(428, 'More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs'), (1541, 'Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering')]
                    ----------------------------------------
                    Label: zero_shot_learning
                    Dimension: methodologies
                    Description: Zero-shot learning is a transfer learning approach where a model is able to recognize and classify objects or tasks it has never seen before by leveraging knowledge from related tasks.
                    Level: 3
                    # of Papers: 12
                    Example Papers: [(68, 'LLMs Are Zero-Shot Context-Aware Simultaneous Translators'), (159, 'Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment'), (627, 'Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations')]
                    ----------------------------------------
               ----------------------------------------
               Label: ensemble_learning
               Dimension: methodologies
               Description: Ensemble learning combines multiple models to improve the overall performance and robustness of predictions in natural language processing tasks.
               Level: 2
               # of Papers: 63
               Example Papers: [(24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (49, "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification"), (129, 'FuseGen: PLM Fusion for Data-generation based Zero-shot Learning')]
               ----------------------------------------
               Children:
                    Label: bagging
                    Dimension: methodologies
                    Description: Bagging, or bootstrap aggregating, is an ensemble method that improves the stability and accuracy of machine learning algorithms by training multiple models on different subsets of the training data and averaging their predictions.
                    Level: 3
                    # of Papers: 1
                    Example Papers: [(1010, 'NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition')]
                    ----------------------------------------
                    Label: boosting
                    Dimension: methodologies
                    Description: Boosting is an ensemble technique that combines the predictions of several base learners, typically weak learners, by sequentially training them to correct the errors made by previous models, thereby enhancing overall performance.
                    Level: 3
                    # of Papers: 1
                    Example Papers: [(1010, 'NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition')]
                    ----------------------------------------
                    Label: stacking
                    Dimension: methodologies
                    Description: Stacking is an ensemble learning method that involves training multiple models (base learners) and then using another model (meta-learner) to combine their predictions, allowing for more complex decision-making.
                    Level: 3
                    # of Papers: 2
                    Example Papers: [(961, 'MedCoT: Medical Chain of Thought via Hierarchical Expert'), (1010, 'NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity Recognition')]
                    ----------------------------------------
                    Label: random_forests
                    Label: blending
                    Dimension: methodologies
                    Description: Blending is a technique similar to stacking, where different models are trained on the same dataset and their predictions are combined using a simple model, often a linear regression, to improve predictive accuracy.
                    Level: 3
                    # of Papers: 23
                    Example Papers: [(24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (249, 'PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval'), (370, 'Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts')]
                    ----------------------------------------
               ----------------------------------------
               Label: generative_models
               Dimension: methodologies
               Description: Generative models are a class of models that learn to generate new data points from the same distribution as the training data.
               Level: 3
               # of Papers: 80
               Example Papers: [(24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (39, 'Tokenization Is More Than Compression'), (48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue')]
               ----------------------------------------
               Label: reinforcement_learning
               Dimension: methodologies
               Description: A type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.
               Level: 2
               # of Papers: 396
               Example Papers: [(6, 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay'), (18, 'RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning'), (19, 'Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing')]
               ----------------------------------------
               Children:
                    Label: value_based_methods
                    Dimension: methodologies
                    Description: Value-based methods focus on estimating the value of states or actions to make decisions, with techniques like Q-learning and Deep Q-Networks (DQN) being prominent examples.
                    Level: 3
                    # of Papers: 2
                    Example Papers: [(975, "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration"), (1688, 'Inference-Time Language Model Alignment via Integrated Value Guidance')]
                    ----------------------------------------
                    Label: policy_gradient_methods
                    Dimension: methodologies
                    Description: Policy gradient methods optimize the policy directly by adjusting the parameters in the direction of higher expected rewards, allowing for more flexible and complex policy representations.
                    Level: 3
                    # of Papers: 1
                    Example Papers: [(1189, 'Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion')]
                    ----------------------------------------
                    Label: actor_critic_methods
                    Dimension: methodologies
                    Description: Actor-critic methods combine the benefits of value-based and policy-based approaches by using an actor to propose actions and a critic to evaluate them, improving learning efficiency.
                    Level: 3
                    # of Papers: 20
                    Example Papers: [(37, 'MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making'), (137, 'Direct Multi-Turn Preference Optimization for Language Agents'), (138, 'Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models')]
                    ----------------------------------------
                    Label: model_based_reinforcement_learning
                    Dimension: methodologies
                    Description: Model-based reinforcement learning involves creating a model of the environment to simulate and plan actions, which can lead to more sample-efficient learning.
                    Level: 3
                    # of Papers: 194
                    Example Papers: [(19, 'Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing'), (37, 'MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making'), (47, 'Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences')]
                    ----------------------------------------
                    Label: hierarchical_reinforcement_learning
                    Dimension: methodologies
                    Description: Hierarchical reinforcement learning decomposes tasks into subtasks, allowing agents to learn and plan at multiple levels of abstraction, improving scalability and efficiency.
                    Level: 3
                    # of Papers: 11
                    Example Papers: [(81, 'Towards Tool Use Alignment of Large Language Models'), (283, 'From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis'), (366, 'EPO: Hierarchical LLM Agents with Environment Preference Optimization')]
                    ----------------------------------------
                    Label: multi_objective_methods
                    Dimension: methodologies
                    Description: Multi-objective methods focus on optimizing multiple conflicting objectives simultaneously in reinforcement learning scenarios.
                    Level: 3
                    # of Papers: 11
                    Example Papers: [(228, 'Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models'), (564, 'Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning'), (759, 'Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction')]
                    ----------------------------------------
                    Label: evolutionary_methods
                    Dimension: methodologies
                    Description: Evolutionary methods apply principles of natural selection and genetics to optimize policies and strategies in reinforcement learning.
                    Level: 3
                    # of Papers: 4
                    Example Papers: [(396, 'Automatic Instruction Evolving for Large Language Models'), (1395, 'Survival of the Safest: Towards Secure Prompt Optimization through Interleaved Multi-Objective Evolution'), (1539, 'Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search')]
                    ----------------------------------------
                    Label: collaborative_methods
                    Dimension: methodologies
                    Description: Collaborative methods involve multiple agents working together to solve tasks, enhancing learning through shared experiences and knowledge.
                    Level: 3
                    # of Papers: 17
                    Example Papers: [(6, 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay'), (226, 'Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting'), (302, 'Safely Learning with Private Data: A Federated Learning Framework for Large Language Model')]
                    ----------------------------------------
                    Label: adaptive_learning_methods
                    Dimension: methodologies
                    Description: Adaptive learning methods adjust learning strategies based on the environment and agent performance, improving efficiency and effectiveness.
                    Level: 3
                    # of Papers: 13
                    Example Papers: [(302, 'Safely Learning with Private Data: A Federated Learning Framework for Large Language Model'), (308, 'Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System'), (312, 'Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning')]
                    ----------------------------------------
                    Label: model_free_methods
                    Dimension: methodologies
                    Description: Model-free methods focus on learning optimal policies directly from interactions with the environment without constructing a model of it.
                    Level: 3
                    # of Papers: 97
                    Example Papers: [(6, 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay'), (40, 'FLIRT: Feedback Loop In-context Red Teaming'), (50, 'In-context Contrastive Learning for Event Causality Identification')]
                    ----------------------------------------
               ----------------------------------------
               Label: multi_task_learning
               Dimension: methodologies
               Description: Multi-task learning is an approach where a model is trained on multiple tasks simultaneously, leveraging shared representations to improve performance across tasks.
               Level: 3
               # of Papers: 44
               Example Papers: [(22, 'An Experimental Analysis on Evaluating Patent Citations'), (147, 'Instruction Pre-Training: Language Models are Supervised Multitask Learners'), (152, 'QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios')]
               ----------------------------------------
          ----------------------------------------
          Label: unsupervised_learning
          Dimension: methodologies
          Description: A technique that involves training a model on data without labeled responses, enabling it to identify patterns and structures within the data.
          Level: 2
          # of Papers: 144
          Example Papers: [(13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (27, 'Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation'), (139, 'In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search')]
          ----------------------------------------
          Children:
               Label: clustering
               Dimension: methodologies
               Description: Clustering is a technique used to group similar data points together based on their features, allowing for the identification of natural groupings within the dataset.
               Level: 3
               # of Papers: 10
               Example Papers: [(27, 'Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation'), (155, 'To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models'), (242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions')]
               ----------------------------------------
               Label: dimensionality_reduction
               Dimension: methodologies
               Description: Dimensionality reduction involves reducing the number of random variables under consideration, simplifying the dataset while preserving its essential structure and relationships.
               Level: 3
               # of Papers: 5
               Example Papers: [(242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (575, 'Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions'), (1100, 'Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval')]
               ----------------------------------------
               Label: anomaly_detection
               Dimension: methodologies
               Description: Anomaly detection is the process of identifying rare items or events in a dataset that differ significantly from the majority, often used for fraud detection or network security.
               Level: 3
               # of Papers: 3
               Example Papers: [(242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (1721, 'Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora'), (2113, 'MDCR: A Dataset for Multi-Document Conditional Reasoning')]
               ----------------------------------------
               Label: topic_modeling
               Dimension: methodologies
               Description: Topic modeling is a method used in natural language processing to discover abstract topics within a collection of documents, helping to summarize and categorize large text corpora.
               Level: 3
               # of Papers: 25
               Example Papers: [(155, 'To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models'), (242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (654, 'MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction')]
               ----------------------------------------
               Label: association_rule_learning
               Dimension: methodologies
               Description: Association rule learning is a rule-based method for discovering interesting relations between variables in large databases, commonly used in market basket analysis.
               Level: 3
               # of Papers: 2
               Example Papers: [(242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (2113, 'MDCR: A Dataset for Multi-Document Conditional Reasoning')]
               ----------------------------------------
               Label: unsupervised_text_simplification
               Dimension: methodologies
               Description: Unsupervised text simplification involves techniques to automatically reduce the complexity of text while retaining its original meaning, making it more accessible.
               Level: 3
               # of Papers: 72
               Example Papers: [(146, 'Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?'), (174, 'How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics'), (199, 'Unsupervised Human Preference Learning')]
               ----------------------------------------
               Label: hierarchical_text_classification
               Dimension: methodologies
               Description: Hierarchical text classification is a method that organizes documents into a tree-like structure based on their content, allowing for more nuanced categorization.
               Level: 3
               # of Papers: 5
               Example Papers: [(242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (1046, "Surprise! Uniform Information Density Isn't the Whole Story: Predicting Surprisal Contours in Long-form Discourse"), (1373, 'Improving Hierarchical Text Clustering with LLM-guided Multi-view Cluster Representation')]
               ----------------------------------------
               Label: keyphrase_generation
               Dimension: methodologies
               Description: Keyphrase generation is the process of automatically identifying and extracting significant phrases from text, which can be used for summarization and indexing.
               Level: 3
               # of Papers: 7
               Example Papers: [(242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (654, 'MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction'), (840, 'Open-world Multi-label Text Classification with Extremely Weak Supervision')]
               ----------------------------------------
               Label: information_extraction
               Dimension: methodologies
               Description: Information extraction involves automatically retrieving structured information from unstructured text, such as identifying entities, relationships, and events.
               Level: 3
               # of Papers: 16
               Example Papers: [(242, 'STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions'), (615, 'What are the Generator Preferences for End-to-end Task-Oriented Dialog System?'), (654, 'MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction')]
               ----------------------------------------
               Label: generative_modeling
               Dimension: methodologies
               Description: Generative modeling refers to techniques that learn to generate new data points from the same distribution as the training data, often used in creative applications like text and image generation.
               Level: 3
               # of Papers: 53
               Example Papers: [(139, 'In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search'), (199, 'Unsupervised Human Preference Learning'), (218, 'Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification')]
               ----------------------------------------
          ----------------------------------------
          Label: semi_supervised_learning
          Dimension: methodologies
          Description: An approach that combines a small amount of labeled data with a large amount of unlabeled data during training, improving learning accuracy.
          Level: 2
          # of Papers: 55
          Example Papers: [(13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (174, 'How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics'), (218, 'Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification')]
          ----------------------------------------
          Children:
               Label: self_training
               Dimension: methodologies
               Description: Self-training is a semi-supervised learning method where a model is initially trained on labeled data and then iteratively predicts labels for unlabeled data, using its own predictions to improve its performance.
               Level: 3
               # of Papers: 2
               Example Papers: [(668, 'Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models'), (1880, 'Semi-Supervised Reward Modeling via Iterative Self-Training')]
               ----------------------------------------
               Label: co_training
               Dimension: methodologies
               Description: Co-training involves training two separate models on different views of the same data, allowing them to label unlabeled instances for each other, thus leveraging the strengths of both models.
               Level: 3
               ----------------------------------------
               Label: graph_based_semi_supervised_learning
               Dimension: methodologies
               Description: Graph-based semi-supervised learning utilizes graph structures to represent data points and their relationships, propagating labels through the graph to improve classification accuracy on unlabeled data.
               Level: 3
               # of Papers: 11
               Example Papers: [(174, 'How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics'), (218, 'Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification'), (615, 'What are the Generator Preferences for End-to-end Task-Oriented Dialog System?')]
               ----------------------------------------
               Label: semi_supervised_support_vector_machines
               Dimension: methodologies
               Description: Semi-supervised support vector machines extend traditional SVMs by incorporating both labeled and unlabeled data to find a hyperplane that maximizes the margin while considering the distribution of the unlabeled data.
               Level: 3
               # of Papers: 2
               Example Papers: [(919, 'LM2: A Simple Society of Language Models Solves Complex Reasoning'), (1324, 'Predicting Entity Salience in Extremely Short Documents')]
               ----------------------------------------
               Label: multi_view_learning
               Dimension: methodologies
               Description: Multi-view learning is a semi-supervised approach that uses multiple representations of the same data to improve learning performance, allowing models to exploit complementary information from different views.
               Level: 3
               # of Papers: 21
               Example Papers: [(218, 'Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification'), (615, 'What are the Generator Preferences for End-to-end Task-Oriented Dialog System?'), (745, 'Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers')]
               ----------------------------------------
          ----------------------------------------
          Label: reinforcement_learning
          Label: transfer_learning
          Label: in_context_learning
          Dimension: methodologies
          Description: A methodology where models learn from the context provided during inference, adapting their responses based on the specific situation.
          Level: 2
          # of Papers: 239
          Example Papers: [(3, 'Prompts have evil twins'), (9, 'Hateful Word in Context Classification'), (13, 'A Usage-centric Take on Intent Understanding in E-Commerce')]
          ----------------------------------------
          Children:
               Label: few_shot_learning
               Dimension: methodologies
               Description: A technique that enables models to learn from a very limited number of training examples, mimicking human-like learning capabilities.
               Level: 2
               # of Papers: 24
               Example Papers: [(73, 'Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process'), (159, 'Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment'), (191, 'How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning')]
               ----------------------------------------
               Children:
                    Label: prototypical_networks
                    Dimension: methodologies
                    Description: Prototypical networks are a few-shot learning approach that learns to represent each class by a prototype, which is the mean of the embedded support examples, allowing for effective classification of new instances.
                    Level: 3
                    ----------------------------------------
                    Label: meta_learning
                    Dimension: methodologies
                    Description: Meta-learning, or learning to learn, focuses on developing models that can adapt quickly to new tasks with minimal data by leveraging experiences from previous tasks.
                    Level: 3
                    # of Papers: 21
                    Example Papers: [(73, 'Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process'), (159, 'Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment'), (191, 'How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning')]
                    ----------------------------------------
                    Label: relation_networks
                    Dimension: methodologies
                    Description: Relation networks are designed to learn a similarity function that can compare support examples with query instances, enabling the model to make predictions based on learned relationships.
                    Level: 3
                    ----------------------------------------
                    Label: transfer_learning
                    Label: self_supervised_learning
                    Dimension: methodologies
                    Description: A learning paradigm where the model generates its own supervisory signals from the input data, enabling it to learn representations without explicit labels.
                    Level: 2
                    # of Papers: 1034
                    Example Papers: [(1, 'Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation'), (3, 'Prompts have evil twins'), (4, 'Table Question Answering for Low-resourced Indic Languages')]
                    ----------------------------------------
                    Children:
                         Label: contrastive_learning
                         Dimension: methodologies
                         Description: Contrastive learning is a self-supervised approach that learns representations by contrasting positive pairs against negative pairs, effectively teaching the model to differentiate between similar and dissimilar data points.
                         Level: 3
                         # of Papers: 6
                         Example Papers: [(50, 'In-context Contrastive Learning for Event Causality Identification'), (1256, 'Nearest Neighbor Normalization Improves Multimodal Retrieval'), (1899, 'An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions')]
                         ----------------------------------------
                         Label: masked_language_modeling
                         Dimension: methodologies
                         Description: Masked language modeling involves training a model to predict missing words in a sentence, allowing it to learn contextual relationships and improve its understanding of language structure.
                         Level: 3
                         # of Papers: 7
                         Example Papers: [(1549, 'Exploring the Best Practices of Query Expansion with Large Language Models'), (1672, 'QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware'), (2140, 'Automating Easy Read Text Segmentation')]
                         ----------------------------------------
                         Label: self_distillation
                         Dimension: methodologies
                         Description: Self-distillation is a technique where a model is trained to predict its own outputs, enabling it to refine its knowledge and improve performance without requiring additional labeled data.
                         Level: 3
                         # of Papers: 3
                         Example Papers: [(702, 'FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation'), (1439, 'GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced Distillation'), (1840, 'Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages')]
                         ----------------------------------------
                         Label: generative_adversarial_networks
                         Dimension: methodologies
                         Description: Generative Adversarial Networks (GANs) consist of two neural networks, a generator and a discriminator, that compete against each other to create realistic data samples, commonly used in image generation.
                         Level: 2
                         # of Papers: 4
                         Example Papers: [(398, 'Generative Models for Automatic Medical Decision Rule Extraction from Text'), (592, 'When Generative Adversarial Networks Meet Sequence Labeling Challenges'), (1420, 'Patentformer: A Novel Method to Automate the Generation of Patent Applications')]
                         ----------------------------------------
                         Label: autoencoders
                         Dimension: methodologies
                         Description: Autoencoders are unsupervised neural networks that learn efficient representations of data by encoding the input into a compressed format and then reconstructing it back to the original form.
                         Level: 2
                         # of Papers: 8
                         Example Papers: [(398, 'Generative Models for Automatic Medical Decision Rule Extraction from Text'), (1464, 'Document Hashing with Multi-Grained Prototype-Induced Hierarchical Generative Model'), (1672, 'QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware')]
                         ----------------------------------------
                         Label: self_supervised_learning_methods_for_numerical_data
                         Dimension: methodologies
                         Description: This cluster focuses on methodologies that apply self-supervised learning techniques specifically to numerical data, enhancing representation and predictive capabilities.
                         Level: 3
                         # of Papers: 699
                         Example Papers: [(1, 'Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation'), (3, 'Prompts have evil twins'), (7, 'When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection')]
                         ----------------------------------------
                         Label: self_supervised_video_learning
                         Dimension: methodologies
                         Description: This cluster encompasses methodologies that leverage self-supervised learning for video data, enabling models to learn from unlabelled video sequences.
                         Level: 3
                         # of Papers: 86
                         Example Papers: [(79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment'), (88, 'UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation'), (90, 'MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering')]
                         ----------------------------------------
                         Label: self_supervised_instruction_tuning
                         Dimension: methodologies
                         Description: This cluster involves methodologies that refine models through self-supervised learning by tuning them based on instructional data, improving their performance on specific tasks.
                         Level: 3
                         # of Papers: 175
                         Example Papers: [(27, 'Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation'), (70, 'ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval'), (75, 'QUDSELECT: Selective Decoding for Questions Under Discussion Parsing')]
                         ----------------------------------------
                         Label: self_supervised_entity_extraction
                         Dimension: methodologies
                         Description: This cluster focuses on methodologies that utilize self-supervised learning to identify and extract entities from unstructured data, enhancing information retrieval and processing.
                         Level: 3
                         # of Papers: 142
                         Example Papers: [(30, 'On Fake News Detection with LLM Enhanced Semantics Mining'), (79, 'Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment'), (88, 'UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation')]
                         ----------------------------------------
                         Label: self_supervised_translation
                         Dimension: methodologies
                         Description: This cluster includes methodologies that apply self-supervised learning techniques to improve machine translation systems by learning from unlabelled bilingual text.
                         Level: 3
                         # of Papers: 443
                         Example Papers: [(3, 'Prompts have evil twins'), (7, 'When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection'), (14, 'Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs')]
                         ----------------------------------------
                    ----------------------------------------
               ----------------------------------------
               Label: zero_shot_learning
               Label: prompt_engineering
               Dimension: methodologies
               Description: Prompt engineering is the process of designing and refining input prompts to guide language models in generating desired outputs, enhancing their performance on specific tasks.
               Level: 3
               # of Papers: 107
               Example Papers: [(3, 'Prompts have evil twins'), (40, 'FLIRT: Feedback Loop In-context Red Teaming'), (50, 'In-context Contrastive Learning for Event Causality Identification')]
               ----------------------------------------
               Label: contextual_embedding
               Dimension: methodologies
               Description: Contextual embedding refers to the technique of generating word representations that capture the meaning of words based on their surrounding context, improving understanding in various NLP tasks.
               Level: 3
               # of Papers: 23
               Example Papers: [(9, 'Hateful Word in Context Classification'), (51, "What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs"), (114, 'CMD: a framework for Context-aware Model self-Detoxification')]
               ----------------------------------------
               Label: meta_learning
          ----------------------------------------
          Label: self_supervised_learning
          Label: federated_learning
          Dimension: methodologies
          Description: A decentralized approach to machine learning that allows models to be trained across multiple devices while keeping data localized for privacy.
          Level: 2
          # of Papers: 176
          Example Papers: [(7, 'When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection'), (13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (40, 'FLIRT: Feedback Loop In-context Red Teaming')]
          ----------------------------------------
          Children:
               Label: federated_averaging
               Dimension: methodologies
               Description: Federated averaging is a method where local models are trained on individual devices and then averaged to create a global model, ensuring data privacy while improving model performance.
               Level: 3
               ----------------------------------------
               Label: federated_distillation
               Dimension: methodologies
               Description: Federated distillation involves training a global model by aggregating knowledge from local models, allowing for efficient learning without sharing raw data.
               Level: 3
               # of Papers: 5
               Example Papers: [(437, 'KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server'), (622, 'BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment'), (1044, 'Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion')]
               ----------------------------------------
               Label: secure_aggregation
               Dimension: methodologies
               Description: Secure aggregation is a technique used in federated learning to combine model updates from multiple clients while ensuring that individual updates remain confidential.
               Level: 3
               # of Papers: 3
               Example Papers: [(1044, 'Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion'), (1785, 'Enhancing Byzantine-Resistant Aggregations with Client Embedding'), (2061, 'Promoting Data and Model Privacy in Federated Learning through Quantized LoRA')]
               ----------------------------------------
               Label: personalized_federated_learning
               Dimension: methodologies
               Description: Personalized federated learning tailors the global model to better fit individual clients' data distributions, enhancing performance on personalized tasks.
               Level: 3
               # of Papers: 3
               Example Papers: [(370, 'Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts'), (371, 'Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning'), (1960, 'Low-Resource Machine Translation through the Lens of Personalized Federated Learning')]
               ----------------------------------------
               Label: federated_transfer_learning
               Dimension: methodologies
               Description: Federated transfer learning leverages knowledge from a source domain to improve learning in a target domain across distributed devices, facilitating better model adaptation.
               Level: 3
               # of Papers: 13
               Example Papers: [(101, 'MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic'), (149, 'Collaborative Performance Prediction for Large Language Models'), (458, 'CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models')]
               ----------------------------------------
               Label: federated_efficient_learning
               Dimension: methodologies
               Description: Federated efficient learning focuses on optimizing resource usage and computational efficiency in federated learning environments.
               Level: 3
               # of Papers: 29
               Example Papers: [(101, 'MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic'), (160, 'MoDULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning'), (214, 'Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale')]
               ----------------------------------------
               Label: multi_modal_federated_learning
               Dimension: methodologies
               Description: Multi-modal federated learning integrates various data modalities to enhance model performance across distributed devices.
               Level: 3
               # of Papers: 11
               Example Papers: [(101, 'MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic'), (417, 'Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale'), (458, 'CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models')]
               ----------------------------------------
               Label: federated_query_learning
               Dimension: methodologies
               Description: Federated query learning aims to improve the accuracy of responses in federated systems by leveraging local query data without compromising privacy.
               Level: 3
               # of Papers: 9
               Example Papers: [(101, 'MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic'), (302, 'Safely Learning with Private Data: A Federated Learning Framework for Large Language Model'), (437, 'KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server')]
               ----------------------------------------
               Label: federated_learning_for_healthcare
               Dimension: methodologies
               Description: Federated learning for healthcare applies distributed learning techniques to medical data, ensuring patient privacy while enhancing healthcare models.
               Level: 3
               # of Papers: 23
               Example Papers: [(7, 'When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection'), (101, 'MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic'), (148, 'LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models')]
               ----------------------------------------
               Label: federated_dialogue_learning
               Dimension: methodologies
               Description: Federated dialogue learning develops conversational agents through collaborative training across multiple devices, preserving user data privacy.
               Level: 3
               # of Papers: 7
               Example Papers: [(101, 'MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic'), (302, 'Safely Learning with Private Data: A Federated Learning Framework for Large Language Model'), (615, 'What are the Generator Preferences for End-to-end Task-Oriented Dialog System?')]
               ----------------------------------------
          ----------------------------------------
          Label: few_shot_learning
          Label: explainable_ai
          Dimension: methodologies
          Description: An area of research focused on making AI systems transparent and understandable to users, ensuring that their decisions can be interpreted.
          Level: 2
          # of Papers: 481
          Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (11, "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning"), (17, 'Uncertainty in Language Models: Assessment through Rank-Calibration')]
          ----------------------------------------
          Children:
               Label: feature_importance
               Dimension: methodologies
               Description: Feature importance techniques help identify which input features significantly influence the predictions made by machine learning models, providing insights into model behavior.
               Level: 3
               # of Papers: 2
               Example Papers: [(1576, 'Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach'), (2474, 'Attribution Patching Outperforms Automated Circuit Discovery')]
               ----------------------------------------
               Label: local_interpretable_model_agnostic_explanations
               Dimension: methodologies
               Description: LIME is a method that explains the predictions of any classifier by approximating it locally with an interpretable model, allowing users to understand individual predictions.
               Level: 3
               # of Papers: 3
               Example Papers: [(568, 'Atomic Inference for NLI with Generated Facts as Atoms'), (1576, 'Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach'), (2657, 'Comparative Study of Explainability Methods for Legal Outcome Prediction')]
               ----------------------------------------
               Label: shapley_values
               Dimension: methodologies
               Description: Shapley values provide a unified measure of feature contribution to model predictions based on cooperative game theory, offering a fair distribution of the prediction value among features.
               Level: 3
               # of Papers: 5
               Example Papers: [(1171, 'BiasWipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability'), (1576, 'Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach'), (2478, 'Investigating Layer Importance in Large Language Models')]
               ----------------------------------------
               Label: counterfactual_explanations
               Dimension: methodologies
               Description: Counterfactual explanations provide insights by showing how input features would need to change for a different prediction outcome, helping users understand decision boundaries.
               Level: 3
               # of Papers: 10
               Example Papers: [(1015, 'Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models'), (1576, 'Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach'), (1651, 'Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals')]
               ----------------------------------------
               Label: saliency_maps
               Dimension: methodologies
               Description: Saliency maps are visual representations that highlight the regions of input data that most influence the output of deep learning models, particularly in image classification tasks.
               Level: 3
               # of Papers: 1
               Example Papers: [(1576, 'Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach')]
               ----------------------------------------
               Label: model_interpretability_metrics
               Dimension: methodologies
               Description: Model interpretability metrics assess the effectiveness and clarity of various explainable AI methods, providing quantitative measures for understanding model behavior.
               Level: 3
               # of Papers: 13
               Example Papers: [(141, 'Backward Lens: Projecting Language Model Gradients into the Vocabulary Space'), (192, 'Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis'), (224, 'Interpretability-based Tailored Knowledge Editing in Transformers')]
               ----------------------------------------
               Label: explainable_knowledge_graphs
               Dimension: methodologies
               Description: Explainable knowledge graphs enhance the interpretability of knowledge representations by providing insights into the relationships and reasoning behind the data.
               Level: 3
               # of Papers: 128
               Example Papers: [(22, 'An Experimental Analysis on Evaluating Patent Citations'), (30, 'On Fake News Detection with LLM Enhanced Semantics Mining'), (35, 'Evaluating Readability and Faithfulness of Concept-based Explanations')]
               ----------------------------------------
               Label: explainable_question_answering
               Dimension: methodologies
               Description: Explainable question answering methods focus on providing transparent and understandable responses from AI systems, allowing users to comprehend the rationale behind the answers.
               Level: 3
               # of Papers: 181
               Example Papers: [(58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs'), (69, 'AgentReview: Exploring Peer Review Dynamics with LLM Agents'), (84, 'Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment')]
               ----------------------------------------
               Label: model_explainability_methods
               Dimension: methodologies
               Description: Model explainability methods encompass a range of techniques designed to clarify how machine learning models make decisions, improving trust and usability.
               Level: 3
               # of Papers: 196
               Example Papers: [(35, 'Evaluating Readability and Faithfulness of Concept-based Explanations'), (58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs'), (69, 'AgentReview: Exploring Peer Review Dynamics with LLM Agents')]
               ----------------------------------------
               Label: explainable_generation
               Dimension: methodologies
               Description: Explainable generation techniques aim to produce outputs from AI systems that are not only coherent but also understandable, providing insights into the generation process.
               Level: 3
               # of Papers: 360
               Example Papers: [(2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (11, "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning"), (17, 'Uncertainty in Language Models: Assessment through Rank-Calibration')]
               ----------------------------------------
          ----------------------------------------
     ----------------------------------------
     Label: deep_learning_techniques
     Dimension: methodologies
     Description: This methodology leverages neural networks, particularly deep learning architectures, to achieve state-of-the-art performance in tasks such as language modeling, translation, and sentiment analysis.
     Level: 1
     # of Papers: 1801
     Example Papers: [(0, 'UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation'), (1, 'Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation'), (2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document')]
     ----------------------------------------
     Children:
          Label: convolutional_neural_networks
          Dimension: methodologies
          Description: Convolutional Neural Networks (CNNs) are specialized deep learning architectures designed for processing structured grid data, such as images, by applying convolutional layers to capture spatial hierarchies.
          Level: 2
          ----------------------------------------
          Label: recurrent_neural_networks
          Dimension: methodologies
          Description: Recurrent Neural Networks (RNNs) are a class of neural networks that are particularly effective for sequential data, allowing information to persist across time steps through their internal memory.
          Level: 2
          # of Papers: 1
          Example Papers: [(2466, 'Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations')]
          ----------------------------------------
          Label: generative_adversarial_networks
          Label: transfer_learning
          Label: autoencoders
          Label: multi_agent_neural_networks
          Dimension: methodologies
          Description: Multi-Agent Neural Networks focus on the design and implementation of neural networks that can operate collaboratively or competitively in multi-agent environments.
          Level: 2
          # of Papers: 36
          Example Papers: [(6, 'LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay'), (69, 'AgentReview: Exploring Peer Review Dynamics with LLM Agents'), (100, 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering')]
          ----------------------------------------
          Label: sequence_models
          Dimension: methodologies
          Description: Sequence Models are methodologies that handle sequential data, capturing temporal dependencies and patterns in various applications.
          Level: 2
          # of Papers: 975
          Example Papers: [(0, 'UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation'), (2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (3, 'Prompts have evil twins')]
          ----------------------------------------
          Children:
               Label: recurrent_neural_networks
               Label: long_short_term_memory
               Dimension: methodologies
               Description: Long Short-Term Memory (LSTM) networks are a type of RNN that are capable of learning long-term dependencies, making them particularly effective for tasks involving sequences with long-range correlations.
               Level: 3
               # of Papers: 1
               Example Papers: [(2827, 'CUNI at WMT24 General Translation Task: LLMs, (Q)LoRA, CPO and Model Merging')]
               ----------------------------------------
               Label: gated_recurrent_units
               Dimension: methodologies
               Description: Gated Recurrent Units (GRUs) are a simplified version of LSTMs that use gating mechanisms to control the flow of information, allowing for efficient training and performance on sequence tasks.
               Level: 3
               # of Papers: 2
               Example Papers: [(2466, 'Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations'), (2827, 'CUNI at WMT24 General Translation Task: LLMs, (Q)LoRA, CPO and Model Merging')]
               ----------------------------------------
               Label: transformer_models
               Dimension: methodologies
               Description: Transformer models utilize self-attention mechanisms to process sequences in parallel, significantly improving efficiency and performance in tasks such as translation and text generation.
               Level: 3
               # of Papers: 100
               Example Papers: [(20, 'Scaling Properties of Speech Language Models'), (68, 'LLMs Are Zero-Shot Context-Aware Simultaneous Translators'), (141, 'Backward Lens: Projecting Language Model Gradients into the Vocabulary Space')]
               ----------------------------------------
               Label: sequence_to_sequence_models
               Dimension: methodologies
               Description: Sequence-to-sequence models are designed to transform input sequences into output sequences, commonly used in applications like machine translation and text summarization.
               Level: 3
               # of Papers: 821
               Example Papers: [(0, 'UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation'), (2, 'FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document'), (4, 'Table Question Answering for Low-resourced Indic Languages')]
               ----------------------------------------
               Label: self_supervised_learning
               Label: generative_language_models
               Dimension: methodologies
               Description: Generative language models are a class of models designed to generate coherent and contextually relevant text based on input prompts.
               Level: 3
               # of Papers: 547
               Example Papers: [(0, 'UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation'), (3, 'Prompts have evil twins'), (5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions')]
               ----------------------------------------
               Label: conversational_models
               Dimension: methodologies
               Description: Conversational models are methodologies focused on enabling machines to engage in dialogue with users, understanding context and intent.
               Level: 3
               # of Papers: 240
               Example Papers: [(9, 'Hateful Word in Context Classification'), (36, 'Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems'), (39, 'Tokenization Is More Than Compression')]
               ----------------------------------------
               Label: knowledge_probing
               Dimension: methodologies
               Description: Knowledge probing refers to techniques used to evaluate and extract knowledge embedded within language models.
               Level: 3
               # of Papers: 150
               Example Papers: [(39, 'Tokenization Is More Than Compression'), (42, 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks'), (56, 'RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning')]
               ----------------------------------------
               Label: multimodal_models
               Dimension: methodologies
               Description: Multimodal models integrate and process information from multiple modalities, such as text, images, and audio, to enhance understanding and generation tasks.
               Level: 3
               # of Papers: 140
               Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (39, 'Tokenization Is More Than Compression'), (42, 'Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks')]
               ----------------------------------------
          ----------------------------------------
          Label: explainable_ai
          Label: cross_modal_learning
          Dimension: methodologies
          Description: Cross-Modal Learning involves methodologies that enable models to learn from and integrate information across different modalities, such as text, images, and audio.
          Level: 2
          # of Papers: 977
          Example Papers: [(0, 'UniGen: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation'), (1, 'Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation'), (4, 'Table Question Answering for Low-resourced Indic Languages')]
          ----------------------------------------
          Children:
               Label: multimodal_representation_learning
               Dimension: methodologies
               Description: This methodology focuses on learning joint representations from multiple modalities, such as text, images, and audio, to enhance the understanding and interaction between different types of data.
               Level: 3
               # of Papers: 79
               Example Papers: [(49, "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification"), (60, 'Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval'), (61, 'RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models')]
               ----------------------------------------
               Label: cross_modal_transfer_learning
               Dimension: methodologies
               Description: This approach involves transferring knowledge from one modality to another, enabling models trained on one type of data to improve performance on another, thereby leveraging the strengths of different data sources.
               Level: 3
               # of Papers: 138
               Example Papers: [(29, 'EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models'), (116, 'TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control'), (123, 'Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models')]
               ----------------------------------------
               Label: multimodal_attention_mechanisms
               Dimension: methodologies
               Description: This technique employs attention mechanisms that can selectively focus on relevant parts of different modalities, allowing models to effectively integrate and process information from diverse sources.
               Level: 3
               # of Papers: 6
               Example Papers: [(487, 'MEANT: Multimodal Encoder for Antecedent Information'), (1272, 'OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents'), (2095, 'AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding')]
               ----------------------------------------
               Label: cross_modal_generation
               Dimension: methodologies
               Description: This methodology involves generating content in one modality based on input from another, such as creating images from textual descriptions or generating text from visual inputs, facilitating creative and informative outputs.
               Level: 3
               # of Papers: 218
               Example Papers: [(5, 'ImageInWords: Unlocking Hyper-Detailed Image Descriptions'), (40, 'FLIRT: Feedback Loop In-context Red Teaming'), (65, 'AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation')]
               ----------------------------------------
               Label: joint_embedding_spaces
               Dimension: methodologies
               Description: This approach creates a shared embedding space where data from different modalities can be represented in a unified manner, enabling better alignment and interaction between the modalities.
               Level: 3
               # of Papers: 5
               Example Papers: [(575, 'Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions'), (1272, 'OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents'), (1328, 'INDUS: Effective and Efficient Language Models for Scientific Applications')]
               ----------------------------------------
               Label: cross_modal_learning_for_machine_translation
               Dimension: methodologies
               Description: This methodology focuses on applying cross-modal learning techniques specifically to enhance machine translation tasks by leveraging multiple data modalities.
               Level: 3
               # of Papers: 487
               Example Papers: [(20, 'Scaling Properties of Speech Language Models'), (24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (29, 'EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models')]
               ----------------------------------------
               Label: cross_modal_learning_for_legal_domain
               Dimension: methodologies
               Description: This approach utilizes cross-modal learning strategies tailored for applications within the legal domain, improving the processing and understanding of legal texts and documents.
               Level: 3
               # of Papers: 120
               Example Papers: [(20, 'Scaling Properties of Speech Language Models'), (72, 'Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation'), (85, 'Mitigating Matthew Effect: Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation')]
               ----------------------------------------
               Label: cross_modal_error_correction
               Dimension: methodologies
               Description: This methodology aims to improve the accuracy of cross-modal systems by implementing error correction techniques that address discrepancies between different modalities.
               Level: 3
               # of Papers: 107
               Example Papers: [(20, 'Scaling Properties of Speech Language Models'), (85, 'Mitigating Matthew Effect: Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation'), (97, 'SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation')]
               ----------------------------------------
               Label: cross_modal_evaluation
               Dimension: methodologies
               Description: This approach focuses on assessing the performance and effectiveness of cross-modal learning systems through various evaluation metrics and methodologies.
               Level: 3
               # of Papers: 198
               Example Papers: [(20, 'Scaling Properties of Speech Language Models'), (40, 'FLIRT: Feedback Loop In-context Red Teaming'), (58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs')]
               ----------------------------------------
               Label: cross_modal_bias_debiasing
               Dimension: methodologies
               Description: This methodology addresses the identification and mitigation of biases in cross-modal learning systems to ensure fair and equitable outcomes across different modalities.
               Level: 3
               # of Papers: 150
               Example Papers: [(20, 'Scaling Properties of Speech Language Models'), (48, 'Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue'), (58, 'HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs')]
               ----------------------------------------
          ----------------------------------------
          Label: reinforcement_learning
     ----------------------------------------
     Label: rule-based_systems
     Dimension: methodologies
     Description: This methodology involves the creation of systems that apply predefined linguistic rules to process and analyze text, often used in applications like grammar checking and information extraction.
     Level: 1
     # of Papers: 64
     Example Papers: [(127, 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models'), (152, 'QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios'), (179, 'BC-Prover: Backward Chaining Prover for Formal Theorem Proving')]
     ----------------------------------------
     Children:
          Label: expert_systems
          Dimension: methodologies
          Description: Expert systems are computer programs that emulate the decision-making ability of a human expert, utilizing a set of rules to solve complex problems within a specific domain.
          Level: 2
          ----------------------------------------
          Label: production_rule_systems
          Dimension: methodologies
          Description: Production rule systems operate on a set of condition-action rules, where the system evaluates conditions and executes corresponding actions to derive conclusions or perform tasks.
          Level: 2
          # of Papers: 37
          Example Papers: [(127, 'LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models'), (152, 'QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios'), (179, 'BC-Prover: Backward Chaining Prover for Formal Theorem Proving')]
          ----------------------------------------
          Label: fuzzy_logic_systems
          Dimension: methodologies
          Description: Fuzzy logic systems extend traditional binary logic to handle the concept of partial truth, allowing for reasoning and decision-making in uncertain or imprecise environments.
          Level: 2
          ----------------------------------------
          Label: decision_trees
          Label: constraint_satisfaction_problems
          Dimension: methodologies
          Description: Constraint satisfaction problems involve finding a solution that satisfies a set of constraints, often represented through rules that define the relationships between variables.
          Level: 2
          # of Papers: 14
          Example Papers: [(152, 'QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios'), (300, 'Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars'), (645, 'Puzzle Solving using Reasoning of Large Language Models: A Survey')]
          ----------------------------------------
     ----------------------------------------
     Label: hybrid_approaches
     Dimension: methodologies
     Description: This methodology combines elements from different paradigms, such as statistical, rule-based, and machine learning methods, to create more robust and flexible natural language processing systems.
     Level: 1
     # of Papers: 387
     Example Papers: [(13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (25, 'Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation')]
     ----------------------------------------
     Children:
          Label: ensemble_learning
          Label: transfer_learning
          Label: multi-task_learning
          Label: rule-based_and_machine_learning
          Dimension: methodologies
          Description: This approach integrates rule-based systems with machine learning techniques to enhance decision-making processes in natural language understanding.
          Level: 2
          # of Papers: 17
          Example Papers: [(313, 'LogicST: A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations'), (378, 'Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution'), (973, 'Symbolic Working Memory Enhances Language Models for Complex Rule Application')]
          ----------------------------------------
          Label: hybrid_recommendation_systems
          Dimension: methodologies
          Description: Hybrid recommendation systems combine collaborative filtering and content-based filtering methods to provide more accurate and personalized recommendations in text-based applications.
          Level: 2
          # of Papers: 7
          Example Papers: [(13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (994, 'PepRec: Progressive Enhancement of Prompting for Recommendation'), (1106, 'Jump Starting Bandits with LLM-Generated Prior Knowledge')]
          ----------------------------------------
          Label: hybrid_model_approaches
          Dimension: methodologies
          Description: Hybrid model approaches integrate various modeling techniques to enhance performance and adaptability in complex tasks.
          Level: 2
          # of Papers: 382
          Example Papers: [(13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (25, 'Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation')]
          ----------------------------------------
          Children:
               Label: ensemble_learning
               Label: transfer_learning
               Label: multi-task_learning
               Label: neural-symbolic_integration
               Dimension: methodologies
               Description: Neural-symbolic integration merges neural networks with symbolic reasoning, enabling models to leverage both data-driven learning and logical inference for enhanced decision-making.
               Level: 3
               # of Papers: 89
               Example Papers: [(30, 'On Fake News Detection with LLM Enhanced Semantics Mining'), (49, "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification"), (100, 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering')]
               ----------------------------------------
               Label: hybrid_reinforcement_learning
               Dimension: methodologies
               Description: Hybrid reinforcement learning combines traditional reinforcement learning techniques with supervised learning methods, allowing for more efficient exploration and exploitation in complex environments.
               Level: 3
               # of Papers: 52
               Example Papers: [(25, 'Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation'), (62, 'CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading'), (100, 'Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering')]
               ----------------------------------------
               Label: human-centered_learning
               Dimension: methodologies
               Description: Human-centered learning focuses on designing models and methodologies that prioritize user experience and interaction in the learning process.
               Level: 3
               # of Papers: 33
               Example Papers: [(117, "Be Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support"), (258, 'Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset'), (279, 'An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records')]
               ----------------------------------------
               Label: agent-based_simulation
               Dimension: methodologies
               Description: Agent-based simulation involves creating models where autonomous agents interact within an environment to study complex systems and behaviors.
               Level: 3
               # of Papers: 26
               Example Papers: [(135, 'Towards Low-Resource Harmful Meme Detection with LMM Agents'), (279, 'An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records'), (508, 'Can Active Label Correction Improve LLM-based Modular AI Systems?')]
               ----------------------------------------
               Label: multi-modal_learning
               Dimension: methodologies
               Description: Multi-modal learning integrates information from various modalities, such as text, audio, and visual data, to enhance model performance and understanding.
               Level: 3
               # of Papers: 238
               Example Papers: [(13, 'A Usage-centric Take on Intent Understanding in E-Commerce'), (24, 'Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing'), (27, 'Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation')]
               ----------------------------------------
               Label: fuzzy_logic_integration
               Dimension: methodologies
               Description: Fuzzy logic integration applies fuzzy set theory to improve decision-making processes in uncertain or imprecise environments.
               Level: 3
               # of Papers: 9
               Example Papers: [(279, 'An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records'), (508, 'Can Active Label Correction Improve LLM-based Modular AI Systems?'), (1075, 'VerifyMatch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp')]
               ----------------------------------------
               Label: knowledge-augmentation
               Dimension: methodologies
               Description: Knowledge-augmentation enhances machine learning models by incorporating external knowledge sources to improve reasoning and contextual understanding.
               Level: 3
               # of Papers: 136
               Example Papers: [(57, 'BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering'), (61, 'RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models'), (80, 'A New Pipeline for Knowledge Graph Reasoning Enhanced by Large Language Models Without Fine-Tuning')]
               ----------------------------------------
          ----------------------------------------
          Label: hybrid_privacy_preserving_methods
          Dimension: methodologies
          Description: These methods focus on combining different strategies to ensure data privacy while maintaining model effectiveness.
          Level: 2
          # of Papers: 4
          Example Papers: [(194, 'GoldCoin: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory'), (370, 'Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts'), (1044, 'Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion')]
          ----------------------------------------
          Label: hybrid_learning_strategies
          Dimension: methodologies
          Description: Hybrid learning strategies merge multiple learning paradigms to optimize knowledge acquisition and application.
          Level: 2
          # of Papers: 36
          Example Papers: [(25, 'Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation'), (157, 'An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making'), (167, 'KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases')]
          ----------------------------------------
          Label: hybrid_generation_methods
          Dimension: methodologies
          Description: Hybrid generation methods utilize a combination of techniques to produce diverse and high-quality outputs in generative tasks.
          Level: 2
          # of Papers: 56
          Example Papers: [(122, 'Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models'), (129, 'FuseGen: PLM Fusion for Data-generation based Zero-shot Learning'), (199, 'Unsupervised Human Preference Learning')]
          ----------------------------------------
          Children:
               Label: template_based_generation
               Dimension: methodologies
               Description: This method combines predefined templates with dynamic content generation to produce coherent and contextually relevant text.
               Level: 3
               ----------------------------------------
               Label: rule_based_generation
               Dimension: methodologies
               Description: This approach utilizes a set of linguistic rules to guide the generation process, ensuring that the output adheres to specific grammatical and stylistic standards.
               Level: 3
               ----------------------------------------
               Label: data_driven_generation
               Dimension: methodologies
               Description: This technique leverages large datasets to inform the generation process, allowing for the creation of text that reflects real-world usage and trends.
               Level: 3
               # of Papers: 47
               Example Papers: [(129, 'FuseGen: PLM Fusion for Data-generation based Zero-shot Learning'), (199, 'Unsupervised Human Preference Learning'), (269, 'Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation')]
               ----------------------------------------
               Label: neural_symbolic_generation
               Dimension: methodologies
               Description: This hybrid method integrates neural networks with symbolic reasoning to enhance the generation of text by combining statistical learning with logical inference.
               Level: 3
               # of Papers: 39
               Example Papers: [(129, 'FuseGen: PLM Fusion for Data-generation based Zero-shot Learning'), (199, 'Unsupervised Human Preference Learning'), (269, 'Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation')]
               ----------------------------------------
               Label: interactive_generation
               Dimension: methodologies
               Description: This approach involves user interaction during the generation process, allowing for real-time adjustments and refinements based on user feedback.
               Level: 3
               # of Papers: 4
               Example Papers: [(901, 'Free your mouse! Command Large Language Models to Generate Code to Format Word Documents'), (1244, 'EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records'), (2598, 'Automated test generation to evaluate tool-augmented LLMs as conversational AI agents')]
               ----------------------------------------
          ----------------------------------------
          Label: multimodal_learning
          Dimension: methodologies
          Description: Multimodal learning involves training models that can process and integrate information from multiple data modalities, such as text and images.
          Level: 2
          # of Papers: 84
          Example Papers: [(49, "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification"), (60, 'Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval'), (61, 'RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models')]
          ----------------------------------------
          Children:
               Label: visual_language_models
               Dimension: methodologies
               Description: Visual language models integrate visual data with textual information to enhance understanding and generate contextually relevant outputs.
               Level: 3
               # of Papers: 2
               Example Papers: [(1116, 'Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities'), (1667, 'Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models')]
               ----------------------------------------
               Label: audio-visual_integration
               Dimension: methodologies
               Description: This methodology focuses on combining audio and visual inputs to improve recognition tasks, such as speech recognition in noisy environments.
               Level: 3
               # of Papers: 1
               Example Papers: [(1272, 'OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents')]
               ----------------------------------------
               Label: multimodal_sentiment_analysis
               Dimension: methodologies
               Description: Multimodal sentiment analysis leverages both text and visual cues to assess emotional tone, providing a more nuanced understanding of user sentiment.
               Level: 3
               # of Papers: 3
               Example Papers: [(557, 'MMoE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts'), (1233, 'M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought'), (2509, 'Continuous Attentive Multimodal Prompt Tuning for Few-Shot Multimodal Sarcasm Detection')]
               ----------------------------------------
               Label: cross-modal_retrieval
               Dimension: methodologies
               Description: Cross-modal retrieval techniques enable the search and retrieval of information across different modalities, such as finding images based on textual descriptions.
               Level: 3
               # of Papers: 17
               Example Papers: [(60, 'Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval'), (61, 'RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models'), (304, 'How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?')]
               ----------------------------------------
               Label: multimodal_representation_learning
          ----------------------------------------
     ----------------------------------------
----------------------------------------
