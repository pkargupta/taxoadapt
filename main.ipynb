{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-23 05:48:00 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 09-23 05:48:00 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 09-23 05:48:00 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 09-23 05:48:01 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 09-23 05:48:02 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e23a3525a04114b72f5bed0b9c0ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 05:48:35 model_runner.py:732] Loading model weights took 14.9888 GB\n",
      "INFO 09-23 05:48:36 gpu_executor.py:102] # GPU blocks: 9057, # CPU blocks: 2048\n",
      "INFO 09-23 05:48:38 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-23 05:48:38 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-23 05:48:46 model_runner.py:1225] Graph capturing finished in 8 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from model_definitions import bertEncode\n",
    "from taxonomy import Node, Taxonomy\n",
    "import subprocess\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import compress\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import torch\n",
    "from scipy import stats\n",
    "import math\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "#from model_definitions import llama_8b_model, sentence_model, promptLlama, constructPrompt, bertEncode\n",
    "from utils import *\n",
    "from prompts import *\n",
    "from main import commonSenseEnrich, computeClassEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"gen_kgc\"\n",
    "        self.data_dir = f\"datasets/{self.dataset}/\"\n",
    "        self.internal = f\"{self.dataset}.txt\"\n",
    "        self.external = f\"{self.dataset}_external.txt\"\n",
    "        self.groundtruth = \"groundtruth.txt\"\n",
    "        \n",
    "        self.length = 512\n",
    "        self.dim = 768\n",
    "\n",
    "        self.iters = 4\n",
    "        self.model = \"bert_full_ft\"\n",
    "        self.override = True\n",
    "        self.max_depth = 5\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create taxonomy from input\n",
    "taxo = Taxonomy(args.data_dir)\n",
    "\n",
    "taxo_dict = taxo.toDict(cur_node=taxo.root)\n",
    "with open(f'datasets/{args.dataset}/initial.json', 'w') as fp:\n",
    "    json.dump(taxo_dict, fp, indent=4)\n",
    "dict_str = json.dumps(taxo_dict, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common-sense enrichment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 7/7 [00:12<00:00,  1.74s/it, est. speed input: 565.68 toks/s, output: 197.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging all enrichment dictionaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 33250.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 13.81446099281311 seconds (0.23024101654688517 minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enrich_start = time.time()\n",
    "prompts, outputs, all_common_phrases, all_common_sentences = commonSenseEnrich(taxo.root, dict_str, True)\n",
    "# prompts, output_dict = commonSenseEnrich(taxo.root, dict_str, True)\n",
    "enrich_end = time.time()\n",
    "\n",
    "if all_common_phrases:\n",
    "    # update vocabulary/embeddings\n",
    "    taxo.updateVocab(all_common_phrases, 'phrases')\n",
    "    taxo.updateVocab(all_common_sentences, 'sentences')\n",
    "    print(f\"Time taken: {enrich_end - enrich_start} seconds ({(enrich_end - enrich_start)/60} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dict = taxo.toDict(cur_node=taxo.root)\n",
    "with open(f'datasets/{args.dataset}/enriched.json', 'w') as fp:\n",
    "    json.dump(updated_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessing/AutoPhrase/data/EN/wiki_quality_orig.txt\", \"r\", encoding='utf-8') as f:\n",
    "    all_phrases = [w.strip() for w in f.readlines() if \" \" in w.strip()]\n",
    "    for a in list(taxo.label2id.keys()) + all_common_phrases:\n",
    "        all_phrases.append(a.strip().replace(\"_\", \" \"))\n",
    "\n",
    "with open(\"preprocessing/AutoPhrase/data/EN/wiki_quality.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for w_id, w in enumerate(all_phrases):\n",
    "        if w_id == (len(all_phrases) - 1):\n",
    "            f.write(f\"{w}\")\n",
    "        else:\n",
    "            f.write(f\"{w}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Corpus Pre-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:01<00:00, 22.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m1.546s\n",
      "user\t0m12.814s\n",
      "sys\t0m0.669s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "No provided expert labels.\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===AutoPhrasing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Enabled\n",
      "Number of threads = 10\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 279831\n",
      "max word token id = 21669\n",
      "# of documents = 34\n",
      "# of distinct POS tags = 55\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 21673\n",
      "# of frequent phrases = 24076\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 43\n",
      "\tThe size of the negative pool = 22639\n",
      "# truth patterns = 21958\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m2.457s\n",
      "user\t0m6.285s\n",
      "sys\t0m0.145s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Saving Model and Results===\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m1.497s\n",
      "user\t0m12.521s\n",
      "sys\t0m0.563s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===Phrasal Segmentation===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Segmentation Model Path = models/NEW/segmentation.model\n",
      "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
      "\tQ(multi-word phrases) >= 0.700000\n",
      "\tQ(single-word phrases) >= 1.000000\n",
      "=======\n",
      "POS guided model loaded.\n",
      "# of loaded patterns = 4516\n",
      "# of loaded truth patterns = 22001\n",
      "POS transition matrix loaded\n",
      "Phrasal segmentation finished.\n",
      "   # of total highlighted quality phrases = 7096\n",
      "   # of total processed sentences = 48201\n",
      "   avg highlights per sentence = 0.147217\n",
      "\n",
      "real\t0m0.410s\n",
      "user\t0m0.380s\n",
      "sys\t0m0.012s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Segmented Corpus Post-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:00, 881.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase segmented corpus written to ../datasets/gen_kgc/phrase_gen_kgc.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Corpus Pre-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8150/8150 [00:09<00:00, 900.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m2.167s\n",
      "user\t0m19.026s\n",
      "sys\t0m0.811s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "No provided expert labels.\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===AutoPhrasing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Enabled\n",
      "Number of threads = 10\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 1664688\n",
      "max word token id = 33742\n",
      "# of documents = 8150\n",
      "# of distinct POS tags = 56\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 33749\n",
      "# of frequent phrases = 53490\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 273\n",
      "\tThe size of the negative pool = 50031\n",
      "# truth patterns = 35522\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m4.893s\n",
      "user\t0m22.369s\n",
      "sys\t0m1.600s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Saving Model and Results===\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m1.906s\n",
      "user\t0m16.551s\n",
      "sys\t0m1.015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===Phrasal Segmentation===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Segmentation Model Path = models/NEW/segmentation.model\n",
      "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
      "\tQ(multi-word phrases) >= 0.700000\n",
      "\tQ(single-word phrases) >= 1.000000\n",
      "=======\n",
      "POS guided model loaded.\n",
      "# of loaded patterns = 24515\n",
      "# of loaded truth patterns = 35795\n",
      "POS transition matrix loaded\n",
      "Phrasal segmentation finished.\n",
      "   # of total highlighted quality phrases = 104679\n",
      "   # of total processed sentences = 189417\n",
      "   avg highlights per sentence = 0.552638\n",
      "\n",
      "real\t0m2.421s\n",
      "user\t0m2.359s\n",
      "sys\t0m0.056s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Segmented Corpus Post-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8150it [00:00, 23559.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase segmented corpus written to ../datasets/gen_kgc/phrase_gen_kgc_external.txt\n"
     ]
    }
   ],
   "source": [
    "if args.override:\n",
    "    # pre-process\n",
    "    os.chdir(\"./preprocessing\")\n",
    "    subprocess.check_call(['./auto_phrase.sh', args.dataset, args.internal])\n",
    "    subprocess.check_call(['./auto_phrase.sh', args.dataset, args.external])\n",
    "    os.chdir(\"../\")\n",
    "else:\n",
    "    print(\"already pre-processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxo.static_emb = {}\n",
    "# taxo.token_lens = {}\n",
    "# taxo.collection = []\n",
    "# taxo.external_collection = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Collections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8184/8184 [00:05<00:00, 1368.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Static Embeddings...\n",
      "Loading in static embeddings and updating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8184/8184 [00:01<00:00, 8148.53it/s] \n"
     ]
    }
   ],
   "source": [
    "collection, external_collection = taxo.createCollections(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**External & Internal Term Enrichment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding sentences...\n",
      "encoding phrases...\n"
     ]
    }
   ],
   "source": [
    "print(\"encoding sentences...\")\n",
    "external_sentences = list(set([sentence for paper in external_collection for sentence in paper.sent_tokenize]))\n",
    "internal_sentences = list(set([sentence for paper in collection for sentence in paper.sent_tokenize]))\n",
    "taxo.updateVocab(external_sentences + internal_sentences, 'sentences')\n",
    "# sent2emb = {sent:idx for idx, sent in enumerate(external_sentences)}\n",
    "# external_sent_emb = {idx:emb for idx, emb in  enumerate(sentence_model.encode(external_sentences))}\n",
    "\n",
    "print(\"encoding phrases...\")\n",
    "external_phrases = list(set([phrase for paper in external_collection for sentence in paper.phrase_tokenize for phrase in sentence]))\n",
    "internal_phrases = list(set([phrase for paper in collection for sentence in paper.phrase_tokenize for phrase in sentence]))\n",
    "taxo.updateVocab(external_phrases + internal_phrases, 'phrases')\n",
    "# phrase2emb = {phrase:idx for idx, phrase in enumerate(external_phrases)}\n",
    "# external_phrase_emb = {idx:emb for idx, emb in  enumerate(sentence_model.encode(external_phrases))}\n",
    "\n",
    "taxo.graph.external['phrases'] = external_phrases\n",
    "taxo.graph.external['sentences'] = external_sentences\n",
    "taxo.graph.internal['phrases'] = internal_phrases\n",
    "taxo.graph.internal['sentences'] = internal_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_pool = external_phrases # list(taxo.vocab['phrases'].keys())\n",
    "pool_emb = np.array([taxo.vocab['phrases'][w] for w in phrase_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:04<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069879466938291\n",
      "F1-Macro Score: 0.6421009098428454\n",
      "F1-Micro Score: 0.7984189723320158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "node_external_phrase_ranks, gt, preds = expandDiscriminative(taxo, phrase_pool, pool_emb, internal=False)\n",
    "f1_scores(gt, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8184/8184 [00:21<00:00, 373.31it/s]\n"
     ]
    }
   ],
   "source": [
    "term_to_idx, td_matrix, co_matrix = constructTermDocMatrix(taxo, collection + external_collection)\n",
    "co_avg = np.true_divide(co_matrix.sum(),(co_matrix!=0).sum())\n",
    "phrase_pool = internal_phrases # list(taxo.vocab['phrases'].keys())\n",
    "pool_emb = np.array([taxo.vocab['phrases'][w] for w in phrase_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_score = computeBM25Cog(co_matrix, co_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:32<00:00,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801139546727782\n",
      "F1-Macro Score: 0.7246886446886448\n",
      "F1-Micro Score: 0.7956204379562044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "node_internal_phrase_ranks, gt, preds = expandInternal(taxo, phrase_pool, pool_emb, term_to_idx, bm_score)\n",
    "f1_scores(gt, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Internal Sentence Enrichment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pool = []\n",
    "sentence_gold = []\n",
    "sentence_phrase_pool = []\n",
    "for paper in collection:\n",
    "    for s_sent, p_sent in zip(paper.sent_tokenize, paper.phrase_tokenize):\n",
    "        if s_sent not in sentence_pool:\n",
    "            sentence_pool.append(s_sent)\n",
    "            sentence_gold.append(paper.gold)\n",
    "            sentence_phrase_pool.append(p_sent)\n",
    "\n",
    "phrase_pool_emb = [np.stack([taxo.vocab['phrases'][w] for w in s], axis=0) for s in sentence_phrase_pool] # S x P x 768\n",
    "sentence_pool_emb = np.array([taxo.vocab['sentences'][s] for s in sentence_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
      "100%|██████████| 5/5 [02:20<00:00, 28.06s/it]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "taxo.root.internal['sentences'] = sentence_pool\n",
    "taxo.root.internal['sent_ids'] = np.arange(len(sentence_pool))\n",
    "queue = deque([taxo.root])\n",
    "\n",
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    # for each child, compute phrase emb and sib emb\n",
    "    for child in curr_node.children:\n",
    "        child.internal['sentences'] = []\n",
    "        child.internal['sent_ids'] = []\n",
    "\n",
    "        child.emb['phrase'] = np.stack([taxo.vocab['phrases'][w] \n",
    "                                        for w in child.getAllTerms(granularity='phrases', children=False)], axis=0)\n",
    "        child.emb['sentence'] = np.stack([taxo.vocab['sentences'][w] \n",
    "                                        for w in child.getAllTerms(granularity='sentences', children=False)], axis=0)\n",
    "    \n",
    "    candidate_phrases = sentence_phrase_pool # [sentence_phrase_pool[i] for i in curr_node.internal['sent_ids']]\n",
    "    candidate_phrase_embs = phrase_pool_emb #[phrase_pool_emb[i] for i in curr_node.internal['sent_ids']]\n",
    "    candidate_sent_embs = sentence_pool_emb #sentence_pool_emb[curr_node.internal['sent_ids']]\n",
    "\n",
    "    sent_ranks = {sent_id:[] for sent_id in np.arange(len(sentence_pool_emb))} # for each candidate: list of ranks across all child nodes\n",
    "\n",
    "    for focus_node in tqdm(curr_node.children):\n",
    "        sibs = [n for n in curr_node.children if n != focus_node]\n",
    "\n",
    "        focus_phrases = focus_node.getAllTerms(granularity='phrases', children=False)\n",
    "        sibling_phrases = [sib.getAllTerms(granularity='phrases', children=False) for sib in sibs]\n",
    "        \n",
    "        # compute target phrase/sentence semantic similarity\n",
    "        focus_phrase_sim = np.stack([cosine_similarity_embeddings(p_embs, focus_node.emb['phrase']).max(axis=0) \n",
    "                                     for p_embs in candidate_phrase_embs], axis=0) # S x [P x N] -> S x N\n",
    "        avg_focus_phrase_sim = average_with_harmonic_series(focus_phrase_sim, axis=1)  # S x 1\n",
    "\n",
    "        focus_sent_sim = cosine_similarity_embeddings(candidate_sent_embs, focus_node.emb['sentence']) # S x N\n",
    "        avg_focus_sent_sim = average_with_harmonic_series(focus_sent_sim, axis=1)  # S x 1\n",
    "\n",
    "        # compute co_occurrence with focus node\n",
    "        target_co_occurrence = np.array([average_with_harmonic_series(getBM25(sent, focus_phrases, term_to_idx, bm_score).mean(axis=0)) \n",
    "                                         for sent in candidate_phrases]) # S x 1\n",
    "        \n",
    "        # compute sibling sentence semantic dissimilarity\n",
    "        sib_phrase_sims = [np.stack([cosine_similarity_embeddings(p_embs, sib.emb['phrase']).max(axis=0)\n",
    "                                      for p_embs in candidate_phrase_embs], axis=0)\n",
    "                                      for sib in sibs] # siblings x sentences x P x N -> sib x sentences x N\n",
    "        sib_sent_sims = [cosine_similarity_embeddings(candidate_sent_embs, sib.emb['sentence']) for sib in sibs] # siblings x sentences x sib_sents\n",
    "        \n",
    "        if len(sibs):\n",
    "            avg_sib_phrase_sim = np.stack([average_with_harmonic_series(sib_sim, axis=1) for sib_sim in sib_phrase_sims], axis=-1).max(axis=1) # sentences x 1\n",
    "            avg_sib_sent_sim = np.stack([average_with_harmonic_series(sib_sim, axis=1) for sib_sim in sib_sent_sims], axis=-1).max(axis=1) # sentences x 1\n",
    "            # compute sibling co-occurrence\n",
    "            sib_co_occurrence = np.array([max([average_with_harmonic_series(getBM25(sent_phrases, sib_terms, term_to_idx, bm_score).mean(axis=0)) \n",
    "                                               for sib_terms in sibling_phrases]) for sent_phrases in candidate_phrases]) # S x 1\n",
    "        else:\n",
    "            avg_sib_phrase_sim = np.zeros_like(avg_focus_phrase_sim)\n",
    "            avg_sib_sent_sim = np.zeros_like(avg_focus_sent_sim)\n",
    "            sib_co_occurrence = np.zeros_like(target_co_occurrence)\n",
    "            \n",
    "        # compute semantic rank\n",
    "        target_sim_phrase_rank = {idx:rank for rank, idx in enumerate((avg_focus_phrase_sim-avg_sib_phrase_sim).argsort()[::-1])}\n",
    "        target_sim_sent_rank = {idx:rank for rank, idx in enumerate((avg_focus_sent_sim-avg_sib_sent_sim).argsort()[::-1])}\n",
    "        \n",
    "        # compute co-occurrence rank\n",
    "        target_co_rank = {idx:rank for rank, idx in enumerate((target_co_occurrence-sib_co_occurrence).argsort()[::-1])}\n",
    "\n",
    "        joint_rank = compute_joint_ranking([target_sim_phrase_rank, target_sim_sent_rank, target_co_rank]) # arr idx: rank\n",
    "\n",
    "        for idx in np.arange(len(sentence_pool)):\n",
    "            sent_ranks[idx].append(joint_rank[idx])\n",
    "\n",
    "    # filter sentences based on rank\n",
    "    for node_id, focus_node in enumerate(curr_node.children):\n",
    "        sorted_ranks = sorted([s_id \n",
    "                               for s_id in np.arange(len(sentence_pool)) \n",
    "                               if (sent_ranks[s_id][node_id] <= min(sent_ranks[s_id])) \n",
    "                               and (len(sentence_phrase_pool[s_id]) > 5) \n",
    "                               and ((sent_ranks[s_id][node_id]) < len(sent_ranks)//2)], \n",
    "                               key=lambda x: sent_ranks[x][node_id])\n",
    "\n",
    "        focus_node.internal['sentences'] = [sentence_pool[s_id] for s_id in sorted_ranks]\n",
    "        focus_node.internal['sent_ids'] = sorted_ranks\n",
    "        queue.append(focus_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(knowledge_graph_completion,\n",
       " ['from discrimination to generation : knowl__edge graph completion with generative_transformer',\n",
       "  'composition_based multi_ relational graph convolutional networks',\n",
       "  '; paper_content : from discrimination to generation : knowledge_graph completion with generative_transformer xin xie zhejiang_university & azft joint lab for knowledge engine hangzhou , china xx2020 @ zju.edu.cnningyu zhang zhejiang_university & azft joint lab for knowledge engine hangzhou , china zhangningyu @ zju.edu.cnzhoubo li , shumin_deng zhejiang_university & azft joint lab for knowledge engine hangzhou , china { zhoubo.li,231sm } @ zju.edu.cn hui chen alibaba_group hangzhou , china weidu.ch @ alibaba_inc.comfeiyu xiong , mosha chen alibaba_group hangzhou , china { feiyu.xfy , chenmosha.cms } @ alibaba_ inc.comhuajun chen zhejiang_university & azft joint lab for knowledge engine hangzhou , china huajunsir @ zju.edu.cn abstract knowledge_graph completion aims to address the problem of ex_ tending a kg with missing triples',\n",
       "  'multi_task_learning for knowl__edge graph completion with pre_trained language4015models',\n",
       "  'cone : cone embeddings for multi_hop_reasoning over knowledge graphs',\n",
       "  'w/o missing indicates the predicted paths contain only edges in the graph , while w/ missing suggests the predicted paths contain valid edges missing from the graph due to incompleteness ( missing edges in bold )',\n",
       "  'we present a detailed analysis of graph structure and semantics for end_to_end explanation_graph generation via pre_trained_language_models',\n",
       "  'on two real_world tasks of explanation_graph generation and temporal graph_generation , with varied node and edge se_ mantics , we observe that our proposed methods and graph perturbation techniques generalize well and lead to improvements in both structural and seman_ tic accuracy of graphs',\n",
       "  '[ sep ] stance : counter `` t5positive negativesgold graphharm the workforce loss of jobsbanning whaling capable of causes humaneis not asynthetic positive graphharm the workforce going of businessbanning whaling capable of causes humaneis not a synthetic semantic ( syse ) harm the workforce loss of jobsbanning whaling antonym of causes humaneis ahuman semantic ( huse ) occupation workforcewhaling part of used for inhumanehas propertybanningnot desires synthetic structural ( syst ) harm the workforce loss of jobsbanning whaling capable of humaneis not a , , figure 2 : our t5_based contrastive_learning framework for graph_generation using positively and three kinds of negatively perturbed graphs',\n",
       "  'firstly , its pre_training is slow due to repeat contextualizing over para_ graphs , leading to 5.6longer gpu hours than ours',\n",
       "  '5.2 max_margin graph_generation model our next model leverages the negatively perturbed graphs in a max_margin formulation',\n",
       "  'it has also been used in unconditional graph repre_ sentation learning ( you et al. , 2020 ; hassani andkhasahmadi , 2020 ; zhu et al. , 2021 )',\n",
       "  'this in_ dicates that in a more complete_graph , most of the generated paths exist in the graph',\n",
       "  'we compute the fraction of acceptable edges for every generated negative graph and choose only those graphs with ae above a certain threshold',\n",
       "  'our methods lead to significant improvements in both structural and semantic accuracy of explanation_graphs and also generalize to other similar graph_generation tasks',\n",
       "  'there is also a large body of work on building generative models for learning unconditional graph distributions ( you et al. , 2018 ; simonovsky and komodakis , 2018 ; grover et al. , 2019 ; liao et al. , 2019 ; shi * et al. , 2020 ) without any semantics at_ tached to the graphs',\n",
       "  '6.4 generalization to other graph_generation tasks we test the generalizability of constructing struc_ turally and semantically perturbed graphs for con_ trastive learning by also experimenting on a tempo_ ral graph_generation task ( madaan and yang , 2021 ) that requires constructing a temporal graph from a document',\n",
       "  '1 introduction knowledge_graph completion ( kgc ) has been a fundamental task to discover unobserved facts from various knowledge_graph ( kg ) structures , includ_ ing static kgc ( skgc ) , temporal kgc ( tkgc ) and few_shot kgc ( fkgc ) ( ji et al. , 2022 )',\n",
       "  'follow_ ing our overall goal of improving graph_generation with limited data , we randomly sample 1.3 % of the overall corpus ( 9.5k samples ) as the train__ing data such that all graphs are connected dags',\n",
       "  'while a general recipe towards improving the structural and semantic aspects of graph_generation can be via large_scale training with more human_ annotated graphs , it is prohibitive under most prac_ tical scenarios because of the cognitive_load associ_ ated with a complex data creation task like graph annotation ( dalvi et al. , 2021 ; saha et al. , 2021b )',\n",
       "  'paper_title : from discrimination to generation : knowledge_graph completion with generative_transformer ; paper_abstract : knowledge_graph completion aims to address the problem of extending a kg with missing triples',\n",
       "  'lego : latent execution_guided reasoning for multi_hop question_answering on knowledge graphs',\n",
       "  'this further motivates us to compare kg_s2s with other kgc methods on fb15k_237n which only has facts with non_cpr.4010wn18rr fb15k_237 fb15k_237n mrr h @ 1 h @ 3 h @ 10 mrr h @ 1 h @ 3 h @ 10 mrr h @ 1 h @ 3 h @ 10 graph_based methods transe ( bordes et al. , 2013 ) .243 .043 .441 .532 .279 .198 .376 .441 .255 .152 .301 .459 distmult ( yang et al. , 2015 ) .444 .412 .470 .504 .281 .199 .301 .446 .209 .143 .234 .330 complex ( trouillon et al. , 2016 ) .449 .409 .469 .530 .278 .194 .297 .450 .249 .180 .276 .380 conve ( dettmers et al. , 2018 ) .456 .419 .470 .531 .312 .225 .341 .497 .273 .192 .305 .429 rotate ( sun et al. , 2019 ) .476 .428 .492 .571 .338 .241 .375 .533 .279 .177 .320 .481 compgcn ( vashishth et al. , 2020 ) .479 .443 .494 .546 .355 .264 .390 .535 .316 .231 .349 .480 plm_based methods kg_bert ( yao et al. , 2019 ) .216 .041 .302 .524 _____ .420 .203 .139 .201 .403 mtl_kgc ( kim et al. , 2020 ) .331 .203 .383 .597 .267 .172 .298 .458 .241 .160 .284 .430 star ( wang et al. , 2021a ) .401 .243 .491 .709 .296 .205 .322 .482 _______ pkgc ( lv et al. , 2022 ) _______ _______ .307 .232 .328 .471 genkgc ( xie et al. , 2022b ) _ .287 .403 .535 _ .192 .355 .439 _______ kgt5 ( saxena et al. , 2022 ) .508 .487 _ .544 .276 .210 _ .414 _______ kg_s2s ( ours ) .574 .531 .595 .661 .336 .257 .373 .498 .353 .282 .385 .495 table 1 : results of static kgc',\n",
       "  'f1 singletagging 0.593 0.381 0.464 0.624 0.317 0.420 copyr 0.569 0.452 0.504 0.610 0.566 0.587sptree 0.492 0.557 0.522 ___ _graphr _____ 0.639 0.600 0.619 hrl 0.692 0.601 0.643 0.781 0.771 0.776wdec 0.777 0.608 0.682 0.881 0.761 0.817 pndec 0.732 0.624 0.673 0.806 0.773 0.789 ensemble hrl 0.764 0.604 0.674 0.842 0.778 0.809wdec 0.846 0.621 0.716 0.945 0.762 0.844 pndec 0.815 0.639 0.716 0.893 0.788 0.838 table 3 : performance comparison on the two datasets',\n",
       "  'structural graph constraints and their semantics',\n",
       "  'for example , consider a recently proposed commonsense expla_ nation graph_generation task shown in fig',\n",
       "  'these graph_structured databases play an important role 1resources are available at https : //github.com/ apoorvumang/kgt5in knowledge_intensive applications including web_search , question_answering and recommendation systems ( ji et al. , 2020 )',\n",
       "  '( 2018 ) , brown curve ) under different graph sizes',\n",
       "  'exploit_ ing structured knowledge in text via graph_guided representation learning',\n",
       "  'most real_world knowledge graphs are incom_ plete',\n",
       "  'specically , in two rounds , if an initial graph g1 4publicly released by saha et al',\n",
       "  'our methods lead to signicant improvements in both structural and semantic accuracy of ex_ planation graphs and also generalize to other similar graph_generation tasks',\n",
       "  'paper_title : knowledge is flat : a seq2seq generative framework for various knowledge_graph completion ; paper_abstract : knowledge_graph completion ( kgc ) has been recently extended to multiple knowledge_graph ( kg ) structures , initiating new research directions , e.g',\n",
       "  'a survey on knowledge graphs : representation , acquisition and applications',\n",
       "  'in addition , compared with the plm_based star model , kg_s2s also obtains higher performance with considerable margins in4011n_shot mrr h @ 1 h @ 5 h @ 10 graph_based methods gmatchingcomplexfive .20 .14 .26 .31 metarfive .26 .17 .35 .44 gmatchingtranseone .17 .12 .21 .26 gmatchingdistmultone .17 .11 .22 .30 gmatchingcomplexone .19 .12 .26 .31 metarone .25 .17 .34 .40 mtranshone .31 .21 .41 .48 plm_based methods starzero .26 .17 .35 .45 kg_s2s ( ours ) zero .31 .22 .41 .49 table 4 : results of few_shot kgc on nell_one',\n",
       "  'one_shot relational learning for knowledge graphs',\n",
       "  'graph_ based models are good at predicting simple struc_ ture yet inferior in absorbing kgs text',\n",
       "  'knowledge_graph completion ( kgc ) aims to complete the knowledge_graph by predicting the missing triples',\n",
       "  'this research was funded by the european unions h2020 marie skodowska_curie project knowledge graphs at scale ( knowgraphs ) under h2020_eu.1.3.1',\n",
       "  'realistic re_ evaluation of knowledge_graph completion methods : an experimental study',\n",
       "  'knowledge graphs on the web _ an overview',\n",
       "  '5 augmentation with perturbed graphs next we propose different methods of leveraging these positive and negative_graphs for explanation_graph generation',\n",
       "  'keywords knowledge_graph completion ; generation ; transformer acm reference format : xin xie , ningyu_zhang , zhoubo li , shumin_deng , hui chen , feiyu xiong , mosha chen , and huajun chen',\n",
       "  'each edge in the graph forms a coher_ ent sentence and the graph , when read as a whole,1192forms reasoning structures explaining why the ar_ gument supports or refutes the belief',\n",
       "  'we signicantly improve both the structural and se_ mantic accuracy of graph_generation by proposing contrastive_learning models that leverage simple yet efcient methods of graph perturbations and also generalize to similar graph_generation tasks.1198ethical considerations from an ethics standpoint , we provide a brief overview and show samples from the datasets that our models are trained on throughout the paper and also in the appendix',\n",
       "  '6.5 analysis of generated graphs fig',\n",
       "  \"as noted earlier , we wish to construct graphs that enable better learning of1193 '' generate an explanation_graph for belief : banning whaling is humane\",\n",
       "  'semi_ supervised classication with graph convolutional networks',\n",
       "  'incorporat_ ing the external knowledge for hybrid span genera__tion , applying more efcient sparse self_attention , and developing better search methods to nd more globally plausible graphs represented by the alter_ nating sequence',\n",
       "  'though multi_hop_reasoning is a harder task than kg completion,1649110521053105410551056105 graph size ( # of edges ) 0102030time ( hours ) method kge rl_based oursfigure 2 : training time ( in hour ) for kge model , rl_ based multi_hop_reasoning model and our multi_hop_reasoning model under varying graph sizes ( measured by the number of edges )',\n",
       "  'in this paper , we take the first step to model the knowledge_graph completion with sequence to sequence generation and propose a novel approach genkgc',\n",
       "  '4.1 setup metrics following sap et al',\n",
       "  'given a ground_truth graph g ( g ) , a positive graphg ( p ) and a set of negative_graphs { g ( n ) i } m i=1 , contrastive learn_ ing aims to learn the graph representations such that the gold graphs representation is close to that of the synthetic positive graph while being distant from those of the negative_graphs',\n",
       "  'since , as we observed , there is no reasonable path in the graph for more than 70 % of the triple queries in fb15k237 , due to the missing of relevant nodesand edges in the graph',\n",
       "  'following prior work ( saha et al. , 2021b ) , we generate explanation_graphs as post_hoc explanations by conditioning on the belief , argument and the predicted stance.2the stance pre__diction model is a ne_tuned roberta model ( liu et al. , 2019 ) which we keep unaltered from prior work and focus on the graph_generation sub_task',\n",
       "  '4.6 no_constraint inference in sparse setting earlier in the paper , we suggest our performance gain on sparse graphs comes from no_constraint generation , where there is no constraint on the path sequence generated by the model',\n",
       "  'kg_bert : bert for knowledge_graph completion',\n",
       "  'at k_th iteration ( k > 1 ) , for each triple in the graph , we partially leverage the current modeldataset # ent # rel # fact # degree mean median fb15k237 14,505 237 272,115 18.71 13 nell995 62,706 198 117,937 1.88 1 fb15k237_20 % 13,166 237 54,423 4.13 3 nell23k 22,925 200 25,445 1.11 1 kacc_m 99,615 209 642,650 6.45 4 fb100k 100,030 471 1,013,470 10.13 7 table 1 : dataset statistics',\n",
       "  'for example , the sec_ ond graph encodes the knowledge that both salads and fast_food are part of mcdonalds and hence mc_ donalds is not greasy and fattening , thus explicitly refuting the belief',\n",
       "  'structural diversity is not a measure of graph correctness ; however , like diverse text gener__ation ( vijayakumar et al. , 2018 ) , generating diverse graphs is an interesting direction for future work',\n",
       "  'stance : counter mcdonaldsfast food part of not has context greasy and fattening gold graph t5_generated graph semantically incorrect salads part offast food greasy and fatteningsalads banning them control obesitycapable of has context part of causesfigure 1 : two representative examples from expla_ graphs ( saha et al. , 2021b ) showing the belief , argu__ment , stance , gold explanation_graph , and t5_generated explanation_graph',\n",
       "  'sun , j. chen , w. zhang , and h. chen , relation adversarial network for low resource knowledge_graph completion , in proc',\n",
       "  'the performance gain of squire on sparse kg is due to the flexibility of our framework , allowing the model to dynamically complete the graph while generating the path ( a more detailed analysis is in sec',\n",
       "  '1 shows a graph generated by t5 that is disconnected and hence structurally incorrect',\n",
       "  'robust knowledge_graph completion with stacked convolutions and a student re_ranking network',\n",
       "  '3 proposed method this section first formulates knowledge_graph completion tasks in sec',\n",
       "  '( 2019 ) advance story ending generation via incremental encoding and multi_level graph convolutional networks',\n",
       "  'the con_ trastive graph_generation model ( sec',\n",
       "  'to strengthen the interpretability of kg comple_ tion , ( das et al. , 2018 ) proposes multi_hop knowl__edge graph reasoning',\n",
       "  'our framework brings about two benefits : ( 1 ) it can learn and predict in an end_to_end fashion , which gives better and faster convergence ; ( 2 ) our trans_ former model does not rely on existing edges to generate the path , and has the flexibility to complete missing edges along the path , espe_ cially in sparse kgs',\n",
       "  '4.1 positive graph perturbations one simple method to augment existing training_data is to create synthetic positive graphs',\n",
       "  '6.4 , we also experiment with another re_ lated task of temporal graph_generation ( madaan et al. , 2020 )',\n",
       "  '( 2021 ) construct a heterogeneous graph of arguments , while defnn ( yang et al. , 2021 ) pre_ dicts arguments via parallel prediction networks',\n",
       "  '5 conclusion and future work in this paper , we present kg_s2s for various knowledge_graph completion tasks',\n",
       "  'valid stca g_bs t5_base 88.8 88.7 54.4 max_margin 89.1 87.7 55.7 contrastive 97.5 96.9 57.2 table 4 : comparison of t5 , max_margin and con_ trastive models for temporal graph_generation',\n",
       "  'corresponding author albert einsteinmilevamari germanhans albert_einstein germanyspouseborn inchild languagechild ? native language ? eth zuricheducated atfigure 1 : an example of multi_hop_reasoning in an in_ complete knowledge_graph',\n",
       "  'the trained graph ( 0.183 0.075 ) , while 75 % of them contain only edges from fb15k237 ( 0.183 0.138 ) , a more complete set of edges',\n",
       "  'otherwise , we only se_ lect the optimally matched gold spans for bipartite_matching loss calculation',\n",
       "  'the max_margin model outperforms the pos data aug model because of the former having access to both structural and semantic supervision while the latter is only augmented with structurally similar graphs',\n",
       "  'our framework brings about two benefits : ( 1 ) it can learn and predict in an end_to_end fashion , which gives better and faster convergence ; ( 2 ) our transformer model does not rely on existing edges to generate the path , and has the flexibility to complete missing edges along the path , especially in sparse kgs',\n",
       "  'both t5_generated graphs shown in fig',\n",
       "  'this can be attributed to our struc_ turally similar positive graphs as the model does not obtain enough supervision to generate diverse graphs',\n",
       "  'the visualization result sug_ gests that the transformer model has memorized the graph during training , and in generation phase it predicts the next token based on the current po_ sition and adjacent nodes or edges ( see results and detailed analysis in appendix d )',\n",
       "  'dackgr ( lv et al. , 2020 ) further applies dynamic anticipation and completion in sparse kgs',\n",
       "  'we rst show that with limited supervision , pre_trained_language_models often generate graphs that either vio_ late these constraints or are semantically inco_ herent',\n",
       "  'to solve these problems , we propose a rst_order approach that invertibly maps the target graph to an alternating_sequence of nodes and edges , and applies a hybrid span generator that directly learns to generate such alternating sequences',\n",
       "  'few_shotslot tagging with collapsed dependency transfer andlabel_enhanced task_adaptive projection network',\n",
       "  'hyte : hyperplane_based temporally aware knowledge_graph embedding',\n",
       "  'we show the versatility of this approach through the task of kgqa over incomplete graphs',\n",
       "  'a bipartite_matching_loss finally optimizes the global span assignment',\n",
       "  'surprisingly , kg_s2s is able to achieve superior performance than all the variations of previous graph_based models , which transfer knowledge from the training_data to the evaluation relations ( i.e. , one_shot and five_shot meta learning )',\n",
       "  'from discrimination to generation : knowledge_graph completion with generative_transformer',\n",
       "  'we furthermore assume that s=s 0 , ... , s n that represent graphs have an alternating struc_ ture , where s 0 , s 2 , s 4 , ... represent nodes v , and s 1 , s 3 , ... represent actual or virtual edges',\n",
       "  'our methods lead to signicant improvements in both structural and semantic accuracy of expla_ nation graphs and also generalize to other similar graph_generation tasks.11912 related work graph_generation from language_models',\n",
       "  'second , our approach does not involve a graph verication stage and thus , the rener model acts on all ( correct and incorrect ) graphs generated in stage 1 and is thus trained with both correct and incorrect graphs',\n",
       "  'the rst generated graph is structurally incor_ rect and the second graph is semantically incorrect',\n",
       "  'we propose simple yet effective graph pertur_ bation techniques for constructing positive and negative_graphs and use them in different graph contrastive_learning models',\n",
       "  'hence , our ap__proach towards improving explanation_graph gener__ation is through data_augmentation techniques that perturb human_curated graphs to construct positive and negative_graphs',\n",
       "  'since curating large amount of human_annotated graphs is expensive and tedious , we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative_graphs',\n",
       "  'previous research ( zuo et al. , 2018 ; lovelace et al. , 2021 ) have shown the utility of the descriptions when integrated with traditional graph_based kgc models',\n",
       "  'we follow this rich line of work to explore their applicabil_ ity in supervised graph_generation tasks from pre__trained language_models in low_resource settings',\n",
       "  'our novelty lies in presenting the rst systematic analysis of structure and se_ mantics of graph_generation for two downstream nlp tasks using pre_trained_language_models and improving them via constrastive learning',\n",
       "  'rotate : knowl__edge graph embedding by relational rotation in complex space',\n",
       "  'improving multi_hop question_answering over knowledge graphs using knowledge_base embeddings',\n",
       "  'a.5 further analysis of bipartite_matching ablation_studies have validated the effectiveness of bipartite_matching loss',\n",
       "  '5.4 contrastive graph_generation model our contrastive graph_generation model ( fig',\n",
       "  'relational learning with gated and attentive neighbor aggregator for few_shot knowledge_graph completion',\n",
       "  'we compare the results on the explagraphs validation set by leveraging synthetic structural ( syst ) , synthetic semantic ( syse ) and human_created semantic ( huse ) graphs with the max_margin graph_generation model',\n",
       "  'stance : counter increases empathycollectivism capable of causes improve human relationship terrible for societyis not aincreases empathycollectivism capable of terrible for societyis not aempathy societynot part of gold grapht5_generated graph structurally and semantically incorrect belief : since fast foods are greasy and fattening , banning them would control obesity',\n",
       "  'drop mrr h @ 10 src tgt rel ent baseline _______ _ .280 .416 _______ .326 .453 _____ .350 .478 ___ .350 .486 ___ .338 .468 kgt5 _______ .226 .335 ___ .233 .341 kg_s2s _ .353 .495 table 5 : ablation for the kg_s2s input components onfb15k_237n',\n",
       "  'hence , in this nal section , we further explore whether it is also possible to automatically imitate and generate more of such harder human_ likeincorrect graphs for the remaining samples as well',\n",
       "  'despite the challenges of com_ monsense modeling , our investigation reveals promising results when implicit knowledge from deep pre_trained_language_models is transferred to generate explicit_knowledge in commonsense knowledge graphs',\n",
       "  'in this paper , we provide an approach genkgc , which converts knowledge_graph completion to sequence_to_sequence generation_task with the pre_trained lan__guage model',\n",
       "  'since curating large amount of human_ annotated graphs is expensive and tedious , we propose simple yet effective ways of graph per_ turbations via node and edge edit operations that lead to structurally and semantically pos_ itive and negative_graphs',\n",
       "  '0 10 20 30 40 sec/ docscirex_p tanl tempgenmodelfigure 7 : inference time comparison on the s cirex 4_ary re task',\n",
       "  'a popular approach for such a challenge is knowledge_graph embedding ( kge ) ( bordes et al. , 2013 ; dettmers et al. , 2018 ) , which infers a missing edge in a complete black_box manner',\n",
       "  'we observe that our contrastive model not only gener_ ates more valid graph encodings but also improves stca by 8 % and g_bs by 3 % .1197stca seca g_bs ged ea syst + syse + huse 49.5 38.4 39.4 0.64 26.1 syst + syse + huse + huse_gen ( ip ) 53.5 38.7 42.1 0.62 28.1 syst + syse + huse + huse_gen ( ae ) 52.0 40.2 41.3 0.62 28.2 table 5 : effect of training the max_margin model with additional human_like semantic negative_graphs on explagraphs dev set',\n",
       "  'we vary the amount of training_data from 500 to 2368 samples ( all ) and report stca and seca along with other metrics like graph_bertscore ( g_ bs ) introduced in prior work ( saha et al. , 2021b )',\n",
       "  '6.2 human evaluation of graph semantics automatically evaluating graphs for semantic cor_ rectness is challenging',\n",
       "  'do pre__trained models benefit knowledge_graph completion ? a reliable evaluation and a reasonable approach',\n",
       "  'compared with graph_based meth__ods , kg_s2s consistently obtains performance gain on wn18rr , though maintaining a modest result on fb15k_237',\n",
       "  '2 shows an example of a positive graph perturbation where the node loss of jobs is re_ placed with going of business',\n",
       "  'finally , our best seca is far from perfect and signicant future work can be done in improving the graph semantics',\n",
       "  '4 graph perturbations most prior works that collect human_annotated graphs for a downstream nlp task have found such collection processes to be quite expensive and tedious ( tandon et al. , 2019 ; dalvi et al. , 2021 ; saha et al. , 2021b )',\n",
       "  'recent work has focused on kgqa over in_ complete kgs , which is also the focus of our work',\n",
       "  'we first show that with limited supervision , pre_trained_language_models often generate graphs that either violate these constraints or are semantically incoherent',\n",
       "  'the semantic aspect deals with common__sense and evaluates whether each edge expresses coherent relational knowledge and if the whole graph explains the stance',\n",
       "  'kg_s2s outperforms several competitive baseline models , including graph_based and plm_based models , and sets new state_of_the_art performance on all three settings',\n",
       "  'we construct positive graphs by replacing edges like a before b with b after a ( more details in appendix c )',\n",
       "  'while the structural accuracy improves with in_ crease in training_data , the gain saturates quickly and even after training on the entire data , we nd a signicant fraction of graphs to violate the struc_ tural constraints',\n",
       "  'generating an alternat_ ing sequence is equivalent to generating the original information graph',\n",
       "  'furthermore , our approach naturally overcomes the missing path problem , since the transformer does not explicitly rely on existing edges in the graph to generate the path sequence',\n",
       "  'the positive graphs , being structurally correct , also reinforces the models be_ lief about structural correlation with correct graphs , thus leading to some improvement in stca as well',\n",
       "  'the obtained traversal embedding is then pointwisely added to the hidden represen__tation of the alternating_sequence hyfor injecting the traversal information of the graph structure',\n",
       "  'note that human_created graphs can only be seman_ tically incorrect , since their structural correctness is already ensured during construction',\n",
       "  'it works on the principle that an explanation_graph is semantically correct if the stance inferred from the belief and the graph matches the gold stance',\n",
       "  'relations mrr h @ 1 h @ 3 h @ 10 rotate cpr .337 .232 .374 .552 non_cpr .340 .254 .376 .504 all .338 .241 .375 .533 kg_s2s cpr .318 .234 .355 .493 non_cpr .363 .292 .398 .504 all .336 .257 .373 .498 table 2 : evaluation of cartesian_product relations ( cprs ) and non_cartesian product relations ( non_cprs ) onfb15k_237 as shown in table 1 , kg_s2s obtains the best re__sults compared with graph_based and plm_based baselines at all metrics',\n",
       "  '4.2 experimental_results static kgc we compare our results with vari_ ous graph_based and plm_based methods on the skgc settings',\n",
       "  'learning sequence encoders for temporal knowledge_graph completion',\n",
       "  'however , they often lead to semantically incon_ sistent graphs',\n",
       "  '7 conclusion we presented an empirical study of graph structure and semantics for end_to_end explanation_graph generation from pre_trained_language_models and showed that the generated graphs often violate structural_constraints or are semantically incorrect',\n",
       "  '9510modelnyt webnlg prec rec f1 prec rec f1 noveltagging .642 .317 .420 .525 .193 .283 copyre_one ( ours ) .612 .530 .571 .312 .272 .291 copyre_mul ( ours ) .610 .566 .587 .319 .273 .294 graphrel_1p .629 .573 .600 .423 .392 .407 graphrel_2p .639 .600 .619 .447 .411 .429 copymtl_one .727 .692 .709 .578 .601 .589 copymtl_mul .757 .687 .720 .580 .549 .564 table 1 : results of the compared models on nyt and webnlg , in which copyre uses less strict evaluation',\n",
       "  'we drop out of the bipartite_matching_loss and ignore the global opti_6764model bipartite matchingmulti_arg promptrole_specific selectorplmarg_c ace05 rams wiki paie bart_b 69.80.9849.50.6563.41.17 paie_ w/o bipartite bart_b 68.91.0349.40.9862.41.09 paie_ w/o multi_prompt bart_b 66.90.6147.61.2059.91.26 eeqa_bart bart_b 67.70.6446.30.7757.10.82 eeqa bert_b 65.4 44 .0 53 .2 table 3 : ablation_study on three benchmarks',\n",
       "  'the uncovered results of graph_based methods are obtained through hyperparameter tuning with libkge ( broscheit et al. , 2020 ) and plm_based methods through official implementations',\n",
       "  'com/chenchens190009/kg_s2s figure 1 : running examples of static ( skgc ) , tempo_ ral ( tkgc ) and few_shot ( fkgc ) knowledge_graph completion tasks',\n",
       "  'note that our node replacement operations will always lead to struc_ turally similar graphs',\n",
       "  'to enable learning these constraints , we generate four types of negative_graphs by performing the following perturbations on each ground_truth graph : ( 1 ) re_ moving an edge at random such that the resultant graph becomes disconnected , ( 2 ) adding an edge between two randomly chosen nodes such that the resultant graph becomes cyclic , ( 3 ) adding and re_ moving one edge at random such that the resultant graph becomes both disconnected and cyclic , ( 4 ) removing a node randomly such that the resultant graph contains less than two concepts from the belief or argument',\n",
       "  'previous works often design kgc models closely coupled with specific graph structures , which inevitably re__sults in two drawbacks : 1 ) structure_specific kgc models are mutually incompatible ; 2 ) ex_ isting kgc methods are not adaptable to emerg_ ing kgs',\n",
       "  '3 motivation and background our primary task of interest is a recently pro_ posed commonsense explanation_graph genera__tion task called explagraphs ( saha et al. , 2021b )',\n",
       "  'extracting summary knowledge graphs from long documents',\n",
       "  'this represen__tation allows us to unambiguously recover the original graph , if we know which type of graph traversal is assumed ( bfs or dfs ) .3algorithm 1 ( which we use to translate graphs in the training_data to sequences ) shows how an alternating se__quence for a given graph can be constructed with bfs traversal',\n",
       "  'the technique works by rst taking a genuine news article , extract_ ing a multimedia knowledge_graph , and replacing or inserting salient nodes or edges in the graph',\n",
       "  'hence , we propose simple yet effective methods of graph perturbations that perform various kinds of node and edge addition , deletion , and replace_ ment operations to construct structurally and se_ mantically positive ( correct ) and negative ( incor_ rect ) graphs',\n",
       "  '3http : //blender.cs.illinois.edu/software/oneie/hyperparameter value batch_size 16 ( ace05 ) / 4 ( others ) weight_decay 0.01 training steps 10000 optimizer adamw adam 1108 adam 1/2 0.9 / 0.999 scheduler linear ( with 0.1 warmup step ) max span length 10 max gradient norm 5.0 window size 250 max encoder seq length 192 ( ace05 ) / 500 ( others ) max decoder seq length 80 table 9 : hyperparameters for paie a.4 details of bipartite_matching loss we formulate the details of bipartite_matching loss in this section',\n",
       "  'paper_title : comet : commonsense transformers for automatic knowledge_graph construction ; paper_abstract : we present the first comprehensive study on automatic knowledge_base construction for two prevalent commonsense knowledge graphs : atomic ( sap et al. , 2019 ) and conceptnet ( speer et al. , 2017 )',\n",
       "  'this task termed knowledge_graph com_ pletion ( kgc ) 2has become a popular area of re_ search in recent years ( wang et al. , 2017 ) and is of_ ten approached using knowledge_graph embedding ( kge ) models',\n",
       "  'while graphs can also be generated rst , followed by the stance , we experiment with one model family for this work.count stca seca g_bs ged ea 500 42.5 20.7 36.3 0.68 20.4 1000 49.2 23.7 42.2 0.63 26.2 1500 50.7 33.2 43.4 0.61 28.2 2368 51.0 34.7 43.9 0.61 29.5 table 1 : performance of t5_large with varying amount of training_data on explagraphs test_set',\n",
       "  'reasoning through memorization : near_ est neighbor knowledge_graph embeddings',\n",
       "  'despite the challenges of commonsense modeling , our investigation reveals promising results when implicit knowledge from deep pre_trained_language_models is transferred to generate explicit_knowledge in commonsense knowledge graphs',\n",
       "  'on re_ training t5 augmented with the positively per_ turbed graphs ( sec',\n",
       "  'structure_augmented text representation learning for efficient knowledge_graph completion',\n",
       "  'tucker : tensor factorization for knowledge_graph completion',\n",
       "  'since we insert multiple slots about this role and each slot generates one prediction , it is a canonical bipartite_matching problem that matches local_optimal pre_ dictions ( of each slot ) and ground_truth as much as possible',\n",
       "  'we observe that while moderate amount of supervision enables the model to learn valid graph encodings , the graphs frequently vio_ late task_specic structural_constraints ( like con_ nectivity )',\n",
       "  'effect of decoding algorithm in table 3 , we show the effect of different generation policies on knowledge quality',\n",
       "  'automatically constructing structurally diverse positive graphs is a challenging problem and we leave that for future work',\n",
       "  'the space of semantically incorrect graphs is fairly large and in order to augment our synthetic negative_graphs with harder structurally_ diverse negatives , we make use of human_created incorrect graphs from prior work ( saha et al. , 2021b ) .4humans make subtle errors , thus mak_ ing them ideal negative candidates for contrastive_learning',\n",
       "  'previous works often design kgc models closely coupled with specific graph structures , which inevitably results in two drawbacks : 1 ) structure_specific kgc models are mutually incompatible ; 2 ) existing kgc methods are not adaptable to emerging kgs',\n",
       "  '2017.model_agnostic meta_learning for fast adaptation ofdeep networks',\n",
       "  'in practice , we observe that most graphs do not change much after renement which we believe stems from the models inability to distinguish between correct and incorrect graphs.effect of positive graph perturbations',\n",
       "  'similarly , the seman_ tically perturbed graphs improves the models rela__tion prediction capability between concepts',\n",
       "  'd egree achieves a much better performance than other baselines',\n",
       "  'the reason why squire benefits more on fb15k237 lies in the density of the two graphs : fb15k237 is denser and thus the two strategies can help distinguish the real evidential_path among a larger set of random paths between handt , with thedataset fb15k237 nell995 kacc_m fb100k minerv a 18.4 11.6 32.4 55.3 multihopkg 19.5 12.0 33.3 57.8 squire 3.2 ( 6x ) 3.5 ( 4x ) 7.5 ( 4x ) 8.5 ( 7x ) table 4 : training time ( in hour ) of rl_based minerv a , multihopkg and our model on four datasets including two standard kgs and two larger kgs',\n",
       "  'this model is trained to learn to en_ code knowledge in both directions : sroand 8https : //ttic.uchicago.edu/~kgimpel/ commonsense.html 9a pre_trained model can be found at https : //ttic.uchicago.edu/~kgimpel/comsense_ resources/ckbc_demo.tar.gzmodel ppl score n/t sro n/tohuman lstm _ s _ 60.83 86.25 7.83 63.86 ckbg ( saito et al. , 2018 ) _ 57.17 86.25 8.67 53.95 comet ( _ pretrain ) 8.05 89.25 36.17 6.00 83.49 comet _ reltok 4.39 95.17 56.42 2.62 92.11 comet 4.32 95.25 59.25 3.75 91.69 table 6 : conceptnet generation results orsto help augment a knowledge_base com_ pletion model',\n",
       "  '4.2 negative graph perturbations in order to enable the model to learn from explicit hard negatives , we construct three diverse types of graphs synthetically constructed structural nega_ tives for learning graph constraints and synthetic 3we also tried similar replacement operations with antonyms',\n",
       "  'relation adversarial network for low resource knowledge_graph completion',\n",
       "  'intuitively , there is no need for negative sampling as we directly optimize by predicting 163from discrimination to generation : knowledge_graph completion with generative_transformer www 22 companion , april 2529 , 2022 , virtual event , lyon , france table 2 : experiment results on wn18rr , fb15k_237 and openbg500',\n",
       "  'we also construct semantically incorrect negative explanation_graphs',\n",
       "  'graphvite : a high_performance cpu_gpu hy_ brid system for node embedding',\n",
       "  'on the other hand , it captures argument interactions via multi_role prompts and conducts joint optimization with optimal span assignments via a bipartite_matching_loss',\n",
       "  'a re_ evaluation of knowledge_graph completion methods',\n",
       "  'thus we dont need to provide any external information about the graph to our model during inference',\n",
       "  '5.1 augmentation with positive graphs in this rst simple approach , we augment the train__ing data with the synthetically created positive graphs and retrain the baseline t5 model',\n",
       "  'our research investigates how pre_trained_language_models can be used for large_scale com_monsense kb construction by generating new graph nodes and edges between nodes',\n",
       "  'two sparse datasets include 4we apply a new training/valid/test split on the whole nell995 graph ( without inverse relations ) , since there is an inconsistency in evaluation in previous studies.1653fb15k237 nell995 fb15k237_20 % nell23k mrr h @ 1 h @ 3 h @ 10 mrr h @ 1 h @ 3 h @ 10 mrr h @ 1 h @ 3 h @ 10 mrr h @ 1 h @ 3 h @ 10 embedding_based methods transe ( bordes et al. , 2013 ) .425 .320 .475 .635 .371 .209 .473 .654 .263 .178 .288 .434 .179 .076 .208 .379 conve ( dettmers et al. , 2018 ) .438 .342 .483 .627 .542 .449 .594 .709 .264 .187 .284 .422 .279 .193 .301 .467 rotate ( sun et al. , 2018 ) .426 .321 .474 .635 .513 .411 .570 .708 .265 .185 .286 .430 .217 .141 .232 .368 tucker ( balaevi c et al. , 2019 ) .451 .357 .495 .635 .511 .422 .556 .682 .246 .178 .261 .384 .207 .143 .224 .338 cone ( bai et al. , 2021 ) .446 .345 .490 .645 .543 .448 .602 .715 .274 .193 .298 .437 .234 .158 .249 .400 rule_based methods anyburl ( meilicke et al. , 2019 ) _ .300 .405 .544 _ .389 .521 .628 _ .159 .240 .359 _ .140 .203 .292 rl_based methods minerv a ( das et al. , 2018 ) .275 .199 .306 .433 .391 .293 .449 .575 .123 .070 .133 .236 .151 .101 .159 .247 multihopkg ( lin et al. , 2018 ) .407 .327 .443 .564 .467 .388 .512 .609 .231 .167 .250 .361 .178 .124 .188 .297 ruleguider ( lei et al. , 2020 ) .387 .297 .428 .563 .417 .344 .476 .582 .094 .042 .094 .21 .112 .030 .140 .273 dackgr ( lv et al. , 2020 ) .347 .274 .382 .493 .421 .347 .464 .554 .246 .180 .270 .386 .197 .133 .211 .337 sequence_based methods squire .421 .329 .465 .606 .498 .409 .550 .668 .249 .180 .272 .401 .233 .157 .256 .389 squire + self_consistency .433 .341 .476 .617 .519 .434 .570 .682 .253 .180 .276 .406 .244 .165 .269 .412 table 2 : link_prediction results on four benchmark datasets',\n",
       "  'in fact , this lets us evaluate the semantic aspect in isolation when both graphs are structurally correct',\n",
       "  'ee has been shown to benet a wide range of applications , e.g. , building knowledge graphs ( zhang et al. , 2020 ) , question_answering ( berant et al. , 2014 ; han et al. , 2021 ) , and other downstream studies ( han et al. , 2019a ; hogenboom et al. , 2016 ; sun and peng , 2021 )',\n",
       "  'therefore , it is intuitive to find a new technical solution for knowledge_graph completion',\n",
       "  'overall , we leverage three types of negative_graphs ( synthetic structural , synthetic se_ mantic , and human_created semantic ) and develop multiple contrastive_learning models ( hjelm et al. , 2018 ; chen et al. , 2020a ; khosla et al. , 2020 ; gunel et al. , 2020 ) for effectively distinguishing between correct and incorrect graphs',\n",
       "  'in this work , we propose ahybrid span gener ator ( hyspa ) that in_ vertibly maps the information graph to an al_ ternating sequence of nodes and edge_types , and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities',\n",
       "  'explanation_graph genera__tion improves the interpretability of neural com_ monsense reasoning systems and could prove to be effective in understanding and debugging such models',\n",
       "  'in table 4 , we report structural correctness accu_ racy ( stca ) ( percentage of connected dags ) and graph_bertscore ( g_bs ) for measuring approxi_ mate semantic correctness wrt gold graphs',\n",
       "  'paper_title : squire : a sequence_to_sequence framework for multi_hop knowledge_graph reasoning ; paper_abstract : multi_hop knowledge_graph ( kg ) reasoning has been widely studied in recent years to provide interpretable predictions on missing links with evidential paths',\n",
       "  'we suspect that this is because the resultant alternating_sequence from dfs is often longer than the one from bfs due to the nature of the knowledge graphs , and thus increases the learning difculty',\n",
       "  'we experiment with two graph_generation tasks , primarily focus_ ing on explagraphs ( saha et al. , 2021b ) because of the clear distinction in the underlying structural_constraints and the semantic aspect dealing with commonsense',\n",
       "  '( 4 ) bipartite match_ ing loss has an average improvement of 0.7 % on three benchmarks',\n",
       "  '( 2018 ) builds upon this work by proposing a joint model for completion and generation of com_ monsense tuples',\n",
       "  'let the graph representations be denoted by h ( g ) , h ( p ) and { h ( n ) i } m i=1',\n",
       "  'we observe that our model outperforms all previous multi_hop_reasoning methods across four datasets by a large margin on most metrics , regardless of the sparsity of the graph and squire does not require additional design as dackgr does',\n",
       "  'next , we leverage these graphs in different contrastive_learning models with max_margin and infonce losses',\n",
       "  'on the task of kgqa on incomplete kgs , we found that our unified ap__proach outperformed baselines on multiple large_ scale benchmark datasets',\n",
       "  'on the other hand , it captures argu__ment interactions via multi_role prompts and conducts joint optimization with optimal span assignments via a bipartite_matching_loss',\n",
       "  'together with the results reported in table 1 , we conclude that much of the improvement in explanation_graph gen__eration comes from increasing the training_data and using a larger model',\n",
       "  'in fig.4 , we report hits @ 1 of models trained on fb15k237 and its subgraph fb15k237_20 % 10 , under constraints of edges in fb15k237 and edges in fb15k237_20 %',\n",
       "  'effect of positive and negative graph pertur_ bations with contrastive_learning',\n",
       "  'virtual edge typesu= { [ sos ] , [ eos ] , [ sep ] } do not repre_ sent edges in g , but serve to control the genera__tion of the sequence , indicating the start/end of sequences and the separation of levels in the graph',\n",
       "  'how_ ever , there has been relatively less work on analyzing their ability to generate structured outputs such as graphs',\n",
       "  'however , there has been relatively less work on analyzing their ability to generate structured outputs such as graphs',\n",
       "  'deep graph convolutional encoders for structured data to text gen_eration',\n",
       "  '( 2021b ) at https : //github.com/swarnahub/explagraphs/blob/ main/data/refinement_graphs_train.tsv .1194is rened into graphs g2andg3successively , then g1andg2are considered as negative_graphs',\n",
       "  'prex length l.we rst study the impact of prex length lby grid search in { l|l= 10 k , knk12 }',\n",
       "  'considering the incomplete kg envi_ ronment that might give low_quality rewards , mul_ tihopkg ( lin et al. , 2018 ) proposes reward shap_ ing from a pretrained kge model',\n",
       "  'overall , these results indicate that there is a signicant scope for improvement both on graph structure and seman_ tics , thus motivating us to develop methods with design choices aimed at improving both aspects',\n",
       "  'this can potentially be improved by incor_ porating more structurally diverse graphs',\n",
       "  'in both these tasks , the structural aspect deals with satisfying certain task_specic constraints on the graph ( like connectivity ) and the semantic aspect deals with the construction of meaningful edges ( that adhere to commonsense )',\n",
       "  'we note that a high 91 % of t5s generations are valid graph encodings i.e. , the gen_ erated strings can be parsed into graphical struc_ tures ( without any post_processing ) , suggesting that t5 is able to learn the graph encoding from a fairly small amount of supervision',\n",
       "  'we keep the type indices in the graph unchanged because they are smaller thanlpandklp',\n",
       "  'however , it is not scalable due to its underlying cross_encoder.3qa meth__ods which leverage kges outperform traditional kgqa approaches on incomplete kgs , but com_ bining kges with the qa pipeline is a non_trivial task ; models that attempt to do this often work on only limited query types ( huang et al',\n",
       "  'the results suggest that our model can in_ deed , walk and complete , which has been shown to be effective in sparse setting',\n",
       "  'this scenario has no effect on the traditional graph_based kgc models because they do not access the text at all',\n",
       "  'morgan kaufmann publishers inc. isbn 1558607781',\n",
       "  'our models either use only posi_ tive graphs as simple data_augmentation , only neg_ ative graphs in a max_margin model , or both in a generate & rene model and a contrastive model',\n",
       "  'paie usually runs 3_4 times faster than eeqa , since it predicts multiple roles simultaneously , while eeqa predicts roles one by one',\n",
       "  '4.4 discussion comparison with previous sota plm_based methods from table 1 and table 4 , kg_s2s outperforms previous sota encoder_only star methods on mrr and hit @ 1,3',\n",
       "  'as shown in table 2 , after we remove the traver_ sal embedding the re f1_scores drop signicantly , which indicates that our traversal embedding can help encode the graph structure and improve rela__tion predictions',\n",
       "  'intuitively , this ensures that a high fraction of the generated edges are actually incorrect and hence when added to the correct graph , will lead to a sufciently different ( human_like ) incorrect graph',\n",
       "  'in this work , we propose a hybrid span generator ( hyspa ) that invertibly maps the information graph to an alternating_sequence of nodes and edge_types , and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities',\n",
       "  '1 : number of generated triplets k=t/3 ; generated triplets list f= [ f1 , ... , f k ] ; already generated triplets set v= { }',\n",
       "  'd dfs traversal embedding since the parent_child information is already con_ tained in the intra_level connections of dfs traver_ sal , we only have the sum of the level embedding and the connection embedding for dfs traversal 5https : //nlp.stanford.edu/projects/ glove/ 6https : //github.com/lorrinwww/ two_are_better_than_one/blob/master/ layers/encodings/embeddings.py 7https : //www.ldc.upenn.edu/ collaborations/past_projects/ace 8https : //github.com/dwadden/dygiepp/ tree/master/scripts/data/ace05/ preprocess figure 7 : the general model architecture of the mixed_ attention transformer',\n",
       "  'following prior work ( sun et al. , 2019a ) we ran_ domly drop 50 % of edges from all kgs to simulate kg incompleteness',\n",
       "  'on few_shot and low_resource settings , uie exhibits strong on_demand adaptation ability : it outperforms baselines dramatically by a large margin',\n",
       "  'huang and jia ( 2021 ) convert documents to unweighted graph and use gat to alleviate the role overlapping is_ sue',\n",
       "  'zero_shot deepstruct 10b outperforms zero_shot gpt_3 175b by a large margin without any prompt engineering',\n",
       "  'note that triplet contrastive_learning and triple generation are two different tasks , and optimizing them jointly is non_trivial , algorithm 1 : n_tuple contrastive_learning',\n",
       "  'these graphs should be created such that all the task_ specic constraints continue to hold upon perturba_ tions',\n",
       "  'all types of negatives graphs lead to consistent increase in seca',\n",
       "  'for fair comparison , we evaluate only those samples where both models predict the correct stance and the graphs are also structurally correct',\n",
       "  'we compare the graphs generated by t5 and our max_margin model on amazon_mechanical_turk where three annotators choose which graph is better or if they are mostly similar ( instructions in appendix f )',\n",
       "  'unlike t5 , our models graphs are both structurally and semantically correct with diverse common__sense nodes ( groupthink , good thing )',\n",
       "  '2017 ) and graphrel ( fu , li , and ma 2019 )',\n",
       "  'ogb_lsc : a large_scale challenge for machine_learning on graphs',\n",
       "  'representative works on graph_generation from lan__guage models include knowledge_graph completion models like comet ( bosselut et al. , 2019 ; hwang et al. , 2021 ) that ne_tune gpt ( radford et al. , 2019 ; brown et al. , 2020 ) and bart ( lewis et al. , 2020 ) , generation of event inuence graphs ( tan_ don et al. , 2019 ; madaan et al. , 2020 ) , partially_ordered scripts ( sakaguchi et al. , 2021 ) , tempo_ ral graphs ( madaan and yang , 2021 ) , entailment trees ( dalvi et al. , 2021 ) , proof graphs ( saha et al. , 2020 ; tafjord et al. , 2021 ; saha et al. , 2021a ) and commonsense explanation_graphs ( saha et al. , 2021b )',\n",
       "  'we treat these rened graphs as negatives',\n",
       "  'similar to cao and wang ( 2021 ) , we use the last layer of the decoder in t5 as the representation of each token in the graph and obtain the graph representation by averaging over the constituent token representations',\n",
       "  'the length |s|is bounded linearly by the size of the graph o ( |s| ) =o ( |v|+|e| ) ( which is also the com_ plexity of typical graph traversal algorithms like bfs/dfs )',\n",
       "  'we see that for the model trained on fb15k237 , out of 34.2 % paths that it correctly predicted ( with_ out constraint ) , 85 % of them contain only edges from the trained graph ( 0.342 0.291 )',\n",
       "  'we create a training set of 6 mil_ lion paragraphs and a validation set of 1000 para_ graphs',\n",
       "  'stance : counter increases empathycollectivism capable of causes improve human relationship terrible for societyis not a increases empathycollectivism capable of terrible for societyis not aempathy societynot part ofincreases empathycollectivism capable of terrible for societyis not agroupthinksynonym ofincreases empathycollectivism capable of good thingis a terrible for societynot has context gold graph t5_generated graph structurally and semantically incorrectmax_margin graphcontrastive graphfigure 3 : qualitative analysis of explanation_graphs',\n",
       "  'e.g. , removing a node that makes the graph disconnected is a prohibitive action',\n",
       "  'that is , our proposed method has the flexibility to walk and complete : automatically infers missing edges along the path',\n",
       "  'after finetuning this model on the task of kgqa over incomplete kgs , our approach outperforms baselines on multiple large_scale datasets without extensive hyperparameter tuning',\n",
       "  'models rely on various transitional relations over graph paths ( trouillon et al. , 2016 ; dettmers et al. , 2018 ; vashishth et al. , 2020 )',\n",
       "  'specically , our approach is a 2_stage pipeline rst , an initial graph is generated by the baseline t5 model and second , an expla_ nation graph renement model conditions on the initial graph , along with the belief , argument and the stance to rene the graph',\n",
       "  'generated output : six days after starting [ acicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviraciclovir |drug ] she exhibited signs of [ [ lithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithium |drug ] toxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicity |disease |effect =aciclovir |effect =lithium ]',\n",
       "  '5 ) , we first apply the pathpred technique ; if the result_ ing answer set is empty which can happen due to kg incompleteness we apply kgt5 to get the answer.2827question type gtq kgt5 gain actormovie 0.96 0.95 _0.01 director movie 0.84 0.92 0.08 movieactor 0.79 0.77 _0.02 moviedirector 0.52 0.64 0.12 moviegenre 0.48 0.63 0.15 movielanguage 0.49 0.63 0.14 movietags 0.72 0.7 _0.02 moviewriter 0.66 0.8 0.14 movieyear 0.46 0.45 _0.01 tagmovie 1 0.96 _0.04 writermovie 0.88 0.94 0.06 all 0.678 0.732 0.054question type gtq kgt5 gain actormoviedirector 0.44 0.39 _0.05 director moviedirector 0.34 0.62 0.28 director movielanguage 0.37 0.77 0.4 writermoviewriter 0.39 0.39 0 actormoviegenre 0.48 0.55 0.07 director moviegenre 0.46 0.7 0.24 actormovieactor 0.57 0.09 _0.48 writermovieactor 0.51 0.31 _0.2 actormoviewriter 0.48 0.44 _0.04 moviedirector movie 0.45 0.21 _0.24 actormovieyear 0.48 0.23 _0.25 writermoviegenre 0.4 0.59 0.19 director movieactor 0.51 0.5 _0.01 movieactormovie 0.73 0.06 _0.67 writermovieyear 0.37 0.35 _0.02 director movieyear 0.45 0.51 0.06 director moviewriter 0.47 0.44 _0.03 moviewritermovie 0.5 0.3 _0.2 writermoviedirector 0.33 0.31 _0.02 writermovielanguage 0.32 0.66 0.34 actormovielanguage 0.4 0.54 0.14 all 0.471 0.363 _0.108 table 15 : hits @ 1 performance on metaqa 1_hop ( left ) and 2_hop ( right ) validation dataset , 50 % kg setting',\n",
       "  'we analyze the quality of the explanation_graphs generated by t5 in ta__ble 1',\n",
       "  'story generationevent process completion size b_4 r_l bert b_4 r_l bert b_1 b_2 b_1 b_2 b_1 b_2 selected task_specific models with competitive performance grf ( ji et al. , 2020 ) _ 11.62 34.62 ___ _______ _______ ie+msa ( guan et al. , 2019 ) _____ _______ 24.40 7.80 _______ plan & write ( yao et al. , 2019 ) _____ _______ 24.40 8.40 30.80 12.60 ___ fine_tuning with pre_trained unified ( generative ) model gpt2_s ( radford et al. , 2019 ) 124m 2.23 22.83 48.74 69.27 65.72 60.53 39.23 13.08 32.20 14.10 35.25 11.75 gpt2_m ( radford et al. , 2019 ) 335m _____ 75.71 72.72 62.39 _______ 45.43 14.81 bart ( lewis et al. , 2020 ) 400m 16.47 38.73 56.36 82.91 76.44 79.50 54.22 18.07 54.22 18.07 56.25 18.75 glm ( du et al. , 2021 ) 335m 7.79 25.54 54.85 75.81 70.03 68.23 57.04 18.45 57.04 18.45 57.34 19.11 claret ( ours ) 400m 17.67 41.04 57.31 87.18 80.74 81.48 57.47 19.16 57.47 19.16 58.88 19.74 table 1 : fine_tuning results on five generation benchmark datasets',\n",
       "  'meta relational learning for few_shot link_prediction in knowledge graphs',\n",
       "  'below we discuss explagraphs briey and ana_ lyze pre_trained_language_models for their ability to generate explanation_graphs',\n",
       "  'during the inference stage , we rst generate triplet sequences via beam_search [ 70 ]',\n",
       "  'in this paper , we provide an approach genkgc , which converts knowledge_graph completion to sequence_to_sequence generation_task with the pre_trained language model',\n",
       "  'different bars correspond to hits @ 1 under different constraints during generation',\n",
       "  'in the nyt dataset , compared with the state_of_the_art model , graphrel_2p , copymtl_one outperforms it by 3https : //github.com/xiangrongzeng/copy re 4as noveltagging is signicantly better than previous works , we do not add more comparisons',\n",
       "  'for exam_ ple , previous classication_based models ( nguyen et al. , 2016 ; wang et al. , 2019 ; yang et al. , 2019b ; wadden et al. , 2019 ; lin et al. , 2020 ) can hardly encode label_semantics and other weak supervision signals',\n",
       "  'graphrel is the state_of_the_art model , which uses a post_editing method to revise the triplets phaseby phase',\n",
       "  '4 without bipartite_matching : p ( start ) k=softmax ( logit ( start ) ( k ) ) p ( end ) k=softmax ( logit ( end ) ( k ) ) ( 10 ) given p ( start ) kandp ( end ) kobtained by eq',\n",
       "  '6.6 generating human_like semantic negatives ( huse_gen ) in explagraphs , human_created negatives account for 38 % of the samples for which the initially con_ structed graph was incorrect and was rened',\n",
       "  'table 5 shows the impact of input com_ ponents and figure 3 shows the ablation_study of decoding components in kg_s2s',\n",
       "  'in this task , given a belief and an argument , an agent has to perform two sub_tasks predict the stance ( sup_ port/counter ) and also generate an explanation_graph explaining the stance',\n",
       "  'matching networks for one shot learning',\n",
       "  '2qincludes a [ null ] node type for the case when the input text does not have an information graph',\n",
       "  'note that , by construction , the positive graphs only differ in the commonsense concepts ( not part of the belief or argument ) while keeping the struc_ ture intact',\n",
       "  'af_ ter generating valid triplets , we may need to gen_ erate na_triplets',\n",
       "  'variational reason_ ing for question_answering with knowledge_graph',\n",
       "  'in con_ trast , kg_s2s follows the common plm fine_ tuning practices , allowing better knowledge trans_ fer from plm',\n",
       "  'hence , squire framework learns and predicts in a complete end_to_end fashion , yielding faster and more stable convergence than rl_based methods ( green curve in training time comparison fig',\n",
       "  'as noted earlier , the baseline model often makes commonsense mistakes in distinguishing between positive and negative relations ( causes vs not causes ) and our relation perturbing negative_graphs and the max_margin loss component facili_ tate learning a better boundary between them.5.3 generate & rene graph_generation explagraphs was constructed using a renement phase wherein the initially constructed graphs that are marked incorrect by human veriers are fur_ ther rened by another set of annotators',\n",
       "  'our rst method is a generate_and_rene model that rst generates an initial graph and further renes it using another t5 model',\n",
       "  '2 modeling information graphs as alternating sequences aninformation graph can be viewed as a het_ erogeneous multigraph ( li et al. , 2014 ; shi et al. , 2017 ) g= ( v , e ) , wherevis a set of nodes ( typ_ ically representing spans ( ts , te ) in the input doc_ ument ) and eis a multiset of edges with a node type mapping function : vqand an edge type mapping function : er',\n",
       "  '4.5 qa over incomplete kgs with kgt5 due to the lack of public kg splits , we compared kgqa methods using gain over ground_truth query model , which is available for both the compari_ son methods ( from ren et al',\n",
       "  'in our settings , bipar_ tite matching loss focuses on multiple arguments of the same role and reassigns the predicted argu_ ments in each prompt slot',\n",
       "  'our work differs from this approach as their models evaluate full tu_ ples rather than learning to generate the phrases to make new nodes in the knowledge_graph',\n",
       "  'although hypergraphs can efciently rep_ resent all spans ( lu and roth , 2015 ; katiyar and cardie , 2018 ; muis and lu , 2016 ) , it suffers from the spurious structure problem , and structural am_ biguity issue during inference and the decoding is quite complicated ( muis and lu , 2017 )',\n",
       "  'in this work , we study pre_trained lan__guage models that generate explanation_graphs in an end_to_end manner and analyze their abil_ ity to learn the structural_constraints and se_ mantics of such graphs',\n",
       "  'here we emulate the graph renement phase with the help of a model',\n",
       "  'explagraphs was constructed via an iter_ ative framework in which the graphs are iteratively rened ( up to two times ) until they are veried as correct',\n",
       "  'a survey on knowl__edge graphs : representation , acquisition , and appli_ cations',\n",
       "  'while tanl mainly works in the multi_task setting , we additionally enable the zero_shot transfer via the task_agnostic structure_pretraining',\n",
       "  'tensor decompositions for temporal knowledge_base completion',\n",
       "  'each example shows a belief , an argument and an explanation_graph explaining how the argument supports or refutes the belief',\n",
       "  '( 2021b ) , we represent graphs as strings composed of concatenated edges and ne_tune t5 to generate graphs in an autore_ gressive manner',\n",
       "  'methodologically , rotate is a typical graph_based model , while kg_s2s regards kgs as plain_text with structure_aware components',\n",
       "  'to evaluate the quality of gen_ erated knowledge , we also report the number of generated positive examples in the test_set that are scored as correct by the pre_trained bilinear a vg model developed by li et al',\n",
       "  'a popular approach involves graph_ based methods , which have the advantage of natu_ rally modelling inter_sentence relations ( peng et al. , 2017 ; song et al. , 2018 ; christopoulou et al. , 2019 ; nan et al. , 2020 ; minh tran et al. , 2020 )',\n",
       "  'prediction results for models with/without bipartite_matching loss',\n",
       "  'knowledge_graph embedding based ques_ tion answering',\n",
       "  'finally , because the ultimate goal of our method is to be able to perform high_quality , diverse knowledge_base construction , we explore how various decoding schemes affect the quality of candidate knowledge tuples',\n",
       "  'however , there have been recent advances in the attention_mechanism that reduce the complexity to o ( llogl ) as in reformer ( kitaev et al. , 2020 ) , or too ( l ) as in linformer ( wang et al. , 2020 )',\n",
       "  'tero : a time_aware knowledge_graph embedding via tem_ poral rotation',\n",
       "  'however , these methods suffer from slow and poor convergence , and they may fail to infer a certain path when there is a missing edge along the path',\n",
       "  'diachronic embedding for temporal knowledge_graph completion',\n",
       "  'for each triple ( s , p , o ) in the train__ing graph , we verbalize the queries ( s , p , ? ) and ( ? , p , o ) according to 3.1 to obtain two input se_ quences',\n",
       "  'degree achieves a much better performance than other baselines',\n",
       "  'in addition , plms have embedded massive real_world knowledge from the pre_training ( petroni et al. , 2019 ) , which is potentially beneficial for the kgc tasks , especially in the data_sparsity scenarios',\n",
       "  'linformer : self_attention with linear complexity',\n",
       "  'for rl_based model , the trade_off on time is still unbearable as the size of the graph grows',\n",
       "  'we aim to test an upper bound of the trans_ fer performance of our structure_pretraining via the additional finetuning phase',\n",
       "  '2.on the task of kgqa over incomplete kgs , our simple seq2seq approach obtains better results than the current_state_of_the_art across multiple datasets',\n",
       "  'for kgqa , we compared against several meth__ods that have been shown to achieve sota on qa over incomplete kgs',\n",
       "  '2 ) and capture semantics not just at the level of each edge but for the graph as a whole ( e.g. , a graph might be rened because it does not explain the stance )',\n",
       "  '1 , we show ex_ amples of structurally incorrect and semantically incorrect graphs generated by t5',\n",
       "  'unlike the human_annotated graphs , which are rened only for semantic correctness , the model_generated graphs can be both structurally and semantically incorrect',\n",
       "  'get to the point : summarization with pointer_ generator networks',\n",
       "  'formally , the generative_transformer obtain contextualized representations and optimize the following object : x0 , x1 , ... , xm , y0 ... , yn = transformer ( x0 , x1 , ... , xm , y0 , ... , yn ) ( 5 ) loss generation = ( m 1xilog ( xi ) +n 1yilog ( yi ) ) ( 6 ) c. n_tuple contrastive_learning the previous generation_based approach usually neglects the fact that triple should be faithful and consistent with the input sentence',\n",
       "  'a question_focused multi_ factor attention network for question_answering',\n",
       "  'beyond [ cls ] through ranking by generation',\n",
       "  'paie requires no threshold tuning since each slot in the prompt only predicts at most one argument span and usually achieves much higher inference speed in practice',\n",
       "  'commonsense knowledge_base completion existing work on generation of novel common__sense knowledge has also used conceptnet and atomic as underlying kbs',\n",
       "  'during train__ing , given a ( belief , argument , stance ) context x , a ground_truth graph g ( g ) and a negative graph g ( n ) , linearized into a sequence of words { y ( g ) i } k i=1and { y ( n ) i } l i=1respectively , we dene the loss_function las a linear_combination of the standard cross_ entropy losslceand a max_margin loss lmm , dened between a word y ( g ) iof the positive graph and a wordy ( n ) iof the negative graph',\n",
       "  'this generated set of incorrect edges is then added to the correct graph to construct the incorrect graph , such that it is structurally correct and hence repre_ sentative of human_like erroneous graphs',\n",
       "  'at the generation stepi , the candidate vocabulary vis the children nodes of the last generated node',\n",
       "  'each directed edge in the graph can be formalized as a triple ( h , r , t ) , where h , t e andr r. lett denote the set of all such triple facts',\n",
       "  '( 2020 ) introduced oneie , which further incorporates global features based on cross subtask and instance constraints , aiming to extract ie results as a graph',\n",
       "  'however , this ap__proach treats the alternating_sequence as an ordi_ nary sequence and ignores the underlying graph structure it encodes',\n",
       "  'whilst standard beam_search always keeps and derives the candidates with the highest beam score from kg_s2s',\n",
       "  'f1 ( metke_jimenez and karimi , 2016 ) 64.4 56.5 60.2 ___ _______ ( tang et al. , 2018 ) 67.8 64.9 66.3 ___ _______ ( dai et al. , 2020 ) [ elmo ] 68.9 69.0 69.0 80.5 75.0 77.7 78.1 81.2 79.6 ( yan et al. , 2021b ) ( bpe ) [ bart_large ] 69.45 70.51 69.97 82.07 76.45 79.16 75.88 84.37 79.90 ours [ t5_base ] ( without_de ) 71.34 70.54 70.94 79.03 78.03 78.53 77.06 83.41 80.11 ours [ t5_base ] ( intra_de ) 71.35 71.86 71.60 81.09 78.13 79.58 77.88 83.77 80.72 ours [ t5_base ] ( inter_de ) 70.44 71.65 71.04 81.31 76.75 78.96 77.51 83.27 80.29 table 3 : results for discontinuous_ner datasets',\n",
       "  'recent studies ( wei et al. , 2021 ) also show that pretrained models finetuned with abundant downstream_tasks can conduct effective zero_shot learning',\n",
       "  'during training , for each triple fact ( h , r , t ) t , we sample a path from all paths with maximum length n= 32between handtand treat it as the target path',\n",
       "  'we utilize cross_entropy_loss generation to optimize the triple generation procedure',\n",
       "  '5 and their effect on the structural and semantic correctness of the gener_ ated graphs',\n",
       "  'to evaluate a model on domaindi , we meta_train the model ond0i= { d1 , ... , d7 } 382we mu pl bo se re cr ave.1_shottransferbert 55.82 38.01 45.65 31.63 21.96 41.79 38.53 39.06mn + bert 21.74 10.68 39.71 58.15 24.21 32.8869.6636.72wpz + bert 46.72 40.07 50.78 68.73 60.81 55.58 67.67 55.77l_tapnet+cdt71.53 60.56 66.27 84.54 76.27 70.7962.8970.41ours + snips82.6277.4671.3385.4983.2284.2382.9281.04ours + onto 56.3967.10 53.49 71.94 66.21 69.04 28.80 59.00ours + no meta 46.42 59.02 47.47 63.79 49.42 64.45 17.60 49.745_shottransferbert 59.41 42.00 46.70 20.74 28.20 67.75 58.61 46.11mn + bert 36.67 33.67 52.60 60.09 38.42 33.28 72.10 47.98wpz + bert 67.82 55.99 46.02 72.17 73.59 60.18 66.89 63.24l_tapnet+cdt71.64 67.16 75.88 84.38 82.58 70.05 73.41 75.01ours + snips91.3586.7387.2095.8592.7191.2391.5590.95ours + onto83.1586.1580.3690.2784.8785.89 68.0882.68ours + no meta73.1482.0278.8284.8683.1486.63 52.5677.31table 2 : our few_shot slot_labeling results on7domains of snips dataset',\n",
       "  'we assume that a better generation order could lead to more valid generated triplets',\n",
       "  'modeltrigger_argument distance d 2 [ 79 ] 1 [ 164 ] 0 [ 1811 ] 1 [ 87 ] 2 [ 47 ] bart_gen 17.7 16.8 44.8 16.6 9.0 docmrc 21.0 20.3 46.6 17.2 12.2 feae 23.7 19.3 49.2 25.0 5.4 eeqa_bart 15.6 24.0 51.7 23.5 8.0 paie_ w/o multi_prompt 21.2 21.4 52.3 27.9 24.6 paie 21.7 27.3 54.7 29.4 25.4 table 5 : performance ( arg_c f1_score ) breakdown by argument_trigger distance don rams development_set',\n",
       "  'figure 1 shows the alternating se__quence for an information multigraph',\n",
       "  '( a ) noisy sample : notice that dur_ ing training , we sample path from htotas the target path , for there is no golden standard for a groundtruth reasoning path',\n",
       "  'kacc_m fb100k mrr h @ 1 h @ 10 mrr h @ 1 h @ 10 multihopkg .576 .492 .713 .652 .601 .744 squire .578 .515 .702 .655 .595 .766 table 5 : link_prediction results on two larger kgs',\n",
       "  'parallel training of knowledge_graph embedding models : a comparison of techniques',\n",
       "  'kges can be utilized to perform kgqa when the back_ ground kgs are incomplete',\n",
       "  '[ 65 ] l. dong et al',\n",
       "  '( 2020 ) [ roberta_large ] ___ 92.40 _____ li et al',\n",
       "  'note that triplet contrastive_learning and triple generation are two different tasks , and optimizing them jointly is non_ trivial , owing to the leakage of generated labels',\n",
       "  'the graphs are encoded as concatenated bracketed edges , in which the edges are ordered according to the depth_first_search ( dfs ) order of the nodes',\n",
       "  'paper_title : explanation_graph generation via pre_trained_language_models : an empirical study with contrastive_learning ; paper_abstract : pre_trained sequence_to_sequence language_models have led to widespread success in many natural_language_generation tasks',\n",
       "  'then 3these rules are a subset of rules in ( meilicke et al. , 2019 ) , referred to as crules in their paper.1652algorithm 1 iterative training 1 : t set of all triples in the graph 2 : initialize query_path training set uby random sampling or rule_enhanced searching 3 : initialize model m 4 : train mfornepochs 5 : fork= 2tondo 6 : foreach triple ( h , r , t ) intdo 7 : q ( h , r ) 8 : 1first ( k1 ) hops of m ( q ) 9 : 2subsequent path of 1towards t , at most ( nk+ 1 ) hops 10 : if2=null then 11 : 1+2 12 : else no valid subsequent path 13 : search for an entire path from htot 14 : add ( q , ) tou 15 : train mforn/k epochs 16 : return trained model m we use these rules to find the evidential_path be__tween handt',\n",
       "  'the top_right component refers to the generative_transformer , and the bottom_right component represents triplet contrastive_learning',\n",
       "  'closed world assumption , pages 415415',\n",
       "  'however , like any other ml model , the graphs generated by our models may not always be completely ac_ curate and hence should be used with caution for real_world applications',\n",
       "  'next , we propose two improved models one that uses the negative_graphs in a max_margin formulation and another that uses both positive and negative_graphs with a infonce ( van den oord et al. , 2018 ) contrastive loss',\n",
       "  'in other words , at least 34 % of all hits @ 1 paths are inferred by dynamically completing missing edges along the path',\n",
       "  'the input text ( on top ) for this graph is he was captured in baghdad late monday_night',\n",
       "  'paper_title : sequence_to_sequence knowledge_graph completion and question_answering ; paper_abstract : knowledge_graph embedding ( kge ) models represent each entity and relation of a knowledge_graph ( kg ) with low_dimensional embedding vectors',\n",
       "  'our structure_pretraining enables zero_shot transfer of the learned knowledge that models have about the structure tasks',\n",
       "  '6 experiments 6.1 impact of different models on graph structural and semantic accuracy in table 2 , we compare the various modeling tech_ niques described in sec',\n",
       "  'this ability distinguishes our models from other few_shot approaches such as prototypical networks ( snell et al. , 2017 ) or matching net_ works ( vinyals et al. , 2016 ) , which are designed only for few_shot scenarios but do not scale well on real_world data which often contains a mix of high and low_resource label types',\n",
       "  'in gen_ eral , kg_s2s achieves higher mrr as the beam width increases , whilst the performance gain be_ comes flat after 40 beams ( red bar )',\n",
       "  '4.2 comparison models for kg completion on wikidata5m , we compared with several standard kge models that have been shown to achieve good performance across mul_ tiple datasets ( ruffinelli et al. , 2020 ) but with a large number of parameters',\n",
       "  '; paper_content : proceedings of the 29th international conference on computational_linguistics , pages 40054017 october 1217 , 2022.4005knowledge is flat : a seq2seq generative framework for various knowledge_graph completion chen chen1 , , yufei wang2 , , bing li3and kwok_yan lam1 nanyang_technological_university , singapore1 macquarie_university , sydney , australia2 centre for frontier ai research ( cfar ) , a * star , singapore3 { s190009 , kwokyan.lam } @ ntu.edu.sg , yufei.wang @ students.mq.edu.au li_bing @ ihpc.a_star.edu.sg abstract knowledge_graph completion ( kgc ) has been recently extended to multiple knowledge_graph ( kg ) structures , initiating new research direc_ tions , e.g',\n",
       "  'in the case of bfs , we exploit the fact that it visits nodes level by level , i.e. , in the order pi , ci1 , ... , cik , pj ( wherecikis thek_th child of parent pi , connected by edgeeik , and pjmay or may not be equal to one of the children of pi ) , which we turn into a sequence , s=pi , ( ei1 ) , ci1 , ... , ( eik ) , cik , [ sep ] , pj , ... where we use the special edge type [ sep ] to de_ lineate the levels in the graph',\n",
       "  'generation_based methods are efficient for generating all arguments , but sequential predic_ tions degrade the performance on long_distance and more arguments',\n",
       "  'multidecoderuses unshared decoders , each decoder predicts one triplet',\n",
       "  'question type gtq kgt5 gain moviedirector movielanguage 0.17 0.85 0.68 moviedirector movieactor 0.37 0.54 0.17 movieactormovielanguage 0.29 0.8 0.51 moviewritermovieyear 0.31 0.47 0.16 movieactormoviedirector 0.65 0.57 _0.08 moviedirector moviegenre 0.37 0.82 0.45 moviewritermoviedirector 0.4 0.52 0.12 movieactormovieyear 0.63 0.72 0.09 movieactormoviewriter 0.63 0.51 _0.12 movieactormoviegenre 0.65 0.83 0.18 moviedirector moviewriter 0.39 0.55 0.16 moviewritermoviegenre 0.42 0.75 0.33 moviewritermovieactor 0.41 0.43 0.02 moviedirector movieyear 0.32 0.56 0.24 moviewritermovielanguage 0.27 0.74 0.47 all 0.443 0.634 0.191 table 16 : hits @ 1 performance on metaqa 3_hop validation dataset , 50 % kg setting',\n",
       "  'hence we do not per_ form knowledge probing experiments on datasets such as t_rex or google_re ( petroni et al. , 2019 ) .2821method wqsp cwq t5_small + qa finetuning 31.3 27.1 kgt5 ( 50 % kg pretraining ) 50.5 34.5 kgt5 ( full kg pretraining ) 56.1 36.5 embedkgqa 66.6 _ cbr_kgqa ( das et al. , 2021b ) 73.1 70.4 table 7 : hits @ 1 in the full_kg kgqa setting',\n",
       "  '2.3 zero_shot the zero_shot deepstruct refers to the setting where the pretrained model is used without any task_specific training at inference time',\n",
       "  'next we com_ pare gtee_d ynprefwith gtee_s taprefto study the advantages of constructing dynamic pre_ x',\n",
       "  '2.1 knowledge_graph embeddings atomic kge models',\n",
       "  'differentiable prompt makes pre_trained_language_models better few_shot learners',\n",
       "  'this helps to better capture subtle yet key tokens and facilitate the ability to ingest other graph structures',\n",
       "  '4 related work knowledge_graph embedding models',\n",
       "  'while our primary metrics of inter_ est are graph structural accuracy ( stca ) and se_ mantic accuracy ( seca ) , following prior work ( saha et al. , 2021b ) , we also report stance accu_ racy ( sa ) , graph_bertscore ( g_bs ) , graph edit_distance ( ged ) and edge accuracy ( ea )',\n",
       "  '5.2 ) leverages all struc_ turally and semantically incorrect graphs and ob_ tains up to 6 % and 9 % improvement in stca and seca respectively over the baseline t5 model',\n",
       "  'from the listed cases , we see that squire can provide reasonable paths , and it can dynamically complete the path during genera__tion , thus yielding superior performance on sparse datasets',\n",
       "  'the dashed nodes represent com_ monsense nodes and the dashed edges are incorrect edges',\n",
       "  'bold : best scores.jia et al',\n",
       "  'instead of compressing the them separately , kg_s2s inter_ acts query and answer in the cross_attention module ofkg_s2s decoder , auto_regressively',\n",
       "  'figure 3 shows a concrete example of our alternating_sequence for a knowledge_graph in the ace05 dataset',\n",
       "  'the bipartite_matching is only applied in training',\n",
       "  '[ 42 ] r. h. zhang et al',\n",
       "  'in contrast , random sampling often pro_ vides low_quality answers due to its randomness in decoding and the outputs of diverse beam_search are distorted due to its diversity encouragement term',\n",
       "  'in this work , we study pre_trained_language_models that generate explanation_graphs in an end_to_end manner and analyze their ability to learn the structural_constraints and semantics of such graphs',\n",
       "  'representation learning of knowledge graphs with entity attributes and multimedia descriptions',\n",
       "  'the state_ of_the_art model , graphrel , also belongs to this genre',\n",
       "  'dpr + bert 72.9 _______ 40.1 60.7 25.0 43.4 _____ dpr 55.3 1.8 0.3 0.5 13.3 28.9 54.3 25.0 44.5 10.7 25.5 23.6 tf_idf 50.9 3.7 0.24 2.1 44.7 60.8 28.1 34.1 46.4 13.7 49.0 30.5 dpr + bart 55.3 75.5 45.2 46.9 13.3 28.9 54.3 25.0 44.4 10.7 25.4 38.6 rag 61.9 72.6 48.1 47.6 28.7 53.7 59.5 30.6 48.7 11.0 57.8 47.3 blink + air 63.7 81.5 80.2 68.8 59.6 78.8 24.5 46.1 65.6 9.3 38.2 56.0 genre 83.6 89.9 87.4 71.2 79.4 95.8 60.3 51.3 69.2 15.8 62.9 69.7 table 3 : r_precision for page_level retrieval on kilt test data',\n",
       "  'e.g. , a causes b does not always imply a not causes not b ornot a not causes not b.and human_created semantic negatives to capture a fairly large space of semantically incorrect graphs',\n",
       "  'while our models generate more correct graphs , they lack in structural diversity the contrastive model gen_ erates 77 % of linear graphs ( i.e. , the nodes are in a linear chain ) which is comparable to 75 % in the t5 model',\n",
       "  '7 shows hits @ 1 performance in the full kg setting',\n",
       "  'learn_ ing attention_based embeddings for relation_prediction in knowledge graphs',\n",
       "  'parents appear once for each child.4068algorithm 1 alternating_sequence construction al_ gorithm with bfs input : ordered adjacency dictionary of an infor__mation graph g , positions of nodes in the input textpq , frequency of edge_types in the training set pr output : an alternating_sequence y sort the nodes in gaccording to pq for each node ving , sort the neighbors and the edges ofvaccording to pqandprrespectively instantiateyas an empty list foruingdo ifuis not visited then initialize an empty queue q markuas visited and enqueue utoq whileqis not empty do dequeue the a node wfromq ifwingthen appendwand all the neighbors of wwith their edge_types to y append the separation edge type , [ sep ] , toy mark all unvisited neighbors of w as visited and enqueue them to q end end end end returny node and edge representations our node and edge representations ( explained below ) rely on the observation that there are only two kinds of objects in an information graph : spans ( as addresses to pieces of input texts ) and types ( as representations of abstract concepts )',\n",
       "  'ness and diversity in these graphs and hence are the best candidates for contrastive_learning',\n",
       "  'thus we expect a large_scale dataset with more multi_arguments and diverse narrative styles in the future , and we believe the bipartite_matching_loss will bring more significant improvement in it',\n",
       "  'however , we also acknowledge the improvement from bipartite_matching loss is somewhat not signif_ icant and robust when compared with other contri_ butions in our paper',\n",
       "  '18653/v1/2020.emnlp_demos.1 [ 32 ] n. zhang et al',\n",
       "  'following star [ 12 ] , we choose the two kinds of kge methods as our base_ line models , which can be classified as graph embedding approach and textual encoding approach',\n",
       "  'even_ tually , using constrained generation and exploiting the candidate sets proved useful',\n",
       "  'novel positional encodings to enable tree_based transformers',\n",
       "  'our main contributions are three_fold : we propose a general technique to invertibly map between an information graph and an al_ ternating sequence ( assuming a given graph traversal algorithm )',\n",
       "  'org/abs/2104.04907 [ 4 ] n. zhang et al',\n",
       "  'all numbers indicate f1_scores except noted otherwise',\n",
       "  'for_ mally , an explanation_graph is a connected dag with nodes as concepts and edges as commonsense relations between two concepts ( see fig',\n",
       "  'dackgr provides the most pre_ cise inference on sparse datasets ( right two ) , since it is specially designed to handle sparse setting , yet it does not perform well in the standard kgs',\n",
       "  'generation byclaret : cora spent thewhole day with herstudents',\n",
       "  'knowledge_graph completion ( kgc ) predicts the missing_entities for the queries ( ? , r , t , m ) or ( h , r , ? , m )',\n",
       "  'on metaqa 1_ and 3_hop , kgt5 was either equal or better than all baselines ( in terms of gain )',\n",
       "  'hence , we exploit beam_search ( bs , sutskever et al. , 2014 ) , an established approximate decoding strategies to efciently navigate the search space',\n",
       "  'below we discuss the construction of these graphs',\n",
       "  'one of the main downstream applications of kges is question_answering over incomplete kgs ( kgqa ) ( choudhary et al. , 2021 )',\n",
       "  '6 analysis on real scenario 6.1 long_range dependencies in d_eae task , arguments could span multiple sentences',\n",
       "  'this enables the zero_shot trans_ fer of knowledge that lms learned about structures during our pretraining to downstream structure pre__diction tasks',\n",
       "  \"as conrmation , we observe that the output from comet ( _ pretrain ) is mango isa spice , which could be a reasonable inference given the information about mango '' in the seed set of knowledge\",\n",
       "  'available : https : //github.com/weizhepei/casrel 3 [ online ]',\n",
       "  'libkge _ a knowledge_graph embedding library for reproducible research',\n",
       "  'simple embedding for link_prediction in knowledge graphs',\n",
       "  'this work is mainly about the pretraining and multi_ task learning of lms for structural prediction',\n",
       "  ', r n. if none of the rules lead to a valid path in the graph , we obtain the path by random sampling',\n",
       "  '[ 55 ] y. lu et al',\n",
       "  'neural bellman_ford net_ works : a general graph neural_network framework for link_prediction',\n",
       "  '9512dataset model prec rec f1 nytgraphrel_2p .639 .600 .619 copyre5 .680 .663 .671 copymtl .727 .692 .709 webnlggraphrel_2p .447 .411 .429 copyre5 .572 .536 .553 copymtl .578 .601 .589 table 5 : results of different multi_token models on nyt and webnlg effects of multi_task_learning copymtl aims to solve the multi_token problem',\n",
       "  'whereand the temperature are the hyperpa_ rameters and sim ( ) denotes the cosine similarity function between the graph representations',\n",
       "  '( b ) thresholding via incorrect probability of a graph ( ip ) : we use our seca metric model ( that classies a graph into support , counter , or incorrect class ) to compute the probability of the generated graph being incorrect and choose those graphs that are above a certain thresholdof incorrect probability',\n",
       "  'explanation_graphs are structured explanations that capture explicit rea_ soning chains between the belief and the argument , thereby making models more interpretable',\n",
       "  'we hypothesize that the constrastive model does not lead to further improvement in seca because of the structurally similar positive1196stca seca g_bs ged ea t5_large 46.5 31.6 36.8 0.66 26.7 + syst 50.2 34.1 40.7 0.64 27.4 + syse 50.7 35.1 40.8 0.63 27.3 + huse 49.5 38.4 39.4 0.64 26.1 table 3 : ablation_study showing the effect of different types of negative_graphs on explagraphs dev set',\n",
       "  'it shows improvements in adding a copy mechanism4638modelen xxar xxzh xxxx enxx arxx zhavg mbart_50_large 51.6 39.8 47.2 48.2 43.2 47.2 46.2 _ w/o copy 50.9 42.2 49.6 50.6 43.5 48.7 47.6 mt5_base 54.3 41.4 51.4 49.4 46.7 51.0 49.1 _ w/o copy 52.1 39.5 47.6 48.1 42.7 48.5 46.4 mt5_large 56.7 44.8 52.6 53.0 48.9 52.1 51.3 _ w/o copy 55.1 45.0 51.5 52.0 46.3 53.2 50.5 table 3 : ablation_study on copy_mechanism for ace_ 2005. enxx indicates the average of en en , enzh , and enar',\n",
       "  'interestingly , degree ( eae ) achieves pretty strong performance and outperforms other baselines with a large margin',\n",
       "  '1 : procedure nc ( t , d , q , gen ) 2 : tq d.get ( q ) 3 : cand t.next ( pre=gen ) 4 : rm t q.next ( pre=gen ) 5 : candcand.remove ( rm ) 6 : return cand g ( x , r , t , m )',\n",
       "  'as seen in figure 5 , both tempgenandtempgen_topk c opy outper_ form grit across all settings with a slightly larger performance margin in low resource settings',\n",
       "  'novelty in addition to being high quality , the generated tuples from comet are also novel , with 59.25 % of the tuples not being present in the train__ing set , showing that the model is capable of gen_ erating new edges between nodes , and even cre_ ating new nodes 3.75 % of onodes are novel to extend the size of the knowledge_graph',\n",
       "  'dataset setting |e| |r| |train| |valid| |test| wn18rr skgc 40,943 11 86,835 3,034 3,134 fb15k_237 skgc 14,541 237 272,115 17,535 20,466 fb15k_237n skgc 14,541 93 87,282 7,041 8,226 icews14 tkgc 6,869 230 72,826 8,941 8,963 nell_one fkgc 68,544 358 189,635 1,004 2,158 table 8 : statistics of the datasets',\n",
       "  'hence , we also develop methods to auto_ matically generate more such human_like semantic negative_graphs , which leads to further improve_ ments',\n",
       "  'we evaluate the structural_understanding ability on multiple structure predic_ tion tasks',\n",
       "  'compared to previous classication_based models ( wang et al. , 2019 ; yang et al. , 2019b ; wadden et al. , 2019 ; lin et al. , 2020 ) , the generation framework provides a exible way to include additional information and guidance',\n",
       "  'if all these con_ straints are satised , the graph is next evaluated for semantic correctness by a model_based met_ ric ( saha et al. , 2021b )',\n",
       "  'the remain_ ing automatic metrics in table 1 measure the pro_ portion of generated tuples and generated objects which are not in the training set',\n",
       "  '2 : foreachi [ 1 , k ] do 3 : fi= [ a3i2 , a3i1 , a3i ] 4 : end for 5 : foreachi [ 1 , k ] do 6 : r3i2=r3i1=r3i= 0 7 : ifi|g|then 8 : iffigandfi/vthen 9 : addfitov 10 : r3i2=r3i1=r3i= 1 11 : end if 12 : else iffi=nathen 13 : r3i2=r3i1=r3i= 0.5 14 : end if 15 : end for reward the reward is used to guide the training , which is critical to rl training',\n",
       "  'it gives the model the flexibility to dynamically complete the path during generation',\n",
       "  '2.2.2 structure generation with uie given ssisand textxas input , uie extracts tar_ geted information by generating a linearized sel',\n",
       "  '[ 30 ] h. zheng et al',\n",
       "  '2these are rationalizing models ( rajani et al. , 2019 ; hase et al. , 2020 ) that rst predict the stance , followed by the graph',\n",
       "  'a complete linearized form decoding process can be represented by executing a trie tree search , as shown in figure 3a',\n",
       "  'lower : performance of our 10b deepstruct zero_shot and multi_task , compared with 175b gpt_3 zero_shot',\n",
       "  'we canp p r f uie_base +11.4179.54 72.63 75.91 w/o rejection 68.13 67.85 66.13 uie_base w/o ssi +9.4178.96 70.50 74.49 w/o rejection 69.55 63.69 66.44 t5_base +17.9574.12 61.72 67.33 w/o rejection 56.17 56.00 55.94 t5_v11 +13.8871.88 51.23 59.67 w/o rejection 58.00 45.04 50.38 table 5 : experiment results of 10_shot setting on the conll 03 development_set',\n",
       "  'as shown in table 5 both thresholding approaches lead to further improvements over us_ ing just the human_created negative_graphs',\n",
       "  'this is problematic for incomplete kgs , where a single missing link can cause the query to fail',\n",
       "  'these explanation_graphs encode structured knowledge ( augmented with commonsense ) and consist of con_ cepts as nodes and relations from conceptnet ( liu and singh , 2004 ) as edges',\n",
       "  'a graph is considered structurally correct if it satises the following constraints : ( 1 ) it is connected , ( 2 ) it is a dag , ( 3 ) the edge relations belong to a pre_dened list , ( 4 ) there are at least two concepts from the belief and two from the argument',\n",
       "  'to pre_train the ability of generating valid struc_ tures dened by sel and schemas , we pre_train uie ondrecord',\n",
       "  'for joint span selection , we design a bipartite_matching_loss that makes the least_cost match between predic_ tions and ground_truth so that each argument will find the optimal role prompt',\n",
       "  'we rst ne_tune a t5 model that conditions on the belief , argument and the stance to generate a set of incor_ rect edges ( which is the set of edges that are present in the incorrect graph and not in the rened graph )',\n",
       "  '[ 68 ] l. dong et al',\n",
       "  'slot labelingsnips atis snips atis conll ontosl/icbi_model ( wang et al.,2018 ) 98.99 96.89joint bert ( chen et al.,2019 ) 98.60 97.5097.0096.10elmo+bilstm ( siddhant et al.,2019 ) 99.2997.42 93.90 95.62nercloze_cnn ( baevski et al.,2019 ) 93.50bert_mrc ( li et al.,2019a ) 93.04 91.11bert_mrc + dsc ( li et al.,2019b ) 93.3392.07bert base ( devlin et al.,2019 ) 92.40 88.95ours : individual 99.00 96.8697.4396.13 90.7090.24ours : snips+atis99.2997.2097.21 95.83ours : conll+ontonotes91.4889.52ours : snips+atis+conll+ontonotes99.14 97.08 96.8296.65 91.4889.67table 1 : results of our models trained on combinations of datasets',\n",
       "  'previous works often focus on asingle setting , while kg_s2s fits all three settings without any architecture modifications',\n",
       "  '3 shows an example of the graphs generated by different models ( more examples in appendix f )',\n",
       "  '5.2 performance analysis ablation_study we conducted ablation_studies by replacing the topk c opy module with other copy mechanisms',\n",
       "  'bart has base and large versions',\n",
       "  'we compare the models in table 5 , from which we can see that copyre5 is worse than copymtl in all evalua_tions , but both outperform graphrel',\n",
       "  'with the candidate spans for each role , we define the bipartite_matching between the candidates and ground_truth annotations as finding the lowest cost of a permutation ofnelements : = arg min nnx kl1 ( ( sk , ek ) , ( s ( k ) , e ( k ) ) ) ( 9 ) where l1 ( ( sk , ek ) , ( s ( k ) , e ( k ) ) ) represents l1_ norm between ( sk , ek ) and ( s ( k ) , e ( k ) )',\n",
       "  'the rener is also a t5 model ne_tuned with the prex rene the explanation_graph for on all positive and negative_graphs described in sec',\n",
       "  '; paper_content : proceedings of the 60th annual meeting of the association_for_computational_linguistics volume 1 : long_papers , pages 2814 _ 2828 may 22_27 , 2022 c2022 association_for_computational_linguistics sequence_to_sequence knowledge_graph completion and question_answering apoorv saxena indian_institute_of_science bangalore apoorvsaxena @ iisc.ac.inadrian kochsiek university of mannheim_germany adrian @ informatik',\n",
       "  'all of these datasets are open_source english_ written sources without any offensive content',\n",
       "  'while several seq2seq_based models ( zhang et al. , 2020 ; zeng et al. , 2018 , 2020 ; wei et al. , 2019 ; zhang et al. , 2019 ) have been proposed to generate triples ( i.e. , node_edge_node ) , our model is fundamentally different from them in that : ( 1 ) it is generating a bfs/dfs traversal of the target graph , which captures dependencies between nodes and edges and has a shorter target sequence , ( 2 ) we model the nodes as the spans in the text , which is independent of the vocabulary , so even if the to_ kens of the nodes are rare or unseen words , we can still generate spans on them based on the context information',\n",
       "  'contrary to many conventional kbs that store knowledge with canonical templates , commonsense kbs only store loosely structured open_text descriptions of knowledge',\n",
       "  'we generate graphs as linearized strings in an end_ to_end manner by leveraging an encoder_decoder pre_trained language model , t5 ( raffel et al. , 2020 )',\n",
       "  'nell_one to conduct zero_shot learning for this dataset , we follow wang et al',\n",
       "  'note that this model renes all graphs ( correct or not ) and can lead to already correct graphs becoming incor_ rect after renement',\n",
       "  'com/renll/hyspa figure 1 : we represent directed multigraphs as alter_ nating sequences of nodes ( blue ) and edges ( orange )',\n",
       "  '( 2020 ) [ bert_large ] 85.42 85.92 85.67 84.50 84.72 84.61 79.43 78.32 78.87 wang et al',\n",
       "  '5gold spans : [ 6 [ 19 , 7 , spain ] , 7 [ 44 , 6 , madrid ] , 8 [ 91 , 7 , spain ] , 9 [ 147 , 11 , real_madrid c',\n",
       "  'this simple but pow_ erful approach , which we call kgt5 , performed competitively with the state_of_the_art methods for kg completion on large kgs while using upto 98 % fewer parameters',\n",
       "  'in this case , gpt_3 only answers with yes or no to produce a task prediction ( figure 7 ) , which is apparently an easier task compared to generating the structures from scratch',\n",
       "  'x_g_ear with random orders still achieve good performance but slightly worse than the original or_ der',\n",
       "  'the template generation problem much resembles summarization , except that generated template sequences contain implicit structures',\n",
       "  'these methods can be divided into : 1 ) pipeline classication ( ahn , 2006 ; ji and grishman , 2008 ; liao and grishman , 2010 ; hong et al. , 2011 , 2018 ; huang and riloff , 2012 ; chen et al. , 2015 ; sha et al. , 2016 ; lin et al. , 2018 ; yang et al. , 2019 ; wang et al. , 2019 ; ma et al. , 2020 ; zhang et al. , 2020c ) , 2 ) multi_task joint mod__els ( mcclosky et al. , 2011 ; li et al. , 2013 , 2014 ; yang and mitchell , 2016 ; nguyen et al. , 2016 ; liu et al. , 2018 ; zhang et al. , 2019a ; zheng et al. , 2019 ) , 3 ) semantic structure grounding ( huang et al. , 2016 , 2018 ; zhang et al. , 2020a ) , and 4 ) question_answering ( chen et al. , 2020b ; du and cardie , 2020 ; li et al. , 2020 ; liu et al. , 2020 )',\n",
       "  'in our knowledge , the relational triplet overlap problem has never been addressed before',\n",
       "  'generated triple : hairport , cityserved , new yorki ground_truth : hairport , cityserved , yorki table 5 : error anslysis',\n",
       "  '2 ) also leverages both positive and negative_graphs but instead of doing so in a 2_stage generate & re_ nemodel , uses a contrastive_learning framework ( khosla et al. , 2020 ; gunel et al. , 2020 )',\n",
       "  '[ 54 ] f. li et al',\n",
       "  'however , as detailed in the original pa_ per ( cao et al. , 2021 ) , genre achieves the best performance when high_quality candidate lists are available',\n",
       "  '( 2020b ) [ bert_large ] 85.83 85.77 85.80 85.01 84.13 84.57 81.25 76.36 78.72 yu et al',\n",
       "  'generation byclaret : ibought anew phone and showed ittomyfriends',\n",
       "  'output : six days after starting [ acycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracycloviracyclovir |drug ] she exhibited signs of [ [ lithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithium |drug ] toxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicity | disease |effect =acyclovir |effect =lithium ]',\n",
       "  'dataset # ent # rel # train # dev # test wn18rr 40,943 11 86,835 3,034 3,134 fb15k_237 14,541 237 272,115 17,535 20,466 openbg500 269,658 500 1,242,550 5,000 5,000 setting',\n",
       "  'we denote the ac_ tion sequence of the source sentence as a= [ a1 , ... , a t ] .371algorithm 1 reward assignment input : sampled action sequence a= [ a1 , ... , a t ] ; gold triplets set g ; na_triplet na',\n",
       "  '4.3 experiments on low_resource settings to verify the quick adaptation ability of uie , we conducted low_resource experiments on six differ__ent partitions of the original training sets ( 1/5/10_ shot , 1/5/10 % ratio ) across 4 tasks',\n",
       "  '2 shows an example of a disconnected graph created as part of the struc_ turally negative_graphs',\n",
       "  'fewrel 2.0 : towards more challenging few_shot relation classication',\n",
       "  'generating sequences we model the distribu_ tionp ( s ) by a sequence generator hwith parame_ ters ( |s|is the length of the s ) : p ( s i|s 0 , ... , s i1 ) =h ( s 0 , ... , s i1 , ) , p ( s ) =|s| i=1p ( s i|s 0 , ... , s i1 ) , we will address in the following sections how to enforce the sequence generator , h , to only gener__ate sequences in the space s , since we do not wanthto assign non_zero probabilities to arbitrary sequences that do not have a corresponding graph',\n",
       "  'ptrnetdecoding achieves the highest f1_scores when we combine the two attention mechanisms with re_spect to the previous hidden vector of the decoder lstm ( h d t1 ) and representation of all previously extracted tuples ( yprev )',\n",
       "  'dataset model prec rec f1 nytcopyre .612 .530 .571 copyre .747 .700 .722 webnlgcopyre .312 .272 .291 copyre .583 .629 .605 table 3 : results of copyre and copyre on nyt and webnlg',\n",
       "  'we also consider the following baselines focus_ ing on the high_resource setting',\n",
       "  'representation learning of knowledge graphs with entity descriptions',\n",
       "  'the parent_child embedding assigns different random initialized em__bedding vectors at the positions of the parent nodes and the child nodes in the bfs traversal levels to help model distinguish between these two kinds of nodes',\n",
       "  'synthetic & structurally negative_graphs ( syst )',\n",
       "  'these methods have recently been applied to kg link_prediction and ques_ tion answering over incomplete kgs ( kgqa )',\n",
       "  '2 shows a semantic negative graph in which the relations marked with dashed lines are perturbed',\n",
       "  'the na_triplets will be generated if the valid triplets number is less than the maximum triplets number.1it takes three_time steps to generate one triplet',\n",
       "  'in this paper , we propose kg_s2s , a seq2seq generative framework that could tackle different verbalizable graph structures by unifying the representation of kg facts into flat text , regardless of their original form',\n",
       "  'among other suc_cesses , gpt_3 outperformsbert_large on thesuperglue benchmark using only 32 examplesper task',\n",
       "  'key_value memory networks for directly read_ ing documents',\n",
       "  'the numbers reported are hits @ 1. the baselines , and even worse than ground_truth querying',\n",
       "  'we enable the zero_shot transfer to structure_prediction_tasks via converting the downstream_tasks to one or a combination of the pretraining tasks',\n",
       "  'input : six days after starting acyclovir she exhibited signs of lithium toxicity',\n",
       "  'dy_ gie++ ( wadden et al. , 2019 ) is a classication_ based model with span graph propagation tech_ nique',\n",
       "  '2 related work kgc has been studied in the static , temporal and few_shot settings',\n",
       "  'conversely , our approach generates new knowledge that is often unstated in text , as com_ monsense information typically is ( gordon and van durme , 2013 )',\n",
       "  'we enable both zero_shot and multi_task transfer_learning',\n",
       "  '( 2021 ) which results in kgs that are much smaller than freebase but can still be used to answer all ques_2818model mrr hits @ 1 hits @ 3 hits @ 10 params transe ( bordes et al. , 2013 ) 0.253 0.170 0.311 0.392 2,400m distmult ( yang et al. , 2015 ) 0.253 0.209 0.278 0.334 2,400m simple ( kazemi and poole , 2018 ) 0.296 0.252 0.317 0.377 2,400m rotate ( sun et al. , 2019b ) 0.290 0.234 0.322 0.390 2,400m quate ( zhang et al. , 2019 ) 0.276 0.227 0.301 0.359 2,400m complex ( trouillon et al. , 2016 ) $ 0.308 0.255 _ 0.398 614m kgt5 ( our method ) 0.300 0.267 0.318 0.365 60m complex 14_dim0.201 0.161 0.211 0.275 67m complex 26_dim0.239 0.187 0.261 0.342 125m kepler ( wang et al. , 2021 ) 0.210 0.173 0.224 0.277 125m dkrl ( xie et al. , 2016a ) 0.160 0.120 0.181 0.229 20m mlmlm ( clouatre et al. , 2021 ) 0.223 0.201 0.232 0.264 355m kgt5_complex ensemble 0.336 0.286 0.362 0.426 674m table 2 : link_prediction results on wikidata5m',\n",
       "  'thus , instead of unavailable arguments , each role in the template serves as a slot for interactions , and during learning , plms tend to fill these slots with exact arguments via a matching loss',\n",
       "  '( iv ) zero_shot : we only use the task_agnostic datasets and exclude the multi_task datasets in the pretrain_ ing',\n",
       "  'ontoprotein : protein pretraining with gene_ontology embedding',\n",
       "  'you { can } teach an old dog new tricks ! on training knowledge_graph embeddings',\n",
       "  'all patients were treated with gefitinib and showed a partial response.ternary dgm relationship crosses a sentence boundary',\n",
       "  'our model could consider the relational triplet overlap problem through copy mecha_ nism',\n",
       "  'c constrained_decoding detailed results table 9 shows the detailed results for x_g_ear us_ ing constrained_decoding algorithm during testing time',\n",
       "  'supposetis the maximum time step of de_ coder , we denote the ground_truth sequence as [ y1 , ... , y t , ... , y t ]',\n",
       "  '( 2019 ) [ bert_large ] ___ 84.33 ___ 83.42 ___ 76.44 shibuya and hovy ( 2020 ) [ bert_large ] 85.23 84.72 84.97 83.30 84.69 83.99 77.46 76.65 77.05 li et al',\n",
       "  'cur_ ran associates , inc. , 2015',\n",
       "  '( 2019 ) [ bert_large ] ___ 93.07 _____ yamada et al',\n",
       "  'given the exibility of the structure of our hybrid span generator , abundant future research directions remain , e.g',\n",
       "  'imojie : itera_ tive memory_based joint open information extrac__tion',\n",
       "  'however , these methods suf_ fer from slow and poor convergence , and they may fail to infer a certain path when there is a missing edge along the path',\n",
       "  'as shown in ta__ble 1 , our proposed claret achieves sota perfor__mance across all generation_tasks',\n",
       "  'together with a cross_attention reranker , our complete el framework achieves strong perfor__mance on four public datasets',\n",
       "  '( 2020b ) [ bert_large ] 92.47 93.27 92.87 91.34 88.39 89.84 yu et al',\n",
       "  'the other paradigm is based on supervised pre_training on similar tasks and then performs knowledge trans_ fer , e.g. , comet ( hwang et al. , 2021 ) , unifiedqa ( khashabi et al. , 2020 ) and unicorn ( lourie et al. , 2021 ) , but they require human_curated data',\n",
       "  'c e xamples 1id : 87d95287707e4bd99633ca0c611a4a3a world without superma :8 2inputs : [',\n",
       "  '2 : zero_ shot and multi_task',\n",
       "  'naval research logistics quar_ terly , 2 ( 1_2 ) :8397',\n",
       "  ': difference to the complete models f1_score',\n",
       "  'obviously , most previous approaches leverage the discrimination paradigm with a pre_defined scoring function to knowledge embeddings',\n",
       "  'next , similar to the findings in table 2 , we find that both task_agnostic training sets ( iv ) and multi_tasks datasets ( v ) contribute to the strength of structure_pretraining',\n",
       "  'previous study [ 18 ] illustrates that concatenating randomly sam_ pled instances as demonstrations can yield better few_shot perfor__mance',\n",
       "  'prefix_tuning : optimizing continuous prompts for generation',\n",
       "  'upper : an overview of deepstruct and the proposed structure pretrain_ ing',\n",
       "  '2.2 knowledge_graph question_answering knowledge_graph question_answering ( kgqa ) has been traditionally solved using semantic pars_ ing ( berant et al',\n",
       "  'we rst compare gtee_s taprefwith gtee_b_ase',\n",
       "  'to remedy this issue and create a model more faithful towards the knowledge present in the in_ complete kg , we devised an ensemble of kgt5 with the pathpred baseline',\n",
       "  'empirical results demonstrate that comet is able to generate novel knowledge that humans rate as high quality , with up to 77.5 % ( atomic ) and 91.7 % ( conceptnet ) precision at top 1 , which approaches human performance for these resources',\n",
       "  'fewrel : a large_scale supervised few_shot relation classication dataset with state_of_the_art evaluation',\n",
       "  'the main drawbacks of these decomposition_based methods are : ( 1 ) they need massive and ne_grained annotations for different subtasks , often resulting in the data inefciency problem',\n",
       "  'for multi_hop kg reasoning task , given query ( h , r , ? ) , we use beam_search to generate reasoning pathof maximum length nfrom q= ( h , r ) : = arg max 1 |||| k=1logp ( k|q , < k ) ( 5 ) our sequence_to_sequence framework brings two challenges',\n",
       "  'in table 4 , gtee_d ynprefachieves competitive arg_c f1_score with oneieon ace05_e+ , while obtaining 7.5 % and 4.6 % gain of f1_scores for trg_c and arg_c , respectively , achieving new state_of_the_art on ere_en',\n",
       "  'available : https : //doi.org/10.24963/ijcai.2021/ 552 [ 5 ] n. zhang et al. , alicg : fine_grained and evolvable conceptual graph con_ struction for semantic_search at alibaba , corr , 2021',\n",
       "  '18653/v1/2021.naacl_main.140 [ 3 ] x. chen et al',\n",
       "  'second,522320 40 60 80 100 120 prefix length l73.073.473.874.274.675.0trg_c f1 53.453.654.054.354.654.9 arg_c f1 trg_c f1 arg_c f1 ( a ) f1_scores of gtee_d ynprefwith different prex length l. 200 400 600 800 1000 embedding dimension d73.273.573.874.174.474.7trg_c f1 53.453.654.054.354.654.9 arg_c f1 trg_c f1 arg_c f1 ( b ) performances of gtee_d ynprefwith differ__ent dimension dof the embedding tensor p. figure 5 : intrinsic evaluation results on ace05_e+',\n",
       "  'recently , generation_based models1have shown strong performances on monolingual structured pre__diction tasks ( yan et al. , 2021 ; huang et al. , 2021b ; paolini et al. , 2021 ) , including eae ( li et al. , 2021 ; hsu et al. , 2021 )',\n",
       "  '3.1 template generation formulation we frame the ree and re tasks as template gen__eration problem , as shown in figure 2',\n",
       "  '] 10 ] 11 predicted spans : [ 12 [ 19 , 7 , spain ] , 13 [ 44 , 6 , madrid ] , 14 [ 91 , 7 , spain ] , 15 [ 128 , 9 , deportivo delacoruna ] , 16 [ 147 , 11 , real_madrid c',\n",
       "  'this helps improve the performance particularly in the case of few_shot scenarios , where the generation of label types can be imperfect since the model has seen only one or few instances of each type',\n",
       "  '( 2 ) it is very challeng_ ing to design the optimal composition architecture of different subtasks manually',\n",
       "  'hence , the model has more supervision about the semantics of the graphs as opposed to the structural_constraints',\n",
       "  'spert ( eberts & ulges , 2019 ) 88.9 71.5 89.3 78.8 dygie ( luan et al. , 2019 ) 88.4 63.2 mrc4ere ( zhao et al. , 2020 ) 88.9 71.9 85.5 62.1 rsan ( yuan et al. , 2020 ) 84.6 tanl 89.4 71.4 90.2 80.6 94.9 90.8 88.9 63.7 tanl ( multi_dataset ) 89.8 72.6 90.0 80.0 94.7 90.5 88.2 62.5 tanl ( multi_task ) 90.3 70.0 91.2 83.8 94.7 90.7nerconll03 ontonotes genia * ace2005 * bert_mrc ( li et al. , 2019a ) 93.0 91.1 83.8 86.9 bert_mrc+dsc ( li et al. , 2019b ) 93.3 92.1 cloze_cnn ( baevski et al. , 2019 ) 93.5 gsl ( athiwaratkun et al. , 2020 ) 90.7 90.2 tanl 91.7 89.8 76.4 84.9 tanl ( multi_dataset ) 92.0 89.8 75.9 84.4 tanl ( multi_task ) 91.7 89.4 76.4relation class.fewrel 1.0 ( validation ) tacred5_way 1_shot5_way 5_shot10_way 1_shot10_way 5_shot bert_em ( soares et al. , 2019 ) 70.1 88.9 82.8 bert em+mtb ( soares et al. , 2019 ) 71.5 90.1 83.4 dg_spanbert ( chen et al. , 2020 ) 71.5 bert_pair ( gao et al. , 2019 ) 85.7 89.5 76.8 81.8 tanl 71.9 94.0 4.1 96.4 4.2 82.64.5 88.25.9 tanl ( multi_task ) 69.1srlconll05 wsj conll05 brown conll2012 dep and span ( li et al. , 2019d ) 86.3 76.4 83.1 bert srl ( shi & lin , 2019 ) 88.8 82.0 86.5 tanl 89.3 82.0 87.7 tanl ( multi_dataset ) 89.4 84.3 87.6 tanl ( multi_task ) 89.1 84.1 87.7event extr.ace2005 trigger id',\n",
       "  'few_shot kgc for one_shot learning on rela__tions , xiong et al',\n",
       "  ') avg ( all ) x_g_ear ( mbart_50_large ) 68.3 48.9 37.7 59.8 30.5 29.2 63.6 45.9 32.3 63.9 37.4 46.2 w/ constrained_decoding 68.0 49.1 37.8 59.5 30.6 29.2 59.7 47.7 31.3 62.4 37.6 45.9 x_g_ear ( mt5_base ) 67.9 53.1 42.0 66.2 27.6 30.5 69.4 52.8 32.0 67.8 39.7 49.1 w/ constrained_decoding 67.9 53.1 42.0 66.2 27.8 30.4 66.7 53.1 33.1 67.0 39.9 48.9 x_g_ear ( mt5_large ) 71.2 54.0 44.8 68.9 32.1 33.3 68.9 55.8 33.1 69.7 42.2 51.3 w/ constrained_decoding 71.2 54.8 45.6 68.9 32.0 33.3 66.2 57.7 35.0 68.8 43.1 51.6 table 9 : the detailed breakdown results for applying constrained_decoding on x_g_ear',\n",
       "  'currently , copyre ( zeng et al',\n",
       "  'experiments on standard and sparse kgs show that our approach yields significant improvement over prior methods , while converging 4x_7x faster',\n",
       "  'f1 ( clark et al. , 2018 ) [ glove300d ] ___ 92.6 _____ ( peters et al. , 2018 ) [ elmo ] ___ 92.22 _____ ( akbik et al. , 2019 ) [ flair ] ___ 93.18 _____ ( strakov et al. , 2019 ) [ bert_large ] ___ 93.07 _____ ( yamada et al. , 2020 ) [ roberta_large ] ___ 92.40 _____ ( li et al. , 2020b ) [ bert_large ] 92.47 93.27 92.87 91.34 88.39 89.84 ( yu et al. , 2020 ) [ bert_large ] 92.85 92.15 92.5 89.92 89.74 89.83 ( yan et al. , 2021b ) ( bpe ) [ bart_large ] 92.60 93.22 92.96 90.00 89.52 89.76 ours [ t5_base ] ( without_de ) 92.68 93.49 93.08 89.58 90.71 90.14 ours [ t5_base ] ( intra_de ) 92.78 93.51 93.14 89.77 91.07 90.42 ours [ t5_base ] ( inter_de ) 92.68 93.57 93.12 89.75 91.02 90.38 table 1 : results for the flat_ner datasets',\n",
       "  '3.1 knowledge_graph completion aknowledge graph ( kg ) ( e , r , t ) includes an entity set e , a relation set rand a tuple list t= [ ( h , r , t , m ) 1 , , ( h , r , t , m ) n ] where h , t eis head and tail_entity , r r is the tuple relation and mis the kg meta_information',\n",
       "  'we leave this as one of the future investigations.3.2 ablation_studies pretraining strategies as the key question of our work is to investigate how structure pretrain_ ing improves the structural_understanding ability of lms , we examine how different pretraining strate_ gies impact the downstream performance',\n",
       "  'they designed a reward to promote three conversational properties : informativity , co_ herence and ease of answering',\n",
       "  'we further exhibit that the claret provides a good initialization for downstream_tasks by zero_ and few_shot learning',\n",
       "  '2.1 generative pretraining while the lm is pretrained to understand a single aspect of the text , structural_understanding aims to recover the entire structure of the text ( figure 2 )',\n",
       "  'so task_wise both datasets should pose a similar challenge',\n",
       "  'd implementation_details we conducted grid search to nd the best learn_ ing rate over { 1105,3105,5105,7 105,9105 } using temp genw/o t opk copy on the muc_4 ree task',\n",
       "  '78717880 , online , july 2020a',\n",
       "  'even though theoretically the beam_ search should help us reduce the exposure_bias , we do not observe any performance gain during grid search of the beam_size and the length penalty on the validation set ( detailed grid search setting is in appendix a )',\n",
       "  'additionally , the limited sampling ( 3.3 ) may not even provide the correct answer if there exist more known positives than sampled answers',\n",
       "  'after the substructure learning stage , we further train our model for the full structure generation_task using the loss in equa_ tion 4',\n",
       "  'struc_ ture pretraining improves the adaptation to those datasets',\n",
       "  'instead of learning from scratch , we perform continual pre_training from bart_large ( lewis et al. , 2020 ) due to limited computation resources',\n",
       "  'besides sota fine_tuning results on 5 generation and 4 clas_ sification tasks , we conduct zero_/few_shot learn_ ing and extensive ablation_studies to exhibit our models effectiveness',\n",
       "  'since slots in our joint prompts usually entail different semantic meanings and matching preferences , even they are about the same roles , the permutation_invariance property of bipartite_matching assures a global_optimization of these arguments.such optimization especially makes sense when the arguments in context have subtle semantic dis_ tinction , and such distinction can not merely be captured by sequential order',\n",
       "  'for larger models , structure pre_ training fully utilizes the model capacity and also teaches the models to generate triples according to the downstream_tasks , allowing them to gener_ alize well to most tasks with the rest capacity',\n",
       "  'improved semantic representations from tree_structured long short_term memory net_ works',\n",
       "  'knowledge_graph embedding by trans_ lating on hyperplanes',\n",
       "  'when we obtained a good triplet after three steps , we assign reward 1 to each of these three steps',\n",
       "  '2013 ; bast and haussmann 2015 ; das et al',\n",
       "  \"apart from the preposition with '' ,897context original after when outlining her tax reform policy , clinton has made clear that she wants to tax the wealthy and make sure they pay their fair share\",\n",
       "  '( iii ) no pretrain , finetune : we remove structure pretrain_ ing , and only finetune the lm on conll04',\n",
       "  'besides , both tanl and deepex explore rela_ tively small_scale pretrained lms',\n",
       "  'few_shot kgc finally , we verify kg_s2s s ability in few_shot learning in the nell_one benchmark , as shown in table 4',\n",
       "  'in figure 3 , we provide an illustration for the attention masks 2_dimensional matrix denotedasm',\n",
       "  'we collect large_scale paral_ lel text_structure pairs by aligning wikidata withenglish wikipedia',\n",
       "  'further analysis of different types of negative_graphs reveal that the human_error graphs are the hardest , most diverse , and hence the best type of negatives to learn from in contrastive_learning',\n",
       "  'multipleworks show that conditioning input representationson slot description embeddings improves multi_domain slot_labeling performance ( bapna et al.,2017 ; lee and jha,2019 )',\n",
       "  '4 related work pretrained lms ( devlin et al. , 2019 ; radford et al. , 2019b ; yang et al. , 2019 ) are the key ingredi_ ents in contemporary nlp',\n",
       "  'we tried several different reward assignments but only this one works.generate triplets in the order of the ground_truth while the reinforcement_learning allows the model generate triplets freely',\n",
       "  'a survey of heterogeneous information network anal_ ysis',\n",
       "  'springer , 2010. doi : 10.1007/ 978_3_642_15939_8\\\\_10',\n",
       "  '29 , 2021 [ 2 ] j. luo , x. chen , j',\n",
       "  'the kgqa finetuning and inference pipeline is explained in 3.4',\n",
       "  '5 shows the per__formance of paie and two other qa_based base_ lines with partial training samples on three bench_ marks',\n",
       "  'this ability helps the models transferknowledge among tasks with limited data , whichare demonstrated through the rest of the paper.4.3 limited resource scenarios andimportance of label semanticsin this section , we show that our model can usethe semantics of labels to learn efciently , which iscrucial for scenarios with limited labeled data',\n",
       "  'unlike supervised_learning with negative log likelihood ( nll ) loss , which forces the model to generate triplets in the order of the ground_truth , reinforcement_learning allows the model generate triplets freely to achieve higher reward',\n",
       "  'we care_ fully tuned the hypermeters on the valid set ( detailed search space in supplementary materials )',\n",
       "  'while the previous category of negative_graphs ( syst ) captures structural con_ straints , syse captures the relational knowledge in graphs',\n",
       "  'available : https : //github.com/microsoft/unilm 5 [ online ]',\n",
       "  'our ndings suggest that using gen_ erative commonsense models for automatic commonsense kb completion could soon be a plausible alternative to extractive methods',\n",
       "  'although pipeline modelshave achieved great progress ( zhang et al',\n",
       "  'cleaned output : six days after starting acicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviracicloviraciclovir she exhibited signs of lithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithiumlithium toxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicitytoxicity',\n",
       "  'this work was supported by the national research founda_ tion , singapore under its strategic capability re_ search centres funding initiative',\n",
       "  'the input to the model is the concatenated belief , argument and the stance along with a prex gen_ erate an explanation_graph for',\n",
       "  'we evaluate the qa capabilities of kgt5 on three large_scale kgqa benchmark datasets : metaqa ( zhang et al. , 2018 ) , webquestionssp ( wqsp ) ( yih et al. , 2016 ) and complexwebques_ tions ( cwq ) ( talmor and berant , 2018 )',\n",
       "  'additionally , comparing with solely training on tgt , transfering the knowledge from src allows gtee_d_ynpref to achieve higher f1_scores than oneieandtext2event',\n",
       "  'during the inference stage , we rst generate triplet se_ quences via beam_search ( wiseman and rush 2016 )',\n",
       "  'by comparison , our model utilizes contrastive_learning to encourage the model to generate faithful triples implicitly.iii',\n",
       "  '6.3 few_shot setting we analyze how paie performs under a scenario without sufficient annotations',\n",
       "  'combined with a sim_ ple cross_attention reranker , our complete el framework achieves state_of_the_art results on three wikidata_based datasets and strong per__formance on tackbp_20101',\n",
       "  '; paper_content : proceedings of the 60th annual meeting of the association_for_computational_linguistics volume 1 : long_papers , pages 1190 _ 1208 may 22_27 , 2022 c2022 association_for_computational_linguistics explanation_graph generation via pre_trained_language_models : an empirical study with contrastive_learning swarnadeep saha prateek yadav mohit bansal unc_chapel_hill { swarna , prateek , mbansal } @ cs.unc.edu abstract pre_trained sequence_to_sequence language_models have led to widespread success in many natural_language_generation tasks',\n",
       "  'by comparison , our model utilizes contrastive_learning to encourage the model to im_ plicitly generate faithful triples',\n",
       "  'to address the above challenges , we employ a curriculum_learning ( bengio et al. , 2009 ; xu et al. , 2020 ) strategy',\n",
       "  'we give four examples of triple query along with the hits @ 1 inferred eviden_ tial path , including paths containing only existing edges ( w/o missing ) and paths containing missing edges ( w/ missing )',\n",
       "  'toward an architecture for never_ending lan__guage learning',\n",
       "  'training the same encoder from scratch ( _ pretraining ) reduces performance by30 %',\n",
       "  'table 8 shows their detailed statistics',\n",
       "  '; paper_content : proceedings of the bionlp 2022 workshop , dublin , ireland , pages 1025 may 26 , 2022',\n",
       "  '2020b ) 89.7 89.5 89.6 93.4 90.1 91.8 generativecopyre ( zeng et al',\n",
       "  'for instance , our 10b lm pro_ duces superior zero_shot performance compared to 175b gpt_3 on a representative structure pre__diction task',\n",
       "  '[ 66 ] y._c. chen , z. gan , y. cheng , j. liu , and j. liu , distilling knowledge learned in bert for text_generation , in proc',\n",
       "  'model10 % data 100 % data arg_i_arg_c arg_i_arg_c oneie 48.3 45.4 73.2 69.3 bart_gen ___ 69.9 66.7 degree ( eae ) 63.3 57.3 76.0 73.5 degree ( eae ) + variant template 1 61.6 55.5 73.4 70.4 degree ( eae ) + variant template 2 63.9 56.9 75.5 72.5 table 8 : study on the effect of different template con_ structing rules',\n",
       "  'our method improves arg_c f1 by 4.6 % and 2.7 % over the sota generation baseline and its extended multi_ task tanl',\n",
       "  'for generation_tasks , we apply eq',\n",
       "  'our framework is general purpose , performing well on few_shot , low_resource , and high_resource tasks',\n",
       "  'dataset train validation test metaqa 1_hop 96,106 9,992 9,947 metaqa 2_hop 118,980 14,872 14,872 metaqa 3_hop 114,196 14,274 14,274 wqsp 2,998 100 1,639 cwq 27,639 3,519 3,531 table 11 : numbers of questions in the kgqa datasets used in our experiments',\n",
       "  'to jointly opti_ mize the triple generation and contrastive object , we intro__duce a batch_wise_dynamic attention_masking mechanism , which allows us to dynamically choose different objects and jointly optimize tasks',\n",
       "  'these methods have recently been applied to kg link_prediction and question_answering over incomplete kgs ( kgqa )',\n",
       "  'leveraging human_created negative_graphs leads to a bigger gain in seca because of the hard_ belief : collectivism is terrible for society',\n",
       "  'human_created & semantic negative_graphs ( huse )',\n",
       "  'd egree has three advantages to learn well with less train__ing data',\n",
       "  '40544060. ijcai.org , 2020. doi : 10.24963/ijcai.2020/561',\n",
       "  'epgel outper_ forms both opentapioca and tweeki ( sec',\n",
       "  'it can also deal with multiple arguments with the same role via flexible role prompts instead of heuristic threshold tuning',\n",
       "  'this is typically accomplished using knowledge_graph embedding ( kge ) models',\n",
       "  '( 2020 ) considers the score of each triple as canonical decomposition of order 4 tensors in complex domain',\n",
       "  'corresponding author 1our source_code is available at https : //github',\n",
       "  'overview of the fourth mes_ sage understanding evaluation and conference',\n",
       "  'moreover , for the fraction of graphs that are structurally correct , the model also makes commonsense mistakes , a type of semantic error , by inferring wrong or incoher_ ent relations between concepts',\n",
       "  'note that our hybrid span decoder is perpendic_ ular to the actual choice of the neural structures of the inner blocks , and we choose the design of mixed_attention transformer ( he et al. , 2018 ) be_ cause its layerwise coordination property is empiri_ cally more suitable for our heterogeneous decoding of two different kinds of sequence elements',\n",
       "  'here we compare against transe , com_ plex and their variants',\n",
       "  'while we choose t5 because of its superior performance ( saha et al. , 2021b ) , we do not make any model_specic as_ sumptions and graphs can be generated via any encoder_decoder style pre_trained language model ( e.g. , see appendix e for results with bart )',\n",
       "  'through our analysis , the main cul_prit is found in eq',\n",
       "  'zero_shot setup for our zero_shot setup , we fol_ low the zero_shot usage in recent pretrained lm studies ( brown et al. , 2020 ; wei et al. , 2021 ; sanh et al. , 2021 )',\n",
       "  'lastly , we show that human errors are the best negatives for contrastive_learning and also that automat_ ically generating more such human_like nega_ tive graphs can lead to further improvements.1 1 introduction pre_trained sequence_to_sequence language mod__els ( plms ) like bart ( lewis et al. , 2020 ) and 1our code and models are publicly available at https : //github.com/swarnahub/explagraphgen',\n",
       "  'compared with the uie model w/o ssi , uie equipped with ssi achieves improvements of 4.16 and 3.30 on average for n_shot and n_ratio experi_ ments',\n",
       "  'worddecoding with sin_ gle attention achieves the highest f1_score on both datasets.we also see that our copy_mechanism improves f1 scoresby around 47 % in each attention_mechanism with bothdatasets',\n",
       "  'mixed_ attention : we remove the mixed_attention layer and use a standard transformer_encoder decoder structure',\n",
       "  '( 2018 ) 67.8 64.9 66.3 ___ _______ dai et al',\n",
       "  'unlike natural lan__guage , graphs have distinct structural and se_ mantic properties in the context of a down__stream nlp task , e.g. , generating a graph that is connected and acyclic can be attributed to its structural_constraints , while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node con_ cepts',\n",
       "  're_ cent work has made efforts in bridging the gap in transferring pretrained models to structure pre__diction tasks with a focus on two directions',\n",
       "  'we further enhance the pretraining with multiple down__stream structure_prediction training sets and obtain state_of_the_art performance on 21 of 28 datasets',\n",
       "  '4.7 kg vs lm pretraining we analyzed how generic corpora pretraining per_ formed compared to kg link_prediction training for the task of kgqa',\n",
       "  '5nell995 is , in fact , sparse ( according to the statistic in table 1 ) , but we consider it a standard kg benchmark as previous multi_hop_reasoning studies do',\n",
       "  '[ 63 ] y._c. chen , z. gan , y. cheng , j. liu , and j. liu , distilling knowledge learned in bert for text_generation , in proc',\n",
       "  '6 conclusion in this work , we propose the hybrid span gener__ation ( hyspa ) model , the rst end_to_end text_to_ graph extraction model that has a linear space and time complexity at the graph decoding stage',\n",
       "  'the model implicitly learns the structural_constraints through relevant supervision and the margin_based loss enables it to learn a better boundary between correct and incorrect graphs',\n",
       "  'to explore further , we investigate by how much novel tuples from the development_set differ from training set phrase objects for the same s , rusing minimum edit_distance of phrase objects',\n",
       "  'we posit that an important step toward automatic common__sense completion is the development of gen_ erative models of commonsense knowledge , and propose com monsensetransformers ( comet ) that learn to generate rich and diverse commonsense descriptions in natural_language',\n",
       "  'when decod_ ing , we can either decode all triplets with one u_ nied decoder or decode every triplet with a sep_ arated decoder',\n",
       "  'for example , in webnlg dataset , multidecoder model only achieves 0.481 and 0.518 f1_score with alpha_ betical and frequency strategy , it achieves 0.564 f1_score with rl',\n",
       "  'second , there is a scarcity of work that has focused on generating faithful triples',\n",
       "  'we compute the metrics , includ_ ing mean reciprocal rank ( mrr ) and hits at n ( h @ n ) under filtered setting ( bordes et al. , 2013 )',\n",
       "  'haitian sun , tania bedrax_weiss , and william w. co_ hen',\n",
       "  'however , since the model has learnt soft rules in the past iterations , making 1much more reasonable than random sampling , then the search space left for 2is more concen_ trated around the groundtruth path',\n",
       "  '( 5 ) graphr ( fu , li , and ma 2019 ) : this model considers each token in a sentence as a node in a graph , and edgesconnecting the nodes as relations between them',\n",
       "  'degree has three advantages to learn well with less training_data',\n",
       "  'more accu_ rate question_answering on freebase',\n",
       "  'the generative method could generate triplets in any order actually',\n",
       "  '4.1 main evaluation fine_tuning for generation',\n",
       "  'meanwhile , for the model trained on fb15k237_20 % , only 41 % of all hits @ 1 paths contain only edges from 9the low reasonable rate ( < 10 % for all models ) is par_ tially due to the non_existence of such reasonable path in the incomplete kg dataset , as we observed during annotation',\n",
       "  'thomas n. kipf and max welling',\n",
       "  'differently , deepstruct aims to generate the triples for a wide set structure_prediction_tasks in an end_to_end fashion thanks to the proposed structure_pretraining',\n",
       "  '5 discussion related models recent studies have provided unified solutions for structural prediction_tasks',\n",
       "  'we consider the same crite_ ria in prior works ( wadden et al. , 2019 ; lin et al. , 2020 )',\n",
       "  'it is based on maximum bipartite_matching algorithm ( kuhn , 1955 ; munkres , 1957 )',\n",
       "  '3.4 learning with prompted span selector given context representation hxand a set of span selectors { k } , each kaims to extract at most one corresponding argument span ( sk , ek ) from hx',\n",
       "  'models p r f1 grit_pipeline 63.88 37.56 47.31 dygie++ ( wadden et al. , 2019 ) 61.90 36.33 45.79 seqtagging ( du and cardie , 2020 ) 46.80 38.30 42.13 gtt 61.69 42.36 50.23 table 2 : micro_average results on the full test_set',\n",
       "  'towards controllable bi_ ases in language generation',\n",
       "  'however , we cant assign a reward to each step directly during the genera__tion since we dont know whether each action we choose is good or not before we nish the gen__eration',\n",
       "  'improving zero_shot cross_lingual_transfer learning via robust training',\n",
       "  'we present the effect of the following gen__eration strategies : argmax greedy decoding , beam_search with beam sizes , b=2 , 5 , 10 , and top_ ksam_ pling with k = 5 , 10',\n",
       "  'we calculate the matching score with the contrastive classier and lter out those triples with the match _score <',\n",
       "  'synthetic & semantic negative_graphs ( syse )',\n",
       "  'we use an enhanced bilou scheme de_ scribed in section 4.1',\n",
       "  'as shown in figure 3 , instead of using the standard pretrain_finetune paradigm for each task , we intro__duce structure_pretraining that aims to teach lms to correspond to structures in a wide spectrum of tasks at the same time',\n",
       "  'we direct_ ly run the code released by zheng et al',\n",
       "  'naive copy refers to comput_ ing copy distributions with the attentions from all cross_attention heads',\n",
       "  'wqsp contains both 1_hop and 2_hop path based questions while cwq contains questions requiring steps such as compositional , conjunctive , comparative and su_ perlative reasoning',\n",
       "  '( 2018 ) [ glove300d ] ___ 92.6 _____ peters et al',\n",
       "  'empirical results demonstrate that comet is able to generate novel knowledge that humans rate as high quality , with up to 77.5 % ( a tomic ) and 91.7 % ( conceptnet ) precision at top 1 , which approaches human performance for these re_ sources',\n",
       "  '39483954. ijcai.org , 2020. doi : 10.24963/ijcai.2020/546',\n",
       "  'the relationship between [ carmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_melis ] and [ sopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosoprano ] is output ( chosen ) : relationship between [ carmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_melis ] and [ sopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosoprano ] =voice_type input ( alternative 1 ) : born in bologna , orlandi was a student of the famous italian [ sopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosoprano ] and voice_teacher [ carmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_melis ] in milan',\n",
       "  'by replacing the trained ic with the oracle gold ic results , we can still observe possible increasements for f1_scores , suggesting the existence of likely chances for further optimizing ic performances',\n",
       "  'figure 2 shows the overview of kg_s2s .4007 figure 2 : the overview of kg_s2s',\n",
       "  'on the fraction of structurally correct graphs , the model makes further semantic errors and a lower seca of 35 % demonstrates that',\n",
       "  'justin lovelace , denis newman_griffis , shikhar vashishth , jill fain lehman , and carolyn ros',\n",
       "  'there have been great numbers of studies on eae tasks since an early stage ( chen et al. , 2015 ; nguyen et al. , 2016 ; huang et al. , 2018 ; yang et al. , 2018 ; sha et al. , 2018 ; zheng et al. , 2019 )',\n",
       "  'the generated sequence corresponds to a path in kg',\n",
       "  'this paradigm naturally induces the language knowledge from pre_trained_language_models by converting eae tasks to fully_ explored reading_comprehension tasks via a ques_ tion template',\n",
       "  'furthermore , comparing with the results in table 1 , we found that grit performs better than our system , reecting that a pointer_network_based model , which has with smaller search space than ours , is 20 40 60 80 100 percentage of training_data ( % ) 4045505560ceaf_ree f1 ( % ) model tempgen tempgen _ topk copy gritfigure 5 : ree test_set performance on muc_4 with regard to different amount of training_data',\n",
       "  '[ hehehehehehehehehehehehehehehehehe|barack_obama ] chose [ herherherherherherherherherherherherherherherherher|hillary_rodham_clinton ] because [ shesheshesheshesheshesheshesheshesheshesheshesheshe|hillary_rodham_clinton ] had foreign_affairs experience as a former [ first ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst ladyfirst lady |hillary_rodham_clinton ]',\n",
       "  'hence we try to answer the followingdatasettrain questionsdistinct qtypesdistinct nl questionstrain qa pairs 1_hop 96,106 11 161 184,884 2_hop 118,980 21 210 739,782 3_hop 114,196 15 150 1,521,495 table 12 : statistics for metaqa qa datasets',\n",
       "  'to jointly opti_ mize the triple generation and contrastive object , we introduce a batch_wise_dynamic attention_masking mechanism , which al_ lows us to choose different objects and jointly optimize tasks dynamically',\n",
       "  'for each of the heads , q , k , andvare uniquely projected prior to the attention being computed : hi=attention ( qwq i , kwk i , v wv i ) ( 6 ) where hiis the output of a single attention head andwq i , wk i , andwv iare head_specic projec_ tions for q , k , and v , respectively',\n",
       "  '1 , long_papers , r. barzilay and m. kan , eds',\n",
       "  'work done during internship with facebook ai research',\n",
       "  'the generation_based model with carefully designed prompts is able to utilize the label_semantics and the additional weakly supervised signals , thus help_ ing learning under the low_resource regime',\n",
       "  '( ju et al. , 2018 ; strakov et al. , 2019 ) , span_ based ( luan et al. , 2019a ; shen et al. , 2021a ) and generative_based ( strakov et al. , 2019 ; paolini et al. , 2021 ; yan et al. , 2021a ) methods',\n",
       "  'input : born in bologna , orlandi was a student of the famous italian [ sopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosoprano ] and voice_teacher [ carmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_melis ] in milan',\n",
       "  '67696781 , online , november 2020',\n",
       "  'this result suggests that comet could be ef_ fective with human evaluators in the loop to con_ rm the correctness of generated tuples',\n",
       "  'dan sanchez reports : the saudis go full isis in their us _ backed takfiri war on the shia saudi_arabia has perpetrated a mass < t > execution < /t > that putsisiss beach beheadings to shame',\n",
       "  'therefore , the output strings have to reect the change of the in_ put language accordingly while remaining well_structured',\n",
       "  'the discussion with colleagues in aws shanghai ai lab was quite fruitful',\n",
       "  'in spite of the added extrac__tion capability , grit requires no additional pa_ rameters beyond those in the pre_trained bert',\n",
       "  'clozecnn ( baevski et al. , 2019 ) leverages a cloze_driven pre_training',\n",
       "  'previous work has addressed this issue with various strategies , including reinforcement_learning ( zeng et al. , 2019 ) , unordered_multi_tree decoders ( zhang et al. , 2020 ) , and non_autoregressive de_ coders ( sui et al. , 2020 )',\n",
       "  'in these works a structured representation , a tree or a graph for instance , is linearized into a sequence of symbols compatible with a seq2seq architecture',\n",
       "  'however , these generation_based methods have two signicant challenges , which impede achiev_ ing competitive results with the classication_based methods',\n",
       "  'on the completion task originally proposed in li et al',\n",
       "  '5.2 results quality our results indicate that high_quality knowledge can be generated by the model : the low perplexity scores in table 6 indicate high model condence in its predictions , while the high clas_ sier score ( 95.25 % ) indicates that the kb com_ pletion model of li et al',\n",
       "  'dotted lines indicate outputs to all future blocks in the next layer and inputs from all preceding blocks in the previous layer',\n",
       "  'we used grid search to nd the best kfortopk c opy and found that k= 10 yields the best overall performance across ree and re',\n",
       "  'the detailed structure of the inner blocks is explained in appendix e. hybrid span decoding for the hybrid span de_ coding module , we rst slice off the hidden rep_4072resentation of the alternating_sequence yfrom the output of the n_layer inner blocks and denote it ashn y',\n",
       "  'following the search , deepex introduces an extra ranking stage to improve the quality of the triples',\n",
       "  'knowledge_graph question_answering ( kgqa ) is the task of answering a natural_language question using a kg as source of knowledge',\n",
       "  'in this work , we representnode types as separate nodes that are connected to their nodevby a special edge type , [ type ] .2 representing information graphs as sequences instead of directly modeling the space of het_ erogeneous multigraphs , g , we build a mapping s=fs ( g , ) fromg , to a sequence_space s. fsdepends on a ( given ) ordering of nodes and their edges in g , constructed by a graph traver_ sal algorithm like breadth_first_search ( bfs ) or depth_first_search ( dfs ) , and an internal order_ ing of nodes and edge_types',\n",
       "  'tkgc and fkgc methods are further integrated with non_trivial com_ ponents or learning paradigms to handle the ex_ tra temporal information or training requirements',\n",
       "  'see also carreras & mrquez ( 2005 ) ; pradhan et al',\n",
       "  'this predic_tion closely matches the true label onto [ jerrysplaylist owner ] [ classical moments in moviesplaylist ] where the true class of classical mo_ments in movies isplaylist',\n",
       "  '; paper_content : proceedings of the 57th annual meeting of the association_for_computational_linguistics , pages 47624779 florence , italy , july 28 _ august 2 , 2019. c2019 association for computational linguistics4762comet : commonsense transformers for automatic knowledge_graph construction antoine bosseluthannah rashkinmaarten sapchaitanya malaviya asli celikyilmazyejin choi allen institute for articial_intelligence , seattle , wa , usa paul g. allen school of computer science & engineering , seattle , wa , usa microsoft_research , redmond , wa , usa abstract we present the rst comprehensive study on automatic knowledge_base construction for two prevalent commonsense knowledge graphs : a tomic ( sap et al. , 2019 ) and con_ ceptnet ( speer et al. , 2017 )',\n",
       "  'one possible ex_ planation could be that the reduced model capacity ofkgt5 which has only 60m parameters does not allow it to memorize facts seen during pretrain_ ing , leading to poor train mrr and full_kg kgqaperformance',\n",
       "  'similar to bfs embedding , the dfs level embedding assigns the same embedding vec_ torlifor each position at the dfs traversal level i , but the value of the embedding vector is randomly initialized instead of lled with the non_parametric sinusoidal position embedding , since the proximity information does not exist between the traversal levels of dfs',\n",
       "  'second , removing the structure pretrain_ ing ( iii ) provides the most direct ablation of how much structure_pretraining helps',\n",
       "  'the hid_ den dimension of the forward and backward lstm of thepointer networks is set at d p= 300',\n",
       "  '* indicates pvalue < 0:01for a paired t_test evaluation',\n",
       "  'also , adding self_consistency during decoding can further boost squires performance for 1 % 2 % on all metrics',\n",
       "  'since , for an information graph , the maximum span length , m , of a mention is often far smaller than the length of the text , i.e. , mn , we can then reduce the bound of the maximum magnitude of kfromo ( n2 ) too ( nm ) by only considering spans of length smaller than m , and thus maintain linear space complexity for our decoder with respect to the length of the input text , n',\n",
       "  'paper_title : hyspa : hybrid span generation for scalable text_to_graph extraction ; paper_abstract : text_to_graph extraction aims to automatically extract information graphs consisting of mentions and types from natural_language texts',\n",
       "  'we also consider low_dimensional versions of the state_ of_the_art method complex',\n",
       "  'table 3 shows the performance of 4 ie tasks under 6 low_resource settings',\n",
       "  'under both setups , we observe similar trends',\n",
       "  'fu , li , and ma ( 2019 ) used a graph convolution network ( gcn ) where they treated each tokenin a sentence as a node in a graph and edges were consideredas relations',\n",
       "  '( 2018 ) 82.4 72.4 34.1 35.2 50.3 38.2 61.9 52.7 53.4 broscheit ( 2019 ) 79.3 _____ _______ martins et al',\n",
       "  'share13 share14 model p r f p r f dai et al',\n",
       "  'datasets comet relies on a seed set of knowl__edge tuples from an existing kb to learn to pro_ duce commonsense knowledge',\n",
       "  'contains two parts : the encoder part introduces an additional crf loss , and in the decoder part the cross entropy loss isused to measure the difference between the output tripletsand the gold triplets',\n",
       "  'however , practical kgs often suffer from in_ completeness , thus proposing the task of kg com_ pletion , such as predicting the tail_entity tgiven ( h , r )',\n",
       "  '2019 ) : this model uses an encoder_decoder approach with n_gram attention mecha_nism for knowledge_base completion using distantly super_vised data',\n",
       "  'most previous kg completion methods , such as transe [ 2 ] , com_ plex [ 11 ] , and rotate [ 9 ] , are knowledge embedding techniques that embed the entities and relations into a vector_space and then obtain the predicted triples by leveraging pre_defined scoring func_ tions to those vectors',\n",
       "  'on our multi_hop_reasoning task , these findings sug_ gest the potential of substituting the previous rl pipeline with transformer.3 methodology 3.1 preliminaries knowledge_graph',\n",
       "  'as an ex_ ample , we assume bfs traversal here and leave the details of dfs traversal embedding in appendix d.4071 figure 4 : the architecture of our hybrid span decoder',\n",
       "  'in international conference on articial neu_ ral networks , pages 549558',\n",
       "  'end_to_end object de_ tection with transformers',\n",
       "  'lcl=logexp ( sim ( h ( g ) , h ( p ) ) / ) hih ( g ) exp ( sim ( h ( g ) , hi ) / ) l=lce+lcl1195sastca seca g_bs ged ea t5_base ( saha et al. , 2021b ) 87.2 38.7 19.0 33.6 0.71 20.8 t5_large 87.2 51.0 34.7 43.9 0.61 29.5 generate & rene 87.2 52.5 37.7 45.3 0.60 30.0 pos data aug 87.2 54.5 41.5 46.9 0.58 30.2 max_margin 87.2 56.7 43.5 48.6 0.57 30.5 contrastive 87.2 60.5 42.5 52.1 0.52 33.1 upper bound 91.0 91.0 83.5 71.1 0.38 46.8 table 2 : comparison of all models across all metrics on the explagraphs ( saha et al. , 2021b ) test_set',\n",
       "  'commonsense knowledge in the following in_ stance with implicit arguments : whether the u.s. extradites gulen or not this will be a political deci_ sion , bozdag said',\n",
       "  'structure_pretraining is designed to bridge the gap via guiding lms to produce structures from the text',\n",
       "  'uni_mannheim.derainer gemulla university of mannheim_germany rgemulla @ uni_mannheim.de abstract knowledge_graph embedding ( kge ) models represent each entity and relation of a knowl__edge graph ( kg ) with low_dimensional em__bedding vectors',\n",
       "  'we argue two advantages contribute to this result : 1 ) pretrain_ ing / finetuning consistency',\n",
       "  'specially , we follow the training suggestions ( li and liang , 2021 ) and reparametrize the embedding tensorpby modeling a mlp and another embed__ding tensorpr|e|ldwith small dimension d < d',\n",
       "  'experiments show that our ap__proach significantly outperforms previous methods , on both standard kgs and sparse kgs',\n",
       "  'in addition , it is interesting to combine kg_s2s with other knowledge_intensive nlp tasks , such as con_ versation recommendation ( li et al. , 2018b ) and commonsense generation ( wang et al. , 2021b ) in the seq2seq framework , and see if the kg knowl__edge could benefit these downstream_tasks',\n",
       "  'formally , the gener_ ative transformer obtain contextualized representations and optimize the following object : ^x0 ; ^x1 ; : : : ; ^xm ; ^y0 : : : ; ^yn = transformer ( x0 ; x1 ; : : : ; xm ; y0 ; : : : ; yn ) ( 5 ) loss generation =x ( mx 1xilog ( ^xi ) +nx 1yilog ( ^yi ) ) ( 6 ) algorithm 1 triplet contrastive_learning 1 : require : train instances x=x1 ; : : : ; x n , labelsy= y1 ; : : : ; y n , batch_size k , pos = , neg = , tem_ peraturet 2 : whilein=k do 3 : batch = [ ( x ; y ) 1 ; : : ; ( x ; y ) k ] 4 : for ( x ; y ) jin batch do 5 : pos = decompose triple ( y j ) 6 : forposin pos do 7 : neg = random permute entity ( pos ) 8 : lpos = contrastive classify ( x , pos ) 9 : lneg = contrastive classify ( x , neg ) 10 : z = cat ( [ l pos , l neg ] , dim=1 ) 11 : labels = zeros ( 2 ) 12 : loss = crossentropyloss ( z/t , labels ) 13 : loss.backward ( ) 14 : update ( contrastive classier.params ) 15 : return dataloader triplet contrastive_learning the previous generation_based approach usually neglects the fact that triple should be faithful and consistent with the input sentence',\n",
       "  'another way to ef_ ciently represent spans is to use the hypergraph ( lu5809and roth , 2015 ; katiyar and cardie , 2018 ; wang and lu , 2018 ; muis and lu , 2016 )',\n",
       "  'structure pre_ training significantly improves the lm in structure_prediction',\n",
       "  'for instance , a typical structure_prediction task , called open in__formation extraction , seeks the entire structural in_ equal contribution',\n",
       "  '2.2 structural_schema_instructor for controllable ie structure generation using sel , uie can uniformly generate differ__ent ie structures',\n",
       "  'to find the optimal span , we design two selectors for the start and end tokens from context : ( s , e ) qk=span_search [ gl ( x ; q ) ] where ( s , e ) qkis the span about k_th query and gl is the span selector',\n",
       "  '2021 association for computational linguistics4066hyspa : hybrid span generation for scalable text_to_graph extraction liliang ren , chenkai sun , heng_ji , julia hockenmaier university of illinois , urbana champaign department of computer science { liliang3 , chenkai5 , hengji , juliahmr } @ illinois.edu abstract text_to_graph extraction aims to automati_ cally extract information graphs consisting of mentions and types from natural_language texts',\n",
       "  '( 2018 ) [ elmo ] ___ 92.22 _____ akbik et al',\n",
       "  'our traver_ sal embedding is the sum of the level embedding , the parent_child embedding and the tree embedding',\n",
       "  'this shows our de_ signs capability to leverage pre_trained knowledge in the generation process',\n",
       "  'dense passage retrieval for open_domain question_answering',\n",
       "  'dpr data corresponds to training only on natural questions ( nq ) and triviaqa ( tqa ) as dpr was trained only for qa tasks on those datasets and two extra ones',\n",
       "  'our tree embedding is then formed by encoding the position information of the depth_3 tree with a tree posi_ tional embedding ( shiv and quirk , 2019 ) for each bfs level',\n",
       "  'efciency of learning from seed tuples be_ cause not all domains will have large available commonsense kbs on which to train , we explore how varying the amount of training_data avail_ able for learning affects the quality and novelty of the knowledge that is produced',\n",
       "  'we further equip our pretraining with multi_task_learning and apply our method to 28 structure_prediction datasets across 10 tasks',\n",
       "  'in small datasets conll04 and 16res , adding structure generation pre_training ( from t5_ v1.1_base to uie_base w/o ltext ) , the performance signicantly increases from 72.12 to 75.70 and 72.03 to 74.28',\n",
       "  '2020 ) or require multi_ stage training and inference pipelines ( ren et al. , 2021 )',\n",
       "  'the beam_size and length penalty is decided by a grid_search on the validation set of the ace05 dataset , and the range for the beam_size is from 1 to 7 with a step size of 1 and the length penalty is from 0.7 to 1.2 with a step size of 0.1',\n",
       "  'as we can see , in nyt dataset , our multide_ coder model achieves the best f1_score , which is 0.587',\n",
       "  '5.2 l ow_resource settings multiple experiments suggest that tanl is data_efcient compared to other baselines',\n",
       "  'therefore , span_based methods usually will set a maximum span length ( xu et al. , 2017 ; luan et al. , 2019 ; wang and lu , 2018 )',\n",
       "  'the relationship between [ carmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_melis ] and [ sopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosoprano ] is output ( alternative 1 ) : voice_type input ( alternative 2 ) : born in bologna , orlandi was a student of the famous italian [ sopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosopranosoprano |tail ] and voice_teacher [ carmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen meliscarmen_meliscarmen_meliscarmen_meliscarmen_meliscarmen_melis |head ] in milan',\n",
       "  'this effect is apparent in table 6 , with a clear improve_ ment on automatic and human evaluations by the pretrained comet over the randomly initializedseed relation completion plausible piece partof machine bread isa food oldsmobile isa car happiness isa feel math isa subject mango isa fruit maine isa state planet atlocation space dust atlocation fridge puzzle atlocation your mind college atlocation town dental chair atlocation dentist nger atlocation your nger sing causes you feel good doctor capableof save life post ofce capableof receive letter dove symbolof purity sun hasproperty big_bird bone hasproperty fragile earth hasa many plant yard usedfor play game get pay hasprerequisite work print on printer hasprerequisite get printer play game hasprerequisite have game live haslastsubevent die swim hassubevent get wet sit down motivatedbygoal you be tire all paper receivesaction recycle chair madeof wood earth definedas planet table 7 : randomly selected and novel generations from the conceptnet development_set',\n",
       "  'the two closest methods above ( li et al. , 2021 ; hsu et al. , 2021 ) both utilize manually designed discrete templates , which caused the sub_optimal problem',\n",
       "  'after finetun_ ing this model on the task of kgqa over in_ complete kgs , our approach outperforms base_ lines on multiple large_scale datasets without extensive hyperparameter tuning.1 1 introduction a knowledge_graph ( kg ) is a multi_relational graph where the nodes are entities from the real world ( e.g',\n",
       "  'while the structure pre__diction requires structural_understanding , the lms are pretrained to understand an independent aspect',\n",
       "  'concretely , genre occupied 14 times less memory than blink and 34 times less memory than dpr',\n",
       "  'generative baseline models : copyre ( zeng et al',\n",
       "  'sel ( ( person : steve ( work for : apple ) ) ( start_position : ... + rm ( ( person : steve ( work for : apple ) ) ( facility : [ null ] ) ... table 1 : an example of rejection mechanism ( rm ) , here ( facility : [ null ] ) is the injected rejection noise during learning stage , and the [ null ] _valued span will be ignored during inference stage',\n",
       "  '( 2019 ) [ elmo ] ___ 84.7 ___ 82.9 ___ 76.2 strakov a et al',\n",
       "  'however , they fail to achieve quality comparable to conven_ tional kges',\n",
       "  'triviaqa : a large scale dis_ tantly supervised challenge dataset for reading_comprehension',\n",
       "  'here , the graph is traversed by breadth_first_search ( bfs ) with an ascending ordering of nodes and edge_types',\n",
       "  'the performances of random sampling and diverse beam_search are much worse than the standard beam_search algo_ rithm',\n",
       "  '12https : //github.com/amazon_research/ tanl4645dataset lang.train dev test # sent',\n",
       "  'since the emergence of large pretrained lms ( radford et al. , 2019a ; devlin et al. , 2019 ; yang et al. , 2019 ) , multi_task training has been shown effective to enhance lms transferability to downstream_tasks ( raffel et al. , 2019 )',\n",
       "  'additional params training cost dygie++ 2h ( # roles + 1 ) 20h nst h ( 2 # roles + 1 ) 1h grit 0 < 40min table 7 : additional parameters and training cost',\n",
       "  'this indicatesthat we can reduce energy consumption when train__ing large pretrained models via employing multi_ task training',\n",
       "  'tanl ( paolini et al. , 2021 ) proposes task_specific data_augmentation ( i.e. , augmented natural lan__guage ) that annotates task information and predic_ tions in the input and output respectively for each structure_prediction task',\n",
       "  '3.2 pre_training we pre_train uie using three sequence generation_tasks with above mentioned pre_training datasets',\n",
       "  'our ndings corroborate a recent trend where tasks typically treated with discriminative methods have been successfully solved using generative ap_ proaches ( brown et al. , 2020 ; izacard & grave , 2020 ; schick & schtze , 2020 )',\n",
       "  'by pretraining on kg link_prediction and finetuning on qa , kgt5 performs similar to or better than much more complex methods on multiple large_ scale kgqa benchmarks',\n",
       "  'additionally , the structure_pretraining takes less than 5 % gradient_steps of the number of pretraining steps of lms , and thus the estimated auxiliary cost for energy is comparatively smaller',\n",
       "  'how decoding strategies affect the veriability of gen_ erated text',\n",
       "  'seq2seq : copyre ( zeng et al',\n",
       "  'each experiment is conducted on nvidia a100 tensor core gpu 40gb',\n",
       "  'model test mrr train mrr params complex 0.308 0.721 614m kgt5 0.300 0.304 60m table 8 : train vs. test performance on link_prediction on wikidata5m',\n",
       "  'all observations above indicate that paie can better utilize plms for few_shot settings',\n",
       "  '708718 , online , november 2020',\n",
       "  'with majority voting on 150 samples , we observe that ourmax_margin models graphs are preferred 13 % more times compared to those of the t5 model ( 43 % vs 30 % and statistically signicant with p < 0.05 ) while in 22 % cases , the graphs are marked similar ( remaining have no majority )',\n",
       "  '5.2 evaluation metric we follow previous work ( lin et al. , 2020 ; ahmad et al. , 2021 ) and consider the argument classica_ tion f1_score to measure the performance of mod__els',\n",
       "  'the gains were the largest on complexwebquestions which is the hardest dataset in terms of complexity and kg size',\n",
       "  'experiments on five benchmarks show that kg_s2s outper_ forms many competitive baselines , setting new state_of_the_art performance',\n",
       "  'finally , we report the same nov_ elty metrics as for a tomic : n/tsroandn/to',\n",
       "  'discontinuous_ner datasets we follow dai et al',\n",
       "  'evaluation metric we adopt two evaluation met_ rics',\n",
       "  'we report micro inkb f1on test sets',\n",
       "  'we observe that squire achieves higher scores on both metrics9',\n",
       "  'as shown in figure 7 , tempgendrastically short_ ens the inference time by around 39 times com_ pared to scirex_p',\n",
       "  'the main difference is that deepstruct trains across multiple structure_prediction datasets in structure_pretraining with task_agnostic corpora , where we cast all datasets into triple formats',\n",
       "  'therefore , the ad_ vantage of degree over degree ( pipe ) becomes less obvious',\n",
       "  'in this paper , we revisit the end_to_end triple ex__traction task for sequence generation',\n",
       "  '2.2 tasks it is resource_intensive to create large_scale struc_ tural understanding datasets from scratch',\n",
       "  'inner blocks with the input text representation htextsliced from the hybrid representation hand the target sequence representation hy , we apply an n_layer transformer structure with mixed_attention ( he et al. , 2018 ) to allow our model to utilize fea_ tures from different attention layers when decoding the edges or the nodes of an alternating_sequence',\n",
       "  'fa8750_19_ 2_1004 , u.s. darpa aida program no',\n",
       "  '( 2019 ) to maintain a valid tree_structure',\n",
       "  'in 3rd international conference on learning represen_ tations , iclr 2015 , san_diego , ca , usa , may 7_9 , 2015 , conference track proceedings',\n",
       "  'table 6 shows the evaluation re__sults on tgt_test under the transfering learning setting and when solely training on tgt_train without transfering knowledge',\n",
       "  'to investigate the effect of different pre_training tasks , table 4 shows ablation experiment results of uie_base on four downstream_tasks',\n",
       "  'we also follow li et al',\n",
       "  'the standard evaluation for this benchmark uses few_shot n_wayk_shot settings , which we follow',\n",
       "  '( bleu _4:34 ) generation bybart : ibought anew phone',\n",
       "  'de_ coders work in a sequential order : the rst decoder generate the rst triplet and then the second de_ coder generate the second triplet',\n",
       "  '( white cell : unmasked ; grey cell : masked )',\n",
       "  'for example , our zero_shot 10b pa_ rameter lm significantly outperforms the zero_shot gpt_3 ( 175b ) on a structure_prediction benchmark dataset ( figure 1 )',\n",
       "  'in comparison , we prompt argument interactions to guide plms and optimize the multiple argument detection by designing a bipartite_matching_loss',\n",
       "  '1 ) pseu_ dopipeline genre , including miwa and bansal ( 2016 ) ; sun et al',\n",
       "  'we did not perform any complex hyperparameter search',\n",
       "  '5 conclusion this paper introduces squire , a sequence_to_ sequence framework for efficient and effective multi_hop knowledge_graph reasoning',\n",
       "  'her research interests include natural_language processing , knowledge_graph , and knowledge_base population under low_resource conditions',\n",
       "  'for a test question ( qtext , h ) , we first get qbase from qtext',\n",
       "  'although in theory wider beam sizes should give improved performance , it has been observed that for beam sizes larger than 5 , performance of generative models suffers drasti_ cally ( yang et al. , 2018 ) and sampling generally produces better results',\n",
       "  'from table 3 , we notice that the performance decayed without contrastive ob_ ject , which illustrates that triplet contrastive_learning can en_ hance the faithfulness of generated triples , thus boosting the performance',\n",
       "  'table 5 shows the results of the different pre_trained models on the development_set of conll 03 under the 10_shot setting',\n",
       "  '( 2018 ) and use the output of the previous layers transformer block as the query input for the multi_headed at_ tention of the next block',\n",
       "  'be_ sides , for each dataset , we build a schema align_ ment between the pretraining dataset and down__stream dataset ( details are described in sec',\n",
       "  'it is only evaluated on the sro tuple generation_task , however',\n",
       "  'although tempgenscores the highest f1 across all three tasks , scirex_p does achieve the highest recall on both re tasks',\n",
       "  'julien leblay and melisachew wudage chekol',\n",
       "  'e transformer with mixed_attention we rst slice off the hidden representation of the input text from the hybrid representation h , and denote it as htext , then the input text representa_ tionhtextand the output from the hybrid span encodinghygets fed into a stack of nmixed_ attention/feedforward blocks that have the follow_ ing structure ( as shown in figure 7 ) : since generating the node and edge_types may need features from different layers , we use mixed attention ( he et al. , 2018 ) , which allows our model to utilize the features from different attention layers when encoding the text segment , htext , and the4078target features , hy , mixedatt ( q , k , v ) =softmax ( qkt dm+m1 ) v rlmdm , m1 ( i , j ) = { 0 , j < nji+n , otherwise wheren=|x|is the length of the input text , lm=|x|+|y|is the total length of the source and the target features',\n",
       "  'as an alternative , we reformulate the structure_prediction as a combination of triple generation_tasks',\n",
       "  'xinya du , alexander m. rush , and claire cardie',\n",
       "  'available : https : //doi.org/10.18653/v1/d16_1137 [ 74 ] y. zhang et al',\n",
       "  'we observe that query trigger plays the most important role among the three and when less training_data is given , the superiority of leveraging any of these weakly_ supervised signal becomes more obvious',\n",
       "  'combined with a simple cross_attention reranker , our complete el framework achieves state_of_the_art results on three wikidata_based datasets and strong performance on tackbp_2010',\n",
       "  'attention connections shown only for the second timestep to reduce clutter',\n",
       "  'her2 ; erbb_2 ; neu @ gene @ breast_cancer @ disease @ @ gda @ n_ary , inter_ sentencethe deletion mutation on exon_19 of egfr gene was present in 16 patients , while the l858e point_mutation on exon_21 was noted in 10',\n",
       "  'its simplicity makes it highly exible to adapt to new domains or longer docu_ ments',\n",
       "  'pedro henrique_martins , zita marinho , and andr e f. t. martins',\n",
       "  'the sequence generationapproach allows us to use the entire model andadapt it to new tasks , where the initial embeddingscontain high quality semantics and help the modeltransfer knowledge efciently.4.5.3 k_shot episode constructiontraditionally , the support setsis often constructedink_shot formats where we use onlykinstancesof each label type',\n",
       "  'formally , the algorithm is as follows : the overall optimization object is as follows : loss=loss generative +loss contrastive ( 8 ) whereis the hyperparameter to balance different objects',\n",
       "  'we adopt two different strategies in decoding process : employing only one united decoder or applying multiple sepa_ rated decoders',\n",
       "  'we conduct the above ablation_studies us_ ing a base version of deepstruct with 220m parameters',\n",
       "  'hence , we choose nodes ( concepts ) that are not part of the belief or the argument ( also termed as common__sense nodes ) and replace them with phrases that are synonymous to the original phrases',\n",
       "  '5.4 ) lever_ ages both positive and negative_graphs and im_ proves stca to 60 % with comparable seca to the max_margin model',\n",
       "  '( 2019 ) [ flair ] ___ 93.18 _____ strakov a et al',\n",
       "  'from table 2 , we no_ tice that genkgc achieves better performance than kg_bert [ 14 ] across all datasets and maintains high speed during inference',\n",
       "  'available : https : //arxiv.org/abs/2011.01675 [ 44 ] r. h. zhang et al',\n",
       "  'traditional kge models fulfill quality and simplicity',\n",
       "  'similar to explagraphs , we create structurally neg_ ative graphs with disconnected and cyclic graphs and semantic negative_graphs by perturbating the temporal relations',\n",
       "  'other works such as jia et al',\n",
       "  'd path predictor on metaqa being an artificially generated template_based dataset , metaqa has far more questions than any other dataset that we compare with ( tab',\n",
       "  'after obtaining the set uof all query_path pairs , we optimize the parameters to maximize ( q , ) up ( |q )',\n",
       "  'thus , the copy_mechanism becomes benecial for mt5',\n",
       "  'mis not a learned parameter , but solved by taking the qr_decomposition of a modied reference vector matrix',\n",
       "  'we randomly pack instances for different tasks in one batch , and details are shown in algorithm 1 in the appendix',\n",
       "  \"kg_bert ( yao et al. , 2019 ) utilizes pretrained bert for link_prediction and holds po_ 2we use the term kgc for the task of kg link_prediction.2814predict tail : john o'connor | position held predict head : blondeliini | parent tax on predict answer : what do jamaican people speakarchbishop euhalida ya jamaican englishfigure 1 : overview of our method kgt5\",\n",
       "  'in the webnlg dataset , multidecoder model achieves the highest f1_score ( 0.371 )',\n",
       "  'by translating the graph extraction_task to a sequence generation_task , we can easily use beam_search decoding to reduce possible exposure_bias ( wiseman and rush , 2016 ) of the sequential decision process and thus nd globally better graph representation',\n",
       "  'as we can see from ta__ble 3 , gtee_d_ynpref achieves the highest f1_scores for trg_c and arg_c on ace05_e , 3the random initialization is implemented in the torch.nn.embeddinglayer class in pytorch v1.7.1.compared with all the generation_based baselines',\n",
       "  '63976407 , online , novem_ ber 2020',\n",
       "  'many tuples , however , are com_ pletely novel , such as bird bone hasproperty fragile and driftwood atlocation beach , which have no related tuples in the training set',\n",
       "  '6 conclusion we revisit the classic nlp problem of template lling and propose an end_to_end learning frame_ work called gtt',\n",
       "  'in the work by miwa and bansal ( 2016 ) , the authors used bilstm ( graves et al. , 2013 ) for ner and conse_quently a tree_lstm ( tai et al. , 2015 ) based on dependency graph for re',\n",
       "  'this simple2819model cwq wqsp gt query 25.2 56.9 pullnet 26.8 ( +1.6 ) 47.4 ( _9.5 ) embedkgqa _ 42.5 ( _14.4 ) lego 29.4 ( +4.2 ) 48.5 ( _8.4 ) gt query 24.5 56.9 kgt5 34.5 ( +10.0 ) 50.5 ( _6.4 ) table 4 : hits @ 1 ( gain vs gt query ) on complexwe_ bquestions ( cwq ) and webquestionssp ( wqsp ) datasets in the 50 % kg setting',\n",
       "  '0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 # parameters 1e8bert_base dygie++ scirex_p grit bart_base tanl tempgenmodel figure 8 : number of parameters of different systems',\n",
       "  'furthermore , we observe that degree is slightly better than degree ( pipe ) under the low_ resource setting',\n",
       "  'in com_ panion of the the web conference 2018 on the web conference 2018 , www 2018 , lyon , france , april 23_27 , 2018 , pages 17711776',\n",
       "  'the intuition is that finetuning is the de facto way to leverage pretrained lms to perform downstream_tasks',\n",
       "  'unlike those downstream_tasks , structure_prediction requires the structural un_ derstanding of the text for further integrating multi_ ple relevant aspects into a structure',\n",
       "  'while tra_ ditional commonsense reasoning tasks are discrim_ inative in nature ( zellers et al. , 2018 ; talmor et al. , 2019 ; sap et al. , 2019 ; bisk et al. , 2020 ; sakaguchi et al. , 2020 ; talmor et al. , 2021 ) , recent focus on generative evaluation have led to the develop_ ment of tasks and benchmarks that explore unstruc_ tured commonsense sentence generation ( lin et al. , 2020 ) , event inuence graph_generation ( madaan et al. , 2020 ) , commonsense explanation_graph gen__eration ( saha et al. , 2021b ) , etc',\n",
       "  '( b ) inside the transformer block , the outputs of all the previous layer blocks from earlier time steps are input to the multi_headed attention with the preceding block for the current time step as the query',\n",
       "  'lastly , we show that human errors are the best negatives for contrastive_learning and also that automatically generating more such human_like negative_graphs can lead to further improvements',\n",
       "  'other researchers introduced multiple strategies , such as multi_task_learning [ 13 ] and one_word generation [ 14 ] to improve copyre',\n",
       "  'to generate faith_ ful results , we propose a novel triplet contrastive training ob_ ject',\n",
       "  'we consider 28 datasets spanning 10 structure_prediction_tasks as shown in figure 5',\n",
       "  'knowledge_graph embedding : a survey of approaches and applications',\n",
       "  '2017a ) 61.5 41.4 49.5 _____ hrl ( takanobu et al',\n",
       "  'in contrast , grit ( du et al. , 2021 ) formulates the problem as sequence generation , and employs a sin_ gle transformer layer whose parameters are shared between encoder and decoder to enrich semantics in the shared parameters',\n",
       "  'curran associates , inc. jing jiang and chengxiang zhai',\n",
       "  'for a can_ didatec , letrd ( c ) indicate its rank in cd ( m ) ( if ccd ( m ) thenrd ( c ) = )',\n",
       "  'recently , some researchers ( katiyar and cardie 2016 ; miwa and bansal 2016 ; bekoulis et al',\n",
       "  'such a methodological discrepancy leads to a great main_ tenance cost and being inadaptable to emerging knowledge queries , ingestion , and presents',\n",
       "  '( 2021 ) employ trained expert annotators for entailment tree construction',\n",
       "  'during evaluation , we perform beam_search to obtain a list of multi_hop paths , along with log_likelihood as their scores',\n",
       "  'acknowledgments we sincerely thank the reviewers for their insight_ ful comments and valuable suggestions',\n",
       "  'however , the flat text could introduce kg struc_ ture loss',\n",
       "  'figure 4 shows the percent_ age of novel development_set tuples that have an edit_distance from the closest training set tuple of at least the value on the x_axis',\n",
       "  '2018 ; nguyen andverspoor 2019 ) tried to bring these two tasks closer to_gether by sharing their parameters and optimizing them to_gether',\n",
       "  'in our model , we didnt using attention_mechanism because we found that the attention_mechanism makes no difference to the results.3.2 reinforcement_learning process we regard the triplets generation process as rl process',\n",
       "  'we attempt to increase the original encoder dropout forkg_s2s training , however , it has little impact on the final performance',\n",
       "  'each class con_ tains sentences that has 1,2,3,4 or > = 5 triplets',\n",
       "  'however , multidecoder performs bet__ter than onedecoder model when generating en__tities',\n",
       "  'see that : ( 1 ) the pre_training of sel ( lrecord ) and sequence_to_structure mapping ( lpair ) is cru_ cial for uie , and such a structure generation pre_training is especially useful for small_scale datasets',\n",
       "  'finally , d egree learns triggers and arguments jointly in an end_to_end manner , which encourages the model to better utilize the shared knowledge and dependencies among them',\n",
       "  'noted that each linearized form has a virtual root root',\n",
       "  'our contributions are as follows : we improve structural_understanding abilities ofpretrained lms',\n",
       "  'the predicted na_triplet will be excluded',\n",
       "  'manual evaluation of each tu_ ple indicates whether the tuple is considered plausible by a human annotator',\n",
       "  'acknowledgement we thank the anonymous reviewers for the help_ ful suggestions',\n",
       "  'we compare four different prompts : three joint prompts introduced in section 3.2 and one single template containing only one role slot , i.e',\n",
       "  'we also evaluate the onedecoder andmultidecoder trick for the seq2seq models ( denoted as _one and _mul )',\n",
       "  'one reason is that for small_scale mod__els , learning across 28 structure_prediction datasets during the structure_pretraining may exceed the model capacity',\n",
       "  ', drop redundant , shrink irrelevant : selective knowl__edge injection for language pretraining , in proc',\n",
       "  'also , with a flexible prompt design , paie can ex_ tract multiple arguments with the same role instead of conventional heuristic threshold tun_ ing',\n",
       "  '334343 , beijing , china , july 2015',\n",
       "  'in addition to the quality improvements , table 1 shows that comet produces more novel tuple objects than the baselines , as well',\n",
       "  'date of publication september 14 , 2021 ; date of current version october 14 , 2021',\n",
       "  'acknowledgements we sincerely thank the reviewers for their insight_ ful comments and valuable suggestions',\n",
       "  'there_ fore , we collect existing datasets in the field of structure_prediction for evaluation',\n",
       "  'this approach presents difcultyin knowledge sharing among tasks',\n",
       "  'repeat this pro_ cess , the decoder could generate multiple triplets',\n",
       "  'we hope it will foster future research along the language structural_understanding direc_ tion',\n",
       "  'row 3_5 show our three types of joint prompts respec_ tively',\n",
       "  'in this work , we cast common__sense acquisition as knowledge_base construction and investigate whether large_scale language mod__els can effectively learn to generate the knowledge personx puts their arms around personyloving towards persony to comfort personycaringpersonx goes to the storebring a walletfeels lovedcommonsense knowledge_bases ( seen events ) automatic kb completionxattrxattr xintentoreactxneed unseen eventspersonx buys lunchto get foodxintentxneed naphaving a restdozing off hassubeventhassubeventgoing to a moviehaving funusedforenergycausesatomic conceptnetthrowing a partycausesfigure 1 : comet learns from an existing knowledge_base ( solid lines ) to be able to generate novel nodes and edges ( dashed lines )',\n",
       "  'please see 3.3 for more details',\n",
       "  '2016 ; lin , liu , and sun 2017 ; li et al',\n",
       "  '( 2019 ) uses token_level pro_totypical network ( snell et al.,2017 ) , which clas_sies by comparing a wordxito each class cen_troid rather than individual sample embeddings.l_tapnet + cdthou et al',\n",
       "  'the arguments follow a long tail distribution and since crf models learn each argument tag separately , it can not leverage the similarity between argument roles to improve the performance on rarely seen roles',\n",
       "  'learning a similarity metric discriminatively , with application to face verification',\n",
       "  'before joining alibaba , he was a re_ search manager and research staff member of ibm_research',\n",
       "  'even training on tgtfrom scratch , the proposed method also outper_ forms strong baselines',\n",
       "  '3.1 i nference with constrained beam_search naturally , at test time , we could compute a score for every element in eand then sort them',\n",
       "  '( 2018 ) ( gpt ) , which uses multiple transformer blocks of multi_headed scaled dot_product attention and fully connected layers to encode input text ( vaswani et al. , 2017 )',\n",
       "  'simple argument enumerations , for example , do not satisfy the con_ dition mentioned above , while contexts with differ__ent syntactic structures are more likely to satisfy it',\n",
       "  'in the second group , recent studies tend to follow the success of pre_trained_language_models ( plms ) and solve eae by question an_ swering ( qa ) ( liu et al. , 2021a ; wei et al. , 2021 ; du and cardie , 2020 ; liu et al. , 2020 ; li et al. , 2020 ) and text_generation ( lu et al. , 2021 ; li et al. , 2021 )',\n",
       "  'such an expansion is non_trivial since new chal_ lenges arise in the unified formulation',\n",
       "  '( 2020 ) propose to improve unsupervised decoding for counterfactual and abductive_reasoning ; huang et al',\n",
       "  'per_slot f1_score is reported in table 1',\n",
       "  'in fact , copyre tries to maximize both thehead and the tail',\n",
       "  'few_shot learningapproaches are evaluated over manyepisodesofdata , which represent a variety of novel tasks',\n",
       "  'xiang lisa li and percy liang',\n",
       "  'inference at test time , we use constrained beam_search with 10 beams , and maximum decoding steps of 15',\n",
       "  'we found that the pre_trained uie model provides a solid foundation for capturing , sharing , and trans_ ferring knowledge between different ie tasks , and new ie tasks can be effectively solved because uie learns general ie ability',\n",
       "  'the symbol _ denotes results not reported in previous papers',\n",
       "  'the result shows that pretrained language_models can handle higher_level understanding ( e.g. , structural_understanding ) , which may benefit more nlp tasks',\n",
       "  'seyed mehran kazemi and david poole',\n",
       "  'although these works show effectiveness in corresponding applications , they are limited to specific scenarios , and can not generalize well to a broad scope of reasoning',\n",
       "  'however , even though our model could be sensitive to the template design , it still outperforms oneie and bart_gen , which are the best classication_ based model and the best generation_based base_ line , respectively',\n",
       "  'yau studied at , uc berkeley.irvine figure 1 : architecture of genkgc',\n",
       "  'keshav kolluru , samarth aggarwal , vipul rathore , soumen chakrabarti , et al',\n",
       "  'this then raises the question whether pre_trained language mod__els , trained on free_form natural_language data , can also adapt themselves to generate structured out_ puts like graphs',\n",
       "  'in addition , we add the following decoding con_ straints , tune probability of generating [ sep ]',\n",
       "  'this vari_ ant of kg_s2s obtain slightly lower result ( 0.351 on mrr ) , but still outperforms star with sub_ stantial margin',\n",
       "  'baselines and evaluation_metrics we compare copymtl with copyre ( zeng et al',\n",
       "  '250259 , berlin , ger_ many , august 2016',\n",
       "  'all rights reserved.input theunited states president trump was raised in the borough of queens innew york_city , and lived there until age 13',\n",
       "  'we find that applying our prefix constraints to the beam_search algorithm further improves the kg_s2s perfor__mance ( i.e. , 0.02 hit @ 10 improvement )',\n",
       "  'this could be explained by the low quality of entities in icews14 , whichmrr h @ 1 h @ 3 h @ 10 graph_based methods ttranse ( leblay and chekol , 2018 ) .255 .074 _ .601 hyte ( dasgupta et al. , 2018 ) .297 .108 .416 .655 atise ( xu et al. , 2019 ) .550 .436 .629 .750 de_simple ( goel et al. , 2020 ) .526 .418 .592 .725 tero ( xu et al. , 2020 ) .562 .468 .621 .732 tcomplex ( lacroix et al. , 2020 ) .560 .470 .610 .730 tntcomplex ( lacroix et al. , 2020 ) .560 .460 .610 .740 t+transe ( han et al. , 2021 ) .553 .437 .627 .765 t+simple ( han et al. , 2021 ) .539 .439 .594 .730 plm_based methods kg_s2s ( ours ) .595 .516 .642 .737 table 3 : results of temporal kgc on icews14',\n",
       "  'in this work , we dene the com monsensetransformer ( comet ) , which constructs commonsense kbs by using existing tuples as a seed set of knowledge on which to train',\n",
       "  'for rebel , we evaluate using free generation in the rc setup',\n",
       "  '5 ) generative genre , including zeng et al',\n",
       "  'we show that an off_the_shelf encoder_decoder transformer model can serve as a scalable and versatile kge model obtaining state_of_the_art results for kg link_prediction and incomplete kg question_answering',\n",
       "  'using only 1 % of the training_data clearly diminishes the quality of the produced gen_ erations , with signicantly lower observed results across both quality and novelty metrics',\n",
       "  'nell_one is a few_shot kgc dataset derived from nell ( carl_ son et al. , 2010 )',\n",
       "  'we experiment on six kg bench_ marks that capture common knowledge',\n",
       "  'the output triples are then decoded as corresponding structure predictions based on the pre_built schema alignment',\n",
       "  '2https : //en.wikipedia.org/wiki/amber_ rileymodel size mrr h @ 10 star 354m .274 .455 kg_s2s ( small ) 60m .351 .485 kg_s2s ( base ) 220m .353 .495 table 7 : comparison of model performance and parame_ ter size between kg_s2s and star on fb15k_237n',\n",
       "  'this is due to the gap between lm pre_ training and downstream structural_understanding',\n",
       "  'fa8750_19_2_1004 , u.s. darpa aida pro_ gram no',\n",
       "  'as shown in table 2 , overall , deepstruct s zero_shot performance isstill far from that of task_specific supervised mod__els on most tasks',\n",
       "  '2.docmrc ( liu et al. , 2021a ) : we report the results from original paper',\n",
       "  'model ( s ) metaqawqsp cwq1_hop 2_hop 3_hop baselines ( lego , embedkgqa , emql , pullnet ) 63.3 45.8 45.3 56.9 25.2 ours ( kgt5 , kgt5 ensemble ) 67.7 48.7 44.4 56.9 24.5 table 14 : percentage of questions answerable using ground_truth query',\n",
       "  'denotes source and target description length',\n",
       "  'however , these methods are designed for the specific reasoning scenarios based on task_specific models so hardly generalize to other scenarios',\n",
       "  'sean welleck , ilia kulikov , stephen roller , emily di_ nan , kyunghyun cho , and jason_weston',\n",
       "  'angela fan , yacine jernite , ethan perez , david grangier , jason_weston , and michael auli',\n",
       "  'similarly , our ptrnetdecoding ( pndec ) model achieves f1_scores that are3.0 % and1.3 % higher than hrl on the nyt29 and nyt24 datasets respectively',\n",
       "  '< sep > talking_heads genre new wave',\n",
       "  'results on selected baselines are presented in ta__ble 2 , as well as additional metrics in tables 3 and 4',\n",
       "  'please see 4.4 for more details',\n",
       "  '2018a ) 61.0 56.6 58.7 37.7 36.4 37.1 pndec ( nayak and ng 2019 ) 80.6 77.3 78.9 38.1 36.9 37.5 copymtl ( zeng , zhang , and liu 2020 ) 75.7 68.7 72.0 58.0 54.9 56.4 ourscgt ( random ) 90.8 77.7 83.7 87.6 70.5 78.1 cgt ( unilm ) 94.7 * 84.2 89.1 92.9 * 75.6 83.4 w/o contrastive 87.3 81.5 84.3 94.6 70.5 80.8 table 3 : main results of nyt and webnlg',\n",
       "  'our bfs traversal embedding is a pointwise sum of the level embedding , l , the parent_child embed__ding , p , and the tree embedding , tof a given alternating_sequence , y , travembed ( y ) =l ( y ) +p ( y ) +t ( y ) r|y|dm where the level embedding assigns the same embed__ding vectorlifor each position at the bfs traversal leveli , and the value of the embedding vector is lled according to the non_parametric sinusoidal position embedding since we want our embedding to extrapolate to the sequence that is longer than any sequences in the training set',\n",
       "  'this issue is caused by the fact that previous kgc benchmarks i ) are not fully ver_ ified by experts ; ii ) are based on the closed_world assumption ( cwa ) ( keet , 2013 )',\n",
       "  'training was done on 32 gpus ( with 32gb of memory ) and it completed in 24h for a total of32 gpu/day',\n",
       "  '1 contain incoherent or non_ commonsensical edges ( marked by dashed arrows ) like fast_food ; has context ; salads',\n",
       "  'in the following , dos san_ tos et al',\n",
       "  '3.3 and sec 3.4 answer the above two questions , respectively',\n",
       "  '( d ) our proposed generative solution to solve all ner_subtasks in a unied way',\n",
       "  'similar to the zero_shot setup , we only train a single model to conduct all the downstream_tasks under the multi_task setting',\n",
       "  'also , with a flexible prompt design , paie can extract multiple arguments with the same role instead of conventional heuristic threshold tuning',\n",
       "  'cls and gen represent classication_ based models and generation_based models , respectively',\n",
       "  ...])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_id = 4\n",
    "taxo.root.children[0].children[node_id], taxo.root.children[0].children[node_id].internal['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in collection:\n",
    "    for child in curr_node.children:\n",
    "        child.emb['phrase'] = np.stack([taxo.vocab['phrases'][w] \n",
    "                                        for w in child.getAllTerms(granularity='phrases', children=False)], axis=0)\n",
    "        child.emb['sentence'] = np.stack([taxo.vocab['sentences'][w] \n",
    "                                        for w in child.getAllTerms(granularity='sentences', children=False)], axis=0)\n",
    "    # rank sentences based on semantic sim\n",
    "    paper.rank\n",
    "\n",
    "    # compute paper embedding based discriminative sentence weights\n",
    "\n",
    "    # for each node, assign papers based on similarity gap → take top 50% of assigned papers as pseudo-labels\n",
    "\n",
    "    # fine-tune classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankSentences(taxo, sents, sent_phrases, phrase_embs, sent_embs, term_to_idx, bm_score, thresh=3, min_freq=3, percentile=0.999, classify=True):\n",
    "    \n",
    "    node_text_ranks = []\n",
    "\n",
    "    for node_id in tqdm(np.arange(0, len(taxo.label2id))):\n",
    "        # gather node and its siblings\n",
    "        focus_node = taxo.root.findChild(str(node_id))\n",
    "        sibling_nodes = taxo.get_sib(focus_node.node_id, granularity='emb')\n",
    "\n",
    "        # get phrases of node and its siblings\n",
    "        focus_phrase = focus_node.getAllTerms(granularity='phrases', children=False)\n",
    "        focus_phrase_embs = np.stack([taxo.vocab['phrases'][w] for w in focus_phrase], axis=0) # P x dim\n",
    "        sibling_phrase = [sib.getAllTerms(granularity='phrases', children=False) for sib in sibling_nodes] # list of list of sibling node terms\n",
    "        sib_phrase_embs = [np.stack([taxo.vocab['phrases'][p] for p in t], axis=0) for t in sibling_phrase] # [(T x dim), ... ()] \n",
    "        # get sentences of node and its siblings\n",
    "        focus_sent = focus_node.getAllTerms(granularity='sentences', children=False)\n",
    "        focus_sent_embs = np.stack([taxo.vocab['sentences'][s] for s in focus_sent], axis=0) # sentences x dim\n",
    "        sibling_sent = [sib.getAllTerms(granularity='sentences', children=False) for sib in sibling_nodes] # list [(list of sibling node i terms), ...]\n",
    "        sib_sent_embs = [np.stack([taxo.vocab['sentences'][s] for s in t], axis=0) for t in sibling_sent] # list [(# of sib sents x dim)]\n",
    "\n",
    "        # compute target phrase/sentence semantic similarity\n",
    "        focus_phrase_sim = np.stack([cosine_similarity_embeddings(s, focus_phrase_embs).max(axis=0) for s in phrase_embs], axis=0) # S x [P x N] -> S x N\n",
    "        avg_focus_phrase_sim = average_with_harmonic_series(focus_phrase_sim, axis=1)  # S x 1\n",
    "\n",
    "        focus_sent_sim = cosine_similarity_embeddings(sent_embs, focus_sent_embs) # S x N\n",
    "        avg_focus_sent_sim = average_with_harmonic_series(focus_sent_sim, axis=1)  # S x 1\n",
    "\n",
    "        # compute sibling sentence semantic dissimilarity\n",
    "        sib_phrase_sims = [np.stack([cosine_similarity_embeddings(sent_phrase_emb, s_emb).max(axis=0) for sent_phrase_emb in phrase_embs], axis=0) for s_emb in sib_phrase_embs] # siblings x sentences x P x N -> sib x sentences x N\n",
    "        sib_sent_sims = [cosine_similarity_embeddings(sent_embs, s_emb) for s_emb in sib_sent_embs] # siblings x sentences x sib_sents\n",
    "        if len(sibling_nodes):\n",
    "            avg_sib_phrase_sim = np.stack([average_with_harmonic_series(sib_sim, axis=1) for sib_sim in sib_phrase_sims], axis=-1).max(axis=1) # sentences x 1\n",
    "            avg_sib_sent_sim = np.stack([average_with_harmonic_series(sib_sim, axis=1) for sib_sim in sib_sent_sims], axis=-1).max(axis=1) # sentences x 1\n",
    "        else:\n",
    "            avg_sib_phrase_sim = np.zeros_like(avg_focus_phrase_sim)\n",
    "            avg_sib_sent_sim = np.zeros_like(avg_focus_sent_sim)\n",
    "\n",
    "        # compute semantic rank\n",
    "        target_sim_phrase_rank = {idx:rank for rank, idx in enumerate((avg_focus_phrase_sim-avg_sib_phrase_sim).argsort()[::-1])}\n",
    "        target_sim_sent_rank = {idx:rank for rank, idx in enumerate((avg_focus_sent_sim-avg_sib_sent_sim).argsort()[::-1])}\n",
    "\n",
    "        # compute target co-occurrence\n",
    "        target_co_occurrence = np.array([average_with_harmonic_series(getBM25(sent, focus_phrase, term_to_idx, bm_score).mean(axis=0)) for sent in sent_phrases]) # S x 1\n",
    "        \n",
    "        # compute sibling co-occurrence\n",
    "        if len(sibling_nodes):\n",
    "            sib_co_occurrence = np.array([max([average_with_harmonic_series(getBM25(sent, sib_terms, term_to_idx, bm_score).mean(axis=0)) for sib_terms in sibling_phrase]) for sent in sent_phrases]) # S x 1\n",
    "        else:\n",
    "            sib_co_occurrence = np.zeros_like(target_co_occurrence)\n",
    "        \n",
    "        # compute co-occurrence rank\n",
    "        target_co_rank = {idx:rank for rank, idx in enumerate((target_co_occurrence-sib_co_occurrence).argsort()[::-1])}\n",
    "\n",
    "        joint_rank = compute_joint_ranking([target_sim_phrase_rank, target_sim_sent_rank, target_co_rank]) # idx: rank\n",
    "        sorted_ranks = sorted(joint_rank.items(), key=lambda x: x[1])\n",
    "\n",
    "        final_ranks = {}\n",
    "        for idx, rank in sorted_ranks:\n",
    "            # if rank > (1-percentile)*len(sents):\n",
    "            #     break\n",
    "            if (len(sent_phrases[idx]) > 5): # and (avg_focus_phrase_sim[idx] > avg_sib_phrase_sim[idx]) and (avg_focus_sent_sim[idx] > avg_sib_sent_sim[idx]) and (target_co_occurrence[idx] > sib_co_occurrence[idx]):\n",
    "                final_ranks[rank] = (sents[idx], avg_focus_phrase_sim[idx], avg_focus_sent_sim[idx], target_co_occurrence[idx])\n",
    "                # focus_node.internal['sentences'].append(text[idx])\n",
    "        \n",
    "        node_text_ranks.append(final_ranks)\n",
    "\n",
    "    return node_text_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(event_extraction,\n",
       " ['event_level',\n",
       "  'event_',\n",
       "  'event_specific',\n",
       "  'event_type',\n",
       "  'event_role',\n",
       "  'event_generation',\n",
       "  'event_types',\n",
       "  'event_extraction',\n",
       "  'event_arguments',\n",
       "  'multi_event',\n",
       "  'event_detection',\n",
       "  'event_argument_extraction',\n",
       "  'event_record',\n",
       "  'event_related',\n",
       "  'event_masked',\n",
       "  'event_records',\n",
       "  'event_relevant',\n",
       "  'event_correlation'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_id = 4\n",
    "taxo.root.findChild(str(node_id)), taxo.root.findChild(str(node_id)).external['phrases']\n",
    "# taxo.root.findChild(str(node_id)).getAllTerms(granularity='phrases', children=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])\n",
    "taxo.root.papers = {paper.id:paper for paper in collection}\n",
    "\n",
    "while queue:\n",
    "    focus_node = queue.popleft()\n",
    "    sibling_nodes = taxo.get_sib(focus_node.node_id, granularity='nodes')\n",
    "\n",
    "    # get phrases of node and its siblings\n",
    "    focus_phrase = focus_node.getAllTerms(granularity='phrases', children=False)\n",
    "    focus_phrase_embs = np.stack([taxo.vocab['phrases'][w] for w in focus_phrase], axis=0) # P x dim\n",
    "    sibling_phrase = [sib.getAllTerms(granularity='phrases', children=False) for sib in sibling_nodes] # list of list of sibling node terms\n",
    "    sib_phrase_embs = [np.stack([taxo.vocab['phrases'][p] for p in t], axis=0) for t in sibling_phrase] # [(T x dim), ... ()]\n",
    "    \n",
    "    # get sentences of node and its siblings\n",
    "    focus_sent = focus_node.getAllTerms(granularity='sentences', children=False)\n",
    "    focus_sent_embs = np.stack([taxo.vocab['sentences'][s] for s in focus_sent], axis=0) # sentences x dim\n",
    "    sibling_sent = [sib.getAllTerms(granularity='sentences', children=False) for sib in sibling_nodes] # list [(list of sibling node i terms), ...]\n",
    "    sib_sent_embs = [np.stack([taxo.vocab['sentences'][s] for s in t], axis=0) for t in sibling_sent] # list [(# of sib sents x dim)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in tqdm(collection):\n",
    "    # for each sentence in paper\n",
    "    ## weigh each phrase in sentence based on how discriminative it is\n",
    "    for sent in paper.phrase_tokenize:\n",
    "        \n",
    "        phrase_sim = cosine_similarity_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: {'0', '1'}; gt: ['0', '1', '2']\n",
      "preds: {'2', '0', '1'}; gt: ['0', '1', '2']\n",
      "preds: {'5', '2', '1', '0', '4', '3'}; gt: ['0', '1', '2', '3', '4']\n",
      "preds: {'3', '0', '6', '1'}; gt: ['0', '1', '2', '3']\n",
      "preds: {'2', '0', '1'}; gt: ['0', '1', '2']\n",
      "preds: {'3', '0', '1'}; gt: ['0', '1', '2', '3', '4']\n",
      "preds: {'2', '0', '1'}; gt: ['0', '1', '2']\n",
      "preds: {'2', '0', '4', '1'}; gt: ['0', '1', '2', '3', '4']\n",
      "preds: {'0', '1'}; gt: ['0', '1', '3']\n",
      "preds: {'0', '1'}; gt: ['0', '1', '3']\n",
      "preds: {'3', '2', '0', '1'}; gt: ['0', '1', '3']\n",
      "preds: {'0', '1'}; gt: ['0', '1', '3']\n",
      "preds: {'1', '0', '4', '3', '6'}; gt: ['0', '1', '3', '4']\n",
      "preds: {'0', '1'}; gt: ['0', '1', '3', '4']\n",
      "preds: {'2', '0', '1'}; gt: ['0', '1', '3']\n",
      "preds: {'3', '5', '0', '1'}; gt: ['0', '1', '3']\n",
      "preds: {'5', '0', '1'}; gt: ['0', '1', '3']\n",
      "preds: {'0', '4', '1'}; gt: ['0', '1', '4']\n",
      "preds: {'0', '4', '1'}; gt: ['0', '1', '4']\n",
      "preds: {'5', '0', '4', '1'}; gt: ['0', '1', '4']\n",
      "preds: {'0', '4', '1'}; gt: ['0', '1', '4']\n",
      "preds: {'0', '4', '1'}; gt: ['0', '1', '4']\n",
      "preds: {'0', '4', '1'}; gt: ['0', '1', '4', '6']\n",
      "preds: {'2', '0', '4', '1'}; gt: ['0', '1', '4']\n",
      "preds: {'3', '0', '4', '1'}; gt: ['0', '1', '4']\n",
      "preds: {'2', '0', '4', '1'}; gt: ['0', '1', '4']\n",
      "preds: {'5', '2', '1', '0', '3', '6'}; gt: ['0', '1', '5']\n",
      "preds: {'5', '2', '0', '1'}; gt: ['0', '1', '5']\n",
      "preds: {'0', '6', '1'}; gt: ['0', '1', '6']\n",
      "preds: {'0', '6', '1'}; gt: ['0', '1', '6']\n",
      "preds: {'0', '6', '1'}; gt: ['0', '1', '6']\n",
      "preds: {'3', '0', '6', '1'}; gt: ['0', '1', '6']\n",
      "preds: {'5', '2', '1', '0', '6'}; gt: ['0', '1', '6']\n",
      "preds: {'0', '6', '1'}; gt: ['0', '1', '6']\n"
     ]
    }
   ],
   "source": [
    "for p in zip(gt, preds):\n",
    "    print(f'preds: {p[1]}; gt: {p[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_pool = internal_sentences\n",
    "# sentence_pool_emb = np.array([taxo.vocab['sentences'][s] for s in sentence_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:28<00:00, 21.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# node_external_sent_ranks = expandDiscriminative(taxo, sentence_pool, sentence_pool_emb, granularity='sentences', classify=False, internal=-1)\n",
    "node_internal_sent_ranks = rankSentences(taxo, sentence_pool, sentence_phrase_pool, phrase_pool_emb, sentence_pool_emb, term_to_idx, bm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generative_knowledge_graph_construction': '0',\n",
       " 'generation_tasks': '1',\n",
       " 'named_entity_recognition': '2',\n",
       " 'relation_extraction': '3',\n",
       " 'event_extraction': '4',\n",
       " 'entity_linking': '5',\n",
       " 'knowledge_graph_completion': '6'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ('[ 8 ] propose a dynamic multi_pooling convolutional neural_network ( dm_ cnn ) , which uses a dynamic multi_pooling layer according to event triggers and arguments for event_extraction', 0.913306785813715, 0.7770894100358857, 0.0031051727799206123)\n",
      "3 ('the argument part is specic to event_type ei', 0.914617693454433, 0.7869675802903486, 0.002727315162633439)\n",
      "5 ('the second group contains generation_based event_extraction methods', 0.9080140282650968, 0.8621444665243786, 0.0024849446481576497)\n",
      "6 ('paper_title : text2event : controllable sequence_to_structure generation for end_to_end event_extraction ; paper_abstract : event_extraction is challenging due to the complex structure of event_records and the semantic gap between text and event', 0.9132553905853479, 0.8347001638762626, 0.0025721657225129344)\n",
      "7 (', text2event : controllable sequence_to_structure generation for end_to_end event_extraction , in proc', 0.9081088541185993, 0.814699993799282, 0.0028579141235849856)\n",
      "8 ('2 t ext2event : end_to_end event_extraction as controllable generation given the token sequence x=x1 , ... , x|x|of the input text , text2event directly generate the event structures e=e1 , ... , e|e|via an encoder_ decoder architecture', 0.9095463804400866, 0.8000682296750994, 0.0021511240786456207)\n",
      "9 ('text2event : controllable sequence_to_ structure generation for end_to_end event_extraction', 0.9081088541185993, 0.8188087881184775, 0.002124171519638873)\n",
      "11 ('degree : a data_efcient generative event_extraction model', 0.907955224561632, 0.8036124302255253, 0.0031794988976541192)\n",
      "13 ('2 method the event_extraction task consists of two subtasks : trigger extraction and argument_extraction', 0.9239054852200754, 0.8567668091893891, 0.0031419418719634148)\n",
      "15 ('event_extraction via dynamic multi_ pooling convolutional neural_networks', 0.9077871849216517, 0.7722936247022086, 0.001926571269909672)\n",
      "16 ('[ 59 ] propose a document_level neural event_argument_extraction model by formulating the task as conditional_generation following event templates', 0.902240807740788, 0.8131498619803018, 0.0028884839442398762)\n",
      "17 ('finally , degree is designed for end_to_end event_extraction and can solve event_detection and event_argument_extraction at the same time', 0.9290012679669113, 0.8373583991294018, 0.0033552936461578703)\n",
      "18 ('3.we release the rst end_to_end zero_shot event_extraction framework by combining our argu__ment extraction model with a zero_shot event trigger classication model', 0.9105894934629265, 0.7919984403914383, 0.003185213019733041)\n",
      "20 ('event_extraction most of the event_extraction works focus on ace sentence level event task [ 44 ] , which rstly detects the event triggers and then extracts arguments from a single sentence', 0.913306785813715, 0.8732341935585839, 0.0034739087583764024)\n",
      "21 ('output text event trigger is detonated', 0.8709893970355488, 0.7387311134036683, 0.0032529783799901553)\n",
      "22 ('2.1 event_extraction as structure generation this section describes how to linearize event struc_ ture so that events can be generated in an end_to_ end manner', 0.9098821985850989, 0.8436178036004834, 0.0029523419116738933)\n",
      "23 ('we propose a document_level neural event_argument_extraction model by formulating the task as conditional_generation following event templates', 0.902240807740788, 0.8049274674277209, 0.0035417553194172827)\n",
      "25 ('speci_ cally , instead of decomposing event structure pre__diction into different subtasks and predicting la_ bels , we uniformly model the whole event extrac__tion process in a neural network_based sequence_to_ structure architecture , and all triggers , arguments , and their labels are universally generated as natural_language words', 0.8791272083345907, 0.8320189076877678, 0.0028282042441546093)\n",
      "27 ('by pairing up our argument_extraction model with a keyword_based zero_shot trigger extraction model , we enable zero_shot transfer for new event_types', 0.919364962784233, 0.7847646097797089, 0.0033398412604288877)\n",
      "29 ('we use the pattern event_type is [ mask ]', 0.9018607935905151, 0.7769493729162791, 0.0027923661360725474)\n",
      "30 ('[ 59 ] propose a document_level neural event_argument_extraction model by formulating the task as conditional genera__tion following event templates', 0.902240807740788, 0.814710567555055, 0.002755220051514587)\n",
      "31 ('in summary , the contri_ butions are as follows : 1.we propose a new paradigm for event ex__traction _ sequence_to_structure generation , which can directly extract events from the text in an end_to_end manner', 0.8717532293238106, 0.8519612068633144, 0.003085993481838518)\n",
      "32 ('only arguments for the bold_ faced event triggers are shown', 0.8785698712025463, 0.7844492968804241, 0.003094041324951503)\n",
      "33 ('dmcnn is an dynamic multi_pooling convolutional neural_network for event_extraction [ 8 ]', 0.9081088541185993, 0.7654555606060341, 0.002754870432284822)\n",
      "34 ('to achieve this , we propose three novel event_centric objectives , i.e. , whole event recovering , contrastive event_ correlation encoding and prompt_based_event_locating , which highlight event_level correla_ tions with effective training', 0.9012215011670016, 0.7901327220377051, 0.0028250715478275344)\n",
      "35 ('cleve : contrastive pre_training for event_extraction', 0.9081088541185993, 0.8113967418799862, 0.0024990448464484215)\n",
      "36 ('the rst group is about classication_based event_extraction methods', 0.9079404809679008, 0.8697441387525371, 0.003049686878428781)\n",
      "37 ('we propose a document_level neu_ ral event_argument_extraction model by for_ mulating the task as conditional_generation following event templates', 0.902240807740788, 0.8343302140373506, 0.0030716267250997264)\n",
      "38 ('6 conclusion & future work in this paper , we advocate document_level event_extraction and propose the rst document_level neural event_argument_extraction model', 0.9214296714556163, 0.8233632098173257, 0.00347371379853543)\n",
      "39 ('figure 2 : examples of three event representations', 0.8709620313044352, 0.8021757239055672, 0.002279949147068614)\n",
      "40 ('unlike previous_works ( yang et al. , 2019a ; li et al. , 2021 ) , which separate event_extraction into two pipelined tasks ( event_detection and event_argument_extraction ) , degree is designed for the end_to_end event ex__traction and predict event triggers and arguments at the same time', 0.9311785695175676, 0.8328360580054261, 0.0028579606632248666)\n",
      "41 ('each sample is a 5_sentence document , with trigger word indicating pre_defined event_type and its argument scattering among the whole document', 0.9157736976493498, 0.8303595781084032, 0.0023864324115603227)\n",
      "42 ('how_ ever , their methods can only be applied to event_detection , which differs from our main focus on studying end_to_end event_extraction', 0.9155522085071713, 0.8439063673952282, 0.0034172929878350795)\n",
      "45 ('paper_title : dynamic prefix_tuning for generative template_based event_extraction ; paper_abstract : we consider event_extraction in a generative manner with template_based conditional_generation', 0.908186455671156, 0.7666378712495836, 0.002450182589911819)\n",
      "46 ('process. , virtual event , 2021 , pp', 0.8671050783487865, 0.7567034440866864, 0.002879570857822175)\n",
      "47 ('ere contains 458 english documents , 38 event_types , and 21 argument_roles', 0.916950188156943, 0.831258113291218, 0.001972173688482467)\n",
      "48 ('document_level event_extraction via parallel prediction networks', 0.9077510003839229, 0.8275087422075158, 0.002118664492503427)\n",
      "49 ('5 related work fully supervised event_extraction', 0.9078425180917051, 0.8535813195581698, 0.0021267809554667565)\n",
      "50 ('there is a rising trend of casting the task of event_extraction as a sequence_generation problem by ap_ plying special decoding strategies ( paolini et al. , 2021 ; lu et al. , 2021 ) or steering pretrained lan_ guage models to output conditional_generation se_ quences with discrete prompts ( li et al. , 2021 ; hsu et al. , 2021 )', 0.9083064900793725, 0.8019391836520537, 0.0021885675292214594)\n",
      "51 ('one template for each event_type is usually pre_dened in the ontology.4 we rst introduce our document_level argument_extraction model in section 2.1 and then intro__duce our zero_shot keyword_based trigger extrac__tion model in section 2.2', 0.9278944154173356, 0.8037659736316615, 0.0028454132930497427)\n",
      "52 ('table 6 demonstrates how different compo_model10 % data 100 % data arg_i_arg_c arg_i_arg_c full d egree ( eae ) 63.3 57.3 76.0 73.5 _ w/o event_type denition 60.3 54.4 74.5 71.1 _ w/o eae template 57.0 51.9 73.8 70.4 _ w/o query trigger 55.2 49.9 71.4 69.0 _ only query trigger 51.9 48.1 71.2 69.4 _ only eae template 51.2 46.9 71.4 68.6 _ only event_type denition 46.7 42.3 71.4 68.2 table 6 : ablation_study for the components in the prompt on event_argument_extraction with ace05_e. nents in prompts affect the performance of event_argument_extraction on ace05_e', 0.9300811121383417, 0.7745876719741347, 0.0011882300469185808)\n",
      "53 ('both contain 33 event_types and 22 argument_roles', 0.9170797502600023, 0.7794537228420972, 0.0018985949421024442)\n",
      "54 ('we propose degree ( data_efcient generation_based event_extraction ) , a generation_based model that takes a passage and a manually designed prompt as thearxiv:2108.12724v3 [ cs.cl ] 4 may 2022prompt event_type description the event is related to conflict and some violent physical act', 0.9210074289496087, 0.837672157721086, 0.0028441511558738174)\n",
      "55 ('intell. , virtual event , z._h. zhou , ed. , montreal , canada , 2021 , pp', 0.8715511152020107, 0.7426400666652923, 0.0026519322539374926)\n",
      "56 ('exploring sen__tence community for document_level event_extraction', 0.9081088537534663, 0.8405824651192915, 0.00247910756217326)\n",
      "57 ('[ 8 ] y. chen , l. xu , k. liu , d. zeng , and j. zhao , event_extraction via dynamic multi_pooling convolutional neural_networks , in proc', 0.9078803895522138, 0.7900015109439091, 0.002212821775381321)\n",
      "59 ('7 conclusion in this paper , we studied event_extraction in the template_based conditional_generation manner', 0.9081795414268191, 0.7999270733513011, 0.0038370728788797086)\n",
      "60 ('compared to the inefficient event_backfilling and contextu_ alizing paradigm in eventbert , our model can explicitly and effectively learn event_level corre_ lations between contexts and events by our novel contrastive and prompt_based objectives', 0.8969768648876721, 0.8179844610252737, 0.0028008614443362505)\n",
      "61 ('controllable generationevent schema arrest _jail person crime agent timetransport destination origin artifact vehicle time event_type arrest _jail trigger capture person the man time tuesday agent bounty huntersevent type transport trigger returned artifact the man destination los_angeles origin mexicosequence _to_structure network constraintfigure 1 : the framework of text2event', 0.9075597594394997, 0.7594490938676985, 0.0009082172553058212)\n",
      "62 ('im_ proving cross_lingual_transfer for event argument ex__traction with language_universal sentence structures', 0.8826984193130776, 0.743856496675892, 0.0023095302515662715)\n",
      "63 ('process. , virtual event , 2020 , pp', 0.8671050783487865, 0.7553694756950665, 0.0028020497791523187)\n",
      "64 ('[ 55 ] propose a sequence_ to_structure network with a constrained_decoding algorithm for event_extraction', 0.9081088541185993, 0.8236417258969654, 0.0032376430630384205)\n",
      "65 ('gets easily distracted by the additional con_ text and does not know which event to focus on', 0.8707863108869363, 0.7841877256640087, 0.00286084542213253)\n",
      "66 ('jrnn is a joint event_extraction framework with bidirectional recurrent neural_networks [ 48 ]', 0.9082342263791255, 0.8277944012832357, 0.002596405936698864)\n",
      "67 ('degree ( pipe ) consists of two models : ( 1 ) degree ( ed ) , which aims to exact event triggers for the given event_type , and ( 2 ) de_ gree ( eae ) , which identies argument_roles for the given event_type and the corresponding trig_ ger.degree ( ed ) anddegree ( eae ) are similar todegree but with different prompts and output formats', 0.9276132409562071, 0.7824960686200809, 0.002836757742933819)\n",
      "68 ('language model prim_ ing for cross_lingual event_extraction', 0.9081445549420861, 0.8127210797486372, 0.0034315035898305024)\n",
      "69 ('we consider three ways to include the event_type information : english tokens', 0.906607728564205, 0.8331536726647317, 0.00234532018783458)\n",
      "70 ('previous work ( yang et al. , 2019a ; fincke et al. , 2021 ) usually divides ee into two sub_ tasks : ( 1 ) event_detection , which identies event triggers and their types , and ( 2 ) event_argument_extraction , which extracts the arguments and their roles for given event triggers', 0.9251216243384556, 0.8333796189739331, 0.0028801709017309335)\n",
      "71 ('paper_title : degree : a data_efficient generation_based event_extraction model ; paper_abstract : event_extraction requires high_quality expert human annotations , which are usually expensive', 0.9080916313430695, 0.8517630023372285, 0.0021793487067832676)\n",
      "72 ('4.1 language_agnostic template we create one language_agnostic template tefor each event_type e , in which we list all possible as_ sociated roles3and form a unique html_tag_style template for that event_type e. for example , in figure 2 , the life : die event is associated with four roles : agent , victim , instrument , and place', 0.9105304710957659, 0.7913863010206871, 0.002401969877534855)\n",
      "73 ('the pre_ vious approach like [ 46 ] leverage hand_designed features for event_extraction', 0.9081088541185993, 0.8573474483654082, 0.002384507469003328)\n",
      "75 ('static t ype information event_type arrest_jail', 0.9006985869181815, 0.7012598332456257, 0.0010982154442557108)\n",
      "76 ('to demonstrate the portability of our model , we also create the first end_to_end zero_shot event_extraction framework and achieve 97 % of fully supervised models trigger extraction performance and 82 % of the argument_extraction performance given only access to 10 out of the 33 types on ace', 0.923884707415641, 0.8020103121898858, 0.0030668902690579895)\n",
      "77 ('describes a conict : attack event', 0.8684608050504434, 0.6764948118299896, 0.0021605462703399637)\n",
      "78 ('event_extraction as machine reading com_ prehension', 0.9078499616394528, 0.824355102436537, 0.0021638528335626976)\n",
      "79 ('although there is a rising trend of casting the task of event_extraction as a sequence_generation problem with prompts , these generation_based methods have two significant challenges , including using suboptimal prompts and static event_type information', 0.9200942876833647, 0.7875722610858411, 0.003067572171172632)\n",
      "80 ('to alleviate the above two challenges , we pro__pose a generative template_based event_extraction method with dynamic prexes , denoted as gtee_ dynpref', 0.9082124322266591, 0.7895790705140119, 0.0029343966093777337)\n",
      "81 ('compared with degree , the event_extraction method using xed templates , and text2event , the generative event_extraction method without prompts , gtee_d_ynpref outperforms them in all the datasets , showing the effectiveness of the trainable dynamic prex with prompts', 0.9081499937852545, 0.8387615118807143, 0.002901623732712154)\n",
      "82 ('document_level event_extraction via heteroge_ neous graph_based interaction model with a tracker', 0.9079516814678257, 0.7871989470690411, 0.002986947765020817)\n",
      "84 ('as shown in figure 2 , it contains the following components : event_type denition describes the denition for the given event_type.1for example , the event is related to conict and some violent phys_ ical act', 0.9056189836035563, 0.764163875697735, 0.002341948538114311)\n",
      "85 ('although there is a rising trend of casting the task of event_extraction as a se__quence generation problem with prompts , these generation_based methods have two signi_ cant challenges , including using suboptimal prompts and static event_type information', 0.9200942876833647, 0.7768653661735401, 0.0028984115533047337)\n",
      "86 ('as demonstrated in figure 1 , we follow5216our methodgeneration_based method generation_based event extraction1 ) extracting t ransport event 2 ) extracting arrest_jail event trigger person time agent capture the man tuesday bounty huntersevent type arrest_jailevent type t ransport trigger artifact destination originreturned the man los_angeles mexico generation_based event_extraction the man returned to los_angeles from mexico following his capture t uesday by bounty_hunters', 0.9105145222760875, 0.7880571679910047, 0.0018554690091455214)\n",
      "87 ('ace 2005 dataset has 599 annotated english documents , 33 event_types , and 22 argument_roles', 0.9168843887495036, 0.819079672792807, 0.002340075363581485)\n",
      "88 ('the nal event prediction is then decoded from the generated output', 0.8692537525040491, 0.7458250335990663, 0.002788670225128013)\n",
      "89 ('6 conclusion & future work in this paper , we present degree , a data_efcient generation_based event_extraction model', 0.9080751609488227, 0.8313314385521365, 0.003725076680663677)\n",
      "90 ('5.3 baseline methods we compare gtee_d_ynpref with two groups of event_extraction work', 0.9082065489956803, 0.8641297498251298, 0.003330456230957871)\n",
      "91 ('compared to ace05_en , ace05_en+and ere_ en further consider pronoun roles and multi_token event triggers', 0.87419548282618, 0.7316509369685551, 0.002112869055976232)\n",
      "92 ('the observed distributions of event_types and argument_roles are shown in figure 3', 0.9170020401543192, 0.796716659021578, 0.002709598126943966)\n",
      "93 ('more recent works leverage pre_trained lan_ guage models [ 51 ] or machine reading_comprehension mech_ anisms [ 52 ] [ 54 ] for event_extraction', 0.9081263679820728, 0.8364862460961527, 0.0014994036870165897)\n",
      "94 ('reference ofthegold event [ e ] : ibought thelatest model ofthe phone iwanted , and showed ittomyfriends', 0.86967694402769, 0.7660772821360885, 0.0016773853283360736)\n",
      "95 ('( 2020 ) , which keeps 38 event_types and 21 argument_roles', 0.9168480260964369, 0.7888633172987857, 0.002767403266063719)\n",
      "96 ('we put the english version of the event_type in the prompt even if we are training or testing on non_english languages , for example , using attack for the event_type attack', 0.904636911329468, 0.6956025231031524, 0.0028977689665266996)\n",
      "97 ('tandrindicate the label name of event_type and ar_ gument role.sindicates the text_span in the raw_text , which is the event trigger or argument mention of the extracted event', 0.9166515321036544, 0.8095738020487195, 0.002953259791337609)\n",
      "98 ('alternative end_ to_end event_extraction models , even those incorpo_ rating pretrained lm representations , only model events in isolation ( wadden et al. , 2019 ; du and cardie , 2020 ) , and are mainly evaluated on ace_ style ( doddington et al. , 2004 ) event_extraction from single sentences ( yang and mitchell , 2016 ; lin et al. , 2020 )', 0.909573602963591, 0.8319109433260314, 0.0024573828643846348)\n",
      "99 ('0 10 20 30 40 50 event types0200400600frequency 0 20 40 argument types0200400600 figure 3 : distribution of event_types and argument types in the w ikievents dataset', 0.9072156266087915, 0.7993770320714932, 0.0015750129472183669)\n",
      "100 ('paper_title : document_level event_argument_extraction by conditional_generation ; paper_abstract : event_extraction has long been treated as a sentence_level task in the ie community', 0.9213351467142507, 0.8392874233978841, 0.0022369044561472754)\n",
      "101 ('in this paper , we propose a sequence_to_ structure generation paradigm for event_extraction text2event , which can directly extract events from the text in an end_to_end manner', 0.909573602963591, 0.8636251396158009, 0.004163319460390191)\n"
     ]
    }
   ],
   "source": [
    "for k,v in node_internal_sent_ranks[4].items():\n",
    "    print(k, v)\n",
    "    if k > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxo.resetTaxo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING generative_knowledge_graph_construction; remaining in queue: deque([])\n",
      "before: ['graph_construction', 'knowledge_graph_building', 'generative_knowledge_graph', 'graph_generation', 'graph_building', 'graph_construction_tasks', 'generative_knowledge', 'graph_construction_techniques', 'knowledge_graph_construction', 'graph_building_tasks', 'graph_construction_methods', 'generative_knowledge_graph_construction_techniques', 'graph_construction_approaches', 'knowledge_graph_generation']\n",
      "after: ['generative_knowledge_graph_construction', 'generative_knowledge_graph_construction_techniques', 'generative_knowledge_graph', 'generative_knowledge', 'knowledge_graph_generation', 'knowledge_graph_construction', 'knowledge_graph_building', 'graph_generation', 'graph_construction_tasks', 'graph_construction_approaches', 'graph_construction', 'graph_construction_techniques', 'graph_construction_methods', 'graph_building', 'graph_building_tasks']\n",
      "PROCESSING generation tasks; remaining in queue: deque([])\n",
      "before: ['text_generation', 'knowledge_graph_construction', 'language_generation', 'text_synthesis', 'graph_generation', 'text_to_graph', 'graph_to_text', 'generation_models', 'text_generation_models', 'graph_generation_models', 'text_synthesis_models', 'graph_construction_models', 'knowledge_graph_generation', 'text_generation_techniques', 'graph_generation_techniques', 'generation_methods', 'graph_methods', 'text_construction_methods', 'graph_construction_methods', 'text_generation_approaches']\n",
      "after: ['generation tasks', 'text_generation_models', 'generation_methods', 'text_generation', 'generation_models', 'text_generation_approaches', 'language_generation', 'text_generation_techniques', 'graph_generation', 'graph_generation_models', 'graph_generation_techniques', 'text_construction_methods', 'knowledge_graph_generation', 'graph_construction_models', 'graph_to_text', 'graph_construction_methods', 'text_to_graph', 'graph_methods', 'knowledge_graph_construction', 'text_synthesis_models', 'text_synthesis']\n",
      "PROCESSING named_entity_recognition; remaining in queue: deque([relation_extraction, event_extraction, entity_linking, knowledge_graph_completion])\n",
      "before: ['named_entity', 'entity_recognition', 'named_entity_extraction', 'entity_identification', 'text_classification', 'named_entities', 'entity_categorization', 'named_entity_detection', 'entity_tagging', 'entity_annotation', 'named_entity_tagging', 'entity_extraction', 'named_entity_categorization', 'entity_detection', 'named_entity_identification', 'entity_classification', 'named_entity_annotation']\n",
      "after: ['named_entity_recognition', 'entity_recognition', 'named_entity_categorization', 'named_entity_detection', 'entity_categorization', 'named_entity_extraction', 'named_entity_tagging', 'named_entity_identification', 'entity_detection', 'entity_classification', 'named_entity_annotation', 'named_entity']\n",
      "PROCESSING relation_extraction; remaining in queue: deque([event_extraction, entity_linking, knowledge_graph_completion])\n",
      "before: ['relation_extraction_techniques', 'entity_relation_discovery', 'relationship_identification', 'entity_pair_extraction', 'relation_classification', 'relation_extraction_models', 'entity_relation_prediction', 'relation_extraction_methods', 'entity_pair_classification', 'relation_discovery', 'entity_relation_classification', 'relation_extraction_approaches', 'entity_pair_relation', 'relation_classification_techniques', 'entity_relation_extraction', 'relation_extraction_tasks', 'entity_pair_relation_extraction', 'relation_extraction_algorithms']\n",
      "after: ['relation_classification_techniques', 'relation_extraction_algorithms', 'relation_extraction_methods', 'relation_extraction_techniques', 'relation_classification', 'relation_extraction_approaches', 'relation_extraction_models', 'relation_extraction', 'relation_extraction_tasks', 'relation_discovery', 'entity_pair_relation_extraction', 'entity_relation_extraction']\n",
      "PROCESSING event_extraction; remaining in queue: deque([entity_linking, knowledge_graph_completion])\n",
      "before: ['event_extraction', 'event_detection', 'event_recognition', 'event_identification', 'event_categorization', 'event_classification', 'event_tagging', 'event_annotation', 'event_summarization', 'event_description', 'event_extraction_methods', 'event_extraction_techniques', 'event_extraction_algorithms', 'event_extraction_tools', 'event_extraction_systems', 'event_extraction_approaches', 'event_extraction_metrics', 'event_extraction_evaluation']\n",
      "after: ['event_identification', 'event_extraction_algorithms', 'event_extraction_tools', 'event_extraction_techniques', 'event_extraction_methods', 'event_extraction_systems', 'event_description', 'event_extraction_approaches', 'event_extraction', 'event_extraction_metrics', 'event_tagging', 'event_detection', 'event_extraction_evaluation', 'event_summarization', 'event_annotation', 'event_classification', 'event_recognition']\n",
      "PROCESSING entity_linking; remaining in queue: deque([knowledge_graph_completion])\n",
      "before: ['entity_linking', 'entity_disambiguation', 'entity_matching', 'entity_recognition', 'entity_classification', 'entity_resolution', 'entity_alignment', 'entity_correlation', 'entity_co-reference', 'entity_grounding', 'entity_linking_task', 'entity_linking_methods', 'entity_linking_algorithms', 'entity_linking_techniques', 'entity_linking_evaluation', 'entity_linking_metrics', 'entity_linking_dataset', 'entity_linking_benchmark', 'entity_linking_evaluation_metrics', 'entity_linking_performance']\n",
      "after: ['entity_linking', 'entity_linking_methods', 'entity_linking_techniques', 'entity_linking_performance', 'entity_linking_algorithms', 'entity_linking_metrics', 'entity_linking_benchmark', 'entity_linking_evaluation_metrics', 'entity_linking_dataset', 'entity_linking_evaluation', 'entity_linking_task', 'entity_co-reference', 'entity_grounding', 'entity_correlation', 'entity_disambiguation', 'entity_resolution', 'entity_matching', 'entity_alignment']\n",
      "PROCESSING knowledge_graph_completion; remaining in queue: deque([])\n",
      "before: ['entity_linking', 'knowledge_graph_inference', 'missing_entities', 'graph_completion', 'entity_linkage', 'graph_inference', 'missing_relationships', 'entity_discovery', 'link_prediction', 'relation_prediction', 'graph_filling', 'graph_completion_task', 'missing_entities_in_kg', 'entity_linkage_task', 'graph_inference_task', 'entity_discovery_task', 'relation_prediction_task', 'link_prediction_task', 'knowledge_graph_filling']\n",
      "after: ['knowledge_graph_completion', 'graph_completion_task', 'knowledge_graph_filling', 'graph_completion', 'knowledge_graph_inference', 'graph_inference_task', 'link_prediction_task', 'graph_filling']\n"
     ]
    }
   ],
   "source": [
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    print(f\"PROCESSING {curr_node.label}; remaining in queue: {queue}\")\n",
    "\n",
    "    # update vocab with label and description\n",
    "    taxo.updateVocab([curr_node.label] + curr_node.description.split(), 'phrases')\n",
    "    taxo.updateVocab(curr_node.description, 'sentences')\n",
    "    taxo.updateVocab(curr_node.label + \" : \" + curr_node.description, 'sentences')\n",
    "\n",
    "    # get common-sense class embedding\n",
    "    print('before:', curr_node.common_sense['phrases'])\n",
    "    top_common_phrases, common_phrase_node_emb = computeClassEmb(curr_node, taxo, granularity='phrases', out_phrases=True)\n",
    "    top_common_sentences, common_sent_node_emb = computeClassEmb(curr_node, taxo, granularity='sentences', out_phrases=True)\n",
    "\n",
    "    print('after:', top_common_phrases)\n",
    "    # curr_node.common_sense['phrases'] = top_common_phrases\n",
    "    # curr_node.common_sense['sentences'] = top_common_sentences\n",
    "\n",
    "    for child in curr_node.children:\n",
    "        queue.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_map = {n.node_id:[] for n in taxo.root.children[0].children}\n",
    "\n",
    "for p in collection + external_collection:\n",
    "    for n in taxo.root.children[0].children:\n",
    "        if any([term in p.vocabulary for term in n.common_sense['phrases']]):\n",
    "            paper_map[n.node_id].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 235\n",
      "3 56\n",
      "4 27\n",
      "5 115\n",
      "6 119\n"
     ]
    }
   ],
   "source": [
    "for n_id, paper_list in paper_map.items():\n",
    "    print(n_id, len(paper_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: 2; title: structured_prediction as translation between augmented natural languages; abstract: we propose a new framework , translation between augmented natural languages ( tanl ) , to solve many structured_prediction language tasks including joint_entity_and_relation_extraction , nested named_entity recognition , relation_classification , semantic_role_labeling , event_extraction , coreference_resolution , and dialogue_state_tracking . instead of tackling the problem by training task_specific discriminative classifiers , we frame it as a translation task between augmented natural languages , from which the task_relevant information can be easily extracted . our approach can match or outperform task_specific models on all tasks , and in particular , achieves new state_of_the_art results on joint_entity_and_relation_extraction ( conll04 , ade , nyt , and ace2005 datasets ) , relation_classification ( fewrel and tacred ) , and semantic_role_labeling ( conll_2005 and conll_2012 ) . we accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time ( multi_task_learning ) . finally , we show that our framework can also significantly improve the performance in a low_resource regime , thanks to better use of label_semantics .,\n",
       " id: 3; title: hyspa : hybrid span generation for scalable text_to_graph extraction; abstract: text_to_graph extraction aims to automatically extract information graphs consisting of mentions and types from natural_language texts . existing approaches , such as table filling and pairwise scoring , have shown impressive performance on various information_extraction tasks , but they are difficult to scale to datasets with longer input texts because of their second_order space/time complexities with respect to the input length . in this work , we propose a hybrid span generator ( hyspa ) that invertibly maps the information graph to an alternating_sequence of nodes and edge_types , and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities . extensive experiments on the ace05 dataset show that our approach also significantly outperforms state_of_the_art on the joint_entity_and_relation_extraction task .,\n",
       " id: 5; title: deepstruct : pretraining of language_models for structure_prediction; abstract: we introduce a method for improving the structural_understanding abilities of language_models . unlike previous approaches that finetune the models with task_specific augmentation , we pretrain language_models to generate structures from the text on a collection of task_agnostic corpora . our structure_pretraining enables zero_shot transfer of the learned knowledge that models have about the structure tasks . we study the performance of this approach on 28 datasets , spanning 10 structure_prediction_tasks including open_information_extraction , joint_entity_and_relation_extraction , named_entity recognition , relation_classification , semantic_role_labeling , event_extraction , coreference_resolution , factual probe , intent detection , and dialogue_state_tracking . we further enhance the pretraining with the task_specific training sets . we show that a 10b parameter language model transfers non_trivially to most tasks and obtains state_of_the_art_performance on 21 of 28 datasets that we evaluate . our code and datasets will be made publicly available .,\n",
       " id: 7; title: unified structure_generation for universal information_extraction; abstract: information_extraction suffers from its varying targets , heterogeneous structures , and demand_specific schemas . in this paper , we propose a unified text_to_structure generation framework , namely uie , which can universally model different ie tasks , adaptively generate targeted structures , and collaboratively learn general ie abilities from different knowledge sources . specifically , uie uniformly encodes different extraction structures via a structured extraction language , adaptively generates target extractions via a schema_based prompt mechanism structural_schema_instructor , and captures the common ie abilities via a large_scale pretrained text_to_structure model . experiments show that uie achieved the state_of_the_art_performance on 4 ie tasks , 13 datasets , and on all supervised , low_resource , and few_shot settings for a wide range of entity , relation , event and sentiment extraction tasks and their unification . these results verified the effectiveness , universality , and transferability of uie .,\n",
       " id: 8; title: extracting relational_facts by an end_to_end neural model with copy_mechanism; abstract: the relational_facts in sentences are often complicated . different relational triplets may have overlaps in a sentence . we divided the sentences into three types according to triplet overlap degree , including normal , entitypairoverlap and singleentiyoverlap . existing_methods mainly focus on normal class and fail to extract relational triplets precisely . in this paper , we propose an end_to_end model based on sequence_to_sequence learning with copy_mechanism , which can jointly extract relational_facts from sentences of any of these classes . we adopt two different strategies in decoding process : employing only one united decoder or applying multiple separated decoders . we test our models in two public datasets and our model outperform the baseline method significantly .,\n",
       " id: 12; title: contrastive information_extraction with generative_transformer; abstract: information_extraction tasks such as entity_relation_extraction and event_extraction are of great importance for natural_language processing and knowledge_graph construction . in this paper , we revisit the end_to_end information_extraction task for sequence_generation . since generative information_extraction may struggle to capture long_term_dependencies and generate unfaithful triples , we introduce a novel model , contrastive information_extraction with a generative_transformer . specifically , we introduce a single shared transformer module for an encoder_decoder_based generation . to generate faithful results , we propose a novel triplet contrastive training object . moreover , we introduce two mechanisms to further improve model performance ( i.e. , batch_wise_dynamic attention_masking and triple_wise calibration ) . experimental_results on five datasets ( i.e. , nyt , webnlg , mie , ace_2005 , and muc_4 ) show that our approach achieves better performance than baselines .,\n",
       " id: 14; title: document_level entity_based extraction as template generation; abstract: document_level entity_based extraction ( ee ) , aiming at extracting entity_centric information such as entity roles and entity relations , is key to automatic knowledge acquisition from text corpora for various domains . most document_level ee systems build extractive models , which struggle to model long_term_dependencies among entities at the document_level . to address this issue , we propose a generative_framework for two document_level ee tasks : role_filler entity_extraction ( ree ) and relation_extraction ( re ) . we first formulate them as a template generation_problem , allowing models to efficiently capture cross_entity dependencies , exploit label_semantics , and avoid the exponential computation complexity of identifying n_ary_relations . a novel cross_attention guided copy_mechanism , topk copy , is incorporated into a pre_trained sequence_to_sequence model to enhance the capabilities of identifying key information in the input document . experiments done on the muc_4 and scirex dataset show new state_of_the_art results on ree ( +3.26 % ) , binary re ( +4.8 % ) , and 4_ary re ( +2.7 % ) in f1_score .,\n",
       " id: 17; title: document_level event_argument_extraction by conditional_generation; abstract: event_extraction has long been treated as a sentence_level task in the ie community . we argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results . we propose a document_level neural event_argument_extraction model by formulating the task as conditional_generation following event templates . we also compile a new document_level event_extraction benchmark dataset wikievents which includes complete event and coreference annotation . on the task of argument_extraction , we achieve an absolute gain of 7.6 % f1 and 5.7 % f1 over the next best model on the rams and wikievents dataset respectively . on the more challenging task of informative argument_extraction , which requires implicit coreference reasoning , we achieve a 9.3 % f1 gain over the best baseline . to demonstrate the portability of our model , we also create the first end_to_end zero_shot event_extraction framework and achieve 97 % of fully supervised models trigger extraction performance and 82 % of the argument_extraction performance given only access to 10 out of the 33 types on ace .,\n",
       " id: 18; title: template filling with generative transformers; abstract: template filling is generally tackled by a pipeline of two separate supervised systems one for role_filler extraction and another for template/event_recognition . since pipelines consider events in isolation , they can suffer from error_propagation . we introduce a framework based on end_to_end generative transformers for this task ( i.e. , gtt ) . it naturally models the dependence between entities both within a single event and across the multiple events described in a document . experiments demonstrate that this framework substantially outperforms pipeline_based approaches , and other neural end_to_end baselines that do not model between_event dependencies . we further show that our framework specifically improves performance on documents containing multiple events .,\n",
       " id: 19; title: grit : generative role_filler transformers for document_level event entity_extraction; abstract: we revisit the classic problem of document_level role_filler entity_extraction ( ree ) for template filling . we argue that sentence_level approaches are ill_suited to the task and introduce a generative transformer_based encoder_decoder framework ( grit ) that is designed to model context at the document_level : it can make extraction decisions across sentence boundaries ; is implicitly aware of noun_phrase coreference structure , and has the capacity to respect cross_role dependencies in the template structure . we evaluate our approach on the muc_4 dataset , and show that our model performs substantially better than prior work . we also show that our modeling choices contribute to model performance , e.g. , by implicitly capturing linguistic knowledge such as recognizing coreferent entity_mentions .,\n",
       " id: 20; title: text2event : controllable sequence_to_structure generation for end_to_end event_extraction; abstract: event_extraction is challenging due to the complex structure of event_records and the semantic gap between text and event . traditional methods usually extract event_records by decomposing the complex structure_prediction task into multiple subtasks . in this paper , we propose text2event , a sequence_to_structure generation paradigm that can directly extract events from the text in an end_to_end manner . specifically , we design a sequence_to_structure network for unified event_extraction , a constrained_decoding algorithm for event knowledge injection during inference , and a curriculum_learning algorithm for efficient model learning . experimental_results show that , by uniformly modeling all tasks in a single model and universally predicting different labels , our method can achieve competitive_performance using only record_level annotations in both supervised_learning and transfer_learning settings .,\n",
       " id: 21; title: degree : a data_efficient generation_based event_extraction model; abstract: event_extraction requires high_quality expert human annotations , which are usually expensive . therefore , learning a data_efficient event_extraction model that can be trained with only a few labeled examples has become a crucial challenge . in this paper , we focus on low_resource end_to_end event_extraction and propose degree , a data_efficient model that formulates event_extraction as a conditional_generation problem . given a passage and a manually designed prompt , degree learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern . the final event predictions are then extracted from the generated sentence with a deterministic algorithm . degree has three advantages to learn well with less training data . first , our designed prompts provide semantic guidance for degree to leverage degree and thus better capture the event arguments . moreover , degree is capable of using additional weakly_supervised information , such as the description of events encoded in the prompts . finally , degree learns triggers and arguments jointly in an end_to_end manner , which encourages the model to better utilize the shared knowledge and dependencies among them . our experimental_results demonstrate the strong performance of degree for low_resource event_extraction .,\n",
       " id: 22; title: claret : pre_training a correlation_aware context_to_event transformer for event_centric generation and classification; abstract: generating new events given context with correlated ones plays a crucial role in many event_centric reasoning tasks . existing works either limit their scope to specific scenarios or overlook event_level correlations . in this paper , we propose to pre_train a general correlation_aware context_to_event transformer ( claret ) for event_centric reasoning . to achieve this , we propose three novel event_centric objectives , i.e. , whole event recovering , contrastive event_correlation encoding and prompt_based_event_locating , which highlight event_level correlations with effective training . the proposed claret is applicable to a wide range of event_centric reasoning scenarios , considering its versatility of ( i ) event_correlation types ( e.g. , causal , temporal , contrast ) , ( ii ) application formulations ( i.e. , generation and classification ) , and ( iii ) reasoning types ( e.g. , abductive , counterfactual and ending reasoning ) . empirical fine_tuning results , as well as zero_ and few_shot_learning , on 9 benchmarks ( 5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations ) , verify its effectiveness and generalization ability .,\n",
       " id: 23; title: dynamic prefix_tuning for generative template_based event_extraction; abstract: we consider event_extraction in a generative manner with template_based conditional_generation . although there is a rising trend of casting the task of event_extraction as a sequence_generation problem with prompts , these generation_based methods have two significant challenges , including using suboptimal prompts and static event_type information . in this paper , we propose a generative template_based event_extraction method with dynamic prefix ( gtee_dynpref ) by integrating context information with type_specific prefixes to learn a context_specific prefix for each context . experimental_results show that our model achieves competitive_results with the state_of_the_art classification_based model oneie on ace 2005 and achieves the best performances on ere . additionally , our model is proven to be portable to new types of events effectively .,\n",
       " id: 24; title: multilingual generative language_models for zero_shot cross_lingual event_argument_extraction; abstract: we present a study on leveraging multilingual_pre_trained generative language_models for zero_shot cross_lingual event_argument_extraction ( eae ) . by formulating eae as a language_generation task , our method effectively encodes event structures and captures the dependencies between arguments . we design language_agnostic templates to represent the event argument structures , which are compatible with any language , hence facilitating the cross_lingual_transfer . our proposed model finetunes multilingual_pre_trained generative language_models to generate sentences that fill in the language_agnostic template with arguments extracted from the input_passage . the model is trained on source languages and is then directly applied to target languages for event_argument_extraction . experiments demonstrate that the proposed model outperforms the current_state_of_the_art models on zero_shot cross_lingual eae . comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language_models for zero_shot cross_lingual_transfer eae .,\n",
       " id: 25; title: prompt for extraction ? paie : prompting argument interaction for event_argument_extraction; abstract: in this paper , we propose an effective yet efficient model paie for both sentence_level and document_level event_argument_extraction ( eae ) , which also generalizes well when there is a lack of training data . on the one hand , paie utilizes prompt_tuning for extractive objectives to take the best advantages of pre_trained_language_models ( plms ) . it introduces two span selectors based on the prompt to select start/end tokens among input texts for each role . on the other hand , it captures argument interactions via multi_role prompts and conducts joint optimization with optimal span assignments via a bipartite_matching_loss . also , with a flexible prompt design , paie can extract multiple arguments with the same role instead of conventional heuristic threshold tuning . we have conducted extensive experiments on three benchmarks , including both sentence_ and document_level eae . the results present promising improvements from paie ( 3.5 % and 2.3 % f1 gains in average on three benchmarks , for paie_base and paie_large respectively ) . further analysis demonstrates the efficiency , generalization to few_shot settings , and effectiveness of different extractive prompt_tuning strategies . our code is available at https : //github.com/mayubo2333/paie .,\n",
       " id: 39; title: 6th china conference on knowledge_graph and semantic computing , ccks 2021; abstract: the proceedings contain 28 papers . the special focus in this conference is on knowledge_graph and semantic computing . the topics include : incorporating complete syntactical knowledge for spoken language_understanding ; nsrl : named_entity recognition withnoisy labels via selective reviewlearning ; knowledge enhanced target_aware stance detection on tweets ; towards nested and fine_grained open_information_extraction ; toward a better text data_augmentation via filtering and transforming augmented instances ; a visual analysis method of knowledge_graph based on the elements and structure ; patentminer : patent vacancy mining via context_enhanced and knowledge_guided graph attention ; multi_task feature learning for social recommendation ; multi_stage knowledge propagation network for recommendation ; federated knowledge_graph embeddings with heterogeneous data ; tgkg : new data graph based on game ontology ; csdqa : diagram question_answering in computer science ; mooper : a large_scale dataset ofpractice_oriented online learning ; meed : a multimodal event_extraction dataset ; c_clue : a benchmark of classical_chinese based on a crowdsourcing system for knowledge_graph construction ; rcwi : a dataset for chinese complex word identification ; diakg : an annotated diabetes dataset for medical_knowledge graph_construction ; weibo_mel , wikidata_mel and richpedia_mel : multimodal entity_linking_benchmark datasets ; makg : a mobile application knowledge_graph for the research of cybersecurity ; text_guided legal knowledge_graph reasoning ; on robustness and bias analysis ofbert_based relation_extraction ; ka_ner : knowledge augmented named_entity recognition ; structural dependency self_attention based hierarchical event model for chinese financial event_extraction ; integrating manifold knowledge forglobal entity_linking withheterogeneous graphs ; content_based open knowledge_graph search : a preliminary study withopenkg.cn ; dependency to semantics : structure transformation and syntax_guided attention for neural semantic_parsing .,\n",
       " id: 352; title: construction andapplication ofevent logic graph : a survey; abstract: since being proposed in 2017 , event_logic_graph has attracted more and more researchers attention and has been gradually applied in various fields such as finance , health_care , transportation , information , politics , etc . unlike the traditional_knowledge graph describing static entities and their attributes and relationships , the event_logic_graph describes the evolution rules and patterns between events . the construction of event logic graphs is significant for understanding human behavior patterns and mining event evolution rules . the survey first systematically combs the work of constructing an event_logic_graph , including event_extraction and event relationship_extraction methods . secondly , the typical application of the event_logic_graph is explained . finally , the challenges of event_logic_graph construction are analyzed , and future_research trends are prospected .,\n",
       " id: 380; title: contrastive information_extraction with generative transformer; abstract: information_extraction tasks such as entity_relation_extraction and event_extraction are of great importance for natural_language processing and knowledge_graph construction . in this paper , we revisit the end_to_end information_extraction task for sequence generation . since generative information_extraction may struggle to capture long_term dependencies and generate unfaithful triples , we introduce a novel model , contrastive information_extraction with a generative transformer . specifically , we introduce a single shared transformer module for an encoder_decoder_based generation . to generate faithful results , we propose a novel triplet contrastive training object . moreover , we introduce two mechanisms to further improve model performance ( i.e. , batch_wise dynamic attention_masking and triple_wise calibration ) . experimental_results on five datasets ( i.e. , nyt , webnlg , mie , ace_2005 , and muc_4 ) show that our approach achieves better performance than baselines.1,\n",
       " id: 527; title: event_logic_graph construction for event mining; abstract: event_logic_graph is a directed and cyclic graph , the nodes in the graph represent events , and the edges represent the logical relationship between events . in essence , event_logic_graph is a knowledge_base of event logic , in order to reveal the evolution law and development mode of the event , we research a method of constructing event_logic_graph , which describes the logical structural relationship between events by adopting knowledge_graph structure . at the same time , in order to describe the event more clearly , we also describe the multi_dimensional attributes of the event . we propose an architecture for constructing event_logic_graph , which includes text_corpus collection , event relationship template construction , event_extraction and structured representation , event similarity calculation and fusion , event trigger word extraction and argument extraction model construction , event relationship pair construction , graph_database storage , and the architecture is used to construct serial event_logic_graph , causal event_logic_graph , conditional event_logic_graph , transitional event_logic_graph , and concurrent event_logic_graph .,\n",
       " id: 539; title: exploiting extensive external information for event_detection through semantic networks word representation and attention map; abstract: event_detection is one of the key tasks to construct knowledge_graph and reason graph , also a hot and difficult problem in information_extraction . automatic event_detection from unstructured natural_language text has far_reaching significance for human cognition and intelligent analysis . however , limited by the source and genre , corpora for event_detection can not provide enough information to solve the problems of polysemy , synonym association and lack of information . to solve these problems , this paper proposes a brand new event_detection model based on extensive external information ( edeei ) . the model employs external corpus , semantic_network , part of speech and attention map to extract complete and accurate triggers . experiments on ace 2005 benchmark dataset show that the model effectively uses the external_knowledge to detect events , and is significantly superior to the state_of_the_art event_detection methods .,\n",
       " id: 552; title: extracting events and their relations from texts : a survey on recent research progress and challenges; abstract: event is a common but non_negligible knowledge type . how to identify events from texts , extract their arguments , even analyze the relations between different events are important for many applications . this paper summaries some constructed event_centric knowledge_graphs and the recent typical approaches for event and event relation_extraction , besides task description , widely used evaluation datasets , and challenges . specifically , in the event_extraction task , we mainly focus on three recent important research problems : 1 ) how to learn the textual semantic representations for events in sentence_level event_extraction ; 2 ) how to extract relations across sentences or in a document level ; 3 ) how to acquire or augment labeled instances for model training . in event relation_extraction , we focus on the extraction approaches for three typical event relation_types , including coreference , causal and temporal relations , respectively . finally , we give out our conclusion and potential research issues in the future .,\n",
       " id: 657; title: ieee international conference on data_mining workshops , icdmw; abstract: the proceedings contain 189 papers . the topics discussed include : detecting performance degradation of software_intensive systems in the presence of trends and long_range dependence ; scalable online_offline stream clustering in apache spark ; distributed mining and modeling of dynamic lead_lag relations in evolving entities ; event_detection for urban dynamic data streams ; segmenting sequences of node_labeled graphs ; inference of partial canonical correlation networks with application to stock_market portfolio selection ; overlapping community detection by local decentralized vertex_centered process ; query_based evolutionary graph cuboid outlier detection ; vertex centric asynchronous belief_propagation algorithm for large_scale graphs ; text network exploration via heterogeneous web of topics ; and classification of normal and pathological brain networks based on similarity in graph partitions .,\n",
       " id: 723; title: joint biomedical entity and relation_extraction with knowledge_enhanced collective inference; abstract: compared to the general news domain , information_extraction ( ie ) from biomedical text requires much broader domain_knowledge . however , many previous ie methods do not utilize any external_knowledge during inference . due to the exponential_growth of biomedical publications , models that do not go beyond their fixed set of parameters will likely fall behind . inspired by how humans look up relevant information to comprehend a scientific text , we present a novel framework that utilizes external_knowledge for joint entity and relation_extraction named keci ( knowledge_enhanced collective inference ) . given an input text , keci first constructs an initial span graph representing its initial understanding of the text . it then uses an entity linker to form a knowledge_graph containing relevant background_knowledge for the the entity mentions in the text . to make the final predictions , keci fuses the initial span graph and the knowledge_graph into a more refined graph using an attention_mechanism . keci takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional_networks . our experimental_results show that the framework is highly effective , achieving new state_of_the_art results in two different benchmark_datasets : biorelex ( binding interaction detection ) and ade ( adverse drug event_extraction ) . for example , keci achieves absolute improvements of 4.59 % and 4.91 % in f1 scores over the state_of_the_art on the biorelex entity and relation_extraction_tasks .,\n",
       " id: 894; title: modeling of unsupervised knowledge_graph of events based on mutual_information among neighbor domains and sparse representation; abstract: text event mining , as an indispensable method of text_mining processing , has attracted the extensive attention of researchers . a modeling method for knowledge_graph of events based on mutual_information among neighbor domains and sparse representation is proposed in this paper , i.e . ukge_ms . specifically , ukge_ms can improve the existing text_mining technology 's ability of understanding and discovering high_dimensional unmarked information , and solves the problems of traditional unsupervised feature_selection methods , which only focus on selecting features from a global perspective and ignoring the impact of local connection of samples . firstly , considering the influence of local information of samples in feature correlation evaluation , a feature clustering algorithm based on average neighborhood mutual_information is proposed , and the feature clusters with certain event correlation are obtained ; secondly , an unsupervised feature_selection method based on the high_order correlation of multi_dimensional statistical data is designed by combining the dimension reduction advantage of local linear embedding algorithm and the feature_selection ability of sparse representation , so as to enhance the generalization_ability of the selected feature items . finally , the events knowledge_graph is constructed by means of sparse representation and l1 norm . extensive_experiments are carried out on five real datasets and synthetic datasets , and the ukge_ms are compared with five corresponding algorithms . the experimental_results show that ukge_ms is better than the traditional method in event clustering and feature_selection , and has some advantages over other methods in text event_recognition and discovery .,\n",
       " id: 902; title: multi_information preprocessing event_extraction with bilstm_crf attention for academic knowledge_graph construction; abstract: academic knowledge_graph is an important application of knowledge_graph in the vertical field of academia . at present , the construction of the academic knowledge_graph is mainly completed by extracting published academic papers , authors , publications , and other information from related databases . however , academic information is not just information of published papers . scholars & # x2019 ; academic activities include participation in academic conferences , visiting and making presentation , and so on . however , the above academic information is hidden in natural_language texts and can not be directly stored in academic knowledge_graph . this article proposes an approach named construct_scholat knowledge_graph to construct an academic event knowledge_graph based on academic social_network scholat . the construction framework mainly consists of two parts : data preprocessing and event_extraction . in the data preprocessing , we propose a knowledge_graph embedding method to represent scholars & # x2019 ; academic social feature . in the event_extraction , we concatenate the preprocessed scholar vector with academic we_media blog text into the extraction model based on bilstm_crf fused with attention_mechanism . the extracted events are added to academic knowledge_graph , and a public relationship exists between the event and the scholar . compared to the previous methods , our framework has an excellent performance after experimental verification . to the best of our knowledge , this is the first study to use the scholar academic social information of the scholar who edited the text as the event_extraction input information . in addition , we publish a chinese event_extraction dataset scholat academic event extraction.https : //www.scholat.com/research/opendata the dataset includes academic we_media blog and the social behavior embedding vector of the scholar . all the data in this dataset are derived from the academic social_network scholat .,\n",
       " id: 1161; title: semi_supervised auto_encoder based event_detection in constructing knowledge_graph for social good; abstract: knowledge_graphs have recently been extensively applied in many different areas ( e.g. , disaster management and relief , disease_diagnosis ) . for example , event_centric knowledge_graphs have been developed to improve decision_making in disaster management and relief . this paper focuses on the task of event_detection , which is the precondition of event_extraction for constructing event_centric knowledge_graphs . event_detection identifies trigger words of events in the sentences of a document and further classifies the types of events . it is straightforward that context information is useful for event_detection . therefore , the feature_based methods adopt cross_sentence information . however , they suffer from the complication of human_designed features . on the other hand , the representation_based methods learn document_level embeddings , which , however , contain much noise caused by unsupervised_learning . to overcome these problems , in this paper we propose a new model based on semi_supervised auto_encoder , which learns context information to enhance event_detection , thus called sae_ceed . this model first applies large_scale unlabeled texts to pre_train an auto_encoder , so that the embeddings of segments learned by the encoder contain the semantic and order information of the original text . it then uses the decoder to extract the context embeddings and fine_tunes them to enhance a bidirectional neural_network model to identify event triggers and their types in sentences . through experiments on the benchmark ace_2005 dataset , we demonstrate the effectiveness of the proposed sae_ceed model . in addition , we systematically conduct a series of experiments to verify the impact of different lengths of text segments in the pre_training of the auto_encoder on event_detection .]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_map[\"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import rankPhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_external, class_emb = rankPhrases(external_sentences, taxo.root, taxo, use_class_emb=True, granularity='sentences', out_phrases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING generative_knowledge_graph_construction; remaining in queue: deque([])\n",
      "PROCESSING generation tasks; remaining in queue: deque([])\n",
      "PROCESSING named_entity_recognition; remaining in queue: deque([relation_extraction, event_extraction, entity_linking, knowledge_graph_completion])\n",
      "PROCESSING relation_extraction; remaining in queue: deque([event_extraction, entity_linking, knowledge_graph_completion])\n",
      "PROCESSING event_extraction; remaining in queue: deque([entity_linking, knowledge_graph_completion])\n",
      "PROCESSING entity_linking; remaining in queue: deque([knowledge_graph_completion])\n",
      "PROCESSING knowledge_graph_completion; remaining in queue: deque([])\n"
     ]
    }
   ],
   "source": [
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    parent_node = curr_node.parents[0]\n",
    "\n",
    "    print(f\"PROCESSING {curr_node.label}; remaining in queue: {queue}\")\n",
    "\n",
    "    # EXTERNAL: get partitioned set of external phrases and sentences\n",
    "    curr_node.external['phrases'] = external_phrases # parent_node.external['phrases']\n",
    "    curr_node.external['sentences'] = external_sentences # parent_node.external['sentences']\n",
    "  \n",
    "    # get top external phrases & external sentences\n",
    "    top_phrases, external_phrase_class_emb = computeClassEmb(curr_node, taxo, class_emb=True, granularity='phrases', out_phrases=True)\n",
    "    top_sentences, external_sentence_class_emb = computeClassEmb(curr_node, taxo, class_emb=True, granularity='sentences', out_phrases=True)\n",
    "\n",
    "    top_external_phrases = [phrase for phrase in top_phrases if phrase in curr_node.external['phrases']]\n",
    "    top_external_sentences = [sent for sent in top_sentences if sent in curr_node.external['sentences']]\n",
    "    curr_node.external['phrases'] = top_external_phrases\n",
    "    curr_node.external['sentences'] = top_external_sentences\n",
    "\n",
    "    # INTERNAL: get partitioned set of internal phrases and sentences\n",
    "    curr_node.internal['phrases'] = internal_phrases # parent_node.internal['phrases']\n",
    "    curr_node.internal['sentences'] = internal_sentences # parent_node.internal['sentences']\n",
    "\n",
    "    # get top internal phrases & sentences\n",
    "    top_phrases, internal_phrase_class_emb = computeClassEmb(curr_node, taxo, class_emb=True, granularity='phrases', out_phrases=True)\n",
    "    top_sentences, internal_sentence_class_emb = computeClassEmb(curr_node, taxo, class_emb=True, granularity='sentences', out_phrases=True)\n",
    "\n",
    "    top_internal_phrases = [phrase for phrase in top_phrases if phrase in curr_node.internal['phrases']]\n",
    "    top_internal_sentences = [sent for sent in top_sentences if sent in curr_node.internal['sentences']]\n",
    "    curr_node.internal['phrases'] = top_internal_phrases\n",
    "    curr_node.internal['sentences'] = top_internal_sentences\n",
    "\n",
    "    for child in curr_node.children:\n",
    "        queue.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phrases': ['entity_recognition',\n",
       "  'bag_of_entity',\n",
       "  'bag_of_entities',\n",
       "  'named',\n",
       "  'anti_nose',\n",
       "  'tokens',\n",
       "  'artificial_neural_networks',\n",
       "  'facial_expression',\n",
       "  '//github.com/wjn1996/mathematical_knowledge_entity_recognition',\n",
       "  'artificial_intelligence',\n",
       "  'andrecognize',\n",
       "  'token',\n",
       "  'murder',\n",
       "  'tokenization',\n",
       "  'negated',\n",
       "  'mention_pronoun',\n",
       "  'word_vector',\n",
       "  'weaponry',\n",
       "  'inflated',\n",
       "  'optical_character_recognition',\n",
       "  'token_based',\n",
       "  \"'phoneme\",\n",
       "  'word_character',\n",
       "  'natural_language',\n",
       "  'intent_classifier',\n",
       "  'entity_classification',\n",
       "  'entity_detection',\n",
       "  'mention',\n",
       "  'name',\n",
       "  'non_named',\n",
       "  'word_by_word',\n",
       "  'weapon',\n",
       "  'named_entity_extraction',\n",
       "  'noun',\n",
       "  'sign_language',\n",
       "  'applications_of_artificial_intelligence',\n",
       "  'noun_phrase',\n",
       "  'intelligent_agent',\n",
       "  'pronoun',\n",
       "  'character_vocabulary',\n",
       "  'word_entity',\n",
       "  'support_vector_machine',\n",
       "  'utterances',\n",
       "  'automatic_speech_recognition',\n",
       "  'naming',\n",
       "  'chinese_language',\n",
       "  'uttered',\n",
       "  'controlled_vocabulary',\n",
       "  'inter_mention',\n",
       "  'negate',\n",
       "  'eye_tracking',\n",
       "  'tagger',\n",
       "  'mention_entity',\n",
       "  'automobile',\n",
       "  'false_negative',\n",
       "  'birds_eye',\n",
       "  'cytokine',\n",
       "  'isolate',\n",
       "  'neural_networks',\n",
       "  'aviation',\n",
       "  'vehicle_pedestrian',\n",
       "  'neural_network',\n",
       "  'microchip',\n",
       "  'uyghur_named',\n",
       "  'bots',\n",
       "  'misdiagnosis',\n",
       "  'lexicon',\n",
       "  'nomenclature',\n",
       "  'masked_entity',\n",
       "  'international_classification_of_diseases',\n",
       "  'mention_context',\n",
       "  'word_vectors',\n",
       "  'character_level',\n",
       "  'named_entity',\n",
       "  'recognize',\n",
       "  'bag_of_word',\n",
       "  'microblog',\n",
       "  'detects',\n",
       "  'bag_level',\n",
       "  'call',\n",
       "  'names',\n",
       "  'decent',\n",
       "  'subword/sub_entity',\n",
       "  'handled',\n",
       "  'support_vector_machines',\n",
       "  'pedestrian_vehicle',\n",
       "  'subword',\n",
       "  'president_obama',\n",
       "  'suspects',\n",
       "  'automobiles',\n",
       "  'recognizes',\n",
       "  'entity',\n",
       "  'identification',\n",
       "  '179',\n",
       "  'tag',\n",
       "  'neural_network_based',\n",
       "  'neural_symbolic',\n",
       "  'neural_based',\n",
       "  'detection',\n",
       "  'wrong_label',\n",
       "  'ethnic',\n",
       "  'cognate',\n",
       "  '#',\n",
       "  'machine_translation',\n",
       "  'number',\n",
       "  'sentiment_controllable',\n",
       "  'sentiment_augmented',\n",
       "  'thebrainof',\n",
       "  'intentional',\n",
       "  'convolutional_neural_network',\n",
       "  'identifying',\n",
       "  'animal_husbandry',\n",
       "  'immunology',\n",
       "  'chinese_characters',\n",
       "  'mentions',\n",
       "  'non_native',\n",
       "  'horn',\n",
       "  'aircraft',\n",
       "  'receptive_field',\n",
       "  'multinomial',\n",
       "  'verbalizer',\n",
       "  'malicious',\n",
       "  'autoimmune_disease',\n",
       "  'molecular',\n",
       "  'word_embedding',\n",
       "  'vector_based',\n",
       "  'infectious',\n",
       "  '320_character',\n",
       "  'wordnet',\n",
       "  'visionlanguage',\n",
       "  'chemical_protein',\n",
       "  'chinese_character',\n",
       "  'boosted',\n",
       "  'describe',\n",
       "  'uncontrollable',\n",
       "  'utterance',\n",
       "  'unit',\n",
       "  'nuclear',\n",
       "  'category',\n",
       "  \"'name\",\n",
       "  'negation',\n",
       "  'word_embeddings',\n",
       "  'microblogging',\n",
       "  'convolutional_neural_networks',\n",
       "  'infections',\n",
       "  'vector',\n",
       "  'differentiates',\n",
       "  'termed',\n",
       "  'bag_of_words',\n",
       "  'pathogenicity',\n",
       "  'intent',\n",
       "  'country',\n",
       "  'beings',\n",
       "  'sentence',\n",
       "  'national',\n",
       "  'semiconductor_industry',\n",
       "  'identifier',\n",
       "  'word/sub_word',\n",
       "  'recognition/normalization',\n",
       "  'conviction',\n",
       "  'token_level',\n",
       "  'name_',\n",
       "  'managed',\n",
       "  'tweet',\n",
       "  'virulent',\n",
       "  'lexicon_based',\n",
       "  'veteran',\n",
       "  'taggers',\n",
       "  'concept_to_sentence',\n",
       "  'anti_money',\n",
       "  'affiliation',\n",
       "  'distinguish',\n",
       "  'vehicle',\n",
       "  'character_feature',\n",
       "  'character_based',\n",
       "  'unique',\n",
       "  'anger',\n",
       "  'vectors',\n",
       "  'boolean_circuit',\n",
       "  'misspelled',\n",
       "  'denoted',\n",
       "  'cruises',\n",
       "  'word_segmentation',\n",
       "  'keyword',\n",
       "  'sentence_embedding',\n",
       "  'detecting',\n",
       "  'neural',\n",
       "  'spurious',\n",
       "  'voice',\n",
       "  'distinguishing',\n",
       "  'accuracy_rate',\n",
       "  'combat',\n",
       "  'named_entities',\n",
       "  'boundaries',\n",
       "  'human_object',\n",
       "  'attention_deficit',\n",
       "  'sentiments',\n",
       "  'molecular_structure',\n",
       "  'bert_imseq2seq_crf',\n",
       "  'terrorist',\n",
       "  'barack_obama',\n",
       "  'elected',\n",
       "  '3188',\n",
       "  'ncbi_disease',\n",
       "  'multilayer_perceptron',\n",
       "  'mobile_phone',\n",
       "  'emotion',\n",
       "  'vectorized',\n",
       "  'deceptive',\n",
       "  'called',\n",
       "  'word_embedding_based',\n",
       "  'bot',\n",
       "  'phone',\n",
       "  'sentiment',\n",
       "  'emotions',\n",
       "  'recognition.to',\n",
       "  'militarycorpus',\n",
       "  'whole_sentence',\n",
       "  'mention_level',\n",
       "  'vehicles',\n",
       "  'language',\n",
       "  'unidentified',\n",
       "  'intelligent_systems',\n",
       "  'speech',\n",
       "  'word_level',\n",
       "  'medical_diagnosis',\n",
       "  'overwhelmed',\n",
       "  'deciphering',\n",
       "  'asthmaenvironment',\n",
       "  'purely',\n",
       "  'word',\n",
       "  'herein',\n",
       "  'string_matching',\n",
       "  'activitynet_caption',\n",
       "  'cause',\n",
       "  'vogue',\n",
       "  'region_based',\n",
       "  'weapons',\n",
       "  'easoning',\n",
       "  'pre_trained_language_models',\n",
       "  'institution',\n",
       "  'birds',\n",
       "  'sentence_specific',\n",
       "  'pre_trained_language',\n",
       "  'veterinary',\n",
       "  'sentences',\n",
       "  'tags',\n",
       "  'fluent',\n",
       "  'tagging',\n",
       "  'label_imbalance',\n",
       "  'pulmonary',\n",
       "  'hospital',\n",
       "  'speaking',\n",
       "  'automate',\n",
       "  'head/tail',\n",
       "  'identifiers',\n",
       "  'military_intelligence',\n",
       "  'telecommunication',\n",
       "  'fighting',\n",
       "  'manage',\n",
       "  'region',\n",
       "  'tweets',\n",
       "  'units',\n",
       "  'multi_neural',\n",
       "  'single_vote',\n",
       "  'unmarked',\n",
       "  'drug_discovery',\n",
       "  'abbreviation',\n",
       "  'tagged',\n",
       "  'flagged',\n",
       "  'intra_sentence',\n",
       "  'lung_cancer',\n",
       "  'domestic',\n",
       "  'detect',\n",
       "  'pedestrians',\n",
       "  'autism_spectrum',\n",
       "  'million_level',\n",
       "  'identify',\n",
       "  're_net',\n",
       "  'attack',\n",
       "  'realm',\n",
       "  'neuro_symbolic',\n",
       "  'indonesian_language',\n",
       "  'obama',\n",
       "  'legitimate',\n",
       "  'chinese_medical',\n",
       "  'spleen',\n",
       "  'id_cnn+crf',\n",
       "  'japanese',\n",
       "  'speer',\n",
       "  'de_identified',\n",
       "  'raise',\n",
       "  'characters',\n",
       "  'neglecting',\n",
       "  'identity',\n",
       "  'attracted',\n",
       "  'spoken',\n",
       "  'character_embedding',\n",
       "  'human_machine',\n",
       "  'serve',\n",
       "  'malware',\n",
       "  'comprise',\n",
       "  'combating',\n",
       "  'sentiment_classification',\n",
       "  'chinese_medicine',\n",
       "  'pronouns',\n",
       "  'distant_supervised',\n",
       "  'non_chinese',\n",
       "  'wordnet_based',\n",
       "  'multi_sentence',\n",
       "  'etiology',\n",
       "  'authenticity',\n",
       "  '!',\n",
       "  'twitter',\n",
       "  'vectorize',\n",
       "  'heroic',\n",
       "  'describing',\n",
       "  'node_embedding',\n",
       "  'topic_embedding',\n",
       "  'sentiment_aware',\n",
       "  'recognized',\n",
       "  '317',\n",
       "  'distantly_supervised',\n",
       "  'asthma',\n",
       "  'boosting',\n",
       "  'character',\n",
       "  '1.1https',\n",
       "  'ep_bot',\n",
       "  'grammatical',\n",
       "  'embeddings_based',\n",
       "  'attackers',\n",
       "  'automated',\n",
       "  'mundane',\n",
       "  'target_language',\n",
       "  'entities',\n",
       "  'automatic',\n",
       "  'recognition.in',\n",
       "  'multi_paragraph',\n",
       "  'calling',\n",
       "  'automates',\n",
       "  'perplexity',\n",
       "  'serves',\n",
       "  'attempt',\n",
       "  'segment',\n",
       "  'distinguished',\n",
       "  'machine_learning',\n",
       "  'maximum_likelihood',\n",
       "  'into',\n",
       "  'out_of_vocabulary',\n",
       "  'ornamental',\n",
       "  'nouns',\n",
       "  'characterize',\n",
       "  'abnormal',\n",
       "  '2017',\n",
       "  'siamese',\n",
       "  'medicine_specific',\n",
       "  'chinese_poetry',\n",
       "  'was',\n",
       "  'species',\n",
       "  'chemistries',\n",
       "  'identifiable',\n",
       "  'cell_cytokine',\n",
       "  'laboratory',\n",
       "  'calls',\n",
       "  'hereinafter',\n",
       "  'base_station',\n",
       "  'report',\n",
       "  'truly',\n",
       "  'picked',\n",
       "  'multi_vote',\n",
       "  'term',\n",
       "  'rare_disease',\n",
       "  'agent',\n",
       "  'mistake',\n",
       "  'infor_mation',\n",
       "  'english_chinese',\n",
       "  'cyber_security',\n",
       "  'cruise',\n",
       "  'italian',\n",
       "  'headway',\n",
       "  'residues',\n",
       "  'idrg_cnn',\n",
       "  'verbalizes',\n",
       "  'patent_medicine',\n",
       "  'inflected',\n",
       "  'director',\n",
       "  'automatics',\n",
       "  'anti_terrorism',\n",
       "  'verbalized',\n",
       "  'description_embodied',\n",
       "  'diagnosis/medications',\n",
       "  'short_term_memory',\n",
       "  'wordties',\n",
       "  '_gram',\n",
       "  'spreading',\n",
       "  'language_understanding',\n",
       "  'topic',\n",
       "  'user_agent',\n",
       "  'sion',\n",
       "  'solely',\n",
       "  'category_based',\n",
       "  'host',\n",
       "  'described',\n",
       "  'words',\n",
       "  'native',\n",
       "  'nodule',\n",
       "  'entity_extraction',\n",
       "  'flavor',\n",
       "  'specificity',\n",
       "  'concatenation',\n",
       "  'distinct',\n",
       "  'mere',\n",
       "  'acknowledged',\n",
       "  'correctly',\n",
       "  'culture',\n",
       "  'cyberbullying',\n",
       "  'characteristic',\n",
       "  'high_precision',\n",
       "  'identifies',\n",
       "  'primarily',\n",
       "  'sentenses',\n",
       "  'affiliations',\n",
       "  'illness',\n",
       "  'vision_language',\n",
       "  'totally',\n",
       "  'cnn_based',\n",
       "  'human_recognizable',\n",
       "  'word_feature',\n",
       "  'vectorization',\n",
       "  'inter_sentence',\n",
       "  'allergies',\n",
       "  'vocabulary',\n",
       "  'chat_bots',\n",
       "  'accuracy',\n",
       "  'identically',\n",
       "  'military',\n",
       "  'entries',\n",
       "  'unrest',\n",
       "  'wang',\n",
       "  'drugbank',\n",
       "  'borders',\n",
       "  'entity_identification',\n",
       "  'asthmakgxe',\n",
       "  'verbalization',\n",
       "  'label_lacking',\n",
       "  'cognates',\n",
       "  'thing',\n",
       "  'numbering',\n",
       "  'specific',\n",
       "  'verbalizing',\n",
       "  'tertiary',\n",
       "  '2019',\n",
       "  'convey',\n",
       "  'animal',\n",
       "  'duconv',\n",
       "  'encompass',\n",
       "  'belonging',\n",
       "  'identities',\n",
       "  'collective',\n",
       "  'topic_word',\n",
       "  'detector',\n",
       "  'category_specific',\n",
       "  'single_sentence',\n",
       "  'dialogre',\n",
       "  'chemical_disease',\n",
       "  'keynote',\n",
       "  'affective_computing',\n",
       "  'united_nations',\n",
       "  'net_',\n",
       "  'foreign',\n",
       "  'uniquely',\n",
       "  'boosts',\n",
       "  'neglect',\n",
       "  'disease_diagnosis',\n",
       "  'transferred',\n",
       "  'succinctly',\n",
       "  'ant_colony',\n",
       "  'volume',\n",
       "  'normalized',\n",
       "  'birth',\n",
       "  'human_like',\n",
       "  'probably',\n",
       "  'ned',\n",
       "  'nabu',\n",
       "  'peak',\n",
       "  'predefined',\n",
       "  'comprises',\n",
       "  'label',\n",
       "  'away',\n",
       "  'direction',\n",
       "  'casrel',\n",
       "  'caselaw',\n",
       "  'operating',\n",
       "  'recognised',\n",
       "  'centers',\n",
       "  'describes',\n",
       "  'characterized',\n",
       "  'cell',\n",
       "  'intra_bag',\n",
       "  'language_agnostic',\n",
       "  'incorrectly',\n",
       "  'google_news',\n",
       "  'social_intelligence',\n",
       "  'netizens',\n",
       "  'accommodate',\n",
       "  'reside',\n",
       "  'said',\n",
       "  'deadly',\n",
       "  'handle',\n",
       "  'mixture_model',\n",
       "  'cyber_threat',\n",
       "  'substring',\n",
       "  'long_short_term_memory',\n",
       "  'tracking',\n",
       "  'tweet_date',\n",
       "  'labeled',\n",
       "  'pattern_recognition',\n",
       "  'segmented',\n",
       "  'diagnostic',\n",
       "  'area',\n",
       "  'stem_cell',\n",
       "  'att_cnn_bigru_crf',\n",
       "  'artificially',\n",
       "  'basically',\n",
       "  'unusual',\n",
       "  'recognition',\n",
       "  'typing',\n",
       "  'radar',\n",
       "  'intelligently',\n",
       "  'vector_space',\n",
       "  'immune',\n",
       "  'microchips',\n",
       "  'allergens',\n",
       "  'politician',\n",
       "  'automating',\n",
       "  'were',\n",
       "  'count',\n",
       "  'unrecognized',\n",
       "  'cells',\n",
       "  'multi_vote_based',\n",
       "  'long_sequence',\n",
       "  'object',\n",
       "  'bert_sentence',\n",
       "  'international',\n",
       "  'stay',\n",
       "  'author',\n",
       "  'dalvi',\n",
       "  'attracting',\n",
       "  'circumvent',\n",
       "  'detected',\n",
       "  'miss',\n",
       "  'automation',\n",
       "  'thailand',\n",
       "  'nuclear_technology',\n",
       "  'flamenco',\n",
       "  'electronic_medical_record',\n",
       "  'intended',\n",
       "  'internal_medicine',\n",
       "  'transportation',\n",
       "  'recurrent_unit',\n",
       "  'distinguishable',\n",
       "  'verbalize',\n",
       "  '168',\n",
       "  'misinformation',\n",
       "  'entity_summarization',\n",
       "  'categorization',\n",
       "  'non_automated',\n",
       "  'normally',\n",
       "  'spontaneous',\n",
       "  'billion_scale',\n",
       "  'recognizing',\n",
       "  'diagnoses/medications',\n",
       "  'delineate',\n",
       "  'net',\n",
       "  'drug_like',\n",
       "  're',\n",
       "  'drug_drug',\n",
       "  'characterizing',\n",
       "  'phrase',\n",
       "  'polarized',\n",
       "  '~100',\n",
       "  'viruses',\n",
       "  'nets',\n",
       "  'negating',\n",
       "  'chinese',\n",
       "  'upon',\n",
       "  'these',\n",
       "  'disease',\n",
       "  'refers',\n",
       "  'inharmonious',\n",
       "  'sentimental',\n",
       "  'auto_emotion',\n",
       "  'quantitized',\n",
       "  'spelling',\n",
       "  'terminology',\n",
       "  'persona',\n",
       "  'unconscious',\n",
       "  'billion_level',\n",
       "  'language_generation',\n",
       "  'negations',\n",
       "  'multi_classifier',\n",
       "  'sentiment_analysis',\n",
       "  'encompasses',\n",
       "  'type_description',\n",
       "  'except',\n",
       "  'govern',\n",
       "  'nltk',\n",
       "  'inter_speaker',\n",
       "  'counter_terrorism',\n",
       "  'intend',\n",
       "  'predict',\n",
       "  'hate_speech',\n",
       "  '\\\\rightarrow',\n",
       "  'embedding_based',\n",
       "  'omitted',\n",
       "  'tweetskb',\n",
       "  'boundary',\n",
       "  'pos',\n",
       "  'succinct',\n",
       "  'bosch',\n",
       "  'part_of_speech',\n",
       "  'imported',\n",
       "  'word_formation',\n",
       "  'entity_agnostic',\n",
       "  'intra_entity',\n",
       "  'abbreviations',\n",
       "  'image_caption',\n",
       "  'threat_intelligence',\n",
       "  '//github.com/zjunlp/deepke/tree/main/example/re/multimodal',\n",
       "  'verbalizable',\n",
       "  'patented',\n",
       "  'cluster',\n",
       "  'st_net',\n",
       "  'kinds',\n",
       "  'mislead',\n",
       "  'hashtags',\n",
       "  'big_data',\n",
       "  'persons',\n",
       "  'verb',\n",
       "  'economic_growth',\n",
       "  'bio_medical',\n",
       "  'resemble',\n",
       "  'born',\n",
       "  'dart',\n",
       "  'cyber_attack',\n",
       "  'conve',\n",
       "  'proper_nouns',\n",
       "  'redefined',\n",
       "  'so_called',\n",
       "  'judicious',\n",
       "  'hamming',\n",
       "  'separation',\n",
       "  'text_matching',\n",
       "  'paragraph',\n",
       "  'image_embodied',\n",
       "  'suspected',\n",
       "  'imperceptible',\n",
       "  'intelligence_based',\n",
       "  'incorporate',\n",
       "  'poisoning',\n",
       "  'wrongly',\n",
       "  'accuracies',\n",
       "  'chinese_based',\n",
       "  'inside',\n",
       "  'institute',\n",
       "  'on_topic',\n",
       "  'paragraphs',\n",
       "  'punishment',\n",
       "  'aligner',\n",
       "  'reranking',\n",
       "  'disease_19',\n",
       "  'semantic_entity',\n",
       "  'mental_illness',\n",
       "  '//github.com/yangxi1016/stroke',\n",
       "  'mes',\n",
       "  'polarity',\n",
       "  'concatenate',\n",
       "  'agents',\n",
       "  'characterizes',\n",
       "  'mass',\n",
       "  'mimic_cxr',\n",
       "  '99',\n",
       "  'multibillion_dollar',\n",
       "  'pedestrian',\n",
       "  'numeric',\n",
       "  'naturalistic',\n",
       "  'lies',\n",
       "  'reorganization',\n",
       "  'posture',\n",
       "  'leave',\n",
       "  'locating',\n",
       "  'abnormality',\n",
       "  'itk_net',\n",
       "  'human_computer',\n",
       "  'indonesian',\n",
       "  'lexicons',\n",
       "  'image_captioning',\n",
       "  'cytokines',\n",
       "  'compounds',\n",
       "  'normalize',\n",
       "  'captures',\n",
       "  'eliminates',\n",
       "  'nominal',\n",
       "  'biomedical',\n",
       "  'assamese',\n",
       "  'isolated',\n",
       "  'language_processing',\n",
       "  'manufacturer',\n",
       "  'multi_typed',\n",
       "  'percent',\n",
       "  'anomaly',\n",
       "  'chiefly',\n",
       "  'symptom_in_chinese',\n",
       "  'position_attention',\n",
       "  'implicitness',\n",
       "  'morphome',\n",
       "  'utterance_query',\n",
       "  'node_labeled',\n",
       "  'mentioned',\n",
       "  'expressions',\n",
       "  'refer',\n",
       "  'agreement',\n",
       "  'chittagonian',\n",
       "  'hosted',\n",
       "  'analyzer',\n",
       "  'automatically',\n",
       "  'gluten_related',\n",
       "  'sequence_based',\n",
       "  'room',\n",
       "  'please',\n",
       "  'language_',\n",
       "  'protein_protein',\n",
       "  'n_gram',\n",
       "  'dynamical',\n",
       "  'year',\n",
       "  '3390',\n",
       "  'alarm',\n",
       "  'embeddings',\n",
       "  'directional',\n",
       "  'overwhelming',\n",
       "  'corpora',\n",
       "  'sentence_pair',\n",
       "  'hidden_markov_model',\n",
       "  'intelligence_analysis',\n",
       "  'random_forest',\n",
       "  'quaternion',\n",
       "  'entry',\n",
       "  'human_human',\n",
       "  'unfamiliarity',\n",
       "  'eliminate',\n",
       "  'winds',\n",
       "  'neglects',\n",
       "  'https',\n",
       "  '~',\n",
       "  'combining_character',\n",
       "  'iden_tifying',\n",
       "  'outlier',\n",
       "  'nubot',\n",
       "  'english_language',\n",
       "  'caption',\n",
       "  'properly',\n",
       "  'something',\n",
       "  'involving',\n",
       "  'named_entityrelation_extraction',\n",
       "  'jiangsu',\n",
       "  'cordis',\n",
       "  'overcome',\n",
       "  'arise',\n",
       "  'matter',\n",
       "  'tweet_topic',\n",
       "  'detect_missing',\n",
       "  'specializes',\n",
       "  'endeavor',\n",
       "  'noted',\n",
       "  'nuclear_power',\n",
       "  'fundamen_tal',\n",
       "  'well_recognized',\n",
       "  'utterance.the',\n",
       "  'child',\n",
       "  'insect',\n",
       "  'nrnet',\n",
       "  'put',\n",
       "  'counting',\n",
       "  'drug_target',\n",
       "  'characteristics',\n",
       "  'disambiguated',\n",
       "  'captions',\n",
       "  'song',\n",
       "  'originate',\n",
       "  'crowdsensing',\n",
       "  'tf',\n",
       "  'human_driving',\n",
       "  'military_oriented',\n",
       "  'descriptor',\n",
       "  'topk_precision',\n",
       "  'log_likelihood',\n",
       "  'multilingual',\n",
       "  'positional',\n",
       "  'criminal',\n",
       "  'entirely',\n",
       "  'things',\n",
       "  'host_',\n",
       "  'unlike',\n",
       "  'taxonomic',\n",
       "  'cyberspace',\n",
       "  'terminals',\n",
       "  'naisc',\n",
       "  'arises',\n",
       "  'resembles',\n",
       "  'plenty',\n",
       "  'object/scene',\n",
       "  'pretrained_language_models',\n",
       "  'target_disease',\n",
       "  'synchronous_motor',\n",
       "  'takes',\n",
       "  '2022',\n",
       "  '__',\n",
       "  'everything',\n",
       "  'indicate',\n",
       "  'suicide_knowledge',\n",
       "  'numbers',\n",
       "  'whom',\n",
       "  'quantify',\n",
       "  'suspicious',\n",
       "  'hyperparameters',\n",
       "  'chemical_compounds',\n",
       "  'throw',\n",
       "  'medical_imaging',\n",
       "  'image_text_matching',\n",
       "  'emphasize',\n",
       "  'respective',\n",
       "  'localize',\n",
       "  'lstm_based',\n",
       "  'thai',\n",
       "  'originally',\n",
       "  'kilt',\n",
       "  'represent',\n",
       "  'wireless_communication',\n",
       "  '_',\n",
       "  'fake',\n",
       "  'carbon_capture',\n",
       "  'spatialnet',\n",
       "  'imbalanced',\n",
       "  'toward',\n",
       "  'single_chain',\n",
       "  'entity_annotation',\n",
       "  'behavior',\n",
       "  'liao_dynasty',\n",
       "  'attention_like',\n",
       "  'forensic',\n",
       "  'chemical_industry',\n",
       "  'latter',\n",
       "  'offensive',\n",
       "  'presence',\n",
       "  'destroy',\n",
       "  'aminer',\n",
       "  'uncommon',\n",
       "  'voltage',\n",
       "  'dropping',\n",
       "  'breast_cancer'],\n",
       " 'sentences': ['finally , based on bilstm_crf benchmark model , a model framework suitable for agricultural named_entity recognition was constructed',\n",
       "  'the experimental_results show that ar+bilstm+crf model has excellent performance for named_entity recognition in adf .',\n",
       "  'furthermore , this study uses automatically labeled datasets to train a deep_learning_based named_entity recognition model',\n",
       "  'experiment results demonstrate that our method can deal with biomedical named_entity recognition and obtain significant performances in both english and chinese biomedical datasets .',\n",
       "  'then , a vietnamese named_entity recognition model is proposed based on residual dense block ( rdb ) convolutional_neural_network ( cnn ) .',\n",
       "  'the experimental_results show that compared with the named_entity recognition method based directly on bilstmcrf on the weakly_supervised named_entity recognition in financial field , our proposed method improves f1_ score in the small data training sample set by nearly 9 % , and it has some generalization_ability .',\n",
       "  'therefore , it is very necessary to improve the recognition performance in the process of named_entity recognition , particularly for the sufficient capture of character position , contextual semantic_features , and long_distance dependency information',\n",
       "  'by designing a optimized fine_tuning method , we have realized the named_entity recognition task in the chinese twenty_four histories',\n",
       "  'conclusions : this paper confirms the effectiveness of using an advanced artificial_intelligence method to carry out named_entity recognition tasks on a corpus of a large number of clinical notes ; this application is promising in the medical setting',\n",
       "  'in this work , we present a simple and effective approach for named_entity recognition',\n",
       "  'a series of experiments were performed to construct the corpus of named_entity recognition',\n",
       "  'the results show that the model performs well in hazop named_entity recognition',\n",
       "  'paper_title : adversarial active_learning for named_entity recognition in cybersecurity ; paper_abstract : owing to the continuous barrage of cyber threats , there is a massive amount of cyber_threat intelligence',\n",
       "  'the test results show that the accuracy , recall , and f1 value of the named_entity recognition task in the aquatic medicine using the bert+cabilstm+crf model reached 93.07 % , 92.85 % , and 92.96 % , respectively',\n",
       "  'flat named_entity recognition can use the vector from pre_trained model to obtain the entity from domain_specific text',\n",
       "  'however , there is no publicly available named_entity recognition dataset in the food_safety domain',\n",
       "  'tests show that the recall_rate and the f_score of the bert_bilstm_crf model are 28.48 % and 18.65 % higher than those of a crf_based entity_recognition model , 13.91 % and 8.69 % higher than those of a bilstm_crf_based entity_recognition model , and 7.08 % and 5.15 % higher than those of a cnn ( convolutional_neural_networks ) _bilstm_crf_based model .',\n",
       "  'paper_title : improving chinese named_entity recognition with semantic_information of character multi_position representation ; paper_abstract : named_entity recognition is an important basic task for information_extraction and construction of knowledge_graph , but the recognition rate needs to be further improved , especially in chinese',\n",
       "  'finally , we propose an artificial_intelligence technique for named_entity recognition for mongolian_language resources .',\n",
       "  'the best f1_score of the named_entity recognition task after training using many rheumatoid_arthritis clinical notes was 0.936',\n",
       "  'experiments on the named_entity recognition task and relational classification task demonstrate that the mda can significantly enhance the efficiency of the deep_learning models compared to cases without augmentation .',\n",
       "  'the proposed framework contains three modules , namely domain feature pre_trained model , lstm_based named_entity recognition and the attention_based nested named_entity recognition',\n",
       "  'to evaluate the proposed method , we conduct named_entity recognition experiments using the status reports of complex equipment in nuclear_power plants',\n",
       "  'the model achieves good results , exhibiting the effectiveness in the task of named_entity recognition in the chemical_industry .',\n",
       "  'in this work , we propose a cdiner model to solve the chinese drug information named_entity recognition',\n",
       "  'paper_title : named_entity recognition method in health preserving field based on bert ; paper_abstract : with the aging of the population development , people pay more attention to health preserving',\n",
       "  'paper_title : research on named_entity recognition method based on transfer_learning for small data_sets ; paper_abstract : aiming at the problem of automatic acquisition of important military target entities , a small sample named_entity recognition method combining a lite bidirectional_encoder_representations_from_transformers ( albert ) , bi_gated recurrent_unit ( bigru ) and conditional_random_field ( crf ) was proposed',\n",
       "  'paper_title : improving neural named_entity recognition with gazetteers ; paper_abstract : the goal of this work is to improve the performance of a neural named_entity recognition system by adding input features that indicate a word is part of a name included in a gazetteer',\n",
       "  'the experimental_results show that the bilstm_crf model has a good effect on grape disease entity_recognition , and the f1 value is as high as 95.44 %',\n",
       "  'moreover , bilstm_crf is introduced for named_entity recognition tasks , and the entity_recognition information is utilized by the multi_head selection structure to solve the problem of overlapping relations',\n",
       "  'traditional named_entity recognition ( ner ) methods based on crf model which relies on large_amounts of hand_crafted features , can not extract more effective features and solve the inconsistency of entity_tagging caused by the diversity of entity names',\n",
       "  'paper_title : multi_neural network collaboration for chinese military named_entity recognition ; paper_abstract : web data contains a large amount of high_value military information which has become an important data source for open_source military_intelligence',\n",
       "  'bi_directional long_short_term_memory ( bilstm ) and iterated dilated convolutional_neural_networks ( idcnn ) combined with conditional_random_field ( crf ) form the model bilstm_idcnn_crf , and it is applied to implement named_entity recognition in chinese pesticide data_sets',\n",
       "  'paper_title : pre_trained_language model based medical named_entity recognition ; paper_abstract : medical named_entity recognition is an important part of structuring chinese electronic_medical_records and construction of medical_knowledge graph',\n",
       "  'the experimental_results prove that the model proposed in this paper has achieved excellent results in the task of named_entity recognition in the field of historical culture',\n",
       "  'paper_title : towards bootstrapping biomedical named_entity recognition using reinforcement_learning ; paper_abstract : named_entity recognition is one of the most fundamental problems in knowledge_graph',\n",
       "  'paper_title : chinese named_entity recognition method in history and culture field based on bert ; paper_abstract : with rapid development of the internet , people have undergone tremendous changes in the way they obtain information',\n",
       "  'the experimental_results show that the roberta + flat compared with the traditional method has a better effect of entity_recognition .',\n",
       "  'paper_title : a multi_layer soft lattice based model for chinese clinical named_entity recognition ; paper_abstract : objective : named_entity recognition ( ner ) is a key and fundamental part of many medical and clinical tasks , including the establishment of a medical_knowledge graph , decision_making support , and question_answering systems',\n",
       "  'and a novel deep neural_network model , named ar+bilstm+crf , which combines attention_mechanism , ranger optimizer , bidirectional lstm , and crf , is proposed for named_entity recognition in adf',\n",
       "  'the average precision of entity_recognition reaches ( formula presented',\n",
       "  'finally , a comparative experiment was designed to verify the effectiveness of the proposed recognition',\n",
       "  'to fully utilize domain_specific features , we present a hybrid method consisting of dedicated rules and a machine_learning model for entity_recognition',\n",
       "  'in order to solve the problem of lack of labeled_data , we propose an end_to_end solution that is not based on domain_knowledge , which instead is based on the pre_trained bert_chinese model and integrates the bilstm_crf model for classical_chinese named_entity recognition',\n",
       "  'the accuracy of entity_recognition reached 87.27 %',\n",
       "  'bert deep_learning classification technology is used to build the sample classification model , the named_entity recognition model is trained based on the bi_lstm technology , as well as the cnc equipment fault corpus sample data is trained , recognized , and modeled',\n",
       "  'paper_title : named_entity recognition in human_nutrition and health domain using rule and bert_flat ; paper_abstract : a nutritious and healthy_diet can be widely expected to reduce the incidence of disease , while improving body health after the disease occurs',\n",
       "  'moreover , the body feature is extracted by convolution neural_network ( cnn )',\n",
       "  'higher recognition was achieved than before',\n",
       "  'paper_title : leveraging multi_source knowledge for chinese clinical named_entity recognition via relational graph_convolutional_network ; paper_abstract : objective : external_knowledge , such as lexicon of words in chinese and domain_knowledge graph ( kg ) of concepts , has been recently adopted to improve the performance of machine_learning methods for named_entity recognition ( ner ) as it can provide additional information beyond context',\n",
       "  'the attention_mechanism was proposed to combine auxiliary classification layer with main classification layer to improve the overall performance.finally , it was sent to conditional_random_field to construct an end_to_end deep_learning model framework suitable for veterinary drug name entity recognition.in the experiment , totally 10 643 sentences and 485 711 characters of veterinary drug text were selected to identify four kinds of entities : drug , adverse effect , intake mode , aimal',\n",
       "  'finally , the label with the highest probability is output through the conditional_random_field ( crf ) layer to obtain each characters category',\n",
       "  'paper_title : chinese drug information named_entity recognition based on mscnn and bert_bilstm_crf ; paper_abstract : in the study of drug_drug interactions , knowledge_graph on drugs plays an important role to extract drug features',\n",
       "  'considering the complexity and ambiguity of data , a named_entity recognition method based on bert in the health_preserving field is proposed',\n",
       "  'the entity_recognition part uses the bidirectional_long_short_term memory_convolutional neural networks_conditions random_field ( bilstm_cnn_crf ) model for entity_extraction',\n",
       "  'especially , it achieves up to 11.46 % improvement in mrr for entity prediction with up to 82 times speedup compared to the state_of_the_art baseline .',\n",
       "  'an ensemble model is constructed based on the hidden_markov_model , conditional_random_field ( crf ) algorithm , bidirectional_long_short_term memory ( bi_lstm ) , and bi_lstm_crf deep_learning network , completing the named_entity recognition of the reports',\n",
       "  'paper_title : bern2 : an advanced neural biomedical named_entity recognition and normalization tool ; paper_abstract : in biomedical natural_language processing , named_entity recognition ( ner ) and named_entity normalization ( nen ) are key tasks that enable the automatic extraction of biomedical entities ( e.g',\n",
       "  'the system is publicly available at github \\\\footnote { \\\\url { https : //github.com/wjn1996/mathematical_knowledge_entity_recognition }',\n",
       "  'in this article , we present bern2 ( advanced biomedical entity_recognition and normalization ) , a tool that improves the previous neural network_based ner tool by employing a multi_task ner model and neural network_based nen models to achieve much faster and more accurate inference',\n",
       "  'since the existing entity_recognition methods only focus on the features of character sequence , it isnt easy to achieve excellent recognition results in a professional and complex electrical knowledge corpus',\n",
       "  'paper_title : deep_learning with language_models improves named_entity recognition for pharmaconer ; paper_abstract : background : the recognition of pharmacological substances , compounds and proteins is essential for biomedical relation_extraction , knowledge_graph construction , drug_discovery , as well as medical question_answering',\n",
       "  'in this paper , we conduct our research for the named_entity recognition , which is an important step of knowledge_graph construction in adf',\n",
       "  'whats more , we present a dataset for chinese named_entity recognition , which contains four categories of entities and consists of 864 sentences from status reports',\n",
       "  'the finding can provide an effective entity_recognition in the field of human_nutrition and health .',\n",
       "  'paper_title : chinese named_entity recognition of geological news based on bert model ; paper_abstract : with the ongoing progress of geological survey work and the continuous accumulation of geological data , extracting accurate information from massive geological data has become increasingly difficult',\n",
       "  'current mainstream recognition methods require lots of manpower , which is time_consuming and laborious',\n",
       "  'firstly , the character and vocabulary information were stitched together and pre_trained in the bert model to improve the recognition ability of the model to entity categories',\n",
       "  'paper_title : a chinese named_entity recognition method based on ernie_bilstm_crf for food_safety domain ; paper_abstract : food_safety is closely related to human health',\n",
       "  'finally , a conditional_random_field ( crf ) is used to realize character_level sequence annotation and then realize the named_entity recognition task of chinese rice variety information',\n",
       "  'furthermore , a large_scale annotated dataset for a chinese named_entity recognition ( ner ) task is established , which provides support for research on chinese ner tasks .',\n",
       "  'correspondingly , the named_entity recognition model in the field of nutrition and health using fusion rules and the bert_flat model presented an accuracy_rate of 95.00 % , a recall_rate of 88.88 % , and an f1_score of 91.81 %',\n",
       "  'however , the traditional named_entity recognition method has certain defects , and it is easy to ignore the association between entities',\n",
       "  'paper_title : recent progress of named_entity recognition over the most popular datasets ; paper_abstract : named_entity recognition ( ner ) has been considered as an initial step for many applications and tasks such as information_retrieval and extraction , question_answering , topic modelling , open_information_extraction , knowledge_graph construction , and so forth',\n",
       "  'the model achieved good result in the agricultural corpus , and the recognition precision , recall , and f_score were respectively 93.48 % , 90.60 % and 92.01 %',\n",
       "  'in order to improve the effect of entity_recognition , this paper proposes entity_recognition based on the bilstm_lpt and bilstm_hanlp models',\n",
       "  'this paper proposes a method for chinese named_entity recognition based on character sequence and word sequence to solve this problem',\n",
       "  'the traditional named_entity recognition ( ner ) model mainly includes hmm , crf , bilstm , bilstm_crf , etc',\n",
       "  'due to the scarcity of labeled ane data , some existing open agricultural entity_recognition models rely on manual features , can reduce the accuracy of entity_recognition',\n",
       "  'the albert_bigru_attention model was based on the results of the named_entity recognition model',\n",
       "  'from a quantitative point of view , we evaluate contextminer as a pre_processing step to perform named_entity recognition ( ner )',\n",
       "  'however , the low recognition accuracy of named_entities has posed a great challenge to the diagnosis and treatment of aquatic_animal diseases',\n",
       "  'the experimental_results show that the recognition accuracy of the embert_bilstm_crf model for the four types of entities was 94.97 % , and the f1_score was 95.93 %',\n",
       "  'the neural cnn_blstm_crf model and the chinese word_segmentation model with dictionary knowledge are jointly trained to build an unified named_entity recognition model which shares the entity types and confidence and changes the computing order from serial to parallel to decrease the error accumulation',\n",
       "  'the nested named_entity recognition based on the attention_mechanism and the weight sliding balance strategy can effectively identify entity types with higher nesting rates',\n",
       "  'in this study , a novel chinese named_entity recognition of agriculture was proposed using embert_bilstm_crf model',\n",
       "  'paper_title : named_entity recognition of agricultural based entity_level masking bert and bilstm_crf ; paper_abstract : an intelligent question_answering of agricultural knowledge can be one of the most important parts of information agriculture',\n",
       "  'compared with the existing named_entity identification methods , the precision rate , recall_rate , and f1 value have been significantly improved .',\n",
       "  'bi_gat_crf also solves the problem of word ambiguity and improves the accuracy of named_entity recognition tasks',\n",
       "  'paper_title : bert_based named_entity recognition in chinese twenty_four histories ; paper_abstract : named_entity recognition in classical_chinese plays a fundamental role in improving the ability of information_extraction and constructing knowledge_graphs from classical_chinese',\n",
       "  'the recognition effect of this model is better than that of lstm_crf , bert_lstm_crf , bert_crf and other models , and the f1=93.81 % .',\n",
       "  'based on these methods this paper proposes a bert_based named_entities identification model bert_bilstm_crf and it is outperforming the established methods',\n",
       "  'for the widely distributed entities , in this paper , we propose an end_to_end named_entity extraction framework , which uses popular deep_learning based approach , known as conditional_random_field ( crf ) , bidirectional_long short_term memory ( bi_ lstm+crf ) and bert+bi_lstm+crf for training and testing the named_entities',\n",
       "  'paper_title : deep_learning_based named_entity recognition and knowledge_graph for accidents of commercial_bank ; paper_abstract : with the diversified development of business , the construction of the banking system has become increasingly complex , which is prone to accidents',\n",
       "  'paper_title : an entity_recognition model based on deep_learning fusion of text feature ; paper_abstract : nowadays a large amount of knowledge has been born on the internet and the way of constructing knowledge_graph is not uniform',\n",
       "  'paper_title : a supervised named_entity recognition method based on pattern_matching and semantic verification ; paper_abstract : named_entity recognition is a basic task in the field of natural_language processing and plays a pivotal role in tasks such as information_extraction , machine_translation , and knowledge_graph construction',\n",
       "  'the experimental_results show that the multi_feature embedding algorithm and local features extracted by cnn can effectively_improve the recognition effect of the entity_recognition model .',\n",
       "  \"compared with other comparison models , the recognition effect of the bilstm_crf model is relatively stable , which to a certain extent verifies that the model has better recognition performance in the task of identifying grape diseases and pests ' entities .\",\n",
       "  'based on the knowledge_enhanced representations of tokens , we deploy a conditional_random_field ( crf ) layer for named_entity label prediction',\n",
       "  'we propose an improved model named boundary detection and category prediction ( bdcp ) for nested ner',\n",
       "  'so , this paper proposes a chinese rice variety information named_entity recognition method based on a bidirectional_long_short_term memory network and conditional_random_field ( bilstm_crf ) , which combines radical features , word_segmentation boundary features , and multi_head attention_mechanism',\n",
       "  'paper_title : named_entity recognition of chinese agricultural text based on attention_mechanism ; paper_abstract : agricultural named_entity recognition is a fundamental tasks for natural_language processing in the agricultural field',\n",
       "  'first , a bidirectional_long_short_term memory neural_network ( bilstm ) was used for two_class entity_identification',\n",
       "  'based on the data_set of this evaluation task , pre_trained_language model based entity_recognition approaches are studied first , select the bilstm_crf model based on random initialized word_embedding as the baseline system ; secondly , apply wordlvec to the baseline system ; thirdly , apply elmo to the baseline system',\n",
       "  'this paper proposes a method of weakly supervised_learning to recognize the complex named_entities ( commonly composed of multiple small entity sequences , hereinafter referred to as cnes ) in the corpus , which makes it difficult to determine the boundaries of such entities',\n",
       "  'paper_title : a novel named_entity recognition algorithm for hot strip rolling based on bert_imseq2seq_crf model ; paper_abstract : named_entity recognition is not only the first step of text information_extraction , but also the key process of constructing domain_knowledge graphs',\n",
       "  'after that , the bilstm+crf model was used for the outer entity_recognition to improve the discrimination of outer entities for the accurate recognition of outer entities',\n",
       "  'in order to build a knowledge_graph of health_preserving field , named_entity recognition is required first',\n",
       "  'secondly , large_scale pretraining language_model , bert was used for agriculture named_entity recognition and provided a pretty well initial parameters containing a lot of basic language knowledge',\n",
       "  'named_entity recognition ( ner ) is a fundamental task for building knowledge_graph',\n",
       "  'in view of the large amount of text data , complex process flow and urgent application needs in the hot strip rolling process , a novel named_entity recognition algorithm based on bert_imseq2seq_crf model is proposed in this paper',\n",
       "  'in order to improve the entity_recognition effect and address the problem of polysemy in nuclear_technology knowledge data_set , an improved nuclear_technology entity_recognition method based on the bert_bilstm_crf combination model was proposed by comparative experiments',\n",
       "  'paper_title : an effective deep_learning method with multi_feature and attention_mechanism for recognition of chinese rice variety information ; paper_abstract : in the process of chinese rice variety information named_entity recognition , traditional methods can not extract potential semantic_information from data and can not capture long_distance dependence',\n",
       "  'the experimental_results showed that our model achieved an f1 of 98.31 % with 4.23 % relative improvement compared to the baseline model ( i.e. , word2vec_based bilstm_crf ) on the self_annotated corpus named chinese named_entity recognition dataset for agricultural diseases and pests ( agcner )',\n",
       "  'this study aims to develop a multi_feature entity_recognition model that considers the differences in text features across different fields',\n",
       "  'this research can not only provide arelatively high entity_recognition accuracy for tasks such as agricultural intelligence question_answering , but also offer new ideas for the identification of chinese named_entities in fishery , animal_husbandry , chinese_medical , and biological fields .',\n",
       "  'a multi_neural network collaboration approach is then developed based on a named_entity recognition model',\n",
       "  'the ccks2019 conference organized a medical named_entity recognition evaluation task to extract six types of medical entities from unstructured chinese electronic_medical_records',\n",
       "  'the framework achieves good results in the field of nuclear_power plant maintenance reports , and the methods for domain pre_trained model and lstm_based flat named_entity recognition have been successfully applied to practical tasks .',\n",
       "  'the training goal for gn is to deceive dn to make wrong classification',\n",
       "  'we first train a convolutional_neural_networks _ long short_term_memory networks ( cnn_lstm ) model to generate a template caption based on the input image',\n",
       "  'named_entity recognition is a crucial step to construct a knowledge_graph',\n",
       "  'the modeling is done using a recurrent neural_network or a factorization machine',\n",
       "  'most existing state_of_the_art_methods are primarily based on convolutional_neural_networks ( cnn ) or long_short_term_memory networks ( lstm )',\n",
       "  'to extract useful information from multi_source data , we adopt the pre_trained bert_crf model to conduct named_entity recognition for incidents records',\n",
       "  'the experimental that results show compared with lattice lstm , id_cnn+crf , crf models , the f1_score of the proposed model reaches 86.86 % on tourism datasets and 95.02 % on resume datasets , displaying high recognition accuracy of the model.02 % on resume datasets , displaying high recognition accuracy of the model.02 % on resume datasets , displaying high recognition accuracy of the model .',\n",
       "  'to identify the entities from these datasets , a rule_based and deep_learning hybrid method is proposed',\n",
       "  'paper_title : recognition of animal drug pathogenicity named_entity based on att_aux_bert_bilstm_crf ; paper_abstract : in order to solve the problems that traditional methods of veterinary drug named_entity recognition rely on artificial design features , which is time_consuming and labor_consuming , and the amount of veterinary drug pathogenic corpus data is less in the process of building veterinary drug pathogenic knowledge_graph , a method based on att_aux_bert_bilstm_crf of veterinary drug text named_entity recognition model was proposed , which combined bert_bilstm_crf models by introducing attention_mechanism and auxiliary classification layer.the text was vectorized by the bert preprocessing model , and then connected to bi_directional long_short term memory network.the auxiliary classification mechanism was introduced , the output of the bert layer was used as the auxiliary classification layer , and the output of the bilstm layer was used as the main classification layer',\n",
       "  'in bert_crf architecture , begin , internal and other ( bio ) labeling rule was used to label the sequence , and the concatenation of character vector and position vector was used as inputs',\n",
       "  'to test the effectiveness of our system , we perform both automatic and manual evaluation of our intent_classifier and slot_filling system on three dialog datasets',\n",
       "  'bi_gat_crf aims to solve the problem of unclear entity boundary recognition and word ambiguity in named_entity recognition tasks',\n",
       "  'the deep_learning method is adopted in this paper and is based on the existing hazop data to recognize the named_entity of hazop text',\n",
       "  'paper_title : named_entity recognition for instructions of chinese_medicine based on pre_trained_language model ; paper_abstract : named_entity recognition ( ner ) of chinese_medicine text is a basic task of constructing medical and health knowledge_graph',\n",
       "  'experiments on two publicly available task oriented dialog datasets show that our proposed fg2seq achieves robust performance on generating appropriate system responses and outperforms the baseline systems .',\n",
       "  \"the well_designed memory component can get rid of the pre_training so that the model does n't depend on the given target entity for training\",\n",
       "  'the bert model combined with the word fusion performed the best , compared with that without the bert model , indicating an effective recognition performance',\n",
       "  'named_entity recognition is a key to construct knowledge_graph',\n",
       "  'paper_title : chinese named_entity recognition method based on albert_bgru_crf ; paper_abstract : named_entity recognition ( ner ) is an important basis for upper_level natural_language processing tasks such as knowledge_graph constructionsearch enginesand recommendation systems.chinese ner labels and classifies proper_nouns or specific named_entities in a text sequence.aiming at the problem that the existing chinese ner methods can not effectively extract long_distance semantic_information and solve the problem of polysemythis study proposes a chinese ner method based on albert pre_training language modelbidirectional gated recurrent_unit ( bgru ) and conditional_random_field ( crf ) called albert_bgru_crf model.firstthe albert pre_trained_language model performs word_embedding on the input text to obtain dynamic word vectorswhich can effectively solve the polysemy problem.secondbgru extracts contextual semantic_features to further understand semantics and obtain semantic_features between long_distance words',\n",
       "  'these models are tested on real medical records , and the experimental_results show that the method can effectively identify the entities , and has certain practical value .',\n",
       "  'the recognition accuracy , recall , and f1 value increased by 12.31 , 12.76 , and 12.53 percentage points , respectively',\n",
       "  \"the generated word_vectors were then inputted into the model , which is composed of traditional bidirectional_long_short_term memory neural_networks and conditional_random_field machine_learning algorithms for the named_entity recognition of clinical notes to improve the model 's effectiveness\",\n",
       "  'the recognition accuracy for 202 documents selected by experts was 79 % for the coincidence of names and 37 % for the coincidence of term identifiers',\n",
       "  'the experimental_results verify the effectiveness of the bertala and knowledge distillation in agricultural entity_recognition .',\n",
       "  'moreover , the current end_to_end neural models trained on small crowd_sourcing datasets ( e.g. , 10k dialogs in the redial dataset ) tend to overfit and have poor chit_chat ability',\n",
       "  'it has been verified that the recognition rate of the model basically meets the requirements , and on this basis , a recommended solution for repairing equipment faults is proposed , which realizes the effective use of fault knowledge .',\n",
       "  'paper_title : bdcp : an improved nested named_entity recognition model based on lstm ; paper_abstract : in construction of knowledge_graphs ( kgs ) , named_entity recognition ( ner ) is a sub_task to identify the boundaries of entities with special meaning and predict their categories in texts',\n",
       "  'under the framework of positive unlabeled learning , the algorithm performs entity_extraction through two stages of entity determination and entity_classification',\n",
       "  'these problems make the recognition of chinese named_entities for online texts more challenging',\n",
       "  'in this study , a diagnosis and treatment of aquatic_animal diseases named_entity recognition was proposed using bert+cabilstm+crf ( bidirectional_encoder_representations_from_transformers+cascade_bi_directional long_short_term_memory+conditional_random_field )',\n",
       "  'in addition , this paper utilizes svm ( support_vector_machines ) to compare the effect of question classification , and utilizes bilstm_crf model to compare the effect of feature words recognition',\n",
       "  'excessive experimental_results show that the kaercnn model proposed in this study is significantly better than traditional machine_learning algorithms in terms of classification accuracy , the f1_score , and practical application effects',\n",
       "  'finally , the results of bert model was compared with the convolutional_neural_network and the piecewise convolutional_network model',\n",
       "  'to overcome the scarcity of labeled named agricultural entity data , weakly named_entity recognition label on agricultural texts crawled from the internet was built with the help of agrikg',\n",
       "  'military named_entity recognition faces some unique challenges not seen in searches for named_entities in other domains , such as military named_entity boundaries being vague and difficult to define , lack of standardized military terms in internet media , extensive use of abbreviations , and the lack of a public military_oriented corpus',\n",
       "  'the results_showed that the model can effectively identify the entities in the veterinary drug pathogenic text , and the f1 value of recognition was 96.7 % .',\n",
       "  'then , neural_networks are used for learning multi_position feature vectors and character_based tagging task',\n",
       "  'based on bert_ala model , bidirectional lstm ( bilstm ) and conditional_random_field ( crf ) were coupled to further improve the recognition precision , giving a bert_ala+bilstm+crf model',\n",
       "  'this is due to the advances in neural_network architectures , increase of computing power and the availability of diverse labeled datasets , which deliver pre_trained , highly accurate models',\n",
       "  'the model performs a multi_scale convolution after the character vector are generated by the bert model , and then to be input to the bi_lstm',\n",
       "  'agriculture named_entity recognition plays a key role in automatic q & a system , which helps obtaining information , understanding agriculture questions and providing answer from the knowledge_graph',\n",
       "  'to solve the above problems and tackle the cner_adp task , a novel chinese named_entity recognition method for agricultural diseases and pests via jointly using radical_embedding and self_attention ( rs_adp ) was proposed',\n",
       "  'firstly , the vietnamese person names , location names , and institution names in vietnamese corpus are collected statistically to build a corresponding entity database to assist the vietnamese named_entity recognition',\n",
       "  'the result showed that the bert_crf model was superior to the others and reported a state_of_the_art_performance',\n",
       "  'furthermore , the chinese named_entity recognition can be confined to the location and semantic_information of characters , due to the long length of agricultural entity and complex naming',\n",
       "  'on the experimental side , we found promising results when the aco_kg model has combined with machine_learning classifiers , namely an increase of 3 % in the classification task .',\n",
       "  'due to scarcity of public dataset for important military targets and many types of entities , albert was used as the generation model of distributed character vectors based on transfer_learning',\n",
       "  'we also fine_tune the proposed masked entity dialogue ( med ) model on smaller corpora which contain dialogues focusing only on the covid_19 disease named as the covid dataset',\n",
       "  'in the first part , bert_bigru_crf model is used to complete the named_entity recognition of power information text',\n",
       "  'finally , to perform personality predictions the resulting embedding matrix was fed to four suggested deep_learning models independently , which are based on convolutional_neural_network ( cnn ) , simple recurrent neural_network ( rnn ) , long short_term_memory ( lstm ) and bidirectional long short_term_memory ( bilstm )',\n",
       "  'paper_title : named_entity recognition for smart_grid operation and inspection domain using attention_mechanism ; paper_abstract : with the rapid development of smart power_grid systems , the field of power_grid operation_and_maintenance is in urgent need of intelligent construction',\n",
       "  'possible intelligent applications based on openkg_covid19 for further development are also described',\n",
       "  'however , the general domain named_entity recognition model can not extract the entities in the network_security domain very well',\n",
       "  'regarding entity_recognition , a bilstm model was used to extract relationships between entities',\n",
       "  'in this work , an approach of model distillation was proposed to recognize agricultural named_entity data',\n",
       "  'experiment results show that the proposed novel model achieves remarkable results for stock_market prediction task .',\n",
       "  'it is urgent to improve the efficiency of breast_cancer diagnosis and treatment through artificial_intelligence technology and improve the postoperative health status of breast_cancer patients',\n",
       "  'many of the existing_methods neglected the role and value of other modal data except images and only relies on low_level image features for disease recognition without utilizing high_level domain_knowledge , leading to poor credibility and interpretability of identification results',\n",
       "  'to explore the named_entity recognition ( ner ) in the field of knowledge_graph construction , the vietnamese grammar and word_formation are analysed deeply in this study , aiming to solve the low recognition precision and low network calculation efficiency in vietnamese named_entity recognition',\n",
       "  'breast_cancer is one of the highest incidences of cancer at present',\n",
       "  'paper_title : crop disease identification and interpretation method based on multimodal deep_learning ; paper_abstract : identification methods of crop diseases based on image modality alone have achieved relative success under limited and restricted conditions',\n",
       "  'it is an important branch in the field of artificial_intelligence',\n",
       "  'experiments on the cosmosqa dataset demonstrate that the proposed cegi model_outperforms the current state_ofthe_ art approaches and achieves the highest accuracy ( 83.6 % ) on the leaderboard .',\n",
       "  'using the multi_head attention_mechanism combined with the bidirectional long and short_term memory network and crf , the accuracy of the neural_network model is further improved',\n",
       "  'therefore , named_entity recognition technology is used to extract named_entities related to food_safety , and building a regulatory knowledge_graph in the field of food_safety can help relevant authorities to regulate food_safety issues and mitigate the hazards caused by food_safety problems',\n",
       "  'given the success of deep_learning and pre_trained_language_models ( lms ) , some lm_based methods are proposed for the kgc task',\n",
       "  'experiments on the cosmosqa dataset demonstrate that the proposed cegi model_outperforms the current state_of_the_art approaches and achieves the accuracy ( 83.6 % ) on the leaderboard .',\n",
       "  'then build a multi_task_learning model , and perform question sentence intent recognition and question sentence entity_recognition at the same time , which can learn the relationship between tasks , improve the effect of the two types of tasks , and shorten the training and inference time , which can effectively reduce training cost',\n",
       "  'paper_title : named_entity recognition in xlnet cyberspace security domain based on dictionary embedding ; paper_abstract : with the increase of network_security incidents , network_security analysts need to analyze massive log information',\n",
       "  'in order to mine the entity information of defect records more efficiently and accurately , this paper proposes a named_entity recognition algorithm to solve the current situation of lack of utilization and analysis of defect records of traction power supply equipment and low efficiency of manual information_processing modes',\n",
       "  'the experimental_results show that the proposed method_achieves better classification results on three open datasets .',\n",
       "  'first , a bert pre_trained_language model is used to encode a single character to obtain a vector representation corresponding to each character',\n",
       "  'methods : inspired by the success of deep_learning with language_models , we compare and explore various representative bert models to promote the development of the pharmaconer task',\n",
       "  'although kg_based approaches prove effective , two issues remain to be solved',\n",
       "  'the experimental_results show that our model is more effective than the current state_of_the_art model ( ccm ) .',\n",
       "  'our experiments demonstrate that the proposed poisoning attacks outperform state_of_art baselines on four kge models for two publicly available datasets',\n",
       "  'therefore , the model can be expected to effectively_improve the accuracy of entity_recognition caused by ambiguity and entity nesting in the task of diagnosis and treatment of aquatic_animal diseases named_entity recognition',\n",
       "  'first , saliency_based attention_mechanism is used in our model , the salient visual_objects and visual features are extracted by using faster region_based convolutional_neural_network ( faster r_cnn )',\n",
       "  'pharmaconer is a named_entity recognition challenge to recognize pharmacological entities from spanish texts',\n",
       "  'entity and relation are the cornerstones of skg ; thus , the task of skg_learning is divided into named_entity recognition and relation_extraction',\n",
       "  'paper_title : multi_feature fusion method for chinese pesticide named_entity recognition ; paper_abstract : chinese pesticide named_entity recognition ( ner ) aims to identify named_entities related to pesticide properties from unstructured chinese pesticide information texts',\n",
       "  'moreover , the transfer matrix of the conditional_random_field ( crf ) algorithm is combined to improve the accuracy of sequence labeling',\n",
       "  'with the increasing computational power , current protein language_models pre_trained with millions of diverse sequences can advance the parameter scale from million_level to billion_level and achieve remarkable improvement',\n",
       "  'compared to existing_methods , mikgi attained the most robust performance with accuracy the highest or near the highest across all tasks',\n",
       "  'finally , crf is used to output the predicted tag sequence',\n",
       "  'paper_title : hierarchical transformer model for scientific named_entity recognition ; paper_abstract : the task of named_entity recognition ( ner ) is an important component of many natural_language processing systems , such as relation_extraction and knowledge_graph construction',\n",
       "  'experimental_results on the dialogre dataset prove that our proposed model bert_mg outperforms the sota baselines .',\n",
       "  'firstly , use the bilstm+crf model of neural_network to realize ner in military field',\n",
       "  'experimental_results have proved that the proposed classifier performs better compared to existing systems .',\n",
       "  'to address this issue , we present a named_entity recognition based framework that accurately extracts covid_19 related information from clinical test results articles , and generates an efficient and interactive visual knowledge_graph',\n",
       "  'compared with traditional machine_learning methods , e.g',\n",
       "  'considering that the task of agriculture named_entity recognition relied heavily on low_end semantic_features but slightly on high_end semantic_features , an attention_based layer aggregation mechanism for bert ( bert_ala ) was designed in this research',\n",
       "  'compared with the four traditional machine_learning algorithms , the accuracy_rate is increased by about 14 % on average , and the f1_score is increased by about 13 %',\n",
       "  'paper_title : semi_supervised geological disasters named_entity recognition using few labeled_data ; paper_abstract : the geological disasters named_entity recognition ( ner ) method aims to recognize entities reflecting disaster event information in unstructured texts to construct a geohazard knowledge_graph that can provide a reference for disaster emergency response',\n",
       "  'the experimental_results show that our model is able to achieve a superior_performance than these existing_methods .',\n",
       "  'to tackle it , in this study , the authors aim to predict a new entity given few reference instances , even only one training instance',\n",
       "  'experiments show that the intelligent question_answering system with the multi_question classification is effective and highly accurate in answering questions in the field of weapon .',\n",
       "  'experimental_results were validated using the hit @ 10 rate entity prediction task',\n",
       "  'then , the bidirectional_long_short_term memory ( bilstm ) and bidirectional_encoder_representations_from_transformers ( bert ) algorithms are adopted to build the named_entity recognition ( ner ) model',\n",
       "  'experiments on forest disease attribute text_corpus show that the precison is 93.15 % , which can be effectively applied to the named_entity recognition task of forest disease text',\n",
       "  'in this paper , our goal is to be able to infer the correct entity given a few training instances , or even only one training instance is available',\n",
       "  'bi_gat_crf can effectively determine entity boundaries and recognize named_entity categories',\n",
       "  'we verify the novelty and accuracy of ep_bot through the experiments .',\n",
       "  'more specifically , we first construct representations for all images of an entity with a neural image encoder',\n",
       "  'paper_title : agricultural named_entity recognition based on semantic aggregation and model distillation ; paper_abstract : with the development of smart agriculture , automatic question and answer ( q & a ) of agricultural knowledge is needed to improve the efficiency of agricultural information acquisition',\n",
       "  'among them , named_entity recognition has been a key technology for intelligent question_answering and knowledge_graph construction in the fields of agricultural domain',\n",
       "  \"bi_lstm improved bert 's insufficient learning ability of the relative position feature , while conditional_random_field models the dependencies of entity_recognition label\",\n",
       "  'paper_title : named_entity recognition in electric_power metering domain based on attention_mechanism ; paper_abstract : named_entity recognition ( ner ) is one key step for constructing power domain_knowledge graph which is increasingly urgent in building smart_grid',\n",
       "  'the methodology is used to create accurately labeled training and test datasets , which are then used to train models for custom entity labeling tasks , centered on the pharmaceutical domain',\n",
       "  'our results show that zsl_kg improves over existing wordnet_based methods on five out of six zero_shot benchmark_datasets in language and vision .',\n",
       "  'but in view of the outstanding nested structure of entities , the model performed better to identify the nested named_entities , such as the clinical symptoms using the named_entity recognition model integrating the bert and cabilstm designed by the hierarchical idea',\n",
       "  'ner ( named_entity recognition ) is the upstream task of knowledge_graph construction , and the quality of the ner model determines the quality of the knowledge_graph to a certain extent',\n",
       "  'paper_title : named_entity recognition in aircraft design field based on deep_learning ; paper_abstract : aircraft design is a kind of knowledge_intensive work involving multi_disciplinary integration , which needs the support of a large amount of knowledge on aircraft design field ( adf )',\n",
       "  'conventional approaches to kgr have achieved promising performance but still have some drawbacks',\n",
       "  'the question analysis module includes named_entity recognition , similarity calculation and question classification',\n",
       "  'we design a branch architecture consisting of the main branch for hoi detection and a supplementary branch for scene recognition',\n",
       "  'research into using kgs for intelligent applications has increased significantly',\n",
       "  'the source_code will be available at https : //github.com/zcy_cqut/macr .',\n",
       "  'paper_title : chinese named_entity recognition in electric_power metering domain based on neural joint learning ; paper_abstract : while the business of electric_power metering is expanding , it is urgent to build an electric_power metering knowledge_graph composed of business information , technical knowledge , industry standards and their internal connections to provide more comprehensive and effective support for the decision_making and development of power_grid',\n",
       "  'the preliminary results suggest that the method_achieves high performance and can help tcm doctors make better diagnosis decisions in practice',\n",
       "  'the accuracy_rate of the proposed approach reaches 95.54 % , and the f1_score reaches 0.901',\n",
       "  'experimental_results_demonstrate that our proposed model_outperforms state_of_the_art models .',\n",
       "  'second , adversarial_training was also introduced to enhance the generalization and robustness in terms of identifying the rare entities',\n",
       "  'furthermore , some promising applications of medsim in drug substitution and drug abuse prevention are presented in case study .',\n",
       "  'however , it remains largely an open_problem how to effectively utilize large and noisy biomedical kg for ddi detection',\n",
       "  'at the same time , compared with the control experiment using only character sequence fea_tures , the f1_score is increased by 2.96 % , and the precision is increased by 4.65 % , which proves the effectiveness of the method proposed in this paper .',\n",
       "  'most of the existing_approaches ignore kgs altogether',\n",
       "  'to alleviate the over_smoothing in high_order chebyshev approximation , a multi_vote based cross_attention ( mvcattn ) with linear computation complexity is also proposed',\n",
       "  'deep_learning have intensively promoted computer vision and natural_language processing ( nlp ) both',\n",
       "  'in order to solve the problem of fast and accurate identification of the named_entities which entity types were pre_defined , a bidirectional encoder representations from transformers_conditional random_field ( bert_crf ) architecture was proposed to solve the task of named_entity recognition ( ner ) in the area of fresh egg supply_chain',\n",
       "  'this intelligent system , which has high value for the treatment of internal_medicine diseases , can effectively solve health issues and reduce the cost of the consultation .',\n",
       "  'this paper proposes an entity_recognition method based on a joint learning model which considers the feature of chinese word_segmentation and ideas of multi_task_learning in the electric_power metering domain',\n",
       "  'to assist the health sector in combating this deadly disease , the authors developed a deep_learning strategy for diabetes named_entity extraction based on a fusion of text characteristic and relationship_extraction utilizing text data as the object',\n",
       "  'it significantly_outperforms the existing_methods and achieves the state_of_the_art_performance .',\n",
       "  'experimental_results show that our proposed system can achieve better performance than other models , and possess great interactivity and accuracy',\n",
       "  'to address the above problem , this paper proposes a deep_learning_based ner model ; namely , the deep , multi_branch bigru_crf model , which combines a multi_branch bidirectional gated recurrent_unit ( bigru ) layer and a conditional_random_field ( crf ) model',\n",
       "  'furthermore , to accommodate the features of both the image and text modalities , we employ a step_by_step training strategy to train the proposed neural_network model',\n",
       "  'paper_title : named_entity recognition of fresh egg supply_chain based on bert_crf architecture ; paper_abstract : recognizing named_entities from raw text is the first step to construct a fresh egg supply_chain knowledge_graph and support a variety of downstream natural_language processing tasks',\n",
       "  'recently , deep neural network_based models have attained very good results in ner',\n",
       "  'the experimental_results showed that the proposed model could effectively recognize the named_entities of agricultural pests and diseases without feature engineering',\n",
       "  'the results show that the entity information recognition algorithm proposed in this paper can effectively and accurately mine the defect record information of traction power supply equipment .',\n",
       "  'results : compared with the traditional word2vec word_vector model , the performance of the bert pre_training model to obtain a word_vector as model input was significantly improved',\n",
       "  'the results demonstrate that our approach achieves state_of_the_art_performance and outperforms many of the existing_approaches .',\n",
       "  'experiments show that the ctd_blstm model obtains higher accuracy and recall_rate than blstm in the chinese_medical named_entity recognition and entity_relationship extraction',\n",
       "  'a variety of artificial_intelligence application products continue to appear , playing various important roles in many fields , such as ai + agriculture , ai + medical , ai + autonomous driving , ai + education , etc',\n",
       "  'in the bilstm model , the context feature of the target entity was learned from the bert output',\n",
       "  \"we aim to develop a model that will be able to give a classification of the disease that the patient might be suffering from after inputting the patient 's symptoms in english or their native language using voice commands or text\",\n",
       "  'the nlp model identified eight types of entities with a recognition accuracy of up to 94.22 %',\n",
       "  'paper_title : panner : pos_aware nested named_entity recognition through heterogeneous graph neural_network ; paper_abstract : nested named_entity recognition ( nested ner ) in knowledge_graph ( kg ) aims at obtaining all meaningful entities , including nested entities for sentences in longer text region',\n",
       "  'emotion recognition is part of affective_computing , which aims to recognize how the person feels , such as happy , sad , anger , disgust , fear , surprise',\n",
       "  'the ckgr successfully narrows the gap between humans and machines',\n",
       "  'aiming at the problem of context memory , a bidirectional gru neural_network is used to fuse the input vectors',\n",
       "  'to alleviate the over_smoothing in high_order chebyshev approximation , a multi_vote_based cross_attention ( mvcattn ) with linear computation complexity is also proposed',\n",
       "  'according to the experiment results , the accuracy_rate , recall_rate , and f1_score of the bilstm_idcnn_crf model in the chinese pesticide data_set were 78.59 % , 68.71 % , and 73.32 % , respectively , which are significantly better than other compared models',\n",
       "  'results : the experimental_results show that deep_learning with language_models can effectively_improve model performance on the pharmaconer dataset',\n",
       "  'these reviewed results could be used to further improve the machine_learning models',\n",
       "  'by further fine_tuning with image_report pairs , kgae consistently outperforms the current state_of_the_art models on two datasets .',\n",
       "  'which compared with the models based on bilstm_crf and bert_bilstm_crf , the recognition performance of embert_bilstm_crf is significantly improved , proved that used pre_trained_language model as the a word_embedding layer can represent the characteristics of characters well and the entity_level masking strategy can alleviate the bias caused by incomplete semantics , thereby enhanced the chinese semantic representation ability of the model , so that enabling the model to more accurately identify chinese agricultural named_entities',\n",
       "  'experiments on five benchmarks show that kg_s2s outperforms many competitive baselines , setting new state_of_the_art_performance',\n",
       "  'the experimental_results of the example are great , which show that the identification method proposed in this paper has theoretical value and practical application value .',\n",
       "  'to improve the recognition accuracy , our method masked_bilstm_crf is proposed to separate the context semantic relationship determination from the entity boundary confirmation',\n",
       "  'in recent_years , deep_learning technologies have developed rapidly such as deep neural_network , attention_mechanism , deep reinforcement_learning and so on',\n",
       "  'the performance of selfkg suggests that self_supervised learning offers great potential for entity_alignment in kgs',\n",
       "  'in order to make better use of these data resources , this paper introduces a process and method to build a knowledge_graph of spleen and stomach diseases in tcm , and takes the emr of tcm for spleen and stomach diseases and related literature inscriptions as data_sources , and selects four commonly used named_entity recognition ( ner ) models for comparative experiments of ner',\n",
       "  'paper_title : chinese named_entity recognition method in electricity based on combining_character sequence and word sequence ; paper_abstract : chinese named_entity recognition in the power field is critical in building a high_quality knowledge_graph of power_equipment fault',\n",
       "  'the source_code is available at https : //github.com/thudm/cogkr .',\n",
       "  'traditional entity_recognition methods of diseases and insect pests highly rely on artificial design features',\n",
       "  'our method_achieves better performance of relationship_extraction and entity name recognition , which helps to construct the knowledge_graph more accurately .',\n",
       "  'we trained custom named_entity recognition ( ner ) model and constructed a cyber_security knowledge_graph ( ckg ) to infer the subjective relevance of the cyber_security text to the user and to generate correlation features',\n",
       "  'the higher quality and the more accurate recognition of the model were achieved , as the deep_learning model was trained to learn more data',\n",
       "  'this model uses bert to fine_tuning character embedding through contextual_information , the problem of polysemy is solved and the performance of entity_recognition of chinese_tea text is improved',\n",
       "  'the model is fine_tuned',\n",
       "  'and the answer accuracy of the question_answering system can reach about 81 % .',\n",
       "  'the improved model is used in chinese named_entity recognition and entity_relationship extraction in the chinese_medical field , named co_training double word_embedding conditioned blstm ( ctd_blstm )',\n",
       "  'military named_entity recognition is a basic , key task for information_extraction , question_answering and knowledge_graphs in the military domain',\n",
       "  'to extract entities from a large amount of historical and cultural information more accurately and efficiently , this paper proposes one named_entity recognition model combining bidirectional_encoder_representations_from_transformers and bidirectional_long_short_term memory_conditional random_field ( bert_bilstm_crf )',\n",
       "  'paper_title : fine grained named_entity recognition via seq2seq framework ; paper_abstract : fine_grained named_entity recognition ( ner ) is crucial to natural_language processing ( nlp ) applications like relation_extraction and knowledge_graph construction',\n",
       "  'paper_title : deep_learning_based named_entity recognition and knowledge_graph construction for geological hazards ; paper_abstract : constructing a knowledge_graph of geological hazards literature can facilitate the reuse of geological hazards literature and provide a reference for geological hazard governance',\n",
       "  \"diabetes is a severe disease that affect people 's health\",\n",
       "  'thus , the qa system based on kg is still faced with difficulties',\n",
       "  'the problem is formulated as a token classification task similar to named_entity extraction',\n",
       "  'the radicalsand drug names were mostly composed of chemical_elements , while the disease names were mostly ended with the word disease , indicating a higher recognition accuracy than that in the nested entities',\n",
       "  'the aim of bert_ala was to adaptively aggregate the output of multiple hidden layers of bert',\n",
       "  'the research method of this study is that we conducted a named_entity recognition task and identified the key relation_types to construct a cryptocurrency anti_money laundering knowledge_graph ( kg )',\n",
       "  'the experimental_results demonstrated that our model outperformed the state_of_the_art work , and achieved a 17.1 % improvement on the f1 values .',\n",
       "  'we propose an approach based on computer vision methods to recognize human_object interaction ( hoi )',\n",
       "  'therefore , these lstm_based models that have achieved high accuracy generally require long training times and extensive training_data , which has obstructed the adoption of lstm_based models in clinical scenarios with limited training time',\n",
       "  'the named_entity recognition task is one of the key tasks in the implementation of the knowledge_graph , which is of great significance for extracting entity information from unstructured_data , namely the hazardous chemical accidents records',\n",
       "  'like any other machine_learning model , these classifiers are very dependent on the size and quality of the training dataset',\n",
       "  'our code_base and the datasets used with detailed instructions for reproducibility is publicly hosted 1.1https : //github.com/mandar_sharma/tcube',\n",
       "  'results : our proposed method_achieves the best performance on ccks2017 and ccks2018 in chinese with f1_scores of 91.88 % and 89.91 % , respectively , significantly outperforming existing_methods',\n",
       "  'specifically , we develop the bag_of_entity ( boe ) loss and the infusion loss to better integrate kg with crs for generating more diverse and informative_responses',\n",
       "  'the segmented maximum pool layer of the pcnn model masked the word unit instead of characters when executing the masked_language model ( mlm )',\n",
       "  'it is also a high demand for the accurate identification of named_entities',\n",
       "  'finally , we completed a historical behavior driven question_answering platform to serve query for elderly',\n",
       "  'although considerable efforts have been made to recognize biomedical entities in english texts , to date , only few limited attempts were made to recognize them from biomedical texts in other languages',\n",
       "  'the pre_training language_model ( bert ) was used to obtain the global features of input sequence , and the crf layer was added at the end of the model to introduce hard constraints',\n",
       "  'paper_title : chinese mineral named_entity recognition based on bert model ; paper_abstract : mineral named_entity recognition ( mner ) is the extraction for the specific types of entities from unstructured chinese mineral text , which is a prerequisite for building a mineral knowledge_graph',\n",
       "  'the proposed model is largely divided into two modules which are synchronized during their training',\n",
       "  'on the other hand , traditional kgr methods , broadly categorized as symbolic and neural , are unable to balance both scalability and interpretability',\n",
       "  'extensive_experiments on the dataset redial show that our macr significantly_outperforms previous state_of_the_art approaches',\n",
       "  'its fusion model architecture is bert+bi_lstm+multi_head_self_attention+fc',\n",
       "  'the results show that our method outperforms all the state_of_the_art algorithms by filtering out wrong results and retaining correct ones .',\n",
       "  'our method_achieves higher performance than the baseline based on the exotic dataset .',\n",
       "  'an empirical evaluation demonstrates the effectiveness of dialokg over state_of_the_art_methods on several standard benchmark_datasets .',\n",
       "  'the results demonstrate that our model_outperforms several state_of_the_art baseline methods in terms of capability and accuracy',\n",
       "  'the experimental_results_demonstrate that the proposed model achieves significantly higher performance than previous models .',\n",
       "  'this study aimed to develop an effective method to identify and classify medical entities in the clinical notes relating to ra and use the entity_identification results in subsequent studies',\n",
       "  'our code and kgs will be made publicly available .',\n",
       "  'the focus is the error accumulation problem brought about by the traditional relational extraction_method of named_entity recognition based on rules or sequence labeling',\n",
       "  'furthermore , we show that the different architectures and training strategies lead to different model biases',\n",
       "  'the bert models can obtain competitive performance by using wordpiece to alleviate the out of vocabulary limitation',\n",
       "  'another challenge is that a deep model requires large_scale manually labelled data , which greatly increases manual_labour',\n",
       "  'in particular , our method surpasses the prior state_of_the_art by a large margin on the grailqa leaderboard',\n",
       "  'experimental_results show that the hit score of proposed approach is higher than that of many competitive state_of_the_art_baselines .',\n",
       "  'in recent_years , kgc methods for chinese have made great progress',\n",
       "  'then , we designed a qa system based on a memory_based neural_network and attention_mechanism',\n",
       "  'however , due to the lack of annotated data and the complexity of grammatical rules , named_entity recognition in classical_chinese has made little progress',\n",
       "  'first , the bert model is used to perform pre_training tasks on massive weaponry corpus',\n",
       "  'the former is used to recognize all candidate head entities and tail entities respectively',\n",
       "  'then , we introduce the latest research progress of deep_learning_based topic models in detail , which can be summed up as three different types of models',\n",
       "  'this can be solved by either fine_tuning the pre_trained_models , or by training custom models',\n",
       "  'we also show the effectiveness of each of the question_answering components in detail , including the query intent recognition and the answer generation .',\n",
       "  'the performance of bert on squad dataset shows that the accuracy of bert can be better than human users',\n",
       "  'the resulting dataset can be used for clinical diagnosis and further research on the disease',\n",
       "  'lasagne also includes a novel entity_recognition module which detects , links , and ranks all relevant entities in the question context',\n",
       "  'then , we benchmarked stonkgs against three baseline models trained on either one of the modalities ( i.e',\n",
       "  'to handle this challenge , we present a novel approach to automatically recognize new biomedical entities',\n",
       "  'we proposed an adversarial contextual embeddings_based model named ace_adp for named_entity recognition in chinese agricultural diseases and pests domain ( cner_adp )',\n",
       "  'experimental_results show performance competitive with published models on the hotpotqa dataset .',\n",
       "  'to classify aspect level sentiment , the memory content is constructed by combining the location information and inputting the multi_level gated recurrent_unit for calculating the sentiment characteristics of aspect terms',\n",
       "  'experimental_results_demonstrate that the improvements in each process are effective and our approach achieves better performance than the best team in ccks2019 competition .',\n",
       "  'extensive_experiments on the benchmark dataset demonstrate that our proposed model achieves much better performance than state_of_the_art_methods .',\n",
       "  'the results show that our model_outperforms others',\n",
       "  'paper_title : named_entity recognition for science and technology policy dynamics ; paper_abstract : dynamic text of science and technology policy reflects the latest intelligence in the field of science and technology policy',\n",
       "  'the experimental_results_demonstrate that our proposed approach outperforms state_of_the_art_methods on both versions of the large_scale benchmark new york times dataset',\n",
       "  'the experimental_results show that the proposed method outperforms other state_of_the_art algorithms .',\n",
       "  'many recent studies focus on improving neural model structures',\n",
       "  'then , the bidirectional_long_short_term memory neural_network is used to extract sequence features before predicting the spans of named_entities',\n",
       "  'concretely , the preliminaries , summaries of kgr models , and typical datasets are introduced and discussed consequently',\n",
       "  'to this end , we capture a complete view of the proposed system .',\n",
       "  'the achievement of this paper can offer a new method for disease identification based on multimodal data and domain_knowledge , which might help improve the intelligence level of crop disease identification .',\n",
       "  'this performance is significantly higher than that of existing pattern_based api misused detectors',\n",
       "  'paper_title : grape diseases and pests named_entity recognition based on bilstm_crf ; paper_abstract : named_entity recognition ( ner ) is one of the foundational and key tasks of knowledge_graph construction',\n",
       "  'then , for the unstructured_text in the data_set , the innovative deep_learning method of bi_gated recurrent_unit ( bigru ) and conditional_random_field ( crf ) model is used to identify the named_entities of forest disease names and therapeutic agents',\n",
       "  'experiments show that our model significantly_outperforms current state_of_the_art_methods',\n",
       "  'finally , the conditional_random_field ( crf ) was utilized to identify entity boundaries and category',\n",
       "  'our source_code is available at https : //github.com/mathisall/hdgcn_pytorch .',\n",
       "  'finally , the model uses the above vectors for classification information',\n",
       "  'experiments on three common kgc datasets_demonstrate the superiority of the proposed ftl_lm , e.g. , it achieves 2.1 % and 3.1 % hits @ 10 improvement over the state_of_the_art lm_based model lp_bert in the wn18rr and fb15k_237 , respectively .',\n",
       "  'experiments show that the f1 value of the proposed model improves 4 % compared with the baseline model .',\n",
       "  'experimental_results_demonstrate the superior_performance of the proposed method over several state_of_the_art_methods .',\n",
       "  'specifically , for entity descriptions , we explore continuous bag_of_words and convolutional_neural_networks models to encode the semantics of entity representations',\n",
       "  'our best performing system achieves a feverous score of 0.23 and 53 % label accuracy on the blind test data .',\n",
       "  'in addition , jieba word_segmentation was used to segment the chinese corpus before the random mask segmentation of the bert model',\n",
       "  'deep_learning has promoted the application of artificial_intelligence ( ai ) techniques to a wide variety of social problems',\n",
       "  'the main method is to change the original sample in a way that is almost imperceptible to the user , and cause an obvious error in the result returned by the model',\n",
       "  'besides , our alignment method can find the correlations between vision and language , resulting in better performance',\n",
       "  'these models achieve an accuracy of 99.27 % and a 98.61 % respectively on the dataset',\n",
       "  'named_entity recognition in network_security domain is an important task to construct knowledge_graph',\n",
       "  'for uni_modal scene ( text modality ) , experiments show that the proposed method surpasses current state_of_the_art_methods on emotion recognition , intent classification , and dialogue act identification tasks',\n",
       "  'at testing stage , the mcts is also combined with the rnn to predict the target node with higher accuracy',\n",
       "  'experimental_results_demonstrate that the proposed method_achieves state of the art results .',\n",
       "  'in addition , we propose the implementation of kgr using a novel neural symbolic framework , with regard to both scalability and interpretability',\n",
       "  'another class of problems exists around target identification tasks',\n",
       "  'firstly , named_entity recognition was realized , then relation_extraction was carried out , and finally , data was imported into the neo4j database to realize the visualization of the knowledge_graph',\n",
       "  'a convolutional attention layer combines the local attention_mechanism and cnn to capture the relationship of local context',\n",
       "  'when classifying the entity tags , we choose crf model as it adds more constraints to avoid position logical problem',\n",
       "  'the proposed model achieved an identification accuracy , precision , sensitivity and specificity of 99.63 % , 99 % , 99.07 % and 99.78 % respectively on a dataset composed of image_text pairs',\n",
       "  'for unstructured_data , bilstm_crf is used for named_entity recognition and then the bert model is used for entity_relationship extraction',\n",
       "  'experiments show that the proposed model performs better than the existing_methods',\n",
       "  'distilling key relations that may affect object recognition is crucially important since treating each region separately leads to a big performance drop when facing heavy long_tail data distributions and plenty of confusing categories',\n",
       "  'the code and data are available at https : //github.com/thudm/selfkg .',\n",
       "  'compared with previous_works , our model achieves an average of 1.6 % improvement ( 2.0 % and 1.5 % improvements in cider and rouge_l , respectively )',\n",
       "  'detailed experimentation shows that our proposed perkg architecture can effectively_improve the performance and alleviate the label sparsity problem of personality analysis .',\n",
       "  'in the system , the question_answering function is realized by template matching , which based on the naive bayes algorithm',\n",
       "  'secondly , according to the classification result , the most similar question template is matched',\n",
       "  'we compare the performance of several choices of methodologies for these sub_tasks using homicide investigation chronologies from los_angeles , california',\n",
       "  'we conduct extensive_experiments on four popular referring segmentation benchmarks and achieve new state_of_the_art performances',\n",
       "  \"results : jieba can accurately identify the herbal name in 'tfds\",\n",
       "  'this kind of kgs generally focus on named_entities , e.g',\n",
       "  'many machine learning_based methods have been proposed for this task',\n",
       "  'the experimental_results_demonstrate that our proposed approach outperforms state_of_the_art the methods on both versions of a large_scale benchmark new york times dataset',\n",
       "  'furthermore , it creates a confusion matrix represents that which intents are ambiguously recognized by approach .',\n",
       "  'experiments on the benchmark dataset show that the effectiveness of nrnet by detailed ablation studies and analysis .',\n",
       "  'our model achieves new state_of_the_art accuracy on the krvqr and fvqa datasets',\n",
       "  'paper_title : model_based clinical note entity_recognition for rheumatoid_arthritis using bidirectional_encoder_representation_from_transformers ; paper_abstract : background : rheumatoid_arthritis ( ra ) is a disease of the immune system with a high rate of disability and there are a large amount of valuable disease_diagnosis and treatment information in the clinical note of the electronic_medical_record',\n",
       "  'many scholars have researched the ner task of electronic_medical_records and drug names , while many factors restrict the research of ner tasks for the instructions of chinese_medicine',\n",
       "  'finally , we compare our model with the state_of_the_art_baselines on two benchmark_datasets , the results of extensive comparison experiments validate the effectiveness of the proposed method .',\n",
       "  'once trained , a machine_learning model is barely portable on a different domain',\n",
       "  'on the experiment of the standard dataset , the bbcm model has a significant_improvement ( f1 value reached 0.9544 ) than the baseline model',\n",
       "  'in the 2021 language and intelligence challenge : multi_skill dialog task , our best model ranked 3rd in the automatic evaluation stage and 5th in the human evaluation stage .',\n",
       "  'although the growth and development of children are rapid , the spleen , stomach and kidney have not yet developed completely',\n",
       "  'however , nearly all previous methods suffer from the problem of error accumulation , i.e. , the boundary recognition error of each entity in step ( 1 ) will be accumulated into the final combined triples',\n",
       "  'experimental_results indicated the proposed deep , multi_branch bigru_crf model outperformed state_of_the_art models',\n",
       "  'experimental_results show that our proposed method_achieves good performance on the agriculture dataset',\n",
       "  'it is found that the precision rate , recall_rate and f1 value of the model are better than pcnn_one and pcnn_ave models',\n",
       "  'meanwhile , this model still maintained over 86 % of f1 value on some other difficultly recognized entities such as weed and pathogeny',\n",
       "  'in experiments , we demonstrate that the kglm achieves significantly better performance than a strong baseline language_model',\n",
       "  'we , then , evaluate the impact of our cognate detection mechanism on neural machine_translation ( nmt ) , as a downstream task',\n",
       "  'first , we release new re model architectures that obtain state_of_the_art f1 scores on 5 out of 7 benchmark_datasets',\n",
       "  'to reduce the cost , insurance inspectors tend to build an intelligent system to detect suspicious claims with inappropriate diagnoses/medications automatically',\n",
       "  'experimental_results_demonstrate that our proposed methods outperform traditional neural symbolic models .',\n",
       "  'furthermore , we propose a novel loss_function to alleviate the false_negative problem during training',\n",
       "  'considering the lack of labeled_data , pretraining language_model was introduced , which is fine tuned with existing labeled_data',\n",
       "  'then , a position code was created for the head and tail position of each character and vocabulary , where the entity position was located with the help of a position vector , in order to improve the recognition of entity boundary',\n",
       "  'the hybrid attention is an adaptation scheme to apply the pre_trained_language model to our model and the copy mechanism is a gate mechanism to control generating a word from generic vocabulary or the input knowledge',\n",
       "  'our system is the winner of track 1 of the lm_kbc challenge , based on bert lm ; it achieves 55.0 % f_1 score on the hidden test set of the challenge .',\n",
       "  'secondly , in order to facilitate the query , this paper establishes entity and relationship/attribute mining based on the continuous bag_of_words ( cbow ) encoding model , bidirectional_long_short_term memory_conditional random_field ( bilstm_crf ) named_entity model , and bidirectional gated recurrent neural_network ( bigru ) intent recognition model for chinese kill chain question and answer ; returns the corresponding entity or attribute values in combination with the knowledge_graph triad form ; and finally constructs the answer return',\n",
       "  'in biomedical field , labeling high_quality biomedical entities requires plenty of linguistic knowledge due to abbreviation and specificity',\n",
       "  'for the spatial representation , we not only adopt a slowfast network to learn global action and scene information , but also exploit the unique cues of face , body and dialogue between characters',\n",
       "  'finally , a model based on the combination of character sequences and word sequences is used to identify chinese named_entities',\n",
       "  'we demonstrate the effectiveness of our system on two datasets in comparison with state_of_the_art models .',\n",
       "  'experiments show that the algorithm proposed in this paper achieves 91.47 % , 88.88 % and 90.16 % in accuracy , recall and f_ { 1 } values while the training convergence speed is smaller than that of the baseline model , which is 1.27 % 4.06 % better than the traditional model',\n",
       "  'in this paper , we propose a named_entity recognition algorithm based on text { bert } +text { bilstm } +text { crf } , which combines attention_mechanism and word_character joint embedding vector , for the text features in the field of grid operation and inspection',\n",
       "  'however , in the medical domain , building a large_scale image_report paired dataset is both time_consuming and expensive',\n",
       "  'the framework combines the perceptual capabilities of computer vision with the cognitive capabilities of kg to improve the accuracy and timeliness of kg updates',\n",
       "  'experimental_results_demonstrate that the proposed approach achieves significantly higher performance compared with other state_of_the_arts .',\n",
       "  'the experimental_results show that the method proposed in this paper is superior than the current mainstream method , which proves the leading role and generalization of the model',\n",
       "  'in this paper , the proposed model combines word_feature into character_feature as characters embedding ; uses a joint model of bilstm and self_attention mechanism to encode characters and a bidirectional label distribution transfer model is used to decode the classification',\n",
       "  'the training_data generator ( tdg ) generates the base training_set for setting up the conversation agent',\n",
       "  'experiments on two opencsr datasets show that the proposed model achieves great performance on benchmark opencsr datasets .',\n",
       "  'yet , this task remains a challenging job for data_driven neural_networks , due to the serious visual and textual data biases',\n",
       "  'the experimental_results_demonstrate that the proposed method can effectively_improve the effectiveness of prediction .',\n",
       "  'specifically , adversarial_training is added to the model training as a regularization method to alleviate the influence of noise on the model , while self_attention is added to the bilstm_crf model to capture features that significant impact entity_classification and improve the accuracy of entity_classification',\n",
       "  'then , accident management based on named_entity recognition and knowledge_graph can be developed',\n",
       "  'the results illustrate that the overall accuracy is 66',\n",
       "  'experimental_results have shown that the proposed classifier outperforms the existing systems , with better domain representation .',\n",
       "  'the bidirectional_encoder_representation_from_transformers ( bert ) pre_training language_model is used as the encoding layer of the word_vector , the bidirectional long short_term_memory ( bilstm ) is used as the character label prediction layer , and the conditional_random_field ( crf ) is used to output the global optimal label',\n",
       "  'recently , some works use graph convolutional_networks to obtain the embeddings of unseen entities for prediction tasks',\n",
       "  'paper_title : improve on entity_recognition method based on bilstm_crf model for the nuclear_technology knowledge_graph ; paper_abstract : the accuracy of entity_recognition is particularly important for knowledge_graph construction',\n",
       "  'the experimental_results show that the proposed approach outperforms previous methods and achieves state_of_the_art_performance',\n",
       "  'inspired by the recent progress of self_supervised learning , we explore the extent to which we can get rid of supervision for entity_alignment',\n",
       "  'experimental_results show that ontoprotein can surpass state_of_the_art_methods with pre_trained protein language_models in tape benchmark and yield better performance compared with baselines in protein_protein interaction and protein function prediction',\n",
       "  'the proposed model_outperforms other popular unsupervised models significantly .',\n",
       "  'in practice , we found that ontology reuse is necessary , and neural_network is the best choice',\n",
       "  'experiments on the large_scale redial dataset demonstrate that the proposed system consistently outperforms state_of_the_art_baselines .',\n",
       "  'with fast developments of deep_learning , this area has attracted great research attention',\n",
       "  'the results show that the model can effectively recognize seven mineral entities with an average f1_score of 0.842 .',\n",
       "  'contextualized representations trained on the mimic_iii database were used to capture context_sensitive meanings of words',\n",
       "  'the result of g_coder on the mimic_iii dataset showed that the micro_f1 score is 69.2 % surpassing the state of art',\n",
       "  'then , using the random_forest algorithm , the standardized classification of entities is accomplished , and the multi_dimensional knowledge_graph network is established',\n",
       "  'conclusion : for the bert models on the pharmaconer dataset , biomedical domain_knowledge has a greater impact on model performance than the native language ( i.e. , spanish )',\n",
       "  'the code and data are publicly available at https : //github.com/macho000/t5_for_kgqg .',\n",
       "  'the second type of the model is named neural_network_based topic model , which employs neural_network structure , such as multilayer_perceptron ( mlp ) or rnn , to model the document generation process with introducing latent topic structure',\n",
       "  'experimental_results show that our method outperforms several state_of_the_art models on benchmark_datasets',\n",
       "  'the generalization_ability of our model is significantly improved by repeating the process of these two components in an alternate way',\n",
       "  'traditional method is tedious , and time_consuming , as well as low accuracy',\n",
       "  'we test the performance of the two models on two chinese_medical datasets : cmeie and cemrds',\n",
       "  'the recognized entities are also used to expand the knowledge_graph generated by dbpedia spotlight for a given pharmaceutical text .',\n",
       "  'the dataset and the word_vectors trained by word2vec are available at github ( https : //github.com/a_mumu/agriculture.git ) .',\n",
       "  'recent_years have witnessed the remarkable success of deep_learning techniques in kg',\n",
       "  'it was shown that the developed architecture is particularly suitable for our field of application .',\n",
       "  'using these data for named_entity recognition to support the construction of knowledge_graphs in the power_grid domain and finally realize the decision intelligence in the power_grid operation and inspection domain is the key of this paper',\n",
       "  'based on the geokg , we propose a bilateral lstm_crf ( long short_term memoryconditional random_field ) model to achieve natural_language question_answering for vges and conduct experiments on the method',\n",
       "  'methods often used to encode sentence , like canonical bidirectional recurrent neural_networks ( birnn ) or convolutional_neural_networks ( cnn ) , are difficult to capture enough information from biomedical text',\n",
       "  'livemedqa system is evaluated in the trec 2017 liveqa medical subtask , where it received an average score of 0.356 on a 3 point scale',\n",
       "  'in experiments , we show that the nklm significantly improves the performance while generating a much smaller number of unknown words .',\n",
       "  'the named_entity recognition task is one of the key tasks for the construction of knowledge_graphs',\n",
       "  'our model improved f1_score better than vncorenlp and underthesea models with 12 % and 17 % respectively',\n",
       "  'the experimental_results_demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans',\n",
       "  'by leveraging this discovery , we develop the self_supervised learning objective for entity_alignment',\n",
       "  'secondly , the method based on dictionary technology and aho_corasick ( ac ) automaton is used to realize fast entity_recognition and question word extraction',\n",
       "  'the results show that our method_achieves superior_performance than baselines',\n",
       "  'the experiment results of our models clearly affirm the effectiveness and superiority of our models against baseline .',\n",
       "  \"based on user 's configuration specified , the system can automatically train and test the model , conduct extensive experimental evaluation of the models selected , and report comprehensive findings\",\n",
       "  'we demonstrate the effectiveness of our system on two datasets in comparison with state_of_the_art models1 .',\n",
       "  'we observe an improvement of up to 18 % points , in terms of f_score , for cognate detection',\n",
       "  'we evaluate our approach on three different tasks : ( i ) standard machine_learning tasks , ( ii ) entity and document modeling , and ( iii ) content_based recommender systems',\n",
       "  'pre_trained large language_models ( plm ) have emerged as a crucial type of approach that provides readily available knowledge for a range of ai applications',\n",
       "  'with the boom of pretrained_language_models , various relative methods have achieved significant improvements in acsa',\n",
       "  'medical dialogue_systems that generate medically appropriate and human_like conversations have been developed using various pre_trained_language_models and a large_scale medical_knowledge base based on unified medical language system ( umls )',\n",
       "  'for the first part , an improved named_entity identification and relationship_extraction method is developed , incorporating a vision sensing pre_training algorithm named bert',\n",
       "  'the character level features are learned in the bert ( bidirectional_encoder_representations_from_transformers ) _based chinese_character embedding representation layer with the context features extracted in the bilstm ( bi_directional long_short_term_memory ) neural_network layer to form the feature matrix',\n",
       "  'for image data , our cmpc_i module first employs entity and attribute words to perceive all the related entities that might be considered by the expression',\n",
       "  'however , in some application fields , it is impossible to achieve the satisfactory results only depending on the traditional ai algorithm',\n",
       "  'this algorithm is suitable for entity_extraction tasks with few training entity samples and reduces the corpus size required for the bilstm_based algorithm entity_extraction',\n",
       "  'experiments with different benchmarks demonstrate the high quality of quint .',\n",
       "  'we also release a neural multi_modal retrieval model that can use images or sentences as inputs and retrieves entities in the kg',\n",
       "  'paper_title : medical question_answering for clinical decision_support ; paper_abstract : the goal of modern clinical decision_support ( cds ) systems is to provide physicians with information relevant to their management of patient care',\n",
       "  'code will be released at https : //github.com/cshizhe/hgr_v2t .',\n",
       "  'experiments on the arc challenge set show that our model_outperforms the previous state_of_the_art qa systems .',\n",
       "  'paper_title : named_entity extraction for chinese electronic_medical_records ; paper_abstract : named_entity_extraction task refers to identifying and extracting proper named_entities from natural_language texts',\n",
       "  'furthermore , experiments on four general chinese ner datasets show that the framework of our approach is transferable .',\n",
       "  'these models are trained and tested in the case study using 879 chemical patents in the carbon_capture domain',\n",
       "  'among them , the character_based method lacks the support of word information , and the word_based method is affected by the word_segmentation efficiency',\n",
       "  'the defect records of the traction power supply equipment of a maintenance department from 2016 to 2019 are used as the data_set for case study , and , the comprehensive evaluation index of entity_recognition of the model has reached 94.66 % , saving 98.50 % time compared with manual recognition',\n",
       "  'we show that kgs are constructed with a single forward_pass of the pre_trained_language_models ( without fine_tuning ) over the corpora',\n",
       "  'then conditional_random_field ( crf ) is introduced to decode the information and obtain the optimal label sequence',\n",
       "  'therefore , chinese word_segmentation is a very difficult task',\n",
       "  'the experimental result shows that our proposed method has achieved a better result .',\n",
       "  'as a result , research on unsupervised models has emerged as an active field recently',\n",
       "  'secondly , a cnns layer with different kernel sizes was considered capturing multi_scale local contextual features',\n",
       "  'as a result , the performance of such models decreases significantly',\n",
       "  'code is available at \\\\url { https : //github.com/urchade/hner } .',\n",
       "  'the final answer is obtained by computing similarity between question and answer',\n",
       "  'firstly , the cbow model was used to pre_train character embedding on a large number of unlabeled agricultural corpora , and alleviate the impact of segmentation accuracy on the performance of the model',\n",
       "  'experimental_results show that the proposed model has high recall_rate 88.16 % and precision rate 89.33 % which is better than the state_of_the_art models .',\n",
       "  'in contrast to prior work , our approach includes a reward function that takes the accuracy , diversity , and efficiency into consideration',\n",
       "  'thanks to the rapid development of computer sciences and telecommunication technologies , this has evolved impressively',\n",
       "  'in the construction of intelligent question_answering system , we use bert_textcnn to realize the task of intention recognition and use bilstm_crf to realize the task of entity_recognition',\n",
       "  'this system consists of three parts',\n",
       "  'for knowledge_graph of chinese history and culture , most researchers adopted traditional named_entity recognition methods to extract entity information from unstructured historical text data',\n",
       "  'the code will be executed on a high_performance cluster ( hpc ) and users can receive the results later on',\n",
       "  'each covid_19 kg was evaluated , and the average precision was found to be above 93 %',\n",
       "  'our model_outperforms unimodal baselines significantly with various evaluation_metrics .',\n",
       "  'the experimental_results on three open datasets show that the proposed model can achieve the most advanced performance compared with previous models .',\n",
       "  'furthermore , we propose to evaluate the crs models in an end_to_end manner , which can reflect the overall performance of the entire system rather than the performance of individual modules , compared to the separate evaluations of the two modules used in previous work',\n",
       "  'extensive_experiments show that our model is effective and competitive with many current state_of_the_art_methods , and also performs well in practice .',\n",
       "  'a benign node labeled as malicious by our approach is a false positive and a correctly identified malicious node is a true positive',\n",
       "  'and it obscures the reasons and details behind specific diagnoses',\n",
       "  'compared with textrcnn , the kaercnn model improves accuracy by about 3 %',\n",
       "  'the crf decoder was used to represent the output of the attention layer in the form of sequence tags',\n",
       "  'while named_entity recognition ( ner ) is an important task for the extraction of factual information and the construction of knowledge_graphs , other information such as terminological concepts and relations between entities are of similar importance in the context of knowledge_engineering , knowledge_base enhancement and semantic_search',\n",
       "  'the experimental_results of tkgc task illustrate the significant performance improvements of our model compared with the existing_approaches',\n",
       "  'furthermore , a three_layered gcn with highway gates is adopted to learn better entity representations from the neighboring structure information',\n",
       "  'we also propose two models an attention rnn and a transformer for the same',\n",
       "  'such modular architectures often come with a complicated and unintuitive connection between the modules , leading to inefficient learning and other issues',\n",
       "  'code and datasets are available in https : //github.com/zjunlp/promptkg/tree/main/genkgc .',\n",
       "  'through experiments on the benchmark ace_2005 dataset , we demonstrate the effectiveness of the proposed sae_ceed model',\n",
       "  'deep_learning based nlp especially large language_models ( llms ) such as bert have found broad acceptance and are used extensively for many applications',\n",
       "  'but most previous methods require that all entities should be seen during training , which is impractical for real_world kgs with new entities emerging daily',\n",
       "  'paper_title : who is mona l. ? identifying mentions of artworks in historical archives ; paper_abstract : named_entity recognition ( ner ) plays an important role in many information_retrieval tasks , including automatic knowledge_graph construction',\n",
       "  'experimental_results on large_scale datasets show that sdt achieves a lower mean rank and higher hits @ 10 than the baseline methods .',\n",
       "  '43 % on the test_dev set , where the accuracy of answering yes or no questions is 83',\n",
       "  'our model achieves the competitive performance compared with the state_of_the_art_methods on flickr30k dataset and ms_coco dataset',\n",
       "  'paper_title : named_entity recognition method in network_security domain based on bert_bilstm_crf ; paper_abstract : with the increase of the number of network threats , the knowledge_graph is an effective method to quickly analyze the network threats from the mass of network_security texts',\n",
       "  'constructing the topic model can represent mentions and candidate entities by using topic distributions',\n",
       "  'the dataset is publicly available under cc by_sa 4.0 in github .',\n",
       "  'hence to contain malware , enterprises must prevent their hosts from accessing malicious domains',\n",
       "  'the bert pre_training model was selected to generate the input word_vector',\n",
       "  'dhge outperforms baseline models on dh_kg , according to experimental_results',\n",
       "  'this poses a challenge to the widely adopted codec_based architectures',\n",
       "  'aiming at the problem that key chinese entity information in network_security related text is difficult to identify , a named_entity recognition model in network_security domain based on bert_bilstm_crf is proposed to identify key named_entities in network_security related text',\n",
       "  'biobert and med_bert are language_models pre_trained for the healthcare domain',\n",
       "  'moreover , entity_recognition and relation recognition , which crf and cnn are applied in , are accessible for professionals before manual annotation in order to increase the efficiency',\n",
       "  'the experimental_results show that our model obtains significant performance gains over several state_of_the_art_baselines .',\n",
       "  'we release the source_code and pre_trained_models .',\n",
       "  'the complexity of the character and the image quality plays a key role in the conversion accuracy',\n",
       "  'secondly , the cabilstm model was designed for the nested named_entity recognition using hierarchical thinking',\n",
       "  'the recognition of named_entity information of the defect record is the premise of constructing the knowledge_graph',\n",
       "  'our approach is to preprocess the structured data followed by named_entity recognition with appropriate finance_related tags',\n",
       "  'extensive_experiments have demonstrated that our approach is superior to existing state_of_the_art algorithms in terms of both efficiency and effectiveness .',\n",
       "  'however , the performance of these models relies heavily on the amount of labeled_data',\n",
       "  'in the vast majority of the existing works , the two tasks are considered separately with different models or algorithms',\n",
       "  'we have implemented the proposed novel algorithm and several typical algorithms for each task',\n",
       "  'moreover , the results of the online a/b test on the large_scale meituan waimai ( mtwm ) kg consistently show our method brings benefits to the industry .',\n",
       "  'the knowledge extracted by regular_expression was used to semi_automatically label the corpus , and then trained by deep_learning model , which achieved good results',\n",
       "  'the extensive_experiments conducted on the covid_19 drug kg dataset show promising results and prove the effectiveness and efficiency of our proposed model',\n",
       "  'experimental_results show that our model_outperforms most metrics compared to prior state_of_the_art_baselines across two benchmarks',\n",
       "  'then a problem base is established for problem template matching',\n",
       "  'information on wide_spread diseases like diabetes and cancer is extensive , heterogeneous and rapidly growing',\n",
       "  'bert only achieved an accuracy of 55.9 % on it',\n",
       "  'these models suffer from higher computational_complexity during training while still losing information beyond the relative distance between entities',\n",
       "  'experiments using real_world kgs show that transalign improves the accuracy of entity_alignment significantly compared to state_of_the_art_methods .',\n",
       "  'the source_code and data of this paper can be obtained from https : //github.com/cciiplab/cet .',\n",
       "  'finally , we discuss the challenges remaining for chinese nqg .',\n",
       "  'paper_title : named_entity recognition for the diagnosis and treatment of aquatic_animal diseases using knowledge_graph construction ; paper_abstract : disease_diagnosis and treatment have been an important support for aquatic_animal health in aquaculture',\n",
       "  'however , there is room for further improvement on overlapping triplet problem in the military domain .',\n",
       "  'meanwhile , our new benchmark could facilitate the further study in this research area .',\n",
       "  'the experiments show that the unsupervised kgae generates desirable medical reports without using any image_report training pairs',\n",
       "  'thus , it is important for the model to be aware of such features to improve the performance',\n",
       "  'in order to verify the effectiveness of the method , we compare the improved algorithm in this paper with the existing classical algorithms and literature algorithms',\n",
       "  'the experimental_results show that compared with the traditional hmm model and bilstm_crf model , the f1_value of our proposed method increases by 7.1 % and 6.5 % respectively',\n",
       "  'as growing kgs inevitably contain non_matchable entities , different from previous_works , the proposed method employs bidirectional nearest neighbor matching to find new entity_alignment and update old alignment',\n",
       "  'paper_title : recognition of chinese agricultural diseases and pests named_entity with joint radical_embedding and self_attention mechanism ; paper_abstract : chinese named_entity recognition in agricultural diseases and pests domain ( cner_adp ) plays an important role in agricultural natural_language processing such as relation_extraction , agricultural knowledge_graph construction , and agricultural knowledge question and answering , but it still presents some problems , i.e. , the neglect of inherent semantic_information and local contextual features and the insufficiency of capturing long_distance dependencies , which will lead to low accuracy and robustness',\n",
       "  'given such a fixed_size vocabulary , it is possible to bootstrap an encoding and embedding for any entity , including those unseen during training',\n",
       "  'therefore , the proposed bert+bi_lstm +multi_head_self_attention+ fc fusion model has certain practical value in chinese_character relationship_extraction .',\n",
       "  'code and data are available at \\\\url { https : //github.com/xnliang98/sms } .',\n",
       "  'as a data_driven technology , its performance depends on a large amount of image labeling data',\n",
       "  'we use six benchmark_datasets to evaluate the proposed method',\n",
       "  'the experimental_results showed that the f1_score of the bert_flat model was 88.99 %',\n",
       "  'the experimental_results have shown that the proposed model_outperforms the current methods and can improve the precision/recall ( pr ) curve area by 8 % to 16 % compared to the state_of_the_art models ; the auc of bg2kga can reach 0.468 in the best case .',\n",
       "  'the performance on the bert model can be further improved by constructing a specific vocabulary based on domain_knowledge',\n",
       "  'extensive_experiments on a public medical dialogue dataset show our kr_ds significantly beats state_of_the_art_methods ( by more than 8 % in diagnosis accuracy )',\n",
       "  'compared with the baseline model , it has increased by 0.43 % and 1.17 % respectively .',\n",
       "  'extensive_experiments conducted over widely_used benchmark_datasets demonstrate the effectiveness of the proposed framework .',\n",
       "  'after the segmentation of the question , the aho_corasick automaton algorithm and semantic_similarity are applied to entity_recognition',\n",
       "  'it offers a core set of software for operating robots that can be extended by creating or using existing packages , making it possible to program robotic software that can be reused on different hardware platforms',\n",
       "  'experimental_results show that our method_achieves state_of_the_art_performance and the effectiveness and reliability of skg .',\n",
       "  'hence , one system might outperform other systems in one dataset and fail to do in another one',\n",
       "  'experiments on benchmark_datasets show our proposed method can achieve superior_performance compared to analogous methods .',\n",
       "  'our codes and models can be found at https : //github.com/changzhisun/probr/ .',\n",
       "  'however , the existing chinese named_entity identification methods are mainly for standardized texts',\n",
       "  'this phenomenon stems from the inherent complexity of biological systems and our poor understanding of human diseases',\n",
       "  'experiment results show that binet outperforms state_of_the_art_methods on a wide range of kgqa and kgc benchmark_datasets .',\n",
       "  'finally , we test bilstm and bert_based pre_trained_language_models ( plms ) on our dataset and propose a baseline for the following studies',\n",
       "  'to fully mine and utilize geological data , this study proposes a geological news named_entity recognition ( gnner ) method based on the bidirectional_encoder_representations_from_transformers ( bert ) pre_trained_language model',\n",
       "  'we conducted several experiments on four benchmark_datasets to evaluate the performance of multpax against different state_of_the_art_baselines',\n",
       "  'however , the existing captioning frameworks basically enumerate the objects in the image',\n",
       "  'the practical relevance of the proposed methodology has been proven in the study of 1.1 million unique messages from > 400,000 distinct users related to one of the most popular dietary fads that evolve into a multibillion_dollar industry , i.e. , gluten_free food',\n",
       "  'empirical experiments demonstrated that mokge can significantly improve the diversity while achieving on par performance on accuracy on two gcr benchmarks , based on both automatic and human evaluations .',\n",
       "  'then , the bilstm layer was used to learn long_distance text information , and the crf was applied to obtain the globally optimal labeling sequence , so as to output the crop disease entities',\n",
       "  'later , the network parameters are trained via the expectation_maximization ( em ) algorithm',\n",
       "  'the joint representation of two different representations of an entity is regarded as the final representation',\n",
       "  'if the system could not match an answer for a question , the question would be added to unsolved question list and the system would alert administrator to deal with it',\n",
       "  'furthermore , the proposed kbqg outperforms all baselines in our experiments on two real_world datasets .',\n",
       "  'finally , we extensively evaluate our method on several large_scale real_world benchmark_datasets , obtaining favorable results compared with state_of_the_art_methods .',\n",
       "  'the experimental_results show that compared with the bilstm_crf model and the bert_bilstm_crf model , the f1_score is increased by 23.15 % and 10.62 % , respectively',\n",
       "  'then by using a tagger based on supervised_learning and an instance selector based on reinforcement_learning , we iteratively generate new biomedical entities',\n",
       "  'extensive_experiments show that the proposed model achieves substantial improvements against the state_of_the_art_baselines .',\n",
       "  'experiments on two real_world kgs demonstrate the effectiveness of our method',\n",
       "  'the experiments show that the algorithm in this paper has certain effectiveness and superiority .',\n",
       "  'sequencematcher tool and the deckard similarity algorithm',\n",
       "  'a crf layer obtains the output tag sequences',\n",
       "  'our proposed approach shows competitive performance on the hotpotqa distractor setting benchmark compared to the recent state_of_the_art models .',\n",
       "  'experimental_results show that the proposed approach outperforms state_of_ the_art methods by a considerable gain',\n",
       "  'we make use of intent classification and slot_filling , the two important components of any dialogue agent , exploit their interconnectedness , and finally construct a kg',\n",
       "  'with military weapons as the research direction , an svm question classification method based on chinese_character algorithm is proposed , and a question_answering system over knowledge_graph of weapons is established',\n",
       "  'nevertheless , the recall_rate of the improved model was dropped slightly in the comparison test',\n",
       "  'besides , the label ( code ) distribution is uneven',\n",
       "  'over the course of its development , the label supervision has been considered necessary for accurate alignments',\n",
       "  'the f1_score of the best_performing method was 0.97 , indicating the effectiveness of the proposed approach',\n",
       "  'traditional works about emotion recognition mainly focus on the characteristic of the person itself , such as audio , text , facial_expression , body posture',\n",
       "  'the results of extensive_experiments based on real_world dataset which is collected from the news show that our model generates image captions closer to the corresponding real_world captions .',\n",
       "  'we will make the code publicly available at https : //github.com/bionlplab/report_generation_amia2022 .',\n",
       "  'the experimental_results show that this model can efficiently and accurately predict the physical label of hot strip rolling , and the model performance index is better than other models , with the f1_score reaching 91.47 %',\n",
       "  'crowdsourcing efforts like paperswithcode among others are devoted to the construction of leaderboards predominantly for various subdomains in artificial_intelligence',\n",
       "  'text_based automatic personality prediction ( app ) is the automated forecasting of the personality of individuals based on the generated/exchanged text contents',\n",
       "  'we study the importance of some modeling choices and criteria for designing the model , and we demonstrate that it can be used to label data for a supervised classifier to achieve an even better performance without relying on any humanly_annotated training_data',\n",
       "  \"parkinson 's disease is the second most common neurodegenerative disorder worldwide , affecting approximately 12 percent of the human population older than 65 years\",\n",
       "  'as a solution , researchers build medical question_answering ( qa ) systems',\n",
       "  'according to the test results , the model performance is good .',\n",
       "  'finally , the experimental validation shows that our approach is able to enhance existing kge models and can provide more robust representations of kgs in noisy scenarios .',\n",
       "  'the test_std set based accuracies calculated are 66',\n",
       "  'however , research on kg updates in the industry is scarce , with most current research focusing on text_based kg updates',\n",
       "  'it performs text_classification using state_of_the_art transfer_learning models , and thoroughly integrates the results obtained through a proposed methodology',\n",
       "  'experiments demonstrate that proposed model_outperforms most state_of_the_art_methods on the vqa v2.0 benchmark_datasets .',\n",
       "  'our method_achieves state_of_the_art_performance on the pharmaconer dataset , with a max f1_score of 92.01 %',\n",
       "  'therefore , they are hard to generalize to a broad set of applications and kgs',\n",
       "  'the results show our proposed framework outperforms several state_of_the_art_baselines .',\n",
       "  'it could be used as a basis for further research on other domain_specific named_entity recog_nition .',\n",
       "  'this work introduces a pre_trained bert model and a dilated gated convolutional_neural_network ( dgcnn ) as an encoder to distinguish the long_range semantics representation from the input sequence',\n",
       "  'the source_code of ctrn will be available at https : //github.com/2399240664/ctrn .',\n",
       "  'our code is released at : https : //github.com/declare_lab/dialog_hgat',\n",
       "  'by using deep_learning models to extract entities',\n",
       "  'while a variety of algorithms have been proposed , most of them are built upon different combinations of image and language features as well as multi_modal attention and fusion',\n",
       "  'at the same time , the model uses a pre_trained_language model as the basis to solve the problem of lack of semantic knowledge in traditional models',\n",
       "  'our code is available at https : //github.com/mm_ir/dualvgr_videoqa .',\n",
       "  'we proposed an innovative approach for recognizing knowledge entities , which included sequence tagging , text_classification , and keyword matching',\n",
       "  'our code is available at https : //github.com/spyflying/cmpc_refseg .',\n",
       "  'codes are available at https : //github.com/zjunlp/deepke/tree/main/example/re/multimodal .',\n",
       "  'the experimental_results show that the orem_af presented a 74.22 % accuracy and 75.12 % f1 value on the agricultural product data_set , while the 84.51 % accuracy and 75.43 % f1 value on the common data_set',\n",
       "  'further usage of the platform is already planned or implemented for many of our projects .',\n",
       "  'thus , they can be great supplements to existing pre_trained_language_models',\n",
       "  'at macro_level , the rs_adp model achieved optimal precision , recall , and f1 values of 94.16 % , 94.47 % , and 94.32 % , respectively',\n",
       "  'the url of this tool is https : //cinnqi.github.io/neo4j_d3_vkg/',\n",
       "  'experiments on benchmark_datasets demonstrate the effectiveness of our method .',\n",
       "  'to solve the above problems , we developed bank accident management from the perspective of knowledge support to introduce relevant methods and technologies in the field of artificial_intelligence',\n",
       "  'we won the 5th place at ccks 2022 track 1 rematch stage , which proved the effectiveness of our method .',\n",
       "  'how to make the automatic question_answering system more intelligent is a popular research direction in the field of natural_language processing',\n",
       "  'existing studies pay more attention to the matching between utterances and responses by calculating the matching score based on learned features , leading to insufficient model reasoning ability',\n",
       "  'paper_title : neural entity_summarization with joint encoding and weak supervision ; paper_abstract : in a large_scale knowledge_graph ( kg ) , an entity is often described by a large number of triple_structured facts',\n",
       "  'weshow that this novel form of medical question_answering ( q/a ) produces very promising resultsin ( a ) identifying accurately the answers and ( b ) it improves medical article rankingby40 % .',\n",
       "  'in experiments , we tested our model using the new york times ( nyt ) public dataset',\n",
       "  'experimental_results on opendialkg show that our approach significantly_outperforms state_of_the_art_methods on both automatic and human evaluation by a large margin , especially in hallucination reduction ( 17.54 % in feqa ) .',\n",
       "  'the proposed work can also provide a basis for the context_aware intelligent question and answer .',\n",
       "  'extensive_experiments justify our model_outperforms other state_of_the_art_baselines substantially .',\n",
       "  'self_supervised learning on a large corpus of data automatically generates deep_learning_based language_models',\n",
       "  'we implement recent typical methods for named_entity recognition and relation_extraction as a benchmark to evaluate the proposed dataset thoroughly',\n",
       "  'vogue comprises four modules that are trained simultaneously through multi_task_learning',\n",
       "  \"for the input questions , the system firstly performs entity_recognition , using entity type labeling combined with entity similarity matching to identify entities in the user 's questions\",\n",
       "  'for each category , representative algorithms and newly proposed algorithms are presented',\n",
       "  'our experiments on real_world datasets , comparison with related work and user study demonstrate the superior efficiency , precision and user satisfaction of our approach in multi_entity resolution ( mer ) .',\n",
       "  'the system has an accuracy of 0.822 , a precision of 0.837 , and a recall of 0.9015 for a set of 500 questions and answers .',\n",
       "  'extensive experimental_results on two widely used datasets_demonstrate that the proposed model performs better than the state_of_the_art_baselines .',\n",
       "  'recently many related models and methods were proposed , such as translational methods , deep_learning based methods , multiplicative approaches',\n",
       "  'the precision , recall and f1_score were 91.82 % , 90.44 % and 91.01 % , respectively',\n",
       "  'the performance and adaptability of the algorithm are further verified with different datasets',\n",
       "  'experimental_results on biomedical dataset and general field dataset show that our method is effective .',\n",
       "  'many sub_problems were raised in this regard , and reasonable efforts have been made to solve them',\n",
       "  'a named_entity recognition was also proposed to accurately obtain six types of entities in text : food , nutrients , population , location , disease , and efficacy in the field of human nutritional health , combining rules with bert_flat ( bidirectional encoder representations from transformers_flat lattice transformer ) model',\n",
       "  'besides having a user_friendly interface , it is fast , supports customization , and is fault_tolerant on both client and server side',\n",
       "  'we adopt the deep neural_network of bi_gat_crf , which has an accuracy of 90.75 , a recall_rate of 91.53 , and an f1_score of 91.14 in the hazop chinese text',\n",
       "  'it was found that the precision of the model was 86.56 % , the recall_rate was 91.01 % , and the f1_score was 88.72 % , compared with the model without location information , indicating improved by 1.55 , 0.20 , and 0.32 percentage points',\n",
       "  'the continuous expansion of large electronic clinical records provides an opportunity to learn medical_knowledge by machine_learning',\n",
       "  'it infers that the active_learning capability led to the strong migration',\n",
       "  'we claim that our algorithm solves some bottlenecks in existing work , and demonstrate that it achieves superior accuracy on real_world datasets .',\n",
       "  'the technique stands on aggregating significant contextual features human_object interactions and scene recognition',\n",
       "  'in this paper we propose esa , a neural_network with supervised attention mechanisms for entity_summarization',\n",
       "  'the experimental_results show that our method remarkably improves the performance compared to several state_of_the_art_baselines .',\n",
       "  'moreover , it had certain generalization and outperformed other models .',\n",
       "  'the experimental_results show that our gtlr method outperforms recent state_of_the_art_methods .',\n",
       "  'we then label the nodes with high marginal probability of being malicious as malicious nodes and benign otherwise',\n",
       "  'furthermore , we evaluate the applicability of our method under a transfer_learning setting and show that bioie achieves promising performance in processing medical text from different formats and writing styles .',\n",
       "  'furthermore , to mitigate the class_imbalance problem that most end_to_end crss face , we propose a new negative_sampling method which could make the proposed crs learn better',\n",
       "  'malicious domain accesses , however , account for majority of host infections',\n",
       "  'the model can be deployed on multiple clouds independently , increasing the systems flexibility , robustness , and security',\n",
       "  'extensive simulations are conducted to evaluate our proposed algorithm in comparison to some state_of_the_art schemes .',\n",
       "  'the results indicated a considerable improvements in prediction accuracies in all of the suggested classifiers .',\n",
       "  'extensive_experiments strongly evidence that our proposed model obtains significant performance compared with state_of_the arts .',\n",
       "  'the experimental_results show that this models precision , recall , and f1_score are 95.78 % , 97.07 % , and 96.42 % , respectively',\n",
       "  'our approach can be readily applied to any other small dataset size like hate_speech or abusive language and text_classification problem using any machine_learning model .',\n",
       "  'then it uses a convolutional_neural_network with an attention_mechanism to select valid information in the text and obtain the overall vector representation of the text',\n",
       "  'as kgs grow , previous alignment results face the need to be revisited while new entity_alignment waits to be discovered',\n",
       "  'specifically , in the biomedical domain , the size of the data has increased exponentially in the last decade , and with the advances in the technologies to collect and generate data , a faster growth rate is expected for the next years',\n",
       "  'our released codes are available at https : //github.com/cgcl_codes/dhunet .',\n",
       "  'existing_approaches mainly adopt a supervised manner and heavily rely on coupled image_report pairs',\n",
       "  'beng builds upon the successful benchmarking platform gerbil , is opensource and is publicly available along with the data it contains .',\n",
       "  'therefore , its application in kgqa can further improve the accuracy of answer prediction',\n",
       "  'the novelties of our work are the five layer model structure and the attention_mechanism',\n",
       "  'in our model , sentences are encoded by recurrent convolutional_neural_network ( rcnn ) , which combines the advantages of birnn and cnn flexibly , containing more information of sentence',\n",
       "  'besides , the ablation study and discussion demonstrated that ace_adp could not only effectively extract rare entities but also maintain a powerful ability to predict new entities in new datasets with high accu_racy',\n",
       "  'experimental_results show a superior_performance than other baselines , especially significant improvements on the automated extracted kg .',\n",
       "  'the tag decoding layer leverages conditional_random fields ( crf ) to solve the dependency between the output tags and obtain the global optimal label sequence',\n",
       "  'to push forward the future_research on expert_sensitive task_oriented_dialogue system , we first release a large_scale medical dialogue consultant benchmark ( mdg_c ) with 16 gastrointestinal diseases for evaluating consultant capability and a medical dialogue diagnosis benchmark ( mdg_d ) with 6 diseases for measuring diagnosis capability of models , respectively',\n",
       "  'the decision_making ability of the system can improve by incorporating human knowledge to guide the vision_based algorithms',\n",
       "  'we introduce qampari , an odqa benchmark , where question answers are lists of entities , spread across many paragraphs',\n",
       "  'the evaluation shows the framework achieves remarkable improvement on f1_score',\n",
       "  'experimental_results_demonstrate the better performance of our method',\n",
       "  'the code is available at ( https : //github.com/ruizhang_ai/gcp/ )',\n",
       "  'different from existing kgc systems , gbuilder provides a flexible and user_defined pipeline to embrace the rapid development of ie models',\n",
       "  'extensively experiments on two benchmarks show that our method improves the performance than the state_of_the_art baseline and some cases study also confirmed the explainability of our model .',\n",
       "  'experimental_results significantly outperform baselines by nearly 1.72.0 in f1 on three public datasets , docred , dialogre , and mpdd',\n",
       "  'firstly , entity type and relation type are defined , and then multi_source data are fused , and then entity_recognition of bio annotated data_sets is carried out by using the bert_bilstm_crf model',\n",
       "  'we further show the superiority of our kr_ds on a newly collected medical dialogue system dataset , which is more challenging retaining original self_reports and conversational data between patients and doctors .',\n",
       "  'second , the word_vector is sent to a bidirectional_long_short_term memory model for further training to obtain contextual features',\n",
       "  'this paper conducts experiments on the public dataset weibo ner and the self_built food domain dataset food',\n",
       "  'in the traditional question_answering system , the quality of answers was not high due to incomplete data and distinctive vocabulary',\n",
       "  'the kgs , both with and without type information , are considered',\n",
       "  'first , kg_based approaches ignore the information in the conversational context but only rely on entity relations and bag of words to recommend items',\n",
       "  'experiment results show that mmm achieves 87.13 % accuracy on the 5_shot text_classification benchmark amazon review sentiment_classification ( arsc ) , outperforming other baselines , such as induction networks ( 85.63 % ) and distributional signatures ( 81.16 % )',\n",
       "  'named_entity recognition ( ner ) , as a core technology for constructing a geological hazard knowledge_graph , has to face the challenges that named_entities in geological hazard literature are diverse in form , ambiguous in semantics , and uncertain in context',\n",
       "  '2018 ; li et al',\n",
       "  'compared to mainstream models ssan , gain , and atlop , fedre_kd improved the f1score by 22.07 , 20.06 , and 22.38 , respectively .',\n",
       "  'besides , our proposed framework could be easily adaptive to various kge models and explain the predicted results .',\n",
       "  'it has received much attention not only in academia but also in industry',\n",
       "  'finally , the personality vector of each entity node is learned for prediction by designing a walk strategy on the personality heterogeneous graph',\n",
       "  'extensive_experiments were conducted on the benchmark dataset , and the results demonstrate that our framework outperforms state_of_the_art baseline models regarding effectiveness and efficiency .',\n",
       "  'the experimental_results show the effectiveness of our proposed method',\n",
       "  'we conduct experiments on the conll dataset and tac dataset , and various datasets provided by gerbil platform',\n",
       "  'experimental_results show that the pre_trained_language model is comparable to the best approach of this evaluation task , and the context_related pre_trained_language model performs better .',\n",
       "  'code is available at https : //github.com/spyflying/cmpc_refseg .',\n",
       "  'since these improvements are reported in aggregate , however , little is known about ( i ) how to select the appropriate knowledge for solid performance across tasks , ( ii ) how to combine this knowledge with neural language_models , and ( iii ) how these pairings affect granular task performance',\n",
       "  'to this end , we utilize the tool of boolean_circuit to obtain all the theoretical results given in this paper',\n",
       "  'the existing research methods mainly apply traditional machine_learning or deep_learning algorithms for short_text classification',\n",
       "  'we learn structure_based representations of entities and relations and explore a deep convolutional_neural_network with attention to encode description_based representations of entities',\n",
       "  'paper_title : a neural topic model with word_vectors and entity vectors for short texts ; paper_abstract : traditional topic models are widely used for semantic discovery from long texts',\n",
       "  'kgs , by contrast , contain only positive samples , necessitating that negative samples are generated by replacing the head/tail of predicates with randomly chosen entities',\n",
       "  'chinese word_segmentation results are the basis for computers to understand natural_language',\n",
       "  'however , existing rl_based methods have some problems , such as unstable training and poor reward function',\n",
       "  'at present , there are many adversarial algorithms for computer vision , but there are few for nlp models , and there is almost no algorithm for question answer task',\n",
       "  'in this paper , we propose an unsupervised method to cast the knowledge contained within language_models into kgs',\n",
       "  'however , recent kge models achieve performance improvements by excessively increasing the embedding dimensions , which may cause enormous training costs and require more storage space',\n",
       "  'extensive_experiments were conducted on the benchmark dataset',\n",
       "  'existing_approaches for this task have yielded impressive results when the training and testing data are from the same domain',\n",
       "  'experimental_results show that our review score predictor reaches 71.4 % _100 % accuracy',\n",
       "  '( code is publicly available at https : //github.com/ruiqingding/knowledgeda',\n",
       "  'paper_title : image_captioning with internal and external_knowledge ; paper_abstract : automatically generating a human_like description for a given image is a potential research in artificial_intelligence , which has attracted a great of attention recently',\n",
       "  'first , we use a small number of manually labeled biomedical entities as seeds to label some biomedical texts and learn their features autonomously',\n",
       "  'extensive_experiments have been conducted and the results demonstrate that our algorithm could achieve better video captioning performance than the state_of_the_art algorithms .',\n",
       "  'experiments on benchmarks verify the state_of_the_art_performance of our method .',\n",
       "  'in this article , we take extra knowledge information of medical encyclopedia into account and we associate the original text in the named_entity recognition task with its encyclopedic knowledge to enhance the ability of entity_recognition through the establishment of the connection and interaction of the joint_network',\n",
       "  'experiments confirm the effectiveness and efficiency of our method on several benchmark data_sets .',\n",
       "  'extensive_experiments demonstrate the effectiveness of our model architecture .',\n",
       "  'the prior methods of dre do not meaningfully leverage speaker information_they just prepend the utterances with the respective speaker names',\n",
       "  'paper_title : context_driven image_caption with global semantic_relations of the named_entities ; paper_abstract : automatic image_captioning has achieved a great progress',\n",
       "  'the obtained results are compared to the fine_tuned bert and biobert models trained on the same dataset',\n",
       "  'finally , we empirically evaluate the performance of the proposed approach on real benchmark data',\n",
       "  'finally , crf is used to implement the sequence labeling task',\n",
       "  'in most previous methods , features are usually extracted by the hand_crafted templates',\n",
       "  'paper_title : leveraging concept_enhanced pre_training model and masked_entity language_model for named_entity disambiguation ; paper_abstract : named_entity disambiguation ( ned ) refers to the task of resolving multiple named_entity mentions in an input_text sequence to their correct references in a knowledge_graph',\n",
       "  'named_entity ) approaches to identify mentions of genes , diseases , drugs , chemicals , symptoms , chinese herbs and patent medicines , etc',\n",
       "  'this paper studies and implements chinese coreference_resolution from two aspects : named_entity recognition and coreference_resolution',\n",
       "  'a small number of labelled data are used to pre_train the model , and then , a large number of unlabelled data are used to fine_tune the pre_training model',\n",
       "  'furthermore , most previous_works focus on binary ddi prediction whereas the multi_typed ddi pharmacological effect prediction is a more meaningful but harder task',\n",
       "  'evaluation results on trec web track ad_hoc task demonstrate that all of the four_way interactions in the duet are useful , the attention_mechanism successfully steers the model away from noisy entities , and together they significantly outperform both word_based and entity_based learning to rank systems .',\n",
       "  'in the field of artificial_intelligence , there is a very important sub field _ natural_language processing ( nlp )',\n",
       "  'experiments are conducted to show the effectiveness of our model .',\n",
       "  'our extensive_experiments have shown that the proposed approach consistently achieves the state_of_the_art_performance across all the test datasets in both unsupervised and supervised settings and the improvement margins are considerable .',\n",
       "  'compared with the modern chinese corpus , it is irrecoverable and specially organized , making it difficult to be learned by existing pre_trained_language_models',\n",
       "  'therefore , this paper synthetically uses deep_learning models such as bidirectional lstms , conditional_random fields and pcnn to carry out entity_recognition and relationship_extraction for text data , such as electronic_medical_record and medical community , to construct visual knowledge_graph',\n",
       "  'according to the results , it can conclude that the application of the bert model instead of the word2vec algorithm for word_vector training is helpful to the model recognition , and the exclusive dictionary word_segmentation and part of speech classification of nuclear_technology texts contribute to improving the quality of labeled_data',\n",
       "  'to address these challenges , we propose a novel and effective closed_loop neural_symbolic learning framework enginekg via incorporating our developed kge and rule learning modules',\n",
       "  'for cross_institutional medical code mapping , the top 1 and top 5 accuracy were 91.0 % and 97.5 % when mapping medication codes at va to rxnorm medication codes at mgb ; 59.1 % and 75.8 % when mapping va local laboratory codes to loinc hierarchy',\n",
       "  'compared with the other existing medical question_answering systems , our system adopts several state_of_the_art technologies including medical entity_disambiguation and medical dialogue_generation , which is more friendly to provide medical services to patients',\n",
       "  'our contributions in this paper are twofold',\n",
       "  'however , real_world kgs do not remain static , but rather evolve and grow in tandem with the development of kg applications',\n",
       "  'some known drugs linked to covid_19 in the literature were identified , as well as some candidate drugs that have not yet been studied',\n",
       "  'experiments conducted on widely baselines show that the proposed framework is superior to the state_of_the_art_methods .',\n",
       "  'further , we propose a hybrid model based on neural_network matrix factorization ( nnmf ) that considers multi_source signals simultaneously',\n",
       "  'by introducing boundary detection unit , our model extracts the boundaries of entities and restrict the number of candidate entities',\n",
       "  'our approach can be generalized to other diseases as well as to other clinical questions .',\n",
       "  'we conducted experiments on the datasets flickr30k and mscoco , which have 31,783 and 123,287 images , respectively',\n",
       "  'in this paper , we proposed a named_entity recognition model , albert_ bigru_ crf , and a relationship_extraction model , albert_bigru_attention , which were embedded with albert ( a lite bidirectional_encoder_representation_from_transformers ) pre_training language_model',\n",
       "  'using dictionary is the simplest way for labeling , but it is difficult to obtain a versatile dictionary and usually a dictionary for one corpus is not suitable for another corpus due to bad transferability',\n",
       "  'we also designed a hamming lower_bound label encoding algorithm to encode the label representations in lower dimensions',\n",
       "  'however , most existing_methods concentrate on modern chinese and ignore the classical_chinese due to its complexity , making research in this field relatively lacking',\n",
       "  'the experimental result proves the effectiveness of the proposed method .',\n",
       "  'finally , the similarity between the input image and text is calculated based on knowledge_fused features to complete the matching process',\n",
       "  'evaluation results show that our approach significantly_outperforms the state of the art on two public benchmarks .',\n",
       "  'owing to the enormous rise in the costs of pharmaceutical r & d , several pharmaceutical companies are leveraging repurposing strategies',\n",
       "  'despite all the notable advancements , current kgqa systems only focus on answer generation techniques and not on answer verbalization',\n",
       "  'however , the precise emotions of one conversational bot are crucial to improve user satisfaction',\n",
       "  'an attention_mechanism was added between the segmented maximum pooling layer and the classification layer , further to extract the high_level semantics',\n",
       "  'experiments on three datasets show that the proposed method clearly outperforms state_of_the_art_methods',\n",
       "  'more specifically , our framework ( diva ) is composed of three modules , i.e',\n",
       "  'in terms of specific categories , it achieved f1 values as high as 95.81 % , 97.76 % , and 97.23 % on easily identifiable entities such as crop , disease , and pest',\n",
       "  'the experimental_results_demonstrate the effectiveness of the proposed model on several datasets .',\n",
       "  'paper_title : ace_adp : adversarial contextual embeddings based named_entity recognition for agricultural diseases and pests ; paper_abstract : entity_recognition tasks , which aim to utilize the deep_learning_based models to identify the agricultural diseases and pests_related nouns such as the names of diseases , pests , and drugs from the texts collected on the internet or input by users , are a fundamental component for agricultural knowledge_graph construction and question_answering , which will be implemented as a web_application and provide the general public with solutions for agricultural diseases and pest_control',\n",
       "  'extensive_experiments on both mdg_c and mdg_d benchmarks demonstrate the superiority of our hgr over state_of_the_art knowledge grounded approaches in general fields of medical dialogue system .',\n",
       "  'experiments with msra datasets show that this method outperforms word_based and character_based baselines and achieves a higher recall_rate compared to other methods .',\n",
       "  'paper_title : incorporating domain_knowledge into language_models by using graph convolutional_networks for assessing semantic textual similarity : model development and performance comparison ; paper_abstract : background : although electronic_health_record systems have facilitated clinical documentation in health_care , they have also introduced new challenges , such as the proliferation of redundant information through the use of copy and paste commands or templates',\n",
       "  'these tasks are generally focused on tagging common entities , but domain_specific use_cases require tagging custom entities which are not part of the pre_trained_models',\n",
       "  'the classifier is equipped with 2 components_ontology and machine_learning data_model',\n",
       "  'the proposed model incorporates category knowledge and local knowledge for improved data representation',\n",
       "  'we conduct extensive_experiments to evaluate existing popular methods , and find that they fail to achieve promising performance',\n",
       "  'we introduce its system design , architecture , algorithms , functions , and implementation',\n",
       "  'firstly , during the stage of topic entity_recognition , a deep transition model is constructed to extract topic entities , and an efficient entity_linking strategy is presented , which combines character matching and entity_disambiguation model',\n",
       "  'the experimental_results on real_world data demonstrate the effectiveness of the proposed model .',\n",
       "  'we finalize this survey with open research problems relevant to mmkgs .',\n",
       "  'while these models are undeniably useful , it is a challenge to quantify their performance beyond traditional accuracy metrics',\n",
       "  'additionally , our model consistently outperforms the state_of_the_art model in domain adaptation settings',\n",
       "  'the question and answer system implemented by this method has a quick query response and can answer relatively complex_questions',\n",
       "  'understanding the nature of this disease , when there is no available cure , is vital to encourage accurate clinical diagnosis and drug_discovery prospects',\n",
       "  'we aim to develop this scheme further for real_world robot intelligence in human_robot interaction .',\n",
       "  'recent studies attempt to employ external_knowledge to improve classification performance , but they ignore the correlation between external_knowledge and have poor interpretability',\n",
       "  'extensive_experiments on common odqa benchmark_datasets ( natural question and triviaqa ) demonstrate that kg_fid can improve vanilla fid by up to 1.5 % on answer exact match score and achieve comparable performance with fid with only 40 % of computation cost .',\n",
       "  'experimental_results on a large_scale dataset demonstrate that kecrs outperforms state_of_the_art chit_chat_based crs , in terms of both recommendation accuracy and response_generation quality .',\n",
       "  'artificial intelligence_based methods are promising in their potential to discover new treatment options',\n",
       "  'when the amount of literature available is vast , it is important to represent the disease domain as completely as possible',\n",
       "  'this approach combines the advantages of prior_knowledge and neural_networks',\n",
       "  'the combination of artificial_intelligence and traditional industries the emergence of artificial_intelligence has made these traditional industries shine',\n",
       "  'in detail , we detect the context using region proposal network ( rpn ) to extract nodes as the input of the graph convolution network ( gcn ) , which transfers the convolution operation from euclidean data_structure to noneuclidean data_structure',\n",
       "  'in this tutorial , we provide a comprehensive overview on recent research and development in this direction',\n",
       "  'by grouping entities by types , we are better able to take advantage of the benefits of mil and further denoise the training signal',\n",
       "  'this pipeline was evaluated on a manually crafted gold_standard , yielding competitive results',\n",
       "  'experimental_results on open_domain corpus ( nlpcc2014 ) demonstrate the validity of the proposed method',\n",
       "  'we test our model on fb15k_237 and nell_995 datasets with different tasks',\n",
       "  'however , these models have not been thoroughly compared , and they were only tested on self_created datasets',\n",
       "  'lastly , a disease identification model based on image_text multimodal collaborative representation and knowledge assistance ( itk_net ) was constructed',\n",
       "  'second , we perform multi_task_learning experiments between open_domain qa and task_oriented dialog , and benchmark our model on a popular nlg dataset',\n",
       "  'after experimental comparison , the proposed method performs better on both fb15k and win18 datasets , with a numerical improvement of about 2.6 % compared with the original translation model , which verifies the reasonableness and effectiveness of the method .',\n",
       "  'for crisis cases , however , kara model achieved a much higher recall than the baseline ( 0.870 vs 0.791 )',\n",
       "  'in these models , token sequence of a text is feeded into a neural_network to generate text with the guidance of latent topics',\n",
       "  'recently , kg has been applied in various fields , such as intelligent search , question_answering systems and so on',\n",
       "  'paper_title : sliqa_i : towards cold_start development of end_to_end spoken language interface for question_answering ; paper_abstract : question_answering ( qa ) has become a key capability for voice enabled personal assistants to automatically answer various user questions',\n",
       "  'additional machine_learning methods can be applied to this representation to make predictions within genomic , pharmaceutical , and clinical domains',\n",
       "  'first , the key technologies for constructing knowledge_graph are explained in detail from named_entity recognition , entity_relation_extraction and entity_alignment',\n",
       "  'in this way , we generate the news image_caption with named_entities',\n",
       "  'the experimental_results show that the proposed methods achieve the state_of_the_art_performance in terms of both automatic and human evaluation .',\n",
       "  'the c_statistics of kara and the baseline were 0.815 and 0.760 , respectively',\n",
       "  'the evaluation results demonstrate that our approach significantly_outperforms the state_of_the_art_baselines , with a significance t_test p & lt ; 0.041',\n",
       "  'the model also predicts ddis using multiple labels rather than single or binary labels',\n",
       "  'we further design a reward based on a multiple_choice cloze test to drive the model to better capture entity interactions',\n",
       "  'the performance of the bert_pcnn_att_jieba model was compared with the classical cnn , pcnn model , as well as the cnn , pcnn , pcnn_att , and pcnn_jieba models combined with bert under the same data_set and the consistent experimental parameters',\n",
       "  'extensive_experiments on public datasets indicate that our supervised method significantly_outperforms the previous methods and the unsupervised one has competitive performance .',\n",
       "  'the prefix att means the model is based on attention_mechanism',\n",
       "  'reliable and scalable identification of malicious domains , how_ ever , is challenging',\n",
       "  'in this paper , we propose a novel method named dhu_net for addressing the time variability challenge and the dynamic unseen entity challenge derived from it',\n",
       "  'experiments on the benchmark dataset redial show our recindial model significantly surpasses the state_of_the_art_methods',\n",
       "  \"then the system predicts the user 's question intention and use the trained question classifier to predict the category number\",\n",
       "  'our approach can be applied to the task of chinese word_segmentation in specific domains containing rare terms .',\n",
       "  'besides , the lack of large labeled datasets has been a serious obstacle impending further research',\n",
       "  'compared to the previous methods , our framework has an excellent performance after experimental verification',\n",
       "  'combining cmpc_i or cmpc_v with tgfe can form our image or video version referring segmentation frameworks and our frameworks achieve new state_of_the_art performances on four referring image_segmentation benchmarks and three referring video segmentation benchmarks respectively',\n",
       "  'among the methods , unified_qa reaches the best performance on the bdd_qa dataset with the adaptation of multiple formats of question answers',\n",
       "  'moreover , acp_based models are shown to outperform the baselines .',\n",
       "  'moreover , the character case also has a certain impact on model performance .',\n",
       "  'our source_code and datasets are public available at https : //github.com/dice_group/multpax .',\n",
       "  'our results demonstrate that stonkgs outperforms both baselines , especially on the more challenging tasks with respect to the number of classes , improving upon the f1_score of the best baseline by up to 0.084 ( i.e',\n",
       "  'intelligent support for mobile_phone detection process with the knowledge_graph .',\n",
       "  'we adopt two benchmarks , i.e. , activitynet_caption and charades_sta , to evaluate our model and conduct comprehensive experiments to analyze the effectiveness of each component',\n",
       "  'the study also presents experimental_results obtained by combining the different components of the strategy',\n",
       "  'our approach obtained the highest results of subtask b compared to the other task participants .',\n",
       "  'result our qg_srgr model is trained , validated and tested on the vqa v2',\n",
       "  'extensive_experiments are carried out on five real datasets and synthetic datasets , and the ukge_ms are compared with five corresponding algorithms',\n",
       "  'extensive_experiments on vmr dataset and visr dataset demonstrate the effectiveness of the proposed framework .',\n",
       "  'we conducted extensive_experiments on multiple kgs',\n",
       "  'the saat outputs were compared with the gold_standard , manually developed by two experts',\n",
       "  'using the training corpus , we design a hierarchical svm classifier to realize the entity knowledge_extraction',\n",
       "  'by accessible to all , we mean that the proposed software_framework is freely available on github and zenodo with an open_source license',\n",
       "  'evaluated on mimic_cxr and iu_xray datasets , our method is able to outperform previous state_of_the_art models on these two datasets .',\n",
       "  'as we evaluated , complex chinese_language processing techniques , such as segmentation and parsing , were not necessary for practice and complex architectures were not necessary to build the qa system',\n",
       "  'experimental_results show the effectiveness of the proposed method .',\n",
       "  'this study dives into a more realistic and challenging setting where new entities emerge in multiple batches',\n",
       "  'then , we discuss the design and implementation of this approach within an automatic community_shared software_framework ( a.k.a',\n",
       "  'experimental_results significantly outperform baselines by nearly 1.7 % 2.0 % in f1 on three public datasets , docred , dialogre , and mpdd',\n",
       "  'this is a comparable result with the known approaches to solving this problem',\n",
       "  'among them , the average accuracy of kgipsl on the yago dataset is 14.9 % higher than that of the baseline method .',\n",
       "  'in contrast to adding context to experiments , a considerable amount of training_data is available to support target identification',\n",
       "  'experiments on wikitext_103 , wmt19 , and enwik8 english datasets_demonstrate that our approach produces a better language_model in terms of perplexity and bits per character',\n",
       "  'finally , we conduct extensive_experiments with state_of_the_art kgqa models and compare their performance on crunchqa',\n",
       "  'according to the audit knowledge_graph , a character_level convolutional_neural_network is trained and an intelligent audit question_answering system is built based on the semantic_similarity',\n",
       "  'then the text prediction sequence was obtained from the crf layer',\n",
       "  'this is important in high_risk domains such as healthcare , intelligence , etc',\n",
       "  'at the end of this paper , we introduce the result of the experiment and a korean template generation module developed using srdf .',\n",
       "  'synchronous modulator is a synchronous_motor in a special operating state',\n",
       "  'first of all , the bilstm+crf model was used to identify the inner entities that appeared frequently , and then the dimension reduction of the identified inner entity feature matrix was connected outer entity feature matrix to retain the complete inner entity feature information',\n",
       "  'it solves the problems that it is difficult to recognize entities from insufficient context , and traditional models usually neglect the relevance between sentiment entities and aspect entities',\n",
       "  'this paper summarizes the existing work in the field of dsre , and pays more attention to the methods based on deep_learning',\n",
       "  'the dataset is available at < uri > https : //cricvqa.github.io < /uri > .',\n",
       "  'thirdly , the bayesian classifier is used to classify the questions and the bidirectional_long_short_term memory ( bilstm ) combined with conditional_random fields ( crf ) method is applied to extract contents from the input questions',\n",
       "  'however , in most enterprises , it is challenging to quickly construct kgs with multi_source and heterogeneous data and apply kgs to meet diverse business demands',\n",
       "  'with back_propagation from ranking labels , the model learns simultaneously how to demote noisy entities and how to rank documents with the word_entity duet',\n",
       "  'however , large_scale and high_quality datasets are rare due to the high complexity and huge workforce cost of making such a dataset',\n",
       "  'in addition , the experimental_results of comparison with deep_learning algorithms also show that the proposed model has better performance in classification of short_text from other fields',\n",
       "  'we also obtain better or comparable performance compared to systems that are fine_tuned from large pretrained_language_models',\n",
       "  'our algorithms accuracy was evaluated against gensim , largely improving its accuracy',\n",
       "  'in medical domain , vqa systems are still in their infancy as the datasets are limited by scale and application scenarios',\n",
       "  'before training , the cnes in the corpus will be masked , and then use the masked corpus training the semantic model through bilstm_crf , which can verify whether the context semantics of the corresponding location entities are correct',\n",
       "  'however , it is not an easy task regarding the huge number of existing applications that are available for use',\n",
       "  'in summary , semnet 2.0 is a comprehensive open_source software for significantly faster , more effective , and user_friendly means of automated biomedical lbd',\n",
       "  'the proposed model is an encoder_decoder architecture',\n",
       "  'the framework is further extended with the capability to extract attribute value across multiple product categories with a single model , by training the decoder to predict both product category and attribute value and conditioning its output on product category',\n",
       "  'the experimental_results show that our proposed method has very good performance on both cross_kg and cross_language data_sets',\n",
       "  'to this end , we propose a novel context_aware graph ( cag ) neural_network',\n",
       "  'comprehensive experimental_results on two real_world crs datasets ( including both english and chinese with about 10,000 dialogues ) show the superiority of our proposed method',\n",
       "  'hence , vector_based similarity representations are learned from multiple perspectives to model multi_level alignments comprehensively',\n",
       "  'experimental_results show that the proposed framework achieves the best performance compared with the state_of_the_art_methods on large_scale kgs .',\n",
       "  'experiments show that kidrr achieves improvements of nearly 2.2 % and 1.6 % relative to recall @ 1 on flicr30k and mscoco , respectively , compared to the current state_of_the_art_baselines .',\n",
       "  'existing works mainly use a sequence_based deep neural encoder to process questions',\n",
       "  'standard rbms , however , operate on bag_of_words assumption , ignoring the inherent underlying relational structures among words',\n",
       "  'to evaluate the proposed approach , we use salzburgerland kg , a real kg describing touristic entities of the region of salzburg , austria',\n",
       "  'then , the multi_head attention_mechanism is introduced to assist the bidirectional_long_short_term memory network ( bilstm ) in acquiring long_distance context_dependence',\n",
       "  'the source_code and datasets of this paper are available at https : //github.com/ngl567/lcge .',\n",
       "  'system testing shew that the q & a system has a high accuracy_rate in response of tree hole rescue questions',\n",
       "  'experiments show that the method is feasible and accurate .',\n",
       "  \"existing language_models such as word2vec , elmo , and bert have various disadvantages , e.g. , they ca n't solve the problem of polysemous words and have poor ability of context fusion and computational efficiency\",\n",
       "  'compared with other models , such as lstm model , lstm_crf model and bilstm_crf model , att_bilstm_crf had obvious advantages in different size corpus , and it can effectively identify entities for agricultural texts .',\n",
       "  'named_entity recognition has achieved an f1_score of 89.37 % , and relation_extraction has achieved an f1_score of 72.64 %',\n",
       "  'experiments with several machine_learning methods and deep_learning methods show that the proposed method has a certain improvement in f_score .',\n",
       "  'the results show that the method proposed in this paper is effective and has certain scalability and popularization .',\n",
       "  'in this paper , we explore multilingual kg completion , which leverages limited seed alignment as a bridge , to embrace the collective knowledge from multiple languages',\n",
       "  'our approach introduces the use of context from a knowledge_graph to generate improved feature representations for cognate detection',\n",
       "  'research related to covid_19 , biomedicine , and many other communities can benefit from openkg_covid19',\n",
       "  'this multi_modal retrieval model can be integrated into any ( neural_network ) model pipeline',\n",
       "  'the application is language and corpus agnostic , but can be tuned for special needs of a specific language or a corpus',\n",
       "  'we find that such resources provide insignificant gains to the performance of fine_tuned language_models',\n",
       "  'when extracting entities from electronic_health_records ( ehrs ) , ner models mostly apply long_short_term_memory ( lstm ) and have surprising performance in clinical ner',\n",
       "  'secondly , the labeled text sequence was vectorized at character level by word_vector technology',\n",
       "  'experiments conducted on widely adopted benchmarks show that the proposed model is superior to the latest kbqa method .',\n",
       "  'data have become an important source of competitive_intelligence',\n",
       "  'the code and datasets are available at https : //github.com/pkusjh/lass .',\n",
       "  'despite the short time on the market , it seems to be quickly well_noticed with 4.6 thousand stars on github at the moment',\n",
       "  'to overcome mvkgf , entity_alignment is the most studied',\n",
       "  '( code is released at https : //github.com/alexa/dstqa )',\n",
       "  'we ranked 9th out of 31 teams in the competition',\n",
       "  \"it fitted the bert_ala+bilstm+crf model 's output of bilstm layer and crf layer\",\n",
       "  'through analysis of extensive experiment results , our method could effectively discover more abnormal data without big loss of time costs , has strong generalization , and correspondingly improves the performance for abnormal detection .',\n",
       "  'extensive evaluations over both an open_released english dataset and our chinese dataset demonstrate that our approach conkadi outperforms the state_of_the_art approach ccm , in most experiments .',\n",
       "  'it enjoys further performance boost when employing a pre_trained bert encoder , outperforming the strongest baseline by 17.5 and 30.2 absolute gain in f1_score on two public datasets nyt and webnlg , respectively',\n",
       "  'our approach is based on a versatile probabilistic formulation _ the restricted boltzmann machine ( rbm ) _ where the underlying graphical_model is an undirected bipartite_graph',\n",
       "  'with thousands of packages available per stable_distribution , encapsulating algorithms , sensor drivers , etc. , it is the de facto middleware for robotics',\n",
       "  'the decoder part consists of a multi_relation classifier for the relation_classification task , and an improved long_short_term_memory for the entity_recognition task',\n",
       "  'boe loss provides an additional supervision signal to guide crs to learn from both human_written utterances and kg',\n",
       "  'in addition , the application status of medical_knowledge maps in clinical decision_support , medical intelligence semantic retrieval , medical question_answering system and other medical services are introduced',\n",
       "  'extensive evaluations showed that our model_outperforms the state_of_the_art models on the opendialkg dataset on multiple metrics .',\n",
       "  'finally , we employ a capsule network to extract different linguistic units ( word and phrase ) from the relations , and dynamically predict the optimal option based on the extracted units',\n",
       "  'we develop an open_source library including 12 representative embedding_based entity_alignment approaches , and extensively evaluate these approaches , to understand their strengths and limitations',\n",
       "  'then one bidirectional_long_short_term memory ( bilstm ) layer is applied to semantically encode the input text',\n",
       "  'experimental_results show the best performance compared to the state_of_art on complex_questions .',\n",
       "  'without training on large_scale labeled_data , current ner methods based on deep_learning models can not identify specific geological disaster entities from geological disaster situation reports',\n",
       "  'as one of the most import research areas , entity_relation_extraction applies usual recurrent neural_networks ( rnns ) and convolutional_neural_networks ( cnns ) and has achieved good results',\n",
       "  'comparative evaluations of instance matching systems can inform us about the performance of such systems regarding artificial benchmarks or real_world data challenges',\n",
       "  'this method uses the weighted graph_convolutional_network to model the semantic dependency_parsing of the entity description and construct the semantic dependency_parsing graph_embedding',\n",
       "  'experimental_results_demonstrate the effectiveness of our method',\n",
       "  'meanwhile , it validated the precision in nuclear_technology knowledge entity_recognition for collected data , which could be used in pre_procedure in nuclear_technology knowledge_graph construction .',\n",
       "  'extensive_experiments demonstrate the effectiveness of our model .',\n",
       "  'paper_title : review of deep_learning_based topic model ; paper_abstract : as a research hotspot for more than twenty years , topic model plays an important role in semantic analysis of multi_documents.the topic model is adept in extracting groups of keywords from documents to represent their core idea , and thus provides crucial support for document_classification , information_retrieval , automatic summarization of multi_documents , sentiment_analysis and so on',\n",
       "  'to encourage reproducible results , we make our code and mwp dataset public available at \\\\url { https : //github.com/tal_ai/make_emnlp2021 } .',\n",
       "  'the experimental_results show that our model achieves a sota performance of 72.64 % and a good performance of 69.68 % for f1 values on the public and self_built datasets , respectively',\n",
       "  'evaluation on the hotpotqa dataset in the distractor setting shows that our method outperforms the published sota entity_based method in five out of six metrics .',\n",
       "  'experimental_results show that the model can achieve good results on test data .',\n",
       "  'the special word attention emphasizes on word importance when focusing on different regions of the input image , and makes full use of the internal annotation knowledge to assist the calculation of visual attention',\n",
       "  'however , there are many named_entities in news text , and existing_approaches are unable to directly generate named_entities in the news image_caption',\n",
       "  'empirically , extensive_experiments on three real_world tkgs demonstrate the superiority of metatkgr over state_of_the_art_baselines by a large margin .',\n",
       "  'we show that our model significantly_outperforms other baselines',\n",
       "  'we conducted fine_tuning training on chinesedailynercorpus',\n",
       "  'our experimental_results also demonstrate the effectiveness and robustness of zs_ska .',\n",
       "  'the datasets including cm3kg at https : //github.com/wengsyx/cm3kg and cmcqa at https : //github.com/wengsyx/cmcqa are also released to further promote future_research .',\n",
       "  'extensive_experiments are conducted on public real_world datasets to show the superior_performance of the proposed method',\n",
       "  'for each module , we discuss the details of our building methods',\n",
       "  'this framework specifically consist of two pre_trained_language_models to provide the embed dings and a sequence labeling model to tag the entity labels',\n",
       "  'moreover , chinese has a very complex grammar , and the word_segmentation criteria are varied with the contexts',\n",
       "  'existing_approaches to solve this problem take a two_step approach',\n",
       "  'at last , the fully connected layer was used to decode the information and output the crop disease entity_relation triples',\n",
       "  'this method usually results in many instances with incorrect labels',\n",
       "  'the experimental_results show that it has a significant performance gain over several different state_of_the_art_baselines .',\n",
       "  'paper_title : entity_aware image_caption generation ; paper_abstract : current image_captioning approaches generate descriptions which lack specific information , such as named_entities that are involved in the images',\n",
       "  'however , these two models inherit the bag_of_word assumption of the standard lda model , which disable the exploitation of more distinguishable n_gram features',\n",
       "  ...],\n",
       " 'examples': []}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root.children[0].children[0].external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_papers = taxo.rankPapers([curr_node.emb['sentences'] for curr_node in taxo.root.children], 2, internal=True, phrase=False, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(66, 0),\n",
       " (65, 1),\n",
       " (71, 2),\n",
       " (69, 3),\n",
       " (51, 4),\n",
       " (47, 5),\n",
       " (55, 6),\n",
       " (50, 7),\n",
       " (48, 8),\n",
       " (70, 9),\n",
       " (24, 10),\n",
       " (54, 11),\n",
       " (4, 12),\n",
       " (62, 13),\n",
       " (25, 14),\n",
       " (11, 15),\n",
       " (22, 16),\n",
       " (53, 17),\n",
       " (67, 18),\n",
       " (59, 19)]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_embs = {idx: paper_emb for idx, paper_emb in enumerate(sentence_model.encode([paper.title for paper in collection]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING using_llms_on_graphs; remaining in queue: deque([])\n",
      "['1', '2', '3']\n",
      "next!\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "label_free node_classification on graphs with large_language_models ( llms )\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "text2mol : cross_modal molecule retrieval with natural_language queries\n",
      "functional output regression for machine_learning in materials_science\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "graph_aware language_model pre_training on a large graph corpus can help multiple graph applications\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "gimlet : a unified graph_text model for instruction_based molecule zero_shot learning\n",
      "next!\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "graph_neural prompting with large_language_models\n",
      "patton : language_model pretraining on text_rich_networks\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "graph_aware language_model pre_training on a large graph corpus can help multiple graph applications\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "disentangled_representation_learning_with_large language_models for text_attributed_graphs\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "next!\n",
      "git_mol : a multi_modal large_language_model for molecular_science with graph , image , and text\n",
      "molca : molecular graph_language modeling with cross_modal projector and uni_modal_adapter\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "interactive molecular discovery with natural_language\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "translation between molecules and natural_language\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "multi_modal molecule structure_text model for text_based retrieval and editing\n",
      "neighborhood contrastive_learning for scientific document representations with citation embeddings\n",
      "unifying molecular and textual_representations via multi_task language_modelling\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "graphtext : graph_reasoning in text_space\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "molfm : a multimodal molecular foundation_model\n",
      "galactica : a large_language_model for science\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING pure_graphs; remaining in queue: deque([text_rich_graphs, text_paired_graphs])\n",
      "['4']\n",
      "PROCESSING text_rich_graphs; remaining in queue: deque([text_paired_graphs])\n",
      "['8', '14', '20']\n",
      "next!\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "neighborhood contrastive_learning for scientific document representations with citation embeddings\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "label_free node_classification on graphs with large_language_models ( llms )\n",
      "text2mol : cross_modal molecule retrieval with natural_language queries\n",
      "a deep_learning system bridging molecule structure and biomedical text with comprehension comparable to human_professionals\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "enhancing activity_prediction models in drug_discovery with the ability to understand human language\n",
      "translation between molecules and natural_language\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "next!\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "gimlet : a unified graph_text model for instruction_based molecule zero_shot learning\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "graphllm : boosting_graph_reasoning_ability_of_large language_model\n",
      "graph_neural prompting with large_language_models\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "language is all a graph needs\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "next!\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "graph_neural prompting with large_language_models\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "gimlet : a unified graph_text model for instruction_based molecule zero_shot learning\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "graphllm : boosting_graph_reasoning_ability_of_large language_model\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "language is all a graph needs\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "interactive molecular discovery with natural_language\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING text_paired_graphs; remaining in queue: deque([llm_as_predictor, llm_as_encoder, llm_as_aligner])\n",
      "['23', '26']\n",
      "next!\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "functional output regression for machine_learning in materials_science\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "graphllm : boosting_graph_reasoning_ability_of_large language_model\n",
      "translation between molecules and natural_language\n",
      "graph_aware language_model pre_training on a large graph corpus can help multiple graph applications\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "next!\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "graph_neural prompting with large_language_models\n",
      "molfm : a multimodal molecular foundation_model\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "molca : molecular graph_language modeling with cross_modal projector and uni_modal_adapter\n",
      "train your own gnn teacher : graph_aware distillation on textual_graphs\n",
      "galactica : a large_language_model for science\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "graphtext : graph_reasoning in text_space\n",
      "interactive molecular discovery with natural_language\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "text2mol : cross_modal molecule retrieval with natural_language queries\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_encoder, llm_as_aligner, llm_as_predictor, llm_as_aligner])\n",
      "['9', '12', '13']\n",
      "next!\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "translation between molecules and natural_language\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "functional output regression for machine_learning in materials_science\n",
      "a deep_learning system bridging molecule structure and biomedical text with comprehension comparable to human_professionals\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "unifying molecular and textual_representations via multi_task language_modelling\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "enhancing activity_prediction models in drug_discovery with the ability to understand human language\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "git_mol : a multi_modal large_language_model for molecular_science with graph , image , and text\n",
      "galactica : a large_language_model for science\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "next!\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "graphtext : graph_reasoning in text_space\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "graph_neural prompting with large_language_models\n",
      "train your own gnn teacher : graph_aware distillation on textual_graphs\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "disentangled_representation_learning_with_large language_models for text_attributed_graphs\n",
      "gimlet : a unified graph_text model for instruction_based molecule zero_shot learning\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "patton : language_model pretraining on text_rich_networks\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "next!\n",
      "language is all a graph needs\n",
      "graph_aware language_model pre_training on a large graph corpus can help multiple graph applications\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_encoder; remaining in queue: deque([llm_as_aligner, llm_as_predictor, llm_as_aligner, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning])\n",
      "['15', '18', '19']\n",
      "next!\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "functional output regression for machine_learning in materials_science\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "translation between molecules and natural_language\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "galactica : a large_language_model for science\n",
      "interactive molecular discovery with natural_language\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "multi_modal molecule structure_text model for text_based retrieval and editing\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "next!\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "graph_neural prompting with large_language_models\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "language is all a graph needs\n",
      "next!\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "graph_neural prompting with large_language_models\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "language is all a graph needs\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "molxpt : wrapping molecules with text for generative pre_training\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_aligner; remaining in queue: deque([llm_as_predictor, llm_as_aligner, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation])\n",
      "['21', '22']\n",
      "next!\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "graphllm : boosting_graph_reasoning_ability_of_large language_model\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "interactive molecular discovery with natural_language\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "graphtext : graph_reasoning in text_space\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "galactica : a large_language_model for science\n",
      "next!\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "neighborhood contrastive_learning for scientific document representations with citation embeddings\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "git_mol : a multi_modal large_language_model for molecular_science with graph , image , and text\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "text2mol : cross_modal molecule retrieval with natural_language queries\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_aligner, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment])\n",
      "['24', '25']\n",
      "next!\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "functional output regression for machine_learning in materials_science\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "translation between molecules and natural_language\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "a deep_learning system bridging molecule structure and biomedical text with comprehension comparable to human_professionals\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "label_free node_classification on graphs with large_language_models ( llms )\n",
      "unifying molecular and textual_representations via multi_task language_modelling\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "next!\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "molfm : a multimodal molecular foundation_model\n",
      "graph_neural prompting with large_language_models\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "graphtext : graph_reasoning in text_space\n",
      "language is all a graph needs\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_aligner; remaining in queue: deque([graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm])\n",
      "['27']\n",
      "PROCESSING graph_as_sequence; remaining in queue: deque([graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm])\n",
      "['10', '11']\n",
      "next!\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "galactica : a large_language_model for science\n",
      "functional output regression for machine_learning in materials_science\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "interactive molecular discovery with natural_language\n",
      "a deep_learning system bridging molecule structure and biomedical text with comprehension comparable to human_professionals\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "multi_modal molecule structure_text model for text_based retrieval and editing\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "translation between molecules and natural_language\n",
      "unifying molecular and textual_representations via multi_task language_modelling\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "next!\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "disentangled_representation_learning_with_large language_models for text_attributed_graphs\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "patton : language_model pretraining on text_rich_networks\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING graph_empowered_llm; remaining in queue: deque([graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based])\n",
      "[]\n",
      "PROCESSING graph_aware_llm_finetuning; remaining in queue: deque([optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based])\n",
      "[]\n",
      "PROCESSING optimization; remaining in queue: deque([data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based])\n",
      "['16', '17']\n",
      "next!\n",
      "translation between molecules and natural_language\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "functional output regression for machine_learning in materials_science\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "label_free node_classification on graphs with large_language_models ( llms )\n",
      "galactica : a large_language_model for science\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "next!\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "language is all a graph needs\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "patton : language_model pretraining on text_rich_networks\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "molfm : a multimodal molecular foundation_model\n",
      "disentangled_representation_learning_with_large language_models for text_attributed_graphs\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "train your own gnn teacher : graph_aware distillation on textual_graphs\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING data_augmentation; remaining in queue: deque([knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING knowledge_distillation; remaining in queue: deque([prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING prediction_alignment; remaining in queue: deque([latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING latent_space_alignment; remaining in queue: deque([graph_as_sequence, graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING graph_as_sequence; remaining in queue: deque([graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING graph_empowered_llm; remaining in queue: deque([rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING rule_based; remaining in queue: deque([gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING gnn_based; remaining in queue: deque([one_step, two_step])\n",
      "[]\n",
      "PROCESSING one_step; remaining in queue: deque([two_step])\n",
      "[]\n",
      "PROCESSING two_step; remaining in queue: deque([])\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "class_labels = {}\n",
    "class_mappings = {}\n",
    "paper_scores = {}\n",
    "\n",
    "while queue:\n",
    "    target_node = queue.popleft()\n",
    "    print(f\"PROCESSING {target_node.label}; remaining in queue: {queue}\")\n",
    "\n",
    "    class_ids = [child.node_id for child in target_node.children]\n",
    "    print(class_ids)\n",
    "\n",
    "    if len(target_node.children) <= 1:\n",
    "        continue\n",
    "    \n",
    "    ranked_papers = []\n",
    "    for idx in np.arange(len(class_ids)):\n",
    "        ranked_papers.append(taxo.rankPapers([curr_node.emb['sentences'] for curr_node in target_node.children], idx, internal=True, phrase=False, top_k=20))\n",
    "    \n",
    "    for p in ranked_papers:\n",
    "        print(\"next!\")\n",
    "        for c in p:\n",
    "            print(collection[c[0]].title)\n",
    "\n",
    "    ranked_paper_embs = [average_with_harmonic_series([paper_embs[p[0]] for p in c]) for c in ranked_papers]\n",
    "\n",
    "\n",
    "    node_labels, node_paper_scores, node_mapping = taxo.mapPapers(np.array(list(paper_embs.values())), target_node.children, ranked_paper_embs)\n",
    "\n",
    "    class_labels[target_node.node_id] = node_labels\n",
    "    paper_scores[target_node.node_id] = node_paper_scores\n",
    "    class_mappings[target_node.node_id] = node_mapping\n",
    "\n",
    "\n",
    "    # full_text_sim = cosine_similarity_embeddings(list(paper_embs.values()), ranked_paper_embs)\n",
    "    # full_text_preds = full_text_sim.argmax(axis=1)\n",
    "    # full_text_scores = [p_id in target_node.children[pred].gold for p_id, pred in enumerate(full_text_preds)]\n",
    "\n",
    "    # for idx, paper_id in enumerate(target_node.papers):\n",
    "    #     target_node.children[winner_idxs[idx]].papers[paper_id] = target_node.papers[paper_id]\n",
    "\n",
    "    print(\"ranking-based classification\", sum(full_text_scores)/len(full_text_scores))\n",
    "\n",
    "    for child in target_node.children:\n",
    "        queue.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection[0].rankSentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [[1],\n",
       "  [0, 1],\n",
       "  [1, 0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [0],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1, 0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [2],\n",
       "  [2],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [2]],\n",
       " '2': [[1, 2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [1, 0],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [2, 1],\n",
       "  [0],\n",
       "  [2, 1],\n",
       "  [1],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [2],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [2],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [1, 0],\n",
       "  [2],\n",
       "  [1, 0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [2],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0, 1],\n",
       "  [0],\n",
       "  [1]],\n",
       " '3': [[1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0]],\n",
       " '8': [[2],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [1],\n",
       "  [1, 2],\n",
       "  [1, 2],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [2, 0],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [0],\n",
       "  [1, 2],\n",
       "  [2, 0],\n",
       "  [1, 2],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [1, 2],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [0, 2],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [2, 0]],\n",
       " '14': [[2],\n",
       "  [2],\n",
       "  [0, 2],\n",
       "  [0],\n",
       "  [2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [0, 2],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [1, 2],\n",
       "  [0, 2],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [2],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [2, 1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [2, 0]],\n",
       " '20': [[0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0]],\n",
       " '23': [[0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0]],\n",
       " '9': [[1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0]],\n",
       " '15': [[1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [0]]}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rank_by_class_discriminative_significance, filter_by_class_discriminative_significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m phrase_reprs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([taxo\u001b[38;5;241m.\u001b[39mstatic_emb[w]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m internal_phrases], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ranks \u001b[38;5;241m=\u001b[39m rank_by_class_discriminative_significance(phrase_reprs, [average_with_harmonic_series([taxo\u001b[38;5;241m.\u001b[39mstatic_emb[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m c\u001b[38;5;241m.\u001b[39mexternal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrases\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m focus_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren], focus_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mindex(focus_node))\n\u001b[1;32m      3\u001b[0m ranked_tok \u001b[38;5;241m=\u001b[39m {internal_phrases[idx]:rank \u001b[38;5;28;01mfor\u001b[39;00m idx, rank \u001b[38;5;129;01min\u001b[39;00m ranks\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m phrase_reprs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([taxo\u001b[38;5;241m.\u001b[39mstatic_emb[w]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m internal_phrases], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ranks \u001b[38;5;241m=\u001b[39m rank_by_class_discriminative_significance(phrase_reprs, [\u001b[43maverage_with_harmonic_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtaxo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphrases\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m focus_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren], focus_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mindex(focus_node))\n\u001b[1;32m      3\u001b[0m ranked_tok \u001b[38;5;241m=\u001b[39m {internal_phrases[idx]:rank \u001b[38;5;28;01mfor\u001b[39;00m idx, rank \u001b[38;5;129;01min\u001b[39;00m ranks\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/utils.py:162\u001b[0m, in \u001b[0;36maverage_with_harmonic_series\u001b[0;34m(representations, axis)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim):\n\u001b[1;32m    161\u001b[0m     weights[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/numpy/lib/function_base.py:548\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    546\u001b[0m     scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw)\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m--> 548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    549\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    551\u001b[0m     avg \u001b[38;5;241m=\u001b[39m avg_as_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(a, wgt,\n\u001b[1;32m    552\u001b[0m                       dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\u001b[38;5;241m.\u001b[39msum(axis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw) \u001b[38;5;241m/\u001b[39m scl\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "phrase_reprs = np.concatenate([taxo.static_emb[w].reshape((-1, 768)) for w in internal_phrases], axis=0)\n",
    "ranks = rank_by_class_discriminative_significance(phrase_reprs, [average_with_harmonic_series([taxo.static_emb[w] for w in c.external['phrases']]) for c in focus_node.parents[0].children], focus_node.parents[0].children.index(focus_node))\n",
    "ranked_tok = {internal_phrases[idx]:rank for idx, rank in ranks.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 0,\n",
       " 'reasoning': 1,\n",
       " 'classification': 2,\n",
       " 'retrieval': 3,\n",
       " 'inference': 4,\n",
       " 'smoothing': 5,\n",
       " 'graph': 6,\n",
       " 'model': 7,\n",
       " 'knowledge': 8,\n",
       " 'representations': 9,\n",
       " 'policy': 10,\n",
       " 'representation': 11,\n",
       " 'based': 12,\n",
       " 'understanding': 13,\n",
       " 'cross': 14,\n",
       " 'analysis': 15,\n",
       " 'models': 16,\n",
       " 'representation_learning': 17,\n",
       " 'taxonomy': 18,\n",
       " 'routing': 19,\n",
       " 'function': 20,\n",
       " 'generation': 21,\n",
       " 'grouping': 22,\n",
       " 'encoding': 23,\n",
       " 'network': 24,\n",
       " 'trajectory': 25,\n",
       " 'dialogue': 26,\n",
       " 'dictionary': 27,\n",
       " 'using': 28,\n",
       " 'discourse': 29,\n",
       " 'algorithm': 30,\n",
       " 'derivation': 31,\n",
       " 'search': 32,\n",
       " 'with': 33,\n",
       " 'estimates': 34,\n",
       " 'modeling': 35,\n",
       " 'attention': 36,\n",
       " 'prompting': 37,\n",
       " 'targets': 38,\n",
       " 'molecular': 39,\n",
       " 'rating': 40,\n",
       " 'programming': 41,\n",
       " 'scene': 42,\n",
       " 'transmission': 43,\n",
       " 'feature': 44,\n",
       " 'information': 45,\n",
       " 'for': 46,\n",
       " 'task': 47,\n",
       " 'semantic': 48,\n",
       " 'collaboration': 49,\n",
       " 'hierarchy': 50,\n",
       " 'conditioned': 51,\n",
       " 'reward': 52,\n",
       " 'action': 53,\n",
       " 'correction': 54,\n",
       " 'searches': 55,\n",
       " 'structure': 56,\n",
       " 'ordering': 57,\n",
       " 'via': 58,\n",
       " 'entity': 59,\n",
       " 'reading': 60,\n",
       " 'is': 61,\n",
       " 'graph_learning': 62,\n",
       " 'prompt': 63,\n",
       " 'training': 64,\n",
       " 'text': 65,\n",
       " 'style': 66,\n",
       " 'resolution': 67,\n",
       " 'through': 68,\n",
       " 'label': 69,\n",
       " 'machine': 70,\n",
       " 'data': 71,\n",
       " 'kernel': 72,\n",
       " 'referring': 73,\n",
       " 'mode': 74,\n",
       " 'variation': 75,\n",
       " 'fragment': 76,\n",
       " 'in': 77,\n",
       " 'passage': 78,\n",
       " 'track': 79,\n",
       " 'acoustic': 80,\n",
       " 'tensor': 81,\n",
       " '.': 82,\n",
       " 'relation': 83,\n",
       " 'assignment': 84,\n",
       " 'matching': 85,\n",
       " 'proximity': 86,\n",
       " 'descriptions': 87,\n",
       " 'proof': 88,\n",
       " 'textual': 89,\n",
       " 'trace': 90,\n",
       " 'consensus': 91,\n",
       " 'definitions': 92,\n",
       " 'directional': 93,\n",
       " 'joint': 94,\n",
       " 'methods': 95,\n",
       " 'tasks': 96,\n",
       " 'and': 97,\n",
       " 'formula': 98,\n",
       " 'route': 99,\n",
       " 'mention': 100,\n",
       " 'shape': 101,\n",
       " 'graph_processing': 102,\n",
       " 'story': 103,\n",
       " 'operating': 104,\n",
       " 'analyses': 105,\n",
       " 'assistance': 106,\n",
       " 'naming': 107,\n",
       " 'attack': 108,\n",
       " 'semantics': 109,\n",
       " 'can': 110,\n",
       " 'to': 111,\n",
       " 'over': 112,\n",
       " 'description': 113,\n",
       " 'interaction': 114,\n",
       " 'message': 115,\n",
       " 'discrete': 116,\n",
       " 'image': 117,\n",
       " 'guided': 118,\n",
       " 'policies': 119,\n",
       " 'domain': 120,\n",
       " 'indicator': 121,\n",
       " 'reinforcement': 122,\n",
       " 'graph_reasoning': 123,\n",
       " 'context': 124,\n",
       " 'gradient': 125,\n",
       " 'graphs': 126,\n",
       " 'optimization': 127,\n",
       " 'coupling': 128,\n",
       " 'by': 129,\n",
       " 'relational': 130,\n",
       " 'separation': 131,\n",
       " 'framework': 132,\n",
       " 'preference': 133,\n",
       " 'collective': 134,\n",
       " 'machine_learning': 135,\n",
       " 'ranks': 136,\n",
       " 'logic': 137,\n",
       " 'event': 138,\n",
       " 'analytics': 139,\n",
       " 'hypothesis': 140,\n",
       " 'link_prediction': 141,\n",
       " 'temporal': 142,\n",
       " 'recommendation': 143,\n",
       " 'classifications': 144,\n",
       " 'tables': 145,\n",
       " 'measurements': 146,\n",
       " 'membership': 147,\n",
       " 'traffic': 148,\n",
       " 'design': 149,\n",
       " 'indicators': 150,\n",
       " 'skills': 151,\n",
       " 'generators': 152,\n",
       " 'synonym': 153,\n",
       " 'sequence': 154,\n",
       " 'genetic': 155,\n",
       " 'forms': 156,\n",
       " 'features': 157,\n",
       " 'anchor': 158,\n",
       " 'instruction': 159,\n",
       " 'correlation': 160,\n",
       " 'comparisons': 161,\n",
       " 'engagement': 162,\n",
       " 'boundary': 163,\n",
       " 'classification_task': 164,\n",
       " 'networks': 165,\n",
       " 'determination': 166,\n",
       " 'catalyst': 167,\n",
       " 'structured': 168,\n",
       " 'process': 169,\n",
       " 'of': 170,\n",
       " 'from': 171,\n",
       " 'morphological': 172,\n",
       " 'driven': 173,\n",
       " 'exploration': 174,\n",
       " 'data_management': 175,\n",
       " 'specification': 176,\n",
       " 'masks': 177,\n",
       " 'automation': 178,\n",
       " 'trigger': 179,\n",
       " 'similarity': 180,\n",
       " 'back': 181,\n",
       " 'multi': 182,\n",
       " 'sentences': 183,\n",
       " 'set': 184,\n",
       " 'patent': 185,\n",
       " 'locally': 186,\n",
       " 'or': 187,\n",
       " '(': 188,\n",
       " 'has': 189,\n",
       " 'exchange': 190,\n",
       " 'node_classification': 191,\n",
       " 'coordination': 192,\n",
       " 'security': 193,\n",
       " 'architectural': 194,\n",
       " 'variations': 195,\n",
       " 'complex': 196,\n",
       " 'tracking': 197,\n",
       " 'oracle': 198,\n",
       " 'free': 199,\n",
       " 'accounts': 200,\n",
       " 'data_integration': 201,\n",
       " 'geographical': 202,\n",
       " 'definition': 203,\n",
       " 'business': 204,\n",
       " 'marketing': 205,\n",
       " 'tracing': 206,\n",
       " 'educational': 207,\n",
       " 'graph_generation': 208,\n",
       " 'node': 209,\n",
       " 'boosting_graph_reasoning_ability_of_large': 210,\n",
       " 'plan': 211,\n",
       " 'distributed': 212,\n",
       " 'indices': 213,\n",
       " 'strongly': 214,\n",
       " 'theme': 215,\n",
       " 'throughout': 216,\n",
       " 'learning_multiplex_representations_on_text_attributed': 217,\n",
       " 'the': 218,\n",
       " 'interdisciplinary': 219,\n",
       " 'transformations': 220,\n",
       " ',': 221,\n",
       " 'switching': 222,\n",
       " 'filtering': 223,\n",
       " 'defect': 224,\n",
       " 'labels': 225,\n",
       " 'convergence': 226,\n",
       " 'aids': 227,\n",
       " 'on': 228,\n",
       " 'traveling': 229,\n",
       " 'probability': 230,\n",
       " 'tag': 231,\n",
       " 'dynamics': 232,\n",
       " 'claims': 233,\n",
       " 'beam': 234,\n",
       " 'trading': 235,\n",
       " 'authorship': 236,\n",
       " 'insertion': 237,\n",
       " 'feature_selection': 238,\n",
       " 'architecture': 239,\n",
       " 'losses': 240,\n",
       " 'evaluation': 241,\n",
       " 'strategic': 242,\n",
       " 'genre': 243,\n",
       " 'concerning': 244,\n",
       " 'prediction': 245,\n",
       " 'homogeneous': 246,\n",
       " 'regular': 247,\n",
       " 'orientation': 248,\n",
       " 'pairing': 249,\n",
       " 'conditioning': 250,\n",
       " 'learned': 251,\n",
       " 'system': 252,\n",
       " 'word': 253,\n",
       " 'dependence': 254,\n",
       " 'examination': 255,\n",
       " 'chemical': 256,\n",
       " 'language_modeling': 257,\n",
       " 'radical': 258,\n",
       " 'pyramid': 259,\n",
       " 'creation': 260,\n",
       " 'filters': 261,\n",
       " 'formulas': 262,\n",
       " 'extreme_multi_label': 263,\n",
       " 'interfaces': 264,\n",
       " 'graph_based': 265,\n",
       " 'label_propagation': 266,\n",
       " 'prefix': 267,\n",
       " 'predictions': 268,\n",
       " 'node_classication': 269,\n",
       " 'surveys': 270,\n",
       " 'data_fusion': 271,\n",
       " 'model_selection': 272,\n",
       " 'in_context_learning': 273,\n",
       " 'reaches': 274,\n",
       " 'enterprise': 275,\n",
       " 'relations': 276,\n",
       " 'weakly': 277,\n",
       " 'orthogonal': 278,\n",
       " 'health': 279,\n",
       " 'biomedical': 280,\n",
       " 'loss': 281,\n",
       " 'regulation': 282,\n",
       " 'expertise': 283,\n",
       " 'research': 284,\n",
       " 'graphics': 285,\n",
       " 'module': 286,\n",
       " 'sequential': 287,\n",
       " 'sparse': 288,\n",
       " 'ne': 289,\n",
       " 'binary_classification': 290,\n",
       " 'assisted': 291,\n",
       " 'fine': 292,\n",
       " 'portal': 293,\n",
       " 'pre_training': 294,\n",
       " 'projections': 295,\n",
       " 'trends': 296,\n",
       " 'ensembles': 297,\n",
       " 'gold': 298,\n",
       " 'language': 299,\n",
       " '?': 300,\n",
       " 'approximation': 301,\n",
       " 'protocol': 302,\n",
       " 'graph_representations': 303,\n",
       " 'compilation': 304,\n",
       " 'perspective': 305,\n",
       " 'positioning': 306,\n",
       " 'environmental': 307,\n",
       " 'ad': 308,\n",
       " 'are': 309,\n",
       " 'deep': 310,\n",
       " 'stems': 311,\n",
       " 'transformers': 312,\n",
       " 'tool': 313,\n",
       " 'social': 314,\n",
       " 'seeds': 315,\n",
       " 'document_classification': 316,\n",
       " 'probe': 317,\n",
       " 'relationship': 318,\n",
       " 'neighborhood': 319,\n",
       " 'without': 320,\n",
       " 'stemming': 321,\n",
       " 'propagation': 322,\n",
       " 'urban': 323,\n",
       " 'instructions': 324,\n",
       " 'signed': 325,\n",
       " 'variance': 326,\n",
       " 'counterpart': 327,\n",
       " 'term': 328,\n",
       " 'data_model': 329,\n",
       " 'generations': 330,\n",
       " 'searching': 331,\n",
       " 'coming': 332,\n",
       " 'tends': 333,\n",
       " 'text_graph': 334,\n",
       " 'across': 335,\n",
       " 'topic': 336,\n",
       " 'into': 337,\n",
       " 'a': 338,\n",
       " 'that': 339,\n",
       " 'patterns': 340,\n",
       " 'hits': 341,\n",
       " 'both': 342,\n",
       " 'spoken': 343,\n",
       " 'translations': 344,\n",
       " 'text_related': 345,\n",
       " 'recovery': 346,\n",
       " 'at': 347,\n",
       " 'data_structure': 348,\n",
       " 'logical': 349,\n",
       " 'visually': 350,\n",
       " 'yield_prediction': 351,\n",
       " 'nonlinear': 352,\n",
       " 'chart': 353,\n",
       " 'working': 354,\n",
       " 'bug': 355,\n",
       " 'approach': 356,\n",
       " 'pattern': 357,\n",
       " 'storage': 358,\n",
       " 'tools': 359,\n",
       " 'graph_related': 360,\n",
       " 'ea': 361,\n",
       " 'knowledge_graph': 362,\n",
       " 'plans': 363,\n",
       " 'query': 364,\n",
       " 'prediction_task': 365,\n",
       " 'principal': 366,\n",
       " 'career': 367,\n",
       " 'simulations': 368,\n",
       " 'linguistics': 369,\n",
       " 'image_based': 370,\n",
       " 'artificial': 371,\n",
       " 'performance': 372,\n",
       " 'content_based': 373,\n",
       " 'walks': 374,\n",
       " 'display': 375,\n",
       " 'global': 376,\n",
       " 'canonical': 377,\n",
       " 'customer': 378,\n",
       " 'coarse': 379,\n",
       " 'data_quality': 380,\n",
       " 'multi_classification': 381,\n",
       " 'plots': 382,\n",
       " 'structure_free': 383,\n",
       " 'audience': 384,\n",
       " 'data_processing': 385,\n",
       " 'equations': 386,\n",
       " 'sensing': 387,\n",
       " 'substantially': 388,\n",
       " 'symmetric': 389,\n",
       " 'competition': 390,\n",
       " 'techniques': 391,\n",
       " 'clean': 392,\n",
       " 'assessments': 393,\n",
       " 'regulatory': 394,\n",
       " 'text_generation': 395,\n",
       " 'sensitivity': 396,\n",
       " 'tolerance': 397,\n",
       " 'revisited': 398,\n",
       " 'sequences': 399,\n",
       " 'gets': 400,\n",
       " 'graph_free': 401,\n",
       " 'preparation': 402,\n",
       " 'precursor': 403,\n",
       " 'graph_text': 404,\n",
       " 'overlap': 405,\n",
       " ';': 406,\n",
       " 'students': 407,\n",
       " 'toward': 408,\n",
       " 'formats': 409,\n",
       " 'star': 410,\n",
       " 'metadata': 411,\n",
       " 'systems': 412,\n",
       " 'problems': 413,\n",
       " 'uniformly': 414,\n",
       " 'non': 415,\n",
       " 'fragments': 416,\n",
       " 'reviewer': 417,\n",
       " 'structural': 418,\n",
       " 'onto': 419,\n",
       " 'relaxation': 420,\n",
       " 'chunk': 421,\n",
       " 'scholar': 422,\n",
       " 'text_molecule': 423,\n",
       " 'information_extraction': 424,\n",
       " 'situation': 425,\n",
       " 'classification_performance': 426,\n",
       " 'biased': 427,\n",
       " 'level': 428,\n",
       " 'impression': 429,\n",
       " 'stack': 430,\n",
       " 'gains': 431,\n",
       " 'subjective': 432,\n",
       " 'text_based': 433,\n",
       " 'transfers': 434,\n",
       " 'medication': 435,\n",
       " 'text_conditional': 436,\n",
       " 'realistic': 437,\n",
       " 'text_representations': 438,\n",
       " 'problem_solving': 439,\n",
       " 'neural': 440,\n",
       " 'translation': 441,\n",
       " 'database': 442,\n",
       " 'selective': 443,\n",
       " 'embodied': 444,\n",
       " 'sequence_based': 445,\n",
       " 'command': 446,\n",
       " 'fitness': 447,\n",
       " 'messages': 448,\n",
       " 'demographic': 449,\n",
       " 'bar': 450,\n",
       " 'venue': 451,\n",
       " 'masked_language': 452,\n",
       " 'meta_learning': 453,\n",
       " 'rational': 454,\n",
       " 'exceeds': 455,\n",
       " 'text_guided': 456,\n",
       " 'progression': 457,\n",
       " 'abilities': 458,\n",
       " 'quantum': 459,\n",
       " 'cell': 460,\n",
       " 'turkish': 461,\n",
       " 'was': 462,\n",
       " 'network_analysis': 463,\n",
       " 'thinking': 464,\n",
       " 'indexed': 465,\n",
       " 'conflicts': 466,\n",
       " 'cooking': 467,\n",
       " 'knowledge_aware': 468,\n",
       " 'signatures': 469,\n",
       " 'acceptance': 470,\n",
       " 'graph_matching': 471,\n",
       " 'web': 472,\n",
       " 'bank': 473,\n",
       " 'evolving': 474,\n",
       " 'type': 475,\n",
       " 'lines': 476,\n",
       " 'themes': 477,\n",
       " 'mining': 478,\n",
       " 'expectation': 479,\n",
       " 'check': 480,\n",
       " 'texts': 481,\n",
       " 'healthcare': 482,\n",
       " 'macro': 483,\n",
       " 'multiple': 484,\n",
       " 'transferring': 485,\n",
       " 'graph_aware': 486,\n",
       " 'runs': 487,\n",
       " 'human': 488,\n",
       " 'user': 489,\n",
       " 'approaches': 490,\n",
       " 'sensitive': 491,\n",
       " 'ranking': 492,\n",
       " 'node_level': 493,\n",
       " 'aggregation': 494,\n",
       " 'format': 495,\n",
       " 'related': 496,\n",
       " 're': 497,\n",
       " 'functions': 498,\n",
       " 'post_processing': 499,\n",
       " 'reached': 500,\n",
       " 'responding': 501,\n",
       " 'learnt': 502,\n",
       " 'offensive': 503,\n",
       " 'such': 504,\n",
       " 'computational': 505,\n",
       " 'knowledge_base': 506,\n",
       " 'documents': 507,\n",
       " 'identification': 508,\n",
       " 'equivalence': 509,\n",
       " 'knowledgebase': 510,\n",
       " 'large_language_model': 511,\n",
       " 'closure': 512,\n",
       " 'edge_classication': 513,\n",
       " 'stacks': 514,\n",
       " 'graph_level': 515,\n",
       " 'relation_extraction': 516,\n",
       " 'subject': 517,\n",
       " 'supply': 518,\n",
       " 'directly': 519,\n",
       " 'calculation': 520,\n",
       " 'improving_feature_representation_through_graph_centric_finetuning': 521,\n",
       " 'extension': 522,\n",
       " 'text_information': 523,\n",
       " 'compact': 524,\n",
       " 'mono_information': 525,\n",
       " 'lecture': 526,\n",
       " 'kg': 527,\n",
       " 'protocols': 528,\n",
       " 'varies': 529,\n",
       " 'examples': 530,\n",
       " 'text_descriptions': 531,\n",
       " 'content': 532,\n",
       " 'construction': 533,\n",
       " 'hints': 534,\n",
       " 'structure_understanding': 535,\n",
       " 'trust': 536,\n",
       " 'stories': 537,\n",
       " 'representational': 538,\n",
       " 'plot': 539,\n",
       " 'stacked': 540,\n",
       " 'closed': 541,\n",
       " 'augmented': 542,\n",
       " 'inspection': 543,\n",
       " 'cache': 544,\n",
       " 'conceptualization': 545,\n",
       " 'catalog': 546,\n",
       " 'arrays': 547,\n",
       " 'conversations': 548,\n",
       " 'generated': 549,\n",
       " 'decreases': 550,\n",
       " 'text_structure': 551,\n",
       " 'guidelines': 552,\n",
       " 'supervision_signals': 553,\n",
       " 'character_level': 554,\n",
       " 'graph_enhanced': 555,\n",
       " 'assumptions': 556,\n",
       " 'size': 557,\n",
       " 'sliding': 558,\n",
       " 'classification_based': 559,\n",
       " 'cyclic': 560,\n",
       " 'feature_based': 561,\n",
       " 'almost': 562,\n",
       " 'agreement': 563,\n",
       " 'sports': 564,\n",
       " 'phenomena': 565,\n",
       " 'column': 566,\n",
       " 'delay': 567,\n",
       " 'effectively': 568,\n",
       " 'local': 569,\n",
       " 'all': 570,\n",
       " 'sentiment': 571,\n",
       " 'character_based': 572,\n",
       " 'depicting': 573,\n",
       " 'metric': 574,\n",
       " 'graph_augmented': 575,\n",
       " 'language_model': 576,\n",
       " 'interacting': 577,\n",
       " 'restrictions': 578,\n",
       " 'isolated': 579,\n",
       " 'cumulative': 580,\n",
       " 'experiences': 581,\n",
       " 'distribution': 582,\n",
       " 'infomax': 583,\n",
       " 'molecule': 584,\n",
       " 'xml': 585,\n",
       " 'micro': 586,\n",
       " 'sentence_bert': 587,\n",
       " 'scaled': 588,\n",
       " 'exhibited': 589,\n",
       " 'control': 590,\n",
       " 'composed': 591,\n",
       " 'steiner': 592,\n",
       " 'semantic_level': 593,\n",
       " 'zero_': 594,\n",
       " 'populated': 595,\n",
       " 'rewards': 596,\n",
       " 'cue': 597,\n",
       " 'qualities': 598,\n",
       " 'text_only': 599,\n",
       " 'anomaly_detection': 600,\n",
       " 'scripts': 601,\n",
       " 'may': 602,\n",
       " 'property': 603,\n",
       " 'ai': 604,\n",
       " 'mesh': 605,\n",
       " 'private': 606,\n",
       " 'counts': 607,\n",
       " 'inside': 608,\n",
       " 'graph_data': 609,\n",
       " 'ol': 610,\n",
       " 'circular': 611,\n",
       " 'an': 612,\n",
       " 'textural': 613,\n",
       " 'entity_level': 614,\n",
       " 'keeps': 615,\n",
       " 'objective': 616,\n",
       " 'smiles': 617,\n",
       " 'chinese': 618,\n",
       " 'textual_representation': 619,\n",
       " 'lessons': 620,\n",
       " 'parameters': 621,\n",
       " 'spatial': 622,\n",
       " 'settings': 623,\n",
       " 'robot': 624,\n",
       " 'patient': 625,\n",
       " 'document_document': 626,\n",
       " 'conflict': 627,\n",
       " 'files': 628,\n",
       " 'entities': 629,\n",
       " 'collecting': 630,\n",
       " 'trade': 631,\n",
       " 'entirely': 632,\n",
       " 'dedicated': 633,\n",
       " 'message_passing': 634,\n",
       " 'unique': 635,\n",
       " 'textgraph': 636,\n",
       " 'result': 637,\n",
       " 'attributes': 638,\n",
       " 'retrieval_reasoning': 639,\n",
       " 'sketch': 640,\n",
       " 'mined': 641,\n",
       " 'euclidean': 642,\n",
       " 'generalisation': 643,\n",
       " 'graph_structure': 644,\n",
       " 'text_level': 645,\n",
       " 'editors': 646,\n",
       " 'entity_aware': 647,\n",
       " 'properly': 648,\n",
       " 'deep_learning': 649,\n",
       " 'relation_paths': 650,\n",
       " 'uncertainty': 651,\n",
       " 'reaction': 652,\n",
       " 'personalized_pagerank': 653,\n",
       " 'diffusion': 654,\n",
       " 'visual_language': 655,\n",
       " 'situations': 656,\n",
       " 'cluster_analysis': 657,\n",
       " 'advertising': 658,\n",
       " 'text_aware': 659,\n",
       " 'knowledge_intensive': 660,\n",
       " 'wi': 661,\n",
       " 'topology': 662,\n",
       " 'guarantees': 663,\n",
       " 'serving': 664,\n",
       " 'entropy': 665,\n",
       " 'more': 666,\n",
       " 'localized': 667,\n",
       " 'machines': 668,\n",
       " 'narratives': 669,\n",
       " 'customized': 670,\n",
       " 'dialogues': 671,\n",
       " 'constructions': 672,\n",
       " 'arguments': 673,\n",
       " 'remains': 674,\n",
       " 'database_systems': 675,\n",
       " 'versatile_reading': 676,\n",
       " 'excel': 677,\n",
       " 'text_attributed_graphs': 678,\n",
       " 'co_reference': 679,\n",
       " 'em': 680,\n",
       " 'as': 681,\n",
       " 'when': 682,\n",
       " 'new': 683,\n",
       " 'strategy': 684,\n",
       " 'automatic': 685,\n",
       " 'auto_encoding': 686,\n",
       " 'vector': 687,\n",
       " 'associated': 688,\n",
       " 'language_models': 689,\n",
       " 'ensemble': 690,\n",
       " 'brand': 691,\n",
       " 'graph_coloring': 692,\n",
       " 'vocabulary': 693,\n",
       " 'multihop': 694,\n",
       " 'multi_scale': 695,\n",
       " 'reasoning_process': 696,\n",
       " 'form': 697,\n",
       " 'reader': 698,\n",
       " 'representation.in': 699,\n",
       " 'balanced': 700,\n",
       " 'beam_search': 701,\n",
       " 'transforms': 702,\n",
       " 'averaging': 703,\n",
       " 'meta_path': 704,\n",
       " 'weight': 705,\n",
       " 'several': 706,\n",
       " 'developers': 707,\n",
       " 'expressed': 708,\n",
       " 'kg_augmented': 709,\n",
       " 'relationships': 710,\n",
       " 'comparison': 711,\n",
       " 'shared': 712,\n",
       " 'consists': 713,\n",
       " 'graph_neural': 714,\n",
       " 'base': 715,\n",
       " 'daily': 716,\n",
       " 'text_summary': 717,\n",
       " 'graph_neural_networks': 718,\n",
       " 'signs': 719,\n",
       " 'satisfying': 720,\n",
       " 'extraction': 721,\n",
       " 'continuously': 722,\n",
       " 'node_representation_learning_on_heterogeneous': 723,\n",
       " 'node_representations': 724,\n",
       " 'physics': 725,\n",
       " 'rep': 726,\n",
       " 'have': 727,\n",
       " 'ke': 728,\n",
       " 'response': 729,\n",
       " 'returns': 730,\n",
       " 'text_classification': 731,\n",
       " 'influences': 732,\n",
       " 'learningbased': 733,\n",
       " 'prospective': 734,\n",
       " 'clip': 735,\n",
       " 'realization': 736,\n",
       " 'parameter': 737,\n",
       " 'loop': 738,\n",
       " 'replacement': 739,\n",
       " 'labelling': 740,\n",
       " 'meta_paths': 741,\n",
       " 'semantic_based': 742,\n",
       " 'instructional': 743,\n",
       " 'visual_linguistic': 744,\n",
       " 'union': 745,\n",
       " 'penalty': 746,\n",
       " 'relevance_modeling': 747,\n",
       " 'concept': 748,\n",
       " 'query_based': 749,\n",
       " 'hit': 750,\n",
       " 'transferable': 751,\n",
       " 'horizontal': 752,\n",
       " 'graph_syntax': 753,\n",
       " 'citation_based': 754,\n",
       " 'multi_relation': 755,\n",
       " 'rc': 756,\n",
       " 'became': 757,\n",
       " 'notions': 758,\n",
       " 'text_to_graph': 759,\n",
       " 'jointly': 760,\n",
       " 'scheme': 761,\n",
       " 'ned': 762,\n",
       " 'feature_extraction': 763,\n",
       " 'cross_information': 764,\n",
       " 'method': 765,\n",
       " 'care': 766,\n",
       " 'multi_relational': 767,\n",
       " 'relevance': 768,\n",
       " 'intent': 769,\n",
       " 'solid': 770,\n",
       " 'its': 771,\n",
       " 'brain': 772,\n",
       " 'network_enhanced': 773,\n",
       " 'contemporary': 774,\n",
       " 'cid': 775,\n",
       " 'apt': 776,\n",
       " 'ability': 777,\n",
       " 'text_rich': 778,\n",
       " 'assignments': 779,\n",
       " 'graph_isomorphism': 780,\n",
       " 'fingerprint': 781,\n",
       " 'properties': 782,\n",
       " 'facilitates': 783,\n",
       " 'concrete': 784,\n",
       " 'knowledge_alignment': 785,\n",
       " 'network_topology': 786,\n",
       " 'ordered': 787,\n",
       " 'graph_to_text': 788,\n",
       " 'author': 789,\n",
       " 'pairs': 790,\n",
       " 'multi_typed': 791,\n",
       " 'machine_generated': 792,\n",
       " 'graph_representation': 793,\n",
       " 'really': 794,\n",
       " 'data_mining': 795,\n",
       " 'structural_information': 796,\n",
       " 'questions': 797,\n",
       " 'indicative': 798,\n",
       " 'representing': 799,\n",
       " 'partially': 800,\n",
       " 'accuracy': 801,\n",
       " 'helped': 802,\n",
       " 'replication': 803,\n",
       " 'hindi': 804,\n",
       " 'text_oriented': 805,\n",
       " 'gnn_architectures': 806,\n",
       " 'controlling': 807,\n",
       " 'prompting_methods': 808,\n",
       " 'edge_detection': 809,\n",
       " 'knowledge_bases': 810,\n",
       " 'computations': 811,\n",
       " 'arrangement': 812,\n",
       " 'persistent': 813,\n",
       " 'contrastive_learning': 814,\n",
       " 'concepts': 815,\n",
       " 'initially': 816,\n",
       " 'thread': 817,\n",
       " 'paper/6703_inductive_representation_learning_on_large_graphs.pdf': 818,\n",
       " 'visual': 819,\n",
       " 'enjoys': 820,\n",
       " 'statistical_model': 821,\n",
       " 'skeleton_based': 822,\n",
       " 'selections': 823,\n",
       " 'dynamic': 824,\n",
       " 'downstream_performance': 825,\n",
       " 'conveyed': 826,\n",
       " 'node_features': 827,\n",
       " 'graph_like': 828,\n",
       " 'reasonably': 829,\n",
       " 'narrows': 830,\n",
       " 'nodes': 831,\n",
       " 'literal': 832,\n",
       " 'mathematical': 833,\n",
       " 'some': 834,\n",
       " 'label_based': 835,\n",
       " 'gathered': 836,\n",
       " 'train': 837,\n",
       " 'graph_extraction': 838,\n",
       " 'chance': 839,\n",
       " 'tree_structured': 840,\n",
       " 'energy': 841,\n",
       " 'tries': 842,\n",
       " 'different': 843,\n",
       " 'machine_learned': 844,\n",
       " 'results': 845,\n",
       " 'japanese': 846,\n",
       " 'inter': 847,\n",
       " 'ns': 848,\n",
       " 'these': 849,\n",
       " 'layers': 850,\n",
       " 'returned': 851,\n",
       " 'explicit_reasoning': 852,\n",
       " 'applications': 853,\n",
       " 'structured_data': 854,\n",
       " 'accurately': 855,\n",
       " 'decoration': 856,\n",
       " 'retrieval_augmented': 857,\n",
       " 'would': 858,\n",
       " 'within': 859,\n",
       " 'class_level': 860,\n",
       " 'accumulation': 861,\n",
       " 'data_analysis': 862,\n",
       " 'sizes': 863,\n",
       " 'versus': 864,\n",
       " 'invoking_linearization_generation': 865,\n",
       " 'string_based': 866,\n",
       " 'traits': 867,\n",
       " 'algorithms': 868,\n",
       " 'figures': 869,\n",
       " 'gan': 870,\n",
       " 'text_to_graph_molecule_generation': 871,\n",
       " 'delivery': 872,\n",
       " 'truth': 873,\n",
       " 'measure': 874,\n",
       " 'text_image': 875,\n",
       " 'failures': 876,\n",
       " 'covers': 877,\n",
       " 'serialized': 878,\n",
       " 'managed': 879,\n",
       " 'economic': 880,\n",
       " 'corpus_level': 881,\n",
       " 'thai': 882,\n",
       " 'advice': 883,\n",
       " 'post_filtering': 884,\n",
       " 'designs': 885,\n",
       " 'neighboring': 886,\n",
       " 'multi_hop': 887,\n",
       " 'zeroshot': 888,\n",
       " 'introductions': 889,\n",
       " 'model_agnostic': 890,\n",
       " 'exploitation': 891,\n",
       " 'person': 892,\n",
       " 'precisely': 893,\n",
       " 'under': 894,\n",
       " 'tb': 895,\n",
       " 'repeatedly': 896,\n",
       " 'repetition': 897,\n",
       " 'questioning': 898,\n",
       " 'their': 899,\n",
       " 'ow': 900,\n",
       " 'multi_round': 901,\n",
       " 'custom': 902,\n",
       " 'materials': 903,\n",
       " 'infrastructure': 904,\n",
       " 'target': 905,\n",
       " 'knowledge_graphs': 906,\n",
       " 'graph_syntax_tree': 907,\n",
       " 'join': 908,\n",
       " 'classifying': 909,\n",
       " 'grows': 910,\n",
       " 'strategically': 911,\n",
       " 'matched': 912,\n",
       " 'root': 913,\n",
       " 'grounding': 914,\n",
       " 'smile': 915,\n",
       " 'receives': 916,\n",
       " 'one': 917,\n",
       " 'domain_knowledge': 918,\n",
       " 'indirectly': 919,\n",
       " 'advanced': 920,\n",
       " 'reasoning_tasks': 921,\n",
       " 'visible': 922,\n",
       " \"'\": 923,\n",
       " 'stores': 924,\n",
       " 'very': 925,\n",
       " 'visualizations': 926,\n",
       " 'specific': 927,\n",
       " 'dice': 928,\n",
       " 'substitution': 929,\n",
       " 'semantic_information': 930,\n",
       " 'document': 931,\n",
       " 'classic': 932,\n",
       " 'successful': 933,\n",
       " 'modeling_language': 934,\n",
       " 'transductive': 935,\n",
       " 'diabetes': 936,\n",
       " 'simulator': 937,\n",
       " 'effortlessly': 938,\n",
       " 'between': 939,\n",
       " 'dataless': 940,\n",
       " 'assigns': 941,\n",
       " 'reading_comprehension': 942,\n",
       " 'parent': 943,\n",
       " 'due': 944,\n",
       " 'walk_based': 945,\n",
       " 'graph_structured_data': 946,\n",
       " 'predictive_text': 947,\n",
       " 'meta_data': 948,\n",
       " 'translated': 949,\n",
       " 'gnn_nested_transformers': 950,\n",
       " 'zeroshot_entail': 951,\n",
       " 'community_detection': 952,\n",
       " 'exactly': 953,\n",
       " 'resulted': 954,\n",
       " 'priority': 955,\n",
       " 'patents': 956,\n",
       " 'graph_theory': 957,\n",
       " 'websites': 958,\n",
       " 'repeated': 959,\n",
       " 'visual_semantic': 960,\n",
       " 'stopping': 961,\n",
       " 'components': 962,\n",
       " 'guides': 963,\n",
       " 'bio': 964,\n",
       " 'textual_descriptions': 965,\n",
       " 'information_retrieval': 966,\n",
       " 'crawl': 967,\n",
       " 'te': 968,\n",
       " 'retention': 969,\n",
       " 'facts': 970,\n",
       " 'polymer': 971,\n",
       " 'tree_based': 972,\n",
       " 'intention': 973,\n",
       " 'defines': 974,\n",
       " 'perfect': 975,\n",
       " 'human_machine': 976,\n",
       " 'pipeline': 977,\n",
       " 'various': 978,\n",
       " 'en': 979,\n",
       " 'multi_agent': 980,\n",
       " 'constantly': 981,\n",
       " 'reranking': 982,\n",
       " 'metabolic': 983,\n",
       " 'node_summary': 984,\n",
       " 'page_rank': 985,\n",
       " 'clustered': 986,\n",
       " 'recorded': 987,\n",
       " 'marks': 988,\n",
       " 'paragraph_level': 989,\n",
       " 'should': 990,\n",
       " 'trained': 991,\n",
       " 'multi_objective': 992,\n",
       " 'text_gcn': 993,\n",
       " 'yielded': 994,\n",
       " 'context_based': 995,\n",
       " 'pseudo_label': 996,\n",
       " 'discovered': 997,\n",
       " 'test': 998,\n",
       " 'sensible': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_augmentation'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_node = taxo.root.findChild(\"18\")\n",
    "focus_node.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['answer_set',\n",
       " 'answer_prediction',\n",
       " 'answer_generation',\n",
       " 'invoices',\n",
       " 'answering_systems',\n",
       " 'image_question',\n",
       " 'answer_candidates',\n",
       " 'answer_answer',\n",
       " 'disengagement',\n",
       " 'conversation_generation',\n",
       " 'answer_contained',\n",
       " 'chatting',\n",
       " 'non_mediated',\n",
       " 'present_participle',\n",
       " 'past_participle',\n",
       " 'soliciting',\n",
       " 'dialogue_generation',\n",
       " 'intolerant',\n",
       " 'donations',\n",
       " 'candidate_answers',\n",
       " 'hate_speech',\n",
       " 'self_distillation',\n",
       " 'dialogue_interaction',\n",
       " 'teacher_forcing',\n",
       " 'know_why',\n",
       " 'exclamation',\n",
       " 'sub_questions',\n",
       " 'canencourage',\n",
       " 'question_comment',\n",
       " 'complex_questions',\n",
       " 'never_before_seen',\n",
       " 'question2subgraph',\n",
       " 'redirected',\n",
       " 'help_seekers',\n",
       " 'query_answering',\n",
       " 'unanswerable',\n",
       " 'frustratingly',\n",
       " 'quests',\n",
       " 'hate_keyword',\n",
       " 'dialogue_management',\n",
       " 'multi_teacher',\n",
       " 'aroused',\n",
       " 'non_directly',\n",
       " 'few/zero_shot',\n",
       " 'weakly_annotated',\n",
       " 'arouse',\n",
       " 'answering_complex',\n",
       " 'dining',\n",
       " 'helpdesks',\n",
       " 'missing_links',\n",
       " 'redirection',\n",
       " 'hop_attention',\n",
       " 'audiences',\n",
       " 'delegate',\n",
       " 'conversational_agent',\n",
       " 'human_authored',\n",
       " 'well_understood',\n",
       " 'dialogue_systems',\n",
       " 'human_given',\n",
       " 'embodying',\n",
       " 'background_answer',\n",
       " 'speaker_centric',\n",
       " 'deaf_community',\n",
       " 'super_sentential',\n",
       " 'helper',\n",
       " 'received_increasing_attention',\n",
       " 'correct_answer',\n",
       " 'dialog_systems',\n",
       " 'persuasive',\n",
       " 'relieving',\n",
       " 'conversational_agents',\n",
       " 'discourage',\n",
       " 'sad',\n",
       " 'emotion',\n",
       " 'projectivity',\n",
       " 'zero_shot',\n",
       " 'speaker_level',\n",
       " 'fake_news',\n",
       " 'quest',\n",
       " 'natural_sounding',\n",
       " 'traveller',\n",
       " 'do_it_yourself',\n",
       " 'human_understandable',\n",
       " 'who_doing_what',\n",
       " 'explainer',\n",
       " 'hesitant',\n",
       " 'overwhelm',\n",
       " 'explain',\n",
       " 'citizen',\n",
       " 'interaction_understanding',\n",
       " 'embody',\n",
       " 'miscommunication',\n",
       " 'polish_language',\n",
       " 'correct_answers',\n",
       " 'arousing',\n",
       " 'fickleness',\n",
       " 'unknowable',\n",
       " 'crowds',\n",
       " 'gluten_free',\n",
       " 'open_ended',\n",
       " 'user_interested',\n",
       " 'invoke',\n",
       " 'anaphors',\n",
       " 'advisor',\n",
       " 'ques_',\n",
       " 'sought_after',\n",
       " 'propagators',\n",
       " 'curators',\n",
       " 'chat_rooms',\n",
       " 'know_what',\n",
       " 'deceiving',\n",
       " 'boring',\n",
       " 'self_supplementation',\n",
       " 'semi_supervisions',\n",
       " 'invisible',\n",
       " '\\\\rho',\n",
       " 'unnoticeable',\n",
       " 'nontext',\n",
       " 'delegates',\n",
       " 'transfers',\n",
       " 'empathy',\n",
       " 'drastic',\n",
       " 'wrongly_linked',\n",
       " 'fake_news_detection',\n",
       " 'chatt_ing',\n",
       " 'delved',\n",
       " 'non_skilled',\n",
       " 'relieves',\n",
       " 'triggering',\n",
       " 'embodiment',\n",
       " 'sender',\n",
       " 'anaphoras',\n",
       " 'desire',\n",
       " 'auto_suggest',\n",
       " 'indivisible',\n",
       " 'activate',\n",
       " 'hyper_',\n",
       " 'user_to_user',\n",
       " 'holder_target_opinion',\n",
       " 'lovers',\n",
       " 'clients',\n",
       " 'voter',\n",
       " 'vision_languageunderstanding',\n",
       " 'gated_attention',\n",
       " 'guess',\n",
       " 'positive_feedback',\n",
       " 'hme_videoqa',\n",
       " 'hate_it',\n",
       " 'whys',\n",
       " 'self_directed',\n",
       " 'hesitancy',\n",
       " 'multi_emotion',\n",
       " 'hypernymy',\n",
       " 'ok_vqa',\n",
       " 'uploads',\n",
       " 'afden',\n",
       " 'excitement',\n",
       " 'chat',\n",
       " 'curating',\n",
       " 'distort',\n",
       " 'relieve',\n",
       " 'scranviz_a',\n",
       " 'information_missing',\n",
       " 'church_rosser',\n",
       " 'conducive',\n",
       " 'adversative',\n",
       " 'video_grounded',\n",
       " 'moral',\n",
       " 'aided_des',\n",
       " 'self_attending',\n",
       " 'brainstorming',\n",
       " 'anti_trustrank',\n",
       " 'near_human',\n",
       " 'hate',\n",
       " 'purchases',\n",
       " 'bachelor',\n",
       " 'semi_supervision',\n",
       " 'buying',\n",
       " 'bi_attention',\n",
       " 'speaker_layer',\n",
       " 'user_experience',\n",
       " 'user_contributed',\n",
       " 'news_comment',\n",
       " 'instilling',\n",
       " 'disobey',\n",
       " 'deluge',\n",
       " 'acception',\n",
       " 'sarcasm_detection',\n",
       " 'utterance_level',\n",
       " 'uploader',\n",
       " \"'opinion_holder_target\",\n",
       " 'backlog',\n",
       " 'relocation',\n",
       " 'multi_head_attention',\n",
       " \"'citizen\",\n",
       " 'knowhrl',\n",
       " 'donation',\n",
       " 'bi_lstm+multi_head_self_attention',\n",
       " 'anaphora',\n",
       " 'dykgchat',\n",
       " 'unhuman',\n",
       " 'hand_authored',\n",
       " 'homophily',\n",
       " 'curing',\n",
       " 'neighbor_attention',\n",
       " 'condemnation',\n",
       " 'interview_based',\n",
       " '//github.com/madaan/thinkaboutit',\n",
       " 'emotion_driven',\n",
       " 'semi_nave',\n",
       " 'learnable',\n",
       " 'grass',\n",
       " 'hypernymic',\n",
       " 'strongly_',\n",
       " 'endusers',\n",
       " 'speaker_independent',\n",
       " 'inbound_directional',\n",
       " 'non_native',\n",
       " 'buyer_generated',\n",
       " 'muslim',\n",
       " 'enjoys',\n",
       " 'indirection',\n",
       " 'laughter',\n",
       " 'illocutionary',\n",
       " 'dialogue_state',\n",
       " 'reintegrates',\n",
       " 'emotion_centered',\n",
       " '//github.com/urchade/hner',\n",
       " 'human_comprehensible',\n",
       " 'expert_annotated',\n",
       " 'deprive',\n",
       " 'absorbing',\n",
       " 'weakly_',\n",
       " 'crowd_sourced',\n",
       " 'cyberbullying',\n",
       " 'immigrants',\n",
       " 'propaganda',\n",
       " 'counselors',\n",
       " 'generated/exchanged',\n",
       " 'whats',\n",
       " 'non_receipt',\n",
       " 'human_to_human',\n",
       " 'easily_ignored',\n",
       " 'injector',\n",
       " 'injective',\n",
       " 'emotions',\n",
       " 'broking',\n",
       " 'amr_enhanced',\n",
       " 'disable',\n",
       " 'sydney_captions',\n",
       " 'smith',\n",
       " '//github.com/dreaminvoker/sire',\n",
       " 'kullbackleibler',\n",
       " 'gaze+',\n",
       " 'emotionally',\n",
       " 'vaccination',\n",
       " 'crowd',\n",
       " 'hurdle_trust',\n",
       " 'unattainable',\n",
       " 'attributive',\n",
       " 'multi_head_self_attention',\n",
       " 'human_ai',\n",
       " 'orem_af',\n",
       " 'insurmountable',\n",
       " 'emotionless',\n",
       " 'hackforums',\n",
       " 'interquestion',\n",
       " '//github.com/14dtj/cocoqa/video',\n",
       " 'emotion_probiotic',\n",
       " 'storied',\n",
       " 'far_reaching',\n",
       " 'connectionists',\n",
       " 'wonderland',\n",
       " 'emotionality',\n",
       " 'anger',\n",
       " 'crisis',\n",
       " 'enlightenment',\n",
       " 'ike_xai',\n",
       " 'inverse',\n",
       " 'few_shot',\n",
       " 'user_expected',\n",
       " 'dialogue_states',\n",
       " 'ai_supported',\n",
       " 'far_away',\n",
       " 'squashed',\n",
       " 'ad_dition',\n",
       " 're_curate',\n",
       " 'cag_distill',\n",
       " 'gone',\n",
       " 'playground',\n",
       " 'hypo_hypernym',\n",
       " 'emotion_specific',\n",
       " 'co_attention',\n",
       " 'aho_corasick',\n",
       " 'hacker',\n",
       " '//github.com/dreaminvoker/gain',\n",
       " 'desires',\n",
       " 'money_laundering',\n",
       " 'inter_play',\n",
       " 'search/browsing',\n",
       " 'autologistic',\n",
       " 'forgetting',\n",
       " 'overcame',\n",
       " 'input_driven',\n",
       " 'unrestricted_hop',\n",
       " '+multi_head_self_attention+',\n",
       " 'empowering',\n",
       " 'gradcam',\n",
       " 'recourse',\n",
       " 'homekeepers',\n",
       " 'competence',\n",
       " 'oke_cnn',\n",
       " 'fromtext',\n",
       " 'debatepedia',\n",
       " 'hateful',\n",
       " 'transfer',\n",
       " 'rejection',\n",
       " 'attention_like',\n",
       " 'response_selection',\n",
       " 'act_r_called',\n",
       " 'achieved_great_success',\n",
       " 'emotion_level',\n",
       " 'resolve',\n",
       " 'embarrassing',\n",
       " 'affective',\n",
       " 'hashtag_generated',\n",
       " 'attention_assisted',\n",
       " 'embarrassingly',\n",
       " 'affect',\n",
       " 'negating',\n",
       " 'inject',\n",
       " 'target_voter_oriented',\n",
       " 'dream',\n",
       " 'gauss_jordan',\n",
       " 'e_participation',\n",
       " 'motivate',\n",
       " 'selfish',\n",
       " 'root_seeking',\n",
       " 'debilitating',\n",
       " 'hatred',\n",
       " 'polarity_sentiment',\n",
       " 'wechat',\n",
       " 'crowd_workers',\n",
       " 'hadith',\n",
       " 'breaking_news',\n",
       " 'yahoo_yahoo',\n",
       " 'purchase',\n",
       " 'spill',\n",
       " 'motivation',\n",
       " 'customer_satisfaction',\n",
       " 'participatory',\n",
       " 'heartburn.we',\n",
       " 'task_agnostic',\n",
       " 'luke',\n",
       " 'delegated',\n",
       " 'ulti_label',\n",
       " 'disene',\n",
       " 'forced',\n",
       " 'guessing',\n",
       " 'fact_augmentation',\n",
       " 'outbound_directional',\n",
       " 'non_opinion',\n",
       " 'explaining',\n",
       " 'never_ending',\n",
       " 'literal_level',\n",
       " 'speech_acts',\n",
       " 'open_chat',\n",
       " 'prestige',\n",
       " 'attributives',\n",
       " 'hadoop',\n",
       " 'ir_qa',\n",
       " 'foreseeable',\n",
       " 'attention_gated',\n",
       " 'one_shot',\n",
       " 'self_bleu',\n",
       " 'pushouts',\n",
       " 'unfaithful',\n",
       " 'cif_ireland',\n",
       " 'crisis/disaster',\n",
       " 'cross_attention',\n",
       " 'unnoticed',\n",
       " 'web_native',\n",
       " 'good_turing',\n",
       " 'subject_verb_object',\n",
       " 'unlinked',\n",
       " 'eat',\n",
       " 'alienation',\n",
       " 'informal',\n",
       " 'enthusiasts',\n",
       " 'floods',\n",
       " 'user_generated',\n",
       " 'injects',\n",
       " 'reskilling',\n",
       " 'citizen_centred',\n",
       " 'forward_pass',\n",
       " 'emotion_sentiment',\n",
       " 'embodiments',\n",
       " 'on_site',\n",
       " 'invigilator',\n",
       " 'undeveloped',\n",
       " 'knowledge_sharing',\n",
       " 'demecarium',\n",
       " 'impede',\n",
       " 'doesnt',\n",
       " 'exertion',\n",
       " 'speech_act',\n",
       " 'creativity',\n",
       " 'hackforum',\n",
       " 'church',\n",
       " 'transfer_enhanced',\n",
       " 'eats',\n",
       " 'backlink',\n",
       " 'hownet',\n",
       " 'bijective',\n",
       " 'yes_no',\n",
       " 'explanationlp',\n",
       " '//turkunlp.org',\n",
       " 'translation_understanding',\n",
       " 'possessives',\n",
       " 'kullback_leibler',\n",
       " 'client/user',\n",
       " 'like_minded',\n",
       " 'movie_data',\n",
       " 'genuinely',\n",
       " 'self_annotated',\n",
       " 'inquiry',\n",
       " 'fiction',\n",
       " 'surfacing',\n",
       " 'inquestion',\n",
       " 'participation',\n",
       " 'okvqa',\n",
       " 're_enhances',\n",
       " 'wrong_labeled',\n",
       " 'eaten',\n",
       " 'sociable',\n",
       " 'achilles_heel',\n",
       " 'crowdsourcing_based',\n",
       " 'murder',\n",
       " 'response_generation',\n",
       " 'impeding',\n",
       " 'feed_forward',\n",
       " 'fact_tampering',\n",
       " 'deceptively',\n",
       " 'endorsements',\n",
       " 'customer_support',\n",
       " 'non_annotated',\n",
       " 'client',\n",
       " 'affection',\n",
       " 'utterance_layer',\n",
       " 'andemotions',\n",
       " 'pullnet',\n",
       " 'inspirational',\n",
       " 'empowerment',\n",
       " 'valence',\n",
       " 'brother',\n",
       " 'attentions',\n",
       " 'actant_relationship',\n",
       " 'laban_bartenieff',\n",
       " 'opinion_role',\n",
       " 'unwritten',\n",
       " 'sadga',\n",
       " 'non_crisis',\n",
       " 'objection',\n",
       " 'indisputable',\n",
       " 'emotion_detection',\n",
       " 'non_recommendation',\n",
       " 'onesidedness',\n",
       " 'a_la_carte',\n",
       " 'detect_missing',\n",
       " 'willingness',\n",
       " 'pretrain',\n",
       " 'emotion_cause',\n",
       " 'mass_media',\n",
       " 'unnatural',\n",
       " 'people',\n",
       " 'non_superordinate',\n",
       " 'user/client',\n",
       " 'victim',\n",
       " 'online_dating',\n",
       " 'exaustive',\n",
       " 'multiliteracies',\n",
       " 'disconnect',\n",
       " 'harassment',\n",
       " 'acceptablity',\n",
       " 'worker',\n",
       " 'reader/viewers',\n",
       " 'feel',\n",
       " 'user_focussed',\n",
       " 'great_success',\n",
       " 'comprehension_analysis',\n",
       " 'feelings',\n",
       " 'know_how',\n",
       " 'sellers',\n",
       " 'explainable',\n",
       " 'enquiry',\n",
       " 'gauges',\n",
       " 'scrutable',\n",
       " 'play',\n",
       " 'opinion_level',\n",
       " 'musdst',\n",
       " 'exquestions11https',\n",
       " 'aristocratic',\n",
       " 'inspiring',\n",
       " 'involvement',\n",
       " 'objectionable',\n",
       " 'word_play',\n",
       " '20_newsgroups',\n",
       " 'profoundness',\n",
       " 'emotional',\n",
       " 'uncanny',\n",
       " 'empowers',\n",
       " 'mention/entity',\n",
       " 'soulmate',\n",
       " 'understanding_based',\n",
       " 'ultimate_voter',\n",
       " 'language_transfer',\n",
       " 'predicting_missing',\n",
       " 'flourishing',\n",
       " 'out_links',\n",
       " 'distiller',\n",
       " 'human_object_interaction',\n",
       " 'crowd_sourcing',\n",
       " 'apollo_13',\n",
       " 'murmur',\n",
       " 'empowered',\n",
       " 'fostering',\n",
       " 'livemedqa',\n",
       " 'pop_up',\n",
       " 'fictions',\n",
       " 'human_interpretable',\n",
       " 'outwit',\n",
       " 'indonesian_language',\n",
       " 'disability',\n",
       " 'engaged',\n",
       " 'visualizer',\n",
       " 'hackers',\n",
       " 'back_translation',\n",
       " 'image_sharing',\n",
       " 'weakly_supervised',\n",
       " 'director_actor_critic',\n",
       " 'adhd_inattentive',\n",
       " 'domain_adversarial',\n",
       " 'followers/followees',\n",
       " 'motivational',\n",
       " 'deprives',\n",
       " 'call_centre',\n",
       " 'podcast',\n",
       " 'delirium',\n",
       " 'okb',\n",
       " 'mutual_distillation',\n",
       " \"'video+subtitles\",\n",
       " 'dordrecht',\n",
       " 'arguable',\n",
       " 'euphemism',\n",
       " 'preventable',\n",
       " 'okvqa_s3',\n",
       " 'inrs_teacute/leacute/com',\n",
       " '//github.com/noahs_ark/neurboparser',\n",
       " 'llocation',\n",
       " 'great_expectations',\n",
       " 'user_selected',\n",
       " 'prelearned',\n",
       " 'prosperity',\n",
       " 'suicide',\n",
       " 'sagm',\n",
       " 'successor',\n",
       " 'poverty',\n",
       " 'hacking',\n",
       " 'nominations',\n",
       " 'ontonotes_5.0',\n",
       " 'struggle',\n",
       " 'teach',\n",
       " 'offense',\n",
       " 'felt',\n",
       " 'sagdre',\n",
       " 'originator',\n",
       " 'sarcasm',\n",
       " 'adhd_200',\n",
       " 'internet_forum',\n",
       " 'continuations',\n",
       " 'untruthful',\n",
       " 'contributor',\n",
       " 'collectibles',\n",
       " 'fakedetector',\n",
       " 'love_it',\n",
       " 'delaunay',\n",
       " '//github.com/lancopku/well_classified_examples_are_underestimated',\n",
       " 'subordinate',\n",
       " 'cooking',\n",
       " 'inviable',\n",
       " 'autoencoder',\n",
       " 'sheet_fed',\n",
       " 'user_review_object',\n",
       " 'inverseconsultation',\n",
       " 'underreporting',\n",
       " 'non_obvious',\n",
       " 'possessions',\n",
       " 'hyperactivity',\n",
       " 'bullish',\n",
       " 'query_log',\n",
       " 'inflection_point',\n",
       " 'morally_disengaged',\n",
       " 'resolved',\n",
       " 't_cross_attention',\n",
       " 'voorhees',\n",
       " 'emotion_analysis',\n",
       " 'www.people.com.cn',\n",
       " 'rumor',\n",
       " 'explainit',\n",
       " 'hourglass',\n",
       " 'hyponymy',\n",
       " 'struggled',\n",
       " 'mutual_reinforcement_based',\n",
       " 'adamic_adar',\n",
       " 'deeming',\n",
       " 'rendering',\n",
       " 'kannada_language',\n",
       " 'injectivity',\n",
       " 'decisive',\n",
       " 'mutual_reinforcement',\n",
       " 'tensorflow2',\n",
       " 'exploring_and_distilling',\n",
       " 'fully_informed',\n",
       " 'expert_labeled',\n",
       " 'norwegian',\n",
       " 'choice_choice',\n",
       " 'walks_with_backward',\n",
       " 'hypernym',\n",
       " 'self_supervision',\n",
       " 'deski',\n",
       " 'attention_heads',\n",
       " 'underrating',\n",
       " 'hater',\n",
       " 'opponent',\n",
       " 'competences',\n",
       " 'autogenerated',\n",
       " 'impulse',\n",
       " 'imagined',\n",
       " 'homicide',\n",
       " 'group_recommendation',\n",
       " 'underestimate',\n",
       " 'actorcritic',\n",
       " 'contributing',\n",
       " 'plays',\n",
       " 'autopruner',\n",
       " 'folksonomy',\n",
       " 'dialokg',\n",
       " 'domain_transferable',\n",
       " 'semiotics',\n",
       " 'gathers',\n",
       " 'export',\n",
       " 'struggles',\n",
       " 'hyper_relations',\n",
       " 'adjudicators',\n",
       " 'argument_roles',\n",
       " 'morality',\n",
       " 'proxies',\n",
       " '\\\\gls',\n",
       " 'non_linguistic',\n",
       " 'graph_conversation',\n",
       " 'frisian_dutch',\n",
       " 'atgir',\n",
       " 'natural_languages',\n",
       " 'multi_shot',\n",
       " 'counter_intuitively',\n",
       " 'natural',\n",
       " 'panner',\n",
       " 'marriage',\n",
       " 'weak_supervision',\n",
       " 'hypernym_hyponym',\n",
       " 'unaware',\n",
       " 'ontonotes',\n",
       " 'non_opinions',\n",
       " 'entity_disambiguation',\n",
       " 'passports',\n",
       " 'unsupervisory',\n",
       " 'unobserved',\n",
       " 'emptiness',\n",
       " 'suicide_related',\n",
       " 'acceptabilities',\n",
       " 'uyghur',\n",
       " 'teacher_model',\n",
       " 'tolkien',\n",
       " '//youtu.be/vqaxi1wydau',\n",
       " 'knowledge_distillation',\n",
       " 'affordable',\n",
       " 'lurking',\n",
       " 'playing',\n",
       " 'non_content',\n",
       " 'anti_money',\n",
       " 'dehrm',\n",
       " 'home_made',\n",
       " 'sadness',\n",
       " 'forward_backward',\n",
       " 'collaborative_writing',\n",
       " 'unobservable',\n",
       " 'prevented',\n",
       " 'query_resolution',\n",
       " 'explained',\n",
       " 'back_propagated',\n",
       " 'imagination',\n",
       " 'autism',\n",
       " 'retailers',\n",
       " 'human_object',\n",
       " 'person',\n",
       " 'humanitarian',\n",
       " 'nonassociative',\n",
       " '//github.com/torchkge_team/torchkge',\n",
       " 'wielding',\n",
       " 'knowledge_transfer',\n",
       " 'task_success',\n",
       " 'at_a_glance',\n",
       " 'missing_facts',\n",
       " 'rewarding',\n",
       " 'semi_automatically',\n",
       " 'laundering',\n",
       " 'her/his',\n",
       " 'ourselves',\n",
       " 'emotion_affective',\n",
       " 'renderings',\n",
       " 'machine_reading_comprehension',\n",
       " 'taught',\n",
       " 'difficulty_controllable',\n",
       " 'possession',\n",
       " 'argument_clusters',\n",
       " 'fication',\n",
       " 'contributors',\n",
       " 'knowledge_injected',\n",
       " 'victoria',\n",
       " 'evtextract',\n",
       " 'chit_chat',\n",
       " 'lection',\n",
       " 'imagine_and_verbalize',\n",
       " 'spanish_to_english',\n",
       " 'thuyg_20',\n",
       " 'suicide_knowledge',\n",
       " 'bird',\n",
       " 'crisis_intervention',\n",
       " 'humanity',\n",
       " 'populate',\n",
       " 'previously_unseen',\n",
       " 'chunyu',\n",
       " 'pho_tos/videos',\n",
       " 'close_or_open',\n",
       " 'misuse',\n",
       " 'influencers',\n",
       " 'bert+bi_lstm+multi_head_self_attention+fc',\n",
       " 'family_enhanced',\n",
       " 'zero/few_shot_learning',\n",
       " 'creative_summ',\n",
       " 'perniciousness',\n",
       " 'hilton_hotels',\n",
       " 'suicidal',\n",
       " 'brothers',\n",
       " 'childhood',\n",
       " 'funds',\n",
       " 'expressibility',\n",
       " 'udt_qa',\n",
       " 'maker',\n",
       " 'role_to_role',\n",
       " 'video_qa',\n",
       " 'global_augmented',\n",
       " 'out_of_domain',\n",
       " 'self_collected',\n",
       " 'exerted',\n",
       " 'attensy_sner',\n",
       " 'emotional_contagion',\n",
       " 'self_adversarial',\n",
       " 'birds_eye',\n",
       " 'emotion_recognition',\n",
       " 'asthmakgxe',\n",
       " '//github.com/wangbing1416/vagr',\n",
       " 'in_links',\n",
       " 'item_level',\n",
       " 'folks',\n",
       " 'decipherable',\n",
       " 'appropri_ate',\n",
       " 'root_oriented',\n",
       " 'outmost',\n",
       " 'homophonic',\n",
       " 'riding',\n",
       " 'american_football',\n",
       " 'informative_responses',\n",
       " 'bottom_up_attention',\n",
       " 'cad/camobject',\n",
       " 'abstactive',\n",
       " 'oppositions',\n",
       " 'citizens',\n",
       " 'actor',\n",
       " 'day_to_day',\n",
       " 'burdensome',\n",
       " 'full_attention',\n",
       " 'subcollection',\n",
       " 'autistic',\n",
       " 'duc_02',\n",
       " 'free_direction',\n",
       " 'forward_connection',\n",
       " 'long_sought',\n",
       " 'drea',\n",
       " 'overburdened',\n",
       " 'choice_dependent',\n",
       " 'beings',\n",
       " 'wh_',\n",
       " 'adhd_',\n",
       " 'creatively',\n",
       " 'semi_structured',\n",
       " 'knowedge',\n",
       " 'facial_expression',\n",
       " 'myhill_nerode',\n",
       " 'attribution',\n",
       " 'motive',\n",
       " 'exert',\n",
       " 'informality',\n",
       " 'creative',\n",
       " 'idssim_wknkn',\n",
       " 'hyponym',\n",
       " 'william',\n",
       " '//github.com/tsafavi/cascader',\n",
       " 'conll_rdf',\n",
       " 'struggling',\n",
       " 'responsibilities',\n",
       " 'feels',\n",
       " 'expert_curated',\n",
       " 'gerrit',\n",
       " 'streaming_media',\n",
       " 'acts',\n",
       " 'activations',\n",
       " 'loses',\n",
       " 'abridged',\n",
       " 'primary_education',\n",
       " 'golfers',\n",
       " 'invoked',\n",
       " 'necker_cube',\n",
       " 'unreachable',\n",
       " 'modal_specific',\n",
       " 'counterproductive',\n",
       " 'root_to_leaf',\n",
       " 'resolves',\n",
       " 'directives',\n",
       " 'drools',\n",
       " 'moods',\n",
       " 'reputable',\n",
       " 'comprehensibility',\n",
       " 'question_reasoning',\n",
       " 'dehumanization',\n",
       " 'hypernyms',\n",
       " 'co_learns',\n",
       " 'suicide_oriented',\n",
       " 'meaningless',\n",
       " 'feeding',\n",
       " 'high_score',\n",
       " 'unmanageable',\n",
       " 'knowledge_demanding',\n",
       " 'surfer',\n",
       " 'imagetext',\n",
       " 'animated',\n",
       " '//asthmakgxe.moreair.info/',\n",
       " 'off_label',\n",
       " 'serverless',\n",
       " 'label_lacking',\n",
       " 'naturalness',\n",
       " 'whitegoods',\n",
       " 'convolve',\n",
       " 'votership',\n",
       " 'ucca_like',\n",
       " 'anglo_american',\n",
       " 'gaming',\n",
       " 'bonuses',\n",
       " 'multi_attention',\n",
       " 'makehuman',\n",
       " 'one_to_one',\n",
       " 'transductive/inductive',\n",
       " 'demonstrable',\n",
       " 'unlink',\n",
       " 'advertisers',\n",
       " 'origin_destination',\n",
       " 'outlying',\n",
       " 'autoencoders',\n",
       " 'virtue',\n",
       " 'sports',\n",
       " 'supervision_based',\n",
       " 'ishizaki',\n",
       " 'success',\n",
       " 'unattributed',\n",
       " 'lack',\n",
       " 'foreverdreaming',\n",
       " 'rewards',\n",
       " 'demonstrator',\n",
       " 'prabhumoye',\n",
       " 'proud',\n",
       " 'multi_user',\n",
       " 'ontonotes4.0',\n",
       " 'ebm',\n",
       " 'endorsement',\n",
       " 'payment',\n",
       " 'goods',\n",
       " 'exams',\n",
       " 'self_service',\n",
       " 'web_directory',\n",
       " 'evoprompting',\n",
       " 'excellent_results',\n",
       " 'uptoyou',\n",
       " 'amber',\n",
       " 'hakia',\n",
       " 'drug_seeking',\n",
       " 'self_unbiased',\n",
       " 'mirflickr_25',\n",
       " 'mturk287',\n",
       " 'speech_perception',\n",
       " 'unveils',\n",
       " 'information_overload',\n",
       " 'umkm',\n",
       " 'intra_cultural',\n",
       " 'michel',\n",
       " 'neutrality',\n",
       " 'semi_',\n",
       " 'non_pretrained',\n",
       " 'lecturers',\n",
       " 'self_made',\n",
       " 'enemies',\n",
       " 'strongly_supervised',\n",
       " 'kermit',\n",
       " 'overlays',\n",
       " 'walk_induced',\n",
       " 'sunnah',\n",
       " 'co_training',\n",
       " 'event_role',\n",
       " '//www.nactem.ac.uk/cord/',\n",
       " 'hard_attention',\n",
       " 'aimlessly',\n",
       " 'literal_aware',\n",
       " 'objectivity',\n",
       " 'non_intrusive',\n",
       " 'suicide_bombing',\n",
       " 'entity_mention',\n",
       " 'customer_service',\n",
       " 'politifact.com',\n",
       " 'ocr_obj',\n",
       " 'person_focused',\n",
       " 'in_house',\n",
       " 'turn_taking',\n",
       " 'charlie_hebdo',\n",
       " 'inbound_direction',\n",
       " 'col_lection',\n",
       " 'observer',\n",
       " 'tatoeba',\n",
       " 'pretrained',\n",
       " 'inputlog',\n",
       " 'lstms',\n",
       " 'krvqr',\n",
       " 'hake',\n",
       " 'hyper_relational',\n",
       " 'bidirectional_encoder_representations_from_transformers',\n",
       " 'semi_gdner',\n",
       " \"'infectious\",\n",
       " 'incor_poration',\n",
       " 'head_qa',\n",
       " 'phishing',\n",
       " 'attention_enhanced',\n",
       " 'human_human',\n",
       " 'connection_attention',\n",
       " \"'naturalness\",\n",
       " 'category_related',\n",
       " 'newcomers',\n",
       " 'happening',\n",
       " 'hawk_r',\n",
       " 'external_information',\n",
       " 'downward',\n",
       " 'advocacy',\n",
       " 'entity_seeking',\n",
       " 're_harnessing',\n",
       " 'illicit',\n",
       " 'climbing',\n",
       " 'activation',\n",
       " 'uke',\n",
       " 'unintuitive',\n",
       " 'text_generation',\n",
       " 'jeopardises',\n",
       " 'non_semantic',\n",
       " 'inspire',\n",
       " 'de_identified',\n",
       " 'sags',\n",
       " 'rumours',\n",
       " 'feder',\n",
       " 'syllabification',\n",
       " 'non_ironic',\n",
       " 'lively',\n",
       " 'ultimate',\n",
       " 'infuses',\n",
       " 'unidentifiable',\n",
       " 'recall_oriented_understudy_for_gisting',\n",
       " 'unidentified',\n",
       " 'video2entities',\n",
       " 'video_level',\n",
       " 'target_centered',\n",
       " 'curation',\n",
       " 'un_annotated',\n",
       " 'strands',\n",
       " 'riders',\n",
       " 'motivations',\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_node.external['phrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_embs = {idx: paper_emb for idx, paper_emb in enumerate(sentence_model.encode([paper.title for paper in collection]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING using_llms_on_graphs; remaining in queue: deque([])\n",
      "['1', '2', '3']\n",
      "before ranking-based classification:  0.6111111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 4096.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.7916666666666666\n",
      "PROCESSING pure_graphs; remaining in queue: deque([text_rich_graphs, text_paired_graphs])\n",
      "['4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2270.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.7142857142857143\n",
      "PROCESSING text_rich_graphs; remaining in queue: deque([text_paired_graphs, llm_as_predictor])\n",
      "['8', '14', '20']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3788.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.3\n",
      "PROCESSING text_paired_graphs; remaining in queue: deque([llm_as_predictor, llm_as_predictor, llm_as_encoder, llm_as_aligner])\n",
      "['23', '26']\n",
      "before ranking-based classification:  0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4433.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.0\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_predictor, llm_as_encoder, llm_as_aligner, llm_as_predictor, llm_as_aligner])\n",
      "['5', '6', '7']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.07142857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 4675.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.21428571428571427\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_encoder, llm_as_aligner, llm_as_predictor, llm_as_aligner, direct_answering, heuristic_reasoning, algorithmic_reasoning])\n",
      "['9', '12', '13']\n",
      "before ranking-based classification:  0.06060606060606061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 5617.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.18181818181818182\n",
      "PROCESSING llm_as_encoder; remaining in queue: deque([llm_as_aligner, llm_as_predictor, llm_as_aligner, direct_answering, heuristic_reasoning, algorithmic_reasoning, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning])\n",
      "['15', '18', '19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 5140.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.0\n",
      "PROCESSING llm_as_aligner; remaining in queue: deque([llm_as_predictor, llm_as_aligner, direct_answering, heuristic_reasoning, algorithmic_reasoning, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation])\n",
      "['21', '22']\n",
      "before ranking-based classification:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4718.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.0\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_aligner, direct_answering, heuristic_reasoning, algorithmic_reasoning, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment])\n",
      "['24', '25']\n",
      "before ranking-based classification:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m full_text_class_embs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c_id \u001b[38;5;129;01min\u001b[39;00m tqdm(class_ids):\n\u001b[0;32m---> 37\u001b[0m     full_text_class_embs\u001b[38;5;241m.\u001b[39mappend(\u001b[43maverage_with_harmonic_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpaper_embs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     39\u001b[0m full_text_sim \u001b[38;5;241m=\u001b[39m cosine_similarity_embeddings(np\u001b[38;5;241m.\u001b[39marray([paper_embs[p_id] \u001b[38;5;28;01mfor\u001b[39;00m p_id \u001b[38;5;129;01min\u001b[39;00m target_node\u001b[38;5;241m.\u001b[39mpapers]), full_text_class_embs)\n\u001b[1;32m     40\u001b[0m full_text_preds \u001b[38;5;241m=\u001b[39m full_text_sim\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/utils.py:136\u001b[0m, in \u001b[0;36maverage_with_harmonic_series\u001b[0;34m(representations, axis)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim):\n\u001b[1;32m    135\u001b[0m     weights[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/numpy/lib/function_base.py:548\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    546\u001b[0m     scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw)\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m--> 548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    549\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    551\u001b[0m     avg \u001b[38;5;241m=\u001b[39m avg_as_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(a, wgt,\n\u001b[1;32m    552\u001b[0m                       dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\u001b[38;5;241m.\u001b[39msum(axis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw) \u001b[38;5;241m/\u001b[39m scl\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "while queue:\n",
    "    target_node = queue.popleft()\n",
    "    print(f\"PROCESSING {target_node.label}; remaining in queue: {queue}\")\n",
    "\n",
    "    class_ids = [child.node_id for child in target_node.children]\n",
    "    print(class_ids)\n",
    "\n",
    "    phrase_class_embs = [computeClassEmb(curr_node, taxo, granularity='phrases') for curr_node in target_node.children]\n",
    "    sent_class_embs = [computeClassEmb(curr_node, taxo, granularity='sentences') for curr_node in target_node.children]\n",
    "    joint_class_embs = [(p+s)/2 for p, s in zip(phrase_class_embs, sent_class_embs)]\n",
    "\n",
    "    encoded_papers = np.array([paper_embs[p_id] for p_id in target_node.papers])\n",
    "    phrase_sim = cosine_similarity_embeddings(encoded_papers, phrase_class_embs) # P x C\n",
    "    sent_sim = cosine_similarity_embeddings(encoded_papers, sent_class_embs)\n",
    "    joint_sim = cosine_similarity_embeddings(encoded_papers, joint_class_embs)\n",
    "\n",
    "    phrase_winner = phrase_sim.argmax(axis=1) # P x 1\n",
    "    sent_winner = sent_sim.argmax(axis=1)\n",
    "    joint_winner = joint_sim.argmax(axis=1)\n",
    "    winner_idxs = stats.mode(np.stack((phrase_winner, sent_winner, joint_winner), axis=1), axis=1, keepdims=False)[0]\n",
    "    labels = [class_ids[winner_idx] for winner_idx in winner_idxs]\n",
    "    correct = [paper_id in target_node.children[winner_idxs[idx]].gold for idx, paper_id in enumerate(target_node.papers)]\n",
    "    print(\"before ranking-based classification: \", sum(correct)/len(correct))\n",
    "\n",
    "    scores = (phrase_sim[np.arange(len(phrase_sim)), \n",
    "                        winner_idxs] + sent_sim[np.arange(len(sent_sim)), \n",
    "                                                winner_idxs] + joint_sim[np.arange(len(joint_sim)), \n",
    "                                                                        winner_idxs])/3\n",
    "\n",
    "    class_map = {i:[] for i in class_ids}\n",
    "    for idx, paper_id in enumerate(target_node.papers):\n",
    "        class_map[labels[idx]].append((scores[idx], target_node.papers[paper_id]))\n",
    "\n",
    "    class_map = {i:sorted(class_map[i], key=lambda x: -x[0]) for i in class_ids}\n",
    "    full_text_class_embs = []\n",
    "    for c_id in tqdm(class_ids):\n",
    "        full_text_class_embs.append(average_with_harmonic_series(np.array([paper_embs[p[1].id] for p in class_map[c_id]])))\n",
    "\n",
    "    full_text_sim = cosine_similarity_embeddings(np.array([paper_embs[p_id] for p_id in target_node.papers]), full_text_class_embs)\n",
    "    full_text_preds = full_text_sim.argmax(axis=1)\n",
    "    full_text_scores = [p_id in target_node.children[pred].gold for p_id, pred in enumerate(full_text_preds)]\n",
    "\n",
    "    for idx, paper_id in enumerate(target_node.papers):\n",
    "        target_node.children[winner_idxs[idx]].papers[paper_id] = target_node.papers[paper_id]\n",
    "\n",
    "    print(\"ranking-based classification\", sum(full_text_scores)/len(full_text_scores))\n",
    "\n",
    "    for child in target_node.children:\n",
    "        queue.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 7, 3, 59, 49, 47]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p[1].id for p in class_map[c_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_class_embs.append(average_with_harmonic_series(np.array([encoded_papers[p[1].id] for p in class_map[c_id]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8', '14', '20']\n",
      "llm_as_predictor: 57 phrases filtered!\n",
      "llm_as_encoder: 75 phrases filtered!\n",
      "llm_as_aligner: 27 phrases filtered!\n",
      "llm_as_predictor: 46 sentences filtered!\n",
      "llm_as_encoder: 12 sentences filtered!\n",
      "llm_as_aligner: 18 sentences filtered!\n"
     ]
    }
   ],
   "source": [
    "target_node = taxo.root#.children[1]\n",
    "\n",
    "class_ids = [child.node_id for child in target_node.children]\n",
    "print(class_ids)\n",
    "\n",
    "phrase_class_embs = [computeClassEmb(curr_node, taxo, granularity='phrases') for curr_node in target_node.children]\n",
    "sent_class_embs = [computeClassEmb(curr_node, taxo, granularity='sentences') for curr_node in target_node.children]\n",
    "# joint_class_embs = [computeClassEmb(curr_node, taxo, granularity='mixed') for curr_node in target_node.children]\n",
    "joint_class_embs = [(p+s)/2 for p, s in zip(phrase_class_embs, sent_class_embs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_rich_graphs: 6586 phrases filtered!\n"
     ]
    }
   ],
   "source": [
    "external_phrase_embs, sim_diff, keep_phrase = compareClasses(external_phrases, taxo, target_node.children[0], 'phrases')\n",
    "filtered_embs = external_phrase_embs[keep_phrase, :]\n",
    "filtered_external_phrases = list(compress(external_phrases, keep_phrase))\n",
    "filtered_diffs = sim_diff[keep_phrase]\n",
    "print(f'{target_node.label}: {len(external_phrases) - len(filtered_external_phrases)} phrases filtered!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.8194444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "encoded_papers = sentence_model.encode([collection[p_id].raw_text for p_id in target_node.papers])\n",
    "phrase_sim = cosine_similarity_embeddings(encoded_papers, phrase_class_embs) # P x C\n",
    "sent_sim = cosine_similarity_embeddings(encoded_papers, sent_class_embs)\n",
    "joint_sim = cosine_similarity_embeddings(encoded_papers, joint_class_embs)\n",
    "\n",
    "phrase_winner = phrase_sim.argmax(axis=1) # P x 1\n",
    "sent_winner = sent_sim.argmax(axis=1)\n",
    "joint_winner = joint_sim.argmax(axis=1)\n",
    "winner_idxs = stats.mode(np.stack((phrase_winner, sent_winner, joint_winner), axis=1), axis=1, keepdims=False)[0]\n",
    "labels = [class_ids[winner_idx] for winner_idx in winner_idxs]\n",
    "correct = [paper_id in target_node.children[winner_idxs[idx]].gold for idx, paper_id in enumerate(target_node.papers)]\n",
    "print(\"before ranking-based classification: \", sum(correct)/len(correct))\n",
    "\n",
    "scores = (phrase_sim[np.arange(len(phrase_sim)), \n",
    "                    winner_idxs] + sent_sim[np.arange(len(sent_sim)), \n",
    "                                            winner_idxs] + joint_sim[np.arange(len(joint_sim)), \n",
    "                                                                      winner_idxs])/3\n",
    "\n",
    "class_map = {i:[] for i in class_ids}\n",
    "for idx, paper_id in enumerate(target_node.papers):\n",
    "    class_map[labels[idx]].append((scores[idx], target_node.papers[paper_id]))\n",
    "\n",
    "class_map = {i:sorted(class_map[i], key=lambda x: -x[0]) for i in class_ids}\n",
    "full_text_class_embs = []\n",
    "for c_id in tqdm(class_ids):\n",
    "    full_text_class_embs.append(average_with_harmonic_series(sentence_model.encode([p[1].raw_text for p in class_map[c_id]])))\n",
    "\n",
    "full_text_sim = cosine_similarity_embeddings(sentence_model.encode([collection[p_id].raw_text for p_id in target_node.papers]), full_text_class_embs)\n",
    "full_text_preds = full_text_sim.argmax(axis=1)\n",
    "full_text_scores = [p_id in target_node.children[pred].gold for p_id, pred in enumerate(full_text_preds)]\n",
    "\n",
    "# for idx, paper_id in enumerate(target_node.papers):\n",
    "#     target_node.children[winner_idxs[idx]].papers[paper_id] = target_node.papers[paper_id]\n",
    "\n",
    "print(\"ranking-based classification\", sum(full_text_scores)/len(full_text_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title: 0.24444444444444444\n",
    "title + abstract: 0.26666666666666666\n",
    "raw_text: 0.28888888888888886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5,6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/')\n",
    "model = BertModel.from_pretrained('/home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/', output_hidden_states=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<00:00, 1444.28it/s]\n",
      "100%|██████████| 51136/51136 [00:06<00:00, 7733.38it/s]\n"
     ]
    }
   ],
   "source": [
    "cnt = defaultdict(int)\n",
    "all_raw_phrases = []\n",
    "token_lens = {}\n",
    "\n",
    "for paper in tqdm(collection):\n",
    "\tall_raw_phrases.extend(paper.raw_text.strip().split())\n",
    "\n",
    "for phrase in tqdm(set(all_raw_phrases)):\n",
    "\ttoken_lens[phrase] = len(tokenizer(phrase).input_ids)\n",
    "\n",
    "cnt = Counter(all_raw_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeProp(phrase_sim, sent_sim):\n",
    "    print('phrase avg:', phrase_sim.mean(axis=0))\n",
    "    print('sent avg:', sent_sim.mean(axis=0))\n",
    "    phrase_count = Counter(phrase_sim.argmax(axis=1))\n",
    "    phrase_list = np.array([phrase_count[i] \n",
    "                            if i in phrase_count \n",
    "                            else 0 \n",
    "                            for i in np.arange(len(taxo.root.children))])\n",
    "    sent_count = Counter(sent_sim.argmax(axis=1))\n",
    "    sent_list = np.array([sent_count[i] \n",
    "                          if i in sent_count \n",
    "                          else 0 \n",
    "                          for i in np.arange(len(taxo.root.children))])\n",
    "    \n",
    "    print(f\"Phrase: {phrase_list/sum(phrase_list)}\")\n",
    "    print(f\"Sentence: {sent_list/sum(sent_list)}\")\n",
    "    return phrase_count, sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(text, token_lens, length=512):\n",
    "\tchunks = [[]]\n",
    "\tsplit_text = text.split()\n",
    "\tcount = 0\n",
    "\tfor word in split_text:\n",
    "\t\tnew_count = count + token_lens[word] + 2 # 2 for [CLS] and [SEP]\n",
    "\t\tif new_count > length:\n",
    "\t\t\tchunks.append([word])\n",
    "\t\t\tcount = token_lens[word]\n",
    "\t\telse:\n",
    "\t\t\tchunks[len(chunks) - 1].append(word)\n",
    "\t\t\tcount = new_count\n",
    "\t\n",
    "\tchunks = [\" \".join(chunk) for chunk in chunks]\n",
    "\treturn chunks if len(chunks) > 1 else chunks[0]\n",
    "\n",
    "def encode(w):\n",
    "\tif type(w) == str:\n",
    "\t\tchunks = chunkify(w, token_lens)\n",
    "\telse:\n",
    "\t\tchunks = w\n",
    "\t\n",
    "\tencoded_data = tokenizer(chunks, padding=True, return_tensors=\"pt\").to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(**encoded_data)\n",
    "\t\n",
    "\t# Get all hidden states\n",
    "\tif (type(w) == str) and (len(output.last_hidden_state.shape) == 3):\n",
    "\t\treturn output.last_hidden_state.mean(axis=(0,1)).cpu().numpy()\n",
    "\telse:\n",
    "\t\treturn output.last_hidden_state.mean(axis=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [05:51<00:00,  4.88s/it]\n"
     ]
    }
   ],
   "source": [
    "cnt = defaultdict(int)\n",
    "vocab_emb = {}\n",
    "vocab_sent_emb = {}\n",
    "\n",
    "for paper in tqdm(collection):\n",
    "\tdata = paper.raw_text.strip().split()\n",
    "\tsent_data = paper.raw_text.strip().split(\" . \")\n",
    "\tp_embs = sentence_model.encode(data)\n",
    "\ts_embs = sentence_model.encode(sent_data)\n",
    "\tfor w_id, word in enumerate(data):\n",
    "\t\tcnt[word] += 1\n",
    "\t\tif word not in vocab_emb:\n",
    "\t\t\tvocab_emb[word] = p_embs[w_id]\n",
    "\tfor s_id, sent in enumerate(sent_data):\n",
    "\t\tvocab_sent_emb[sent] = s_embs[s_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 423/423 [00:03<00:00, 135.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for w in tqdm(all_common_phrases):\n",
    "    if w not in vocab_emb:\n",
    "        vocab_emb[w] = sentence_model.encode(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 46604),\n",
       " ('.', 31280),\n",
       " ('the', 30334),\n",
       " ('and', 14675),\n",
       " (')', 14552),\n",
       " ('(', 14263),\n",
       " ('of', 12858),\n",
       " ('to', 10450),\n",
       " ('in', 10030),\n",
       " ('a', 9576),\n",
       " ('for', 6881),\n",
       " (':', 6764),\n",
       " ('we', 6445),\n",
       " ('is', 5707),\n",
       " ('on', 5222),\n",
       " ('as', 4366),\n",
       " ('with', 4331),\n",
       " ('[', 3910),\n",
       " ('graph', 3906),\n",
       " (']', 3898),\n",
       " ('et', 3473),\n",
       " ('that', 3272),\n",
       " ('are', 3188),\n",
       " (';', 2906),\n",
       " ('model', 2896),\n",
       " ('al.', 2762),\n",
       " ('from', 2608),\n",
       " ('this', 2602),\n",
       " ('by', 2589),\n",
       " ('can', 2191),\n",
       " ('1', 1978),\n",
       " ('our', 1948),\n",
       " ('be', 1889),\n",
       " ('%', 1851),\n",
       " ('tasks', 1850),\n",
       " ('models', 1797),\n",
       " ('llms', 1727),\n",
       " ('which', 1720),\n",
       " ('an', 1640),\n",
       " ('text', 1515),\n",
       " ('2', 1514),\n",
       " ('performance', 1494),\n",
       " ('it', 1456),\n",
       " ('reasoning', 1379),\n",
       " ('information', 1368),\n",
       " ('node', 1330),\n",
       " ('molecule', 1328),\n",
       " ('data', 1311),\n",
       " ('nodes', 1249),\n",
       " ('language', 1242),\n",
       " ('task', 1206),\n",
       " ('3', 1181),\n",
       " ('each', 1145),\n",
       " ('or', 1129),\n",
       " ('table', 1107),\n",
       " ('results', 1087),\n",
       " ('have', 1038),\n",
       " ('not', 1023),\n",
       " ('dataset', 1015),\n",
       " ('paper', 1003),\n",
       " ('different', 996),\n",
       " ('these', 993),\n",
       " ('2022', 978),\n",
       " ('more', 959),\n",
       " ('4', 952),\n",
       " ('all', 946),\n",
       " ('such', 926),\n",
       " ('at', 919),\n",
       " ('also', 916),\n",
       " ('two', 910),\n",
       " ('between', 898),\n",
       " ('training', 897),\n",
       " ('2023', 886),\n",
       " ('using', 879),\n",
       " ('input', 861),\n",
       " ('datasets', 857),\n",
       " ('use', 852),\n",
       " ('graphs', 848),\n",
       " ('2021', 830),\n",
       " ('learning', 828),\n",
       " ('figure', 828),\n",
       " ('prediction', 821),\n",
       " ('2020', 820),\n",
       " ('has', 812),\n",
       " ('both', 805),\n",
       " ('knowledge', 795),\n",
       " ('#', 795),\n",
       " ('*', 786),\n",
       " ('into', 771),\n",
       " ('>', 765),\n",
       " ('5', 762),\n",
       " ('2019', 758),\n",
       " ('where', 749),\n",
       " ('their', 729),\n",
       " ('al', 699),\n",
       " ('molecules', 697),\n",
       " ('its', 684),\n",
       " ('based', 661),\n",
       " ('other', 656),\n",
       " ('methods', 649),\n",
       " ('c', 639),\n",
       " ('representations', 638),\n",
       " ('used', 630),\n",
       " ('work', 629),\n",
       " ('only', 625),\n",
       " (\"''\", 615),\n",
       " ('given', 612),\n",
       " ('one', 604),\n",
       " ('0', 589),\n",
       " ('prompt', 586),\n",
       " ('gnn', 583),\n",
       " ('@', 582),\n",
       " ('natural_language', 579),\n",
       " ('llm', 579),\n",
       " ('while', 577),\n",
       " ('<', 571),\n",
       " ('set', 565),\n",
       " ('will', 555),\n",
       " ('``', 547),\n",
       " ('number', 543),\n",
       " ('however', 539),\n",
       " ('when', 535),\n",
       " ('?', 526),\n",
       " ('6', 522),\n",
       " ('molecular', 522),\n",
       " ('generate', 514),\n",
       " ('transformer', 514),\n",
       " ('pre_training', 513),\n",
       " ('structure', 511),\n",
       " ('then', 505),\n",
       " ('method', 501),\n",
       " ('gnns', 500),\n",
       " ('than', 499),\n",
       " ('}', 494),\n",
       " ('bert', 491),\n",
       " ('{', 488),\n",
       " ('framework', 486),\n",
       " ('example', 481),\n",
       " ('generation', 475),\n",
       " ('tokens', 470),\n",
       " ('representation', 469),\n",
       " ('approach', 467),\n",
       " ('shown', 464),\n",
       " ('lm', 462),\n",
       " ('zhang', 456),\n",
       " ('large', 451),\n",
       " ('experiments', 449),\n",
       " ('first', 447),\n",
       " ('three', 443),\n",
       " ('g', 441),\n",
       " ('i', 439),\n",
       " ('accuracy', 438),\n",
       " ('&', 438),\n",
       " ('new', 435),\n",
       " ('embeddings', 432),\n",
       " ('most', 430),\n",
       " ('features', 428),\n",
       " ('various', 425),\n",
       " ('e', 424),\n",
       " ('embedding', 415),\n",
       " ('same', 415),\n",
       " ('but', 412),\n",
       " ('function', 410),\n",
       " ('conference', 408),\n",
       " ('understanding', 408),\n",
       " ('retrieval', 406),\n",
       " ('zero_shot', 404),\n",
       " ('li', 403),\n",
       " ('encoder', 401),\n",
       " ('output', 399),\n",
       " ('lms', 398),\n",
       " ('smiles', 398),\n",
       " ('query', 396),\n",
       " ('following', 395),\n",
       " ('2018', 394),\n",
       " ('label', 394),\n",
       " ('further', 393),\n",
       " ('7', 392),\n",
       " ('edges', 389),\n",
       " ('research', 386),\n",
       " ('wang', 385),\n",
       " ('pretraining', 385),\n",
       " ('over', 382),\n",
       " ('edge', 377),\n",
       " ('n', 376),\n",
       " ('_', 372),\n",
       " ('generated', 371),\n",
       " ('e.g.', 367),\n",
       " ('t', 366),\n",
       " ('section', 366),\n",
       " ('propose', 363),\n",
       " ('better', 363),\n",
       " ('been', 363),\n",
       " ('pre_trained', 362),\n",
       " ('8', 361),\n",
       " ('b', 360),\n",
       " ('fine_tuning', 359),\n",
       " ('10', 358),\n",
       " ('may', 352),\n",
       " ('node_classification', 352),\n",
       " ('r', 351),\n",
       " ('sequence', 350),\n",
       " ('h', 350),\n",
       " ('proposed', 347),\n",
       " ('token', 340),\n",
       " ('time', 340),\n",
       " ('neighbors', 338),\n",
       " ('s', 336),\n",
       " ('through', 335),\n",
       " ('baselines', 334),\n",
       " ('attention', 334),\n",
       " ('protein', 334),\n",
       " ('there', 332),\n",
       " ('trained', 332),\n",
       " ('how', 328),\n",
       " ('prompts', 327),\n",
       " ('without', 327),\n",
       " ('them', 326),\n",
       " ('api', 325),\n",
       " ('including', 324),\n",
       " ('via', 324),\n",
       " ('question', 323),\n",
       " ('answer', 323),\n",
       " ('process', 322),\n",
       " ('liu', 322),\n",
       " ('examples', 320),\n",
       " ('similar', 320),\n",
       " ('property', 319),\n",
       " ('test', 318),\n",
       " ('labels', 318),\n",
       " ('they', 317),\n",
       " ('chemical', 317),\n",
       " ('network', 316),\n",
       " ('papers', 316),\n",
       " ('study', 314),\n",
       " ('semantic', 312),\n",
       " ('large_language_models', 311),\n",
       " ('across', 311),\n",
       " ('d', 310),\n",
       " ('ability', 308),\n",
       " ('encoding', 305),\n",
       " ('some', 303),\n",
       " ('parameters', 303),\n",
       " ('user', 301),\n",
       " ('evaluation', 300),\n",
       " ('learn', 300),\n",
       " ('2017', 299),\n",
       " ('=', 298),\n",
       " ('v', 297),\n",
       " ('i.e.', 297),\n",
       " ('textual', 293),\n",
       " ('p', 291),\n",
       " ('like', 290),\n",
       " ('context', 289),\n",
       " ('+', 289),\n",
       " ('iclr', 286),\n",
       " ('no', 284),\n",
       " ('proceedings', 284),\n",
       " ('best', 282),\n",
       " ('perform', 281),\n",
       " ('layer', 281),\n",
       " ('similarity', 281),\n",
       " ('https', 280),\n",
       " ('provide', 280),\n",
       " ('within', 278),\n",
       " ('compared', 276),\n",
       " ('chen', 276),\n",
       " ('classification', 276),\n",
       " ('relation', 276),\n",
       " ('kg', 275),\n",
       " ('structures', 271),\n",
       " ('modeling', 271),\n",
       " ('...', 270),\n",
       " ('description', 270),\n",
       " ('problem', 269),\n",
       " ('existing', 268),\n",
       " ('texts', 267),\n",
       " ('feature', 267),\n",
       " ('design', 263),\n",
       " ('applications', 262),\n",
       " ('appendix', 262),\n",
       " ('9', 262),\n",
       " ('well', 262),\n",
       " ('corpus', 261),\n",
       " ('published', 261),\n",
       " ('respectively', 260),\n",
       " ('since', 259),\n",
       " ('could', 258),\n",
       " ('analysis', 257),\n",
       " ('score', 256),\n",
       " ('about', 253),\n",
       " ('any', 252),\n",
       " ('domain', 252),\n",
       " ('multiple', 251),\n",
       " ('specific', 251),\n",
       " ('prompting', 250),\n",
       " ('chatgpt', 250),\n",
       " ('show', 249),\n",
       " ('types', 249),\n",
       " ('specifically', 247),\n",
       " ('corresponding', 247),\n",
       " ('train', 246),\n",
       " ('evaluate', 245),\n",
       " ('document', 243),\n",
       " ('discovery', 242),\n",
       " ('machine_learning', 240),\n",
       " ('descriptions', 238),\n",
       " ('link_prediction', 238),\n",
       " ('random', 236),\n",
       " ('o', 236),\n",
       " ('connected', 235),\n",
       " ('potential', 235),\n",
       " ('thus', 234),\n",
       " ('original', 234),\n",
       " ('system', 234),\n",
       " ('entities', 233),\n",
       " ('search', 231),\n",
       " ('related', 231),\n",
       " ('pairs', 230),\n",
       " ('layers', 230),\n",
       " ('architecture', 230),\n",
       " ('do', 229),\n",
       " ('baseline', 229),\n",
       " ('!', 228),\n",
       " ('target', 227),\n",
       " ('predict', 227),\n",
       " ('downstream_tasks', 227),\n",
       " ('neighbor', 226),\n",
       " ('properties', 226),\n",
       " ('pp', 226),\n",
       " ('improve', 224),\n",
       " ('objective', 224),\n",
       " ('demonstrate', 222),\n",
       " ('complex', 221),\n",
       " ('if', 220),\n",
       " ('capture', 219),\n",
       " ('due', 218),\n",
       " ('setting', 218),\n",
       " ('was', 218),\n",
       " ('after', 217),\n",
       " ('j.', 217),\n",
       " ('available', 216),\n",
       " ('general', 216),\n",
       " ('neighborhood', 216),\n",
       " ('conduct', 216),\n",
       " ('several', 214),\n",
       " ('entity', 214),\n",
       " ('result', 213),\n",
       " ('12', 212),\n",
       " ('he', 210),\n",
       " ('directly', 210),\n",
       " ('even', 209),\n",
       " ('networks', 209),\n",
       " ('few_shot', 208),\n",
       " ('20', 208),\n",
       " ('size', 206),\n",
       " ('effectiveness', 206),\n",
       " ('during', 206),\n",
       " ('processing', 205),\n",
       " ('q', 205),\n",
       " ('does', 204),\n",
       " ('representation_learning', 204),\n",
       " ('abstract', 203),\n",
       " ('2024', 203),\n",
       " ('many', 203),\n",
       " ('strategy', 202),\n",
       " ('type', 201),\n",
       " ('approaches', 200),\n",
       " ('relations', 200),\n",
       " ('a.', 200),\n",
       " ('experimental', 198),\n",
       " ('inference', 198),\n",
       " ('chemistry', 198),\n",
       " ('path', 197),\n",
       " ('pretrained', 197),\n",
       " ('science', 196),\n",
       " ('yang', 196),\n",
       " ('average', 195),\n",
       " ('samples', 195),\n",
       " ('l', 193),\n",
       " ('outperforms', 192),\n",
       " ('comparison', 192),\n",
       " ('loss', 192),\n",
       " ('focus', 191),\n",
       " ('settings', 190),\n",
       " ('see', 190),\n",
       " ('human', 189),\n",
       " ('introduce', 189),\n",
       " ('among', 188),\n",
       " ('predictions', 188),\n",
       " ('/', 187),\n",
       " ('shows', 187),\n",
       " ('documents', 187),\n",
       " ('x', 186),\n",
       " ('11', 186),\n",
       " ('drug', 186),\n",
       " ('kgs', 185),\n",
       " ('graph_neural_networks', 184),\n",
       " ('utilize', 184),\n",
       " ('transformers', 184),\n",
       " ('j', 182),\n",
       " ('wu', 182),\n",
       " ('need', 182),\n",
       " ('domains', 181),\n",
       " ('category', 181),\n",
       " ('instruction', 180),\n",
       " ('gpt_4', 179),\n",
       " ('module', 179),\n",
       " ('instructions', 178),\n",
       " ('novel', 178),\n",
       " ('effective', 178),\n",
       " ('reaction', 178),\n",
       " ('significant', 177),\n",
       " ('include', 177),\n",
       " ('space', 177),\n",
       " ('modalities', 177),\n",
       " ('state_of_the_art', 176),\n",
       " ('additional', 176),\n",
       " ('tog', 176),\n",
       " ('topic', 176),\n",
       " ('present', 175),\n",
       " ('significantly', 175),\n",
       " ('therefore', 175),\n",
       " ('knowledge_graph', 175),\n",
       " ('provided', 174),\n",
       " ('capabilities', 173),\n",
       " ('graph_toolformer', 173),\n",
       " ('ogbn_arxiv', 173),\n",
       " ('biomedical', 173),\n",
       " ('y', 172),\n",
       " ('follows', 172),\n",
       " ('here', 172),\n",
       " ('cora', 172),\n",
       " ('because', 170),\n",
       " ('obtain', 170),\n",
       " ('contrastive_learning', 170),\n",
       " ('benchmark', 169),\n",
       " ('what', 169),\n",
       " ('under', 169),\n",
       " ('effectively', 169),\n",
       " ('associated', 168),\n",
       " ('international_conference', 167),\n",
       " ('find', 166),\n",
       " ('structural', 166),\n",
       " ('understand', 166),\n",
       " ('questions', 164),\n",
       " ('role', 164),\n",
       " ('title', 164),\n",
       " ('2016', 164),\n",
       " ('above', 163),\n",
       " ('m', 163),\n",
       " ('leverage', 163),\n",
       " ('relevant', 163),\n",
       " ('generative', 163),\n",
       " ('represent', 163),\n",
       " ('downstream', 163),\n",
       " ('product', 163),\n",
       " ('galactica', 163),\n",
       " ('diverse', 162),\n",
       " ('very', 162),\n",
       " ('gl', 162),\n",
       " ('pubmed', 162),\n",
       " ('so', 160),\n",
       " ('make', 160),\n",
       " ('stage', 160),\n",
       " ('small', 159),\n",
       " ('gr', 159),\n",
       " ('15', 158),\n",
       " ('k', 158),\n",
       " ('mlp', 158),\n",
       " ('node_features', 158),\n",
       " ('overall', 157),\n",
       " ('name', 157),\n",
       " ('large_scale', 157),\n",
       " ('whether', 156),\n",
       " ('note', 156),\n",
       " ('0.00', 156),\n",
       " ('molt5', 156),\n",
       " ('consider', 155),\n",
       " ('atom', 155),\n",
       " ('instance', 154),\n",
       " ('still', 154),\n",
       " ('were', 154),\n",
       " ('scores', 154),\n",
       " ('problems', 153),\n",
       " ('way', 153),\n",
       " ('yu', 153),\n",
       " ('base', 152),\n",
       " ('second', 151),\n",
       " ('achieves', 151),\n",
       " ('improvement', 151),\n",
       " ('30', 151),\n",
       " ('strategies', 151),\n",
       " ('joint', 151),\n",
       " ('positive', 150),\n",
       " ('proteins', 150),\n",
       " ('achieve', 148),\n",
       " ('single', 148),\n",
       " ('contains', 147),\n",
       " ('correct', 147),\n",
       " ('detailed', 147),\n",
       " ('adopt', 146),\n",
       " ('should', 146),\n",
       " ('details', 146),\n",
       " ('comprehensive', 145),\n",
       " ('wei', 145),\n",
       " ('metrics', 145),\n",
       " ('sample', 145),\n",
       " ('compare', 145),\n",
       " ('studies', 145),\n",
       " ('enhance', 144),\n",
       " ('14', 144),\n",
       " ('xu', 144),\n",
       " ('sun', 143),\n",
       " ('four', 143),\n",
       " ('computer', 143),\n",
       " ('case', 143),\n",
       " ('key', 143),\n",
       " ('association_for_computational_linguistics', 143),\n",
       " ('f', 143),\n",
       " ('vector', 143),\n",
       " ('provides', 142),\n",
       " ('high', 141),\n",
       " ('calls', 141),\n",
       " ('instead', 140),\n",
       " ('encoders', 140),\n",
       " ('simple', 139),\n",
       " ('queries', 139),\n",
       " ('center', 139),\n",
       " ('distribution', 139),\n",
       " ('employ', 138),\n",
       " ('found', 138),\n",
       " ('functions', 138),\n",
       " ('answers', 136),\n",
       " ('generating', 136),\n",
       " ('deep', 136),\n",
       " ('select', 136),\n",
       " ('step', 135),\n",
       " ('format', 135),\n",
       " ('powerful', 135),\n",
       " ('usa', 135),\n",
       " ('sequences', 135),\n",
       " ('ablation', 135),\n",
       " ('future', 134),\n",
       " ('furthermore', 134),\n",
       " ('ai', 134),\n",
       " ('`', 134),\n",
       " ('validation', 134),\n",
       " (\"'\", 132),\n",
       " ('hu', 132),\n",
       " ('smiles_strings', 132),\n",
       " ('much', 131),\n",
       " ('16', 131),\n",
       " ('13', 131),\n",
       " ('structural_information', 131),\n",
       " ('form', 131),\n",
       " ('applied', 131),\n",
       " ('c.', 131),\n",
       " ('paradigm', 131),\n",
       " ('matching', 130),\n",
       " ('metric', 130),\n",
       " ('improves', 130),\n",
       " ('observe', 130),\n",
       " ('main', 130),\n",
       " ('negative', 130),\n",
       " ('arxiv', 130),\n",
       " ('neural_network', 130),\n",
       " ('interaction', 130),\n",
       " ('captioning', 130),\n",
       " ('together', 129),\n",
       " ('works', 129),\n",
       " ('length', 129),\n",
       " ('full', 129),\n",
       " ('21', 129),\n",
       " ('limited', 128),\n",
       " ('nlp', 127),\n",
       " ('systems', 127),\n",
       " ('acm', 127),\n",
       " ('ground_truth', 127),\n",
       " ('specter', 127),\n",
       " ('question_answering', 126),\n",
       " ('selected', 126),\n",
       " ('uses', 126),\n",
       " ('community', 126),\n",
       " ('finetuning', 126),\n",
       " ('molecular_graph', 126),\n",
       " ('designed', 125),\n",
       " ('open', 125),\n",
       " ('those', 125),\n",
       " ('experiment', 125),\n",
       " ('jointly', 125),\n",
       " ('end', 124),\n",
       " ('higher', 124),\n",
       " ('take', 124),\n",
       " ('additionally', 124),\n",
       " ('s.', 124),\n",
       " ('supervised', 124),\n",
       " ('order', 123),\n",
       " ('previous', 123),\n",
       " ('class', 123),\n",
       " ('labeled', 123),\n",
       " ('application', 122),\n",
       " ('relationships', 122),\n",
       " ('products', 122),\n",
       " ('cross_modal', 122),\n",
       " ('standard', 121),\n",
       " ('denotes', 121),\n",
       " ('common', 121),\n",
       " ('categories', 121),\n",
       " ('selection', 121),\n",
       " ('call', 121),\n",
       " ('techniques', 120),\n",
       " ('would', 120),\n",
       " ('multimodal', 120),\n",
       " ('according', 119),\n",
       " ('able', 119),\n",
       " ('semantics', 119),\n",
       " ('another', 119),\n",
       " ('less', 119),\n",
       " ('important', 119),\n",
       " ('predicting', 119),\n",
       " ('m.', 119),\n",
       " ('amazon', 119),\n",
       " ('$', 119),\n",
       " ('extensive', 118),\n",
       " ('32', 118),\n",
       " ('values', 118),\n",
       " ('neural_networks', 118),\n",
       " ('often', 118),\n",
       " ('enables', 118),\n",
       " ('either', 118),\n",
       " ('neural', 118),\n",
       " ('annotations', 118),\n",
       " ('sentence', 117),\n",
       " ('out', 117),\n",
       " ('|', 117),\n",
       " ('capability', 116),\n",
       " ('citation', 116),\n",
       " ('report', 116),\n",
       " ('local', 115),\n",
       " ('100', 115),\n",
       " ('cot', 115),\n",
       " ('aim', 115),\n",
       " ('introduced', 115),\n",
       " ('academic', 115),\n",
       " ('outputs', 115),\n",
       " ('current', 115),\n",
       " ('learned', 115),\n",
       " ('quality', 115),\n",
       " ('w', 115),\n",
       " ('url_https', 115),\n",
       " ('multi_modal', 115),\n",
       " ('raw_text', 115),\n",
       " ('=o', 115),\n",
       " ('up', 114),\n",
       " ('indicates', 114),\n",
       " ('you', 114),\n",
       " ('w/o', 114),\n",
       " ('molecular_property_prediction', 114),\n",
       " ('recent', 113),\n",
       " ('users', 113),\n",
       " ('aggregation', 113),\n",
       " ('help', 113),\n",
       " ('roberta', 113),\n",
       " ('tag', 113),\n",
       " ('50', 112),\n",
       " ('zhao', 112),\n",
       " ('recommendation', 112),\n",
       " ('attributes', 112),\n",
       " ('devlin', 112),\n",
       " ('fine_tuned', 112),\n",
       " ('heterogeneous', 112),\n",
       " ('steps', 111),\n",
       " ('obtained', 111),\n",
       " ('encode', 111),\n",
       " ('parameter', 111),\n",
       " ('atoms', 111),\n",
       " ('huang', 110),\n",
       " ('randomly', 110),\n",
       " ('limitations', 110),\n",
       " ('finally', 110),\n",
       " ('predicted', 110),\n",
       " ('text_encoder', 110),\n",
       " ('accurate', 109),\n",
       " ('top', 109),\n",
       " ('explore', 109),\n",
       " ('contrast', 109),\n",
       " ('initial', 109),\n",
       " ('fig', 109),\n",
       " ('galm', 109),\n",
       " ('next', 108),\n",
       " ('classes', 108),\n",
       " ('dynamic', 108),\n",
       " ('list', 107),\n",
       " ('17', 107),\n",
       " ('inputs', 107),\n",
       " ('sampling', 107),\n",
       " ('zhu', 107),\n",
       " ('advances_in_neural_information_processing_systems', 107),\n",
       " ('ours', 107),\n",
       " ('moreover', 107),\n",
       " ('code', 106),\n",
       " ('zhou', 106),\n",
       " ('larger', 106),\n",
       " ('structured_data', 106),\n",
       " ('twitter', 106),\n",
       " ('field', 106),\n",
       " ('t5', 106),\n",
       " ('regression', 106),\n",
       " ('large_language_model', 105),\n",
       " ('addition', 105),\n",
       " ('strong', 105),\n",
       " ('impact', 105),\n",
       " ('allows', 105),\n",
       " ('apply', 105),\n",
       " ('acc', 105),\n",
       " ('augmentation', 105),\n",
       " ('23', 105),\n",
       " ('dragon', 105),\n",
       " ('leveraging', 104),\n",
       " ('online', 104),\n",
       " ('self_supervised', 104),\n",
       " ('total', 103),\n",
       " ('improving', 103),\n",
       " ('gpt_models', 103),\n",
       " ('besides', 103),\n",
       " ('vectors', 103),\n",
       " ('raw', 103),\n",
       " ('momu', 103),\n",
       " ('range', 102),\n",
       " ('lee', 102),\n",
       " ('real_world', 102),\n",
       " ('scientific', 102),\n",
       " ('before', 101),\n",
       " ('requires', 101),\n",
       " ('22', 101),\n",
       " ('24', 101),\n",
       " ('address', 101),\n",
       " ('require', 101),\n",
       " ('tags', 101),\n",
       " ('modality', 101),\n",
       " ('contrastive', 101),\n",
       " ('graphformers', 101),\n",
       " ('ogbn_products', 101),\n",
       " ('algorithm', 100),\n",
       " ('notably', 100),\n",
       " ('represents', 100),\n",
       " ('extract', 100),\n",
       " ('aims', 100),\n",
       " ('jiang', 100),\n",
       " ('2015', 100),\n",
       " ('retrieve', 100),\n",
       " ('us', 100),\n",
       " ('classication', 100),\n",
       " ('rt', 100),\n",
       " ('possible', 99),\n",
       " ('d.', 99),\n",
       " ('long', 99),\n",
       " ('terms', 99),\n",
       " ('19', 99),\n",
       " ('pipeline', 99),\n",
       " ('memory', 99),\n",
       " ('database', 99),\n",
       " ('cost', 99),\n",
       " ('z', 99),\n",
       " ('giant', 99),\n",
       " ('abilities', 98),\n",
       " ('valid', 98),\n",
       " ('construct', 98),\n",
       " ('challenges', 98),\n",
       " ('gcn', 98),\n",
       " ('links', 98),\n",
       " ('split', 98),\n",
       " ('lower', 97),\n",
       " ('instances', 97),\n",
       " ('18', 97),\n",
       " ('incorporate', 97),\n",
       " ('sets', 97),\n",
       " ('social_network', 97),\n",
       " ('denoted', 97),\n",
       " ('names', 97),\n",
       " ('text_classification', 97),\n",
       " ('scientic', 97),\n",
       " ('caption', 97),\n",
       " ('likely', 96),\n",
       " ('rich', 96),\n",
       " ('scibert', 96),\n",
       " ('chemformer', 96),\n",
       " ('solve', 95),\n",
       " ('cases', 95),\n",
       " ('probability', 95),\n",
       " ('explanations', 95),\n",
       " ('generates', 95),\n",
       " ('optimization', 95),\n",
       " ('mean', 95),\n",
       " ('per', 95),\n",
       " ('micol', 95),\n",
       " ('crucial', 94),\n",
       " ('level', 94),\n",
       " ('investigate', 94),\n",
       " ('includes', 94),\n",
       " ('although', 94),\n",
       " ('performs', 94),\n",
       " ('demonstrates', 94),\n",
       " ('instruction_tuning', 94),\n",
       " ('grad', 94),\n",
       " ('40', 93),\n",
       " ('few', 93),\n",
       " ('goal', 93),\n",
       " ('challenging', 93),\n",
       " ('backbone', 93),\n",
       " ('plms', 93),\n",
       " ('e2eg', 93),\n",
       " ('recently', 92),\n",
       " ('scenarios', 92),\n",
       " ('challenge', 92),\n",
       " ('smaller', 92),\n",
       " ('l.', 92),\n",
       " ('training_data', 92),\n",
       " ('training_set', 92),\n",
       " ('rog', 92),\n",
       " ('hence', 92),\n",
       " ('in_context_learning', 91),\n",
       " ('relevance', 91),\n",
       " ('especially', 91),\n",
       " ('augmented', 91),\n",
       " ('dynamic_graph', 91),\n",
       " ('content', 91),\n",
       " ('social', 91),\n",
       " ('k.', 91),\n",
       " ('being', 90),\n",
       " ('statistics', 90),\n",
       " ('presented', 90),\n",
       " ('benchmarks', 90),\n",
       " ('scale', 90),\n",
       " ('reported', 90),\n",
       " ('h.', 90),\n",
       " ('learning_rate', 90),\n",
       " ('cls', 90),\n",
       " ('enhancing', 89),\n",
       " ('reference', 89),\n",
       " ('improved', 89),\n",
       " ('deep_learning', 89),\n",
       " ('raph', 89),\n",
       " ('matrix', 89),\n",
       " ('graphsage', 89),\n",
       " ('unlabeled', 89),\n",
       " ('denote', 88),\n",
       " ('topology', 88),\n",
       " ('fundamental', 88),\n",
       " ('languages', 88),\n",
       " ('consists', 88),\n",
       " ('25', 88),\n",
       " ('image', 88),\n",
       " ('modules', 88),\n",
       " ('translation', 88),\n",
       " ('0.01', 88),\n",
       " ('tweet', 88),\n",
       " ('remains', 87),\n",
       " ('complexity', 87),\n",
       " ('introduction', 87),\n",
       " ('hard', 87),\n",
       " ('distinct', 87),\n",
       " ('maximum', 87),\n",
       " ('indicate', 87),\n",
       " ('value', 87),\n",
       " ('components', 87),\n",
       " ('confidence', 87),\n",
       " ('lead', 87),\n",
       " ('utilizing', 87),\n",
       " ('continuous', 87),\n",
       " ('fine_tune', 87),\n",
       " ('required', 86),\n",
       " ('capacity', 86),\n",
       " ('pair', 86),\n",
       " ('part', 86),\n",
       " ('prior', 86),\n",
       " ('despite', 86),\n",
       " ('considering', 86),\n",
       " ('incorporating', 86),\n",
       " ('text_attributed_graphs', 86),\n",
       " ('volume', 86),\n",
       " ('retrieved', 86),\n",
       " ('g.', 86),\n",
       " ('glem', 86),\n",
       " ('kv_plm', 86),\n",
       " ('max', 85),\n",
       " ('weights', 85),\n",
       " ('source', 85),\n",
       " ('demonstrated', 85),\n",
       " ('towards', 85),\n",
       " ('direct', 85),\n",
       " ('get', 85),\n",
       " ('vanilla', 85),\n",
       " ('described', 85),\n",
       " ('usually', 85),\n",
       " ('r.', 85),\n",
       " ('rst', 85),\n",
       " ('hamilton', 84),\n",
       " ('along', 84),\n",
       " ('making', 84),\n",
       " ('development', 84),\n",
       " ('t.', 84),\n",
       " ('2014', 84),\n",
       " ('node_representations', 84),\n",
       " ('candidate', 84),\n",
       " ('resulting', 83),\n",
       " ('compute', 83),\n",
       " ('text_attributes', 83),\n",
       " ('hyperparameters', 83),\n",
       " ('plm', 83),\n",
       " ('captions', 83),\n",
       " ('structured', 82),\n",
       " ('topological', 82),\n",
       " ('4.2', 82),\n",
       " ('represented', 82),\n",
       " ('importance', 82),\n",
       " ('efficient', 82),\n",
       " ('interactions', 82),\n",
       " ('improvements', 82),\n",
       " ('inductive', 82),\n",
       " ('images', 82),\n",
       " ('decoder', 82),\n",
       " ('0.03', 82),\n",
       " ('objectives', 81),\n",
       " ('widely', 81),\n",
       " ('node_embeddings', 81),\n",
       " ('particularly', 81),\n",
       " ('identify', 81),\n",
       " ('3.2', 81),\n",
       " ('___', 81),\n",
       " ('link', 81),\n",
       " ('icl', 81),\n",
       " ('hand', 80),\n",
       " ('might', 80),\n",
       " ('increase', 80),\n",
       " ('yield', 80),\n",
       " ('ieee', 80),\n",
       " ('follow', 80),\n",
       " ('2023a', 80),\n",
       " ('fully', 80),\n",
       " ('b.', 80),\n",
       " ('certain', 79),\n",
       " ('effect', 79),\n",
       " ('achieved', 79),\n",
       " ('final', 79),\n",
       " ('university', 79),\n",
       " ('pages', 79),\n",
       " ('procedure', 79),\n",
       " ('text_based', 79),\n",
       " ('scaffold', 79),\n",
       " ('advanced', 78),\n",
       " ('citations', 78),\n",
       " ('batch_size', 78),\n",
       " ('graph_based', 77),\n",
       " ('combined', 77),\n",
       " ('takes', 77),\n",
       " ('evaluated', 77),\n",
       " ('position', 77),\n",
       " ('27', 77),\n",
       " ('exploration', 77),\n",
       " ('handle', 77),\n",
       " ('seed', 77),\n",
       " ('llama', 77),\n",
       " ('vision', 77),\n",
       " ('unsupervised', 77),\n",
       " ('eq', 77),\n",
       " ('n.', 77),\n",
       " ('0.05', 77),\n",
       " ('grenade', 77),\n",
       " ('moleculestm', 77),\n",
       " ('last', 76),\n",
       " ('produce', 76),\n",
       " ('patterns', 76),\n",
       " ('particular', 76),\n",
       " ('good', 76),\n",
       " ('enabling', 76),\n",
       " ('difference', 76),\n",
       " ('textual_information', 76),\n",
       " ('unified', 76),\n",
       " ('graphllm', 76),\n",
       " ('nd', 76),\n",
       " ('statement', 76),\n",
       " ('adsgnn', 76),\n",
       " ('text2mol', 76),\n",
       " ('brown', 75),\n",
       " ('version', 75),\n",
       " ('inspired', 75),\n",
       " ('group', 75),\n",
       " ('authors', 75),\n",
       " ('exhibit', 75),\n",
       " ('words', 75),\n",
       " ('35', 75),\n",
       " ('26', 75),\n",
       " ('statements', 75),\n",
       " ('lin', 75),\n",
       " ('energy', 75),\n",
       " ('journal', 75),\n",
       " ('component', 75),\n",
       " ('ppr', 75),\n",
       " ...]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cnt.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_phrases = [w for w in cnt if (w not in stopwords.words('english'))]\n",
    "phrase_reprs = np.concatenate([vocab_emb[w].reshape((-1, 768)) for w in iv_phrases], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(vocab_sent_emb.keys())\n",
    "sent_reprs = np.concatenate([vocab_sent_emb[s].reshape((-1, 768)) for s in sents], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"SeeTopic/{args.dataset}\"):\n",
    "    os.makedirs(f\"SeeTopic/{args.dataset}\")\n",
    "\n",
    "with open(f\"SeeTopic/{args.dataset}/{args.dataset}.txt\", \"w\") as f:\n",
    "    for p in taxo.external_collection:\n",
    "        f.write(f\"paper_title : {p.title} ; paper_abstract : {p.abstract}\\n\")\n",
    "    for p in taxo.collection:\n",
    "        f.write(f\"paper_title : {p.title} ; paper_abstract : {p.abstract}\\n\")\n",
    "\n",
    "children_with_terms = taxo.root.getChildren(terms=True)\n",
    "\n",
    "with open(f\"SeeTopic/{args.dataset}/keywords_0.txt\", \"w\") as f:\n",
    "    for idx, c in enumerate(children_with_terms):\n",
    "        str_c = \",\".join(c[1])\n",
    "        f.write(f\"{idx}:{c[0]},{str_c}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Get PLM Embeddings===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### CONSTRUCTING AND TOKENIZING VOCAB #######\n",
      "####### COMPUTING STATIC EMBEDDINGS #######\n",
      "####### COMPUTING RAW WORD EMBEDDINGS #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:30<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Iter 0: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../llm_graph/llm_graph.txt\n",
      "Reading topics from file llm_graph_1/keywords.txt\n",
      "Vocab size: 14396\n",
      "Words in train file: 1290798\n",
      "Read 3 topics\n",
      "graph_context\trepresentation_learning\tgraph_search\t\n",
      "augmentation_methods\talignment_methods\tgraph_neural_networks\t\n",
      "representation_enhancement\tsequence_graph\tgraph_to_sequence\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000099  Progress: 99.67%  Words/thread/sec: 13.26k  Topic mining results written to file llm_graph_1/res_cate.txt\n",
      "\u001b[32m===Iter 1: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 2: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 2: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../llm_graph/llm_graph.txt\n",
      "Reading topics from file llm_graph_2/keywords.txt\n",
      "Vocab size: 14396\n",
      "Words in train file: 1290798\n",
      "Read 3 topics\n",
      "graph_context\trepresentation_learning\tgraph_search\tgraph_reasoning\tanswer_generation\tgraph_generation\t\n",
      "augmentation_methods\talignment_methods\tgraph_neural_networks\taugmentation_techniques\ttwo_stage\tpredefined_rules\t\n",
      "representation_enhancement\tsequence_graph\tgraph_to_sequence\tgraph_to_sequence_model\ttextual_graphs\tsql_to_text\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000099  Progress: 99.67%  Words/thread/sec: 13.02k  Topic mining results written to file llm_graph_2/res_cate.txt\n",
      "\u001b[32m===Iter 2: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 3: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 3: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../llm_graph/llm_graph.txt\n",
      "Reading topics from file llm_graph_3/keywords.txt\n",
      "Vocab size: 14396\n",
      "Words in train file: 1290798\n",
      "Read 3 topics\n",
      "graph_context\trepresentation_learning\tgraph_search\tgraph_reasoning\tanswer_generation\tgraph_generation\tpath_finding\tgraph_representation\tgtrl\t\n",
      "augmentation_methods\talignment_methods\tgraph_neural_networks\taugmentation_techniques\ttwo_stage\tpredefined_rules\trepresentation_learning\tlogical_rules\tencoder_based\t\n",
      "representation_enhancement\tsequence_graph\tgraph_to_sequence\tgraph_to_sequence_model\ttextual_graphs\tsql_to_text\tword_relation\tgraph_aware\tsyntax_enhanced\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000099  Progress: 100.14%  Words/thread/sec: 13.28k  Topic mining results written to file llm_graph_3/res_cate.txt\n",
      "\u001b[32m===Iter 3: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 4: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 4: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../llm_graph/llm_graph.txt\n",
      "Reading topics from file llm_graph_4/keywords.txt\n",
      "Vocab size: 14396\n",
      "Words in train file: 1290798\n",
      "Read 3 topics\n",
      "graph_context\trepresentation_learning\tgraph_search\tgraph_reasoning\tanswer_generation\tgraph_generation\tpath_finding\tgraph_representation\tgtrl\tgraph_optimization\tqas\tgraph_based\t\n",
      "augmentation_methods\talignment_methods\tgraph_neural_networks\taugmentation_techniques\ttwo_stage\tpredefined_rules\trepresentation_learning\tlogical_rules\tencoder_based\tsocial_networks\tgraph_optimization\tgraph_reasoning\t\n",
      "representation_enhancement\tsequence_graph\tgraph_to_sequence\tgraph_to_sequence_model\ttextual_graphs\tsyntax_enhanced\tsql_to_text\tgraph_aware\tword_relation\tgtae\ttemporal_knowledge_graph_reasoning\tdependency_structures\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000073  Progress: 99.91%  Words/thread/sec: 13.19k  Topic mining results written to file llm_graph_4/res_cate.txt\n",
      "\u001b[32m===Iter 4: Ensemble===\u001b[m\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"./SeeTopic\")\n",
    "subprocess.check_call(['./seetopic.sh', args.dataset, str(args.iters), args.model])\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
