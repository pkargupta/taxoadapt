{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 12:12:23 config.py:729] Defaulting to use mp for distributed inference\n",
      "WARNING 11-01 12:12:23 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-01 12:12:23 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-01 12:12:23 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 11-01 12:12:23 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-01 12:12:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-01 12:12:24 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 11-01 12:12:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:24 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:24 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-01 12:12:24 custom_all_reduce_utils.py:203] generating GPU P2P access cache in /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_5,7.json\n",
      "INFO 11-01 12:12:50 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_5,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:50 custom_all_reduce_utils.py:234] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_5,7.json\n",
      "INFO 11-01 12:12:50 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f55db80f6a0>, local_subscribe_port=37433, remote_subscribe_port=None)\n",
      "INFO 11-01 12:12:50 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:50 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-01 12:12:51 weight_utils.py:225] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:51 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8396e295a6b94272a7066a051f2f4116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 12:12:56 model_runner.py:732] Loading model weights took 7.5122 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:56 model_runner.py:732] Loading model weights took 7.5122 GB\n",
      "INFO 11-01 12:12:57 distributed_gpu_executor.py:56] # GPU blocks: 15716, # CPU blocks: 4096\n",
      "INFO 11-01 12:12:59 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-01 12:12:59 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:59 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:12:59 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-01 12:13:07 custom_all_reduce.py:219] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:13:07 custom_all_reduce.py:219] Registering 1040 cuda graph addresses\n",
      "INFO 11-01 12:13:07 model_runner.py:1225] Graph capturing finished in 8 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3832099)\u001b[0;0m INFO 11-01 12:13:07 model_runner.py:1225] Graph capturing finished in 8 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model_definitions import bertEncode\n",
    "from taxonomy import Node, Taxonomy\n",
    "import subprocess\n",
    "import pickle as pk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import compress\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import torch\n",
    "from scipy import stats\n",
    "import math\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "#from model_definitions import llama_8b_model, sentence_model, promptLlama, constructPrompt, bertEncode\n",
    "from utils import *\n",
    "from expansion import llmExpansion\n",
    "from prompts import *\n",
    "from main import commonSenseEnrich, computeClassEmb\n",
    "import sys  \n",
    "sys.path.insert(1, './lbm')\n",
    "from prepare_training_data import construct_samples, create_dataset, prepare_data\n",
    "from model import *\n",
    "from classifier_training import *\n",
    "from eval_classifier import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"ner_event_kgc\"\n",
    "        self.data_dir = f\"datasets/gen_kgc/{self.dataset}/\"\n",
    "        self.internal = f\"{self.dataset}.txt\"\n",
    "        self.external = f\"{self.dataset}_external.txt\"\n",
    "        self.groundtruth = \"groundtruth.txt\"\n",
    "        \n",
    "        self.length = 512\n",
    "        self.dim = 768\n",
    "\n",
    "        self.iters = 4\n",
    "        self.model = \"bert_full_ft\"\n",
    "        self.override = True\n",
    "        self.max_depth = 5\n",
    "\n",
    "        # training classifier\n",
    "        self.batch_size = 16\n",
    "        self.epoch = 5\n",
    "        self.lr = 5e-5\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create taxonomy from input\n",
    "taxo = Taxonomy(args.data_dir)\n",
    "\n",
    "taxo_dict = taxo.toDict(cur_node=taxo.root)\n",
    "with open(os.path.join(args.data_dir, 'initial.json'), 'w') as fp:\n",
    "    json.dump(taxo_dict, fp, indent=4)\n",
    "dict_str = json.dumps(taxo_dict, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'generative_knowledge_graph_construction',\n",
       " '1': 'generation_tasks',\n",
       " '2': 'relation_extraction',\n",
       " '3': 'entity_linking',\n",
       " '4': 'knowledge_graph_completion'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM Expansion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5/5 [00:01<00:00,  3.64it/s, est. speed input: 1333.02 toks/s, output: 133.88 toks/s]\n"
     ]
    }
   ],
   "source": [
    "expansion_prompts, candidate_nodes = llmExpansion(taxo, sentence_model, temperature=0.3)\n",
    "taxo_dict = taxo.toDict(cur_node=taxo.root)\n",
    "with open(os.path.join(args.data_dir, 'expanded.json'), 'w') as fp:\n",
    "    json.dump(taxo_dict, fp, indent=4)\n",
    "dict_str = json.dumps(taxo_dict, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'parent_node': 'generative_knowledge_graph_construction',\n",
       "  'candidate_nodes': ['graph_construction',\n",
       "   'knowledge_graph',\n",
       "   'graph_representation',\n",
       "   'generative_modeling',\n",
       "   'graph_learning',\n",
       "   'graph_generation',\n",
       "   'graph_representation_learning',\n",
       "   'graph_construction_methods']},\n",
       " {'parent_node': 'generation_tasks',\n",
       "  'candidate_nodes': ['text_generation',\n",
       "   'language_modeling',\n",
       "   'data_to_text_synthesis']},\n",
       " {'parent_node': 'relation_extraction',\n",
       "  'candidate_nodes': ['relation_prediction',\n",
       "   'relation_inference',\n",
       "   'relation_identification']},\n",
       " {'parent_node': 'entity_linking',\n",
       "  'candidate_nodes': ['entity_disambiguation',\n",
       "   'entity_typing',\n",
       "   'entity_resolution']},\n",
       " {'parent_node': 'knowledge_graph_completion',\n",
       "  'candidate_nodes': ['knowledge_graph_completion_with_context',\n",
       "   'knowledge_graph_completion_with_graph',\n",
       "   'knowledge_graph_completion_with_relation']}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common-sense enrichment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m enrich_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m prompts, outputs, all_common_phrases, all_common_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mcommonSenseEnrich\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaxo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# prompts, output_dict = commonSenseEnrich(taxo.root, dict_str, True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m enrich_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/main.py:30\u001b[0m, in \u001b[0;36mcommonSenseEnrich\u001b[0;34m(taxo, dict_str, batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m nodes\u001b[38;5;241m.\u001b[39mappend(current_node)\n\u001b[1;32m     29\u001b[0m sibs \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m current_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m current_node]\n\u001b[0;32m---> 30\u001b[0m prompts\u001b[38;5;241m.\u001b[39mappend(constructPrompt(init_enrich_prompt, \u001b[43mmain_simple_enrich_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaxo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msibs\u001b[49m\u001b[43m)\u001b[49m, api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m current_node\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[1;32m     33\u001b[0m     queue\u001b[38;5;241m.\u001b[39mappend(child)\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/prompts.py:55\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(taxo, node, sibs)\u001b[0m\n\u001b[1;32m     40\u001b[0m one_shot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124mExample Input:\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124mHere is an example output for the topic, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti_agent_reinforcement_learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, a subtopic of topic \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreinforcement learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. The terms are irrelevant to the sibling subtopics, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhierarchical_reinforcement_learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_reinforcement_learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     53\u001b[0m parent_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m taxo, node: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and is the subtopic of topics: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(taxo\u001b[38;5;241m.\u001b[39mget_par(node\u001b[38;5;241m.\u001b[39mnode_id,\u001b[38;5;250m \u001b[39mnode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnode_id \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 55\u001b[0m main_simple_enrich_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m taxo, node, sibs: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is a topic in Natural Language Processing (NLP)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent_prompt(taxo,\u001b[38;5;250m \u001b[39mnode)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please generate 20 realistic key terms and sentences about the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m topic that are relevant to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but irrelevant to the topics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msibs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The terms should be short (1-3 words), concise, and distinctive to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The sentences should have specific details and resemble realistic sentences found in NLP research papers.\u001b[39m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124mYour output format should be in the following JSON format (where node_to_enrich, id and description for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should match their respective values in the input taxonomy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_taxo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m aka COPIED OVER FROM input_taxo):\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;130;01m{{\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_to_enrich: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mnode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mnode\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<string where value is a 1-sentence description of node_to_enrich>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_key_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: <list of 20 diverse, short terms where values are realistic and relevant to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and DISSIMILAR to any of the following: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msibs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>,\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: <list of 10 diverse sentences where values are sentences used in papers about \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and DISSIMILAR to any of the following: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msibs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     69\u001b[0m main_long_enrich_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m node, sibs, dict_str: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mI am providing you a JSON which contains a taxonomy detailing concepts in NLP research papers (tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_taxo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m). Each JSON key within the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dictionary represents a taxonomy concept node. Can you fill in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_key_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m fields for the specified node (tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)? A research paper relevant to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m will be relevant to all concept nodes present in the taxonomy path to the node, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, as listed in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_node\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Here are your instructions on how to enrich the fields for node, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_key_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: This is a list (Python-formatted) of 20 key, realistic phrases (e.g., SUBTOPICS of the given \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) commonly written within NLP research papers that EXCLUSIVELY DISCUSS \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms key phrases/subtopics should be highly relevant to all of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms ancestors listed in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_node\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, and NOT be relevant to ANY other non-ancestor or siblings of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (siblings of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are specified in tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiblings\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m below). All added key phrases/subtopics should be 1-3 words, lowercase, and have spaces replaced with underscores (e.g., \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_phrase\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m). Each key phrase should be unique.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     97\u001b[0m main_classify_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m node, paper: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mGiven the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (provided below) of an NLP research paper that uses large language models for graphs, select the class labels (tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_options\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) that should be assigned to this paper (multi-label classification). If the research paper SHOULD NOT be labeled with any of the classes in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_options\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, then output an empty list. We provide additional descriptions (tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_descriptions\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) for each class option for your reference.\u001b[39m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124m'''\u001b[39m\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/prompts.py:53\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(taxo, node)\u001b[0m\n\u001b[1;32m     21\u001b[0m bulk_enrich_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m dict_str: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mI am providing you a JSON which contains a taxonomy detailing concepts for NLP research papers. Each JSON key within the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dictionary represents a taxonomy concept node. Can you fill in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_key_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m fields for each concept node (enrichment of both the root node and its children/descendants) that contains these fields? The required fields are already present for you, so you do not need to create any new keys for concepts without them. Here are the instructions for each field under concept A:\u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_key_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: This is a list (Python-formatted) of 20 key phrases (e.g., SUBTOPICS of the given concept node A) commonly used amongst NLP research papers that EXCLUSIVELY DISCUSS that concept node (concept A\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms key phrases/subtopics should be highly relevant to its concept A\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms parent concept, and NOT OR RARELY be mentioned in ANY of its SIBLING concepts B; A and B share the same parent concept). All added key phrases/subtopics should be 1-3 words, lowercase, and have spaces replaced with underscores (e.g., \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_phrase\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m). Each key phrase should be unique.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     40\u001b[0m one_shot \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124mExample Input:\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124mHere is an example output for the topic, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti_agent_reinforcement_learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, a subtopic of topic \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreinforcement learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. The terms are irrelevant to the sibling subtopics, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhierarchical_reinforcement_learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_reinforcement_learning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 53\u001b[0m parent_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m taxo, node: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and is the subtopic of topics: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(taxo\u001b[38;5;241m.\u001b[39mget_par(node\u001b[38;5;241m.\u001b[39mnode_id,\u001b[38;5;250m \u001b[39mnode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnode_id \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m main_simple_enrich_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m taxo, node, sibs: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is a topic in Natural Language Processing (NLP)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent_prompt(taxo,\u001b[38;5;250m \u001b[39mnode)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please generate 20 realistic key terms and sentences about the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m topic that are relevant to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but irrelevant to the topics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msibs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The terms should be short (1-3 words), concise, and distinctive to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The sentences should have specific details and resemble realistic sentences found in NLP research papers.\u001b[39m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124mYour output format should be in the following JSON format (where node_to_enrich, id and description for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should match their respective values in the input taxonomy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_taxo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m aka COPIED OVER FROM input_taxo):\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     69\u001b[0m main_long_enrich_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m node, sibs, dict_str: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mI am providing you a JSON which contains a taxonomy detailing concepts in NLP research papers (tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_taxo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m). Each JSON key within the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dictionary represents a taxonomy concept node. Can you fill in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_key_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m fields for the specified node (tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)? A research paper relevant to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m will be relevant to all concept nodes present in the taxonomy path to the node, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, as listed in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_node\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Here are your instructions on how to enrich the fields for node, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_key_phrases\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: This is a list (Python-formatted) of 20 key, realistic phrases (e.g., SUBTOPICS of the given \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) commonly written within NLP research papers that EXCLUSIVELY DISCUSS \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms key phrases/subtopics should be highly relevant to all of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms ancestors listed in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_node\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, and NOT be relevant to ANY other non-ancestor or siblings of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (siblings of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_to_enrich\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are specified in tag \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiblings\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m below). All added key phrases/subtopics should be 1-3 words, lowercase, and have spaces replaced with underscores (e.g., \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_phrase\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m). Each key phrase should be unique.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124m'''\u001b[39m\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/taxonomy.py:315\u001b[0m, in \u001b[0;36mTaxonomy.get_par\u001b[0;34m(self, idx, node)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_par\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 315\u001b[0m     cur_node, parents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindChild\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parents\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/taxonomy.py:142\u001b[0m, in \u001b[0;36mNode.findChild\u001b[0;34m(self, node_id, parent, node)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parent:\n\u001b[0;32m--> 142\u001b[0m         ans, ancestors \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindChild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         ans \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mfindChild(node_id)\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/taxonomy.py:142\u001b[0m, in \u001b[0;36mNode.findChild\u001b[0;34m(self, node_id, parent, node)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parent:\n\u001b[0;32m--> 142\u001b[0m         ans, ancestors \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mfindChild(node_id, parent, node)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         ans \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mfindChild(node_id)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "enrich_start = time.time()\n",
    "prompts, outputs, all_common_phrases, all_common_sentences = commonSenseEnrich(taxo, dict_str, True)\n",
    "# prompts, output_dict = commonSenseEnrich(taxo.root, dict_str, True)\n",
    "enrich_end = time.time()\n",
    "\n",
    "if all_common_phrases:\n",
    "    # update vocabulary/embeddings\n",
    "    taxo.updateVocab(all_common_phrases, 'phrases')\n",
    "    taxo.updateVocab(all_common_sentences, 'sentences')\n",
    "    print(f\"Time taken: {enrich_end - enrich_start} seconds ({(enrich_end - enrich_start)/60} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generative_knowledge_graph_construction': '0',\n",
       " 'generation_tasks': '1',\n",
       " 'relation_extraction': '2',\n",
       " 'entity_linking': '3',\n",
       " 'knowledge_graph_completion': '4',\n",
       " 'knowledge_graph_representation': 5,\n",
       " 'graph_generation': 6,\n",
       " 'graph_construction': 7,\n",
       " 'text_generation': 8,\n",
       " 'language_modeling': 9,\n",
       " 'dialog_systems': 10,\n",
       " 'relation_prediction': 11,\n",
       " 'entity_discovery': 12,\n",
       " 'relation_identification': 13,\n",
       " 'entity_disambiguation': 14,\n",
       " 'entity_typing': 15,\n",
       " 'entity_normalization': 16,\n",
       " 'graph_completion_with_context': 17,\n",
       " 'knowledge_graph_inference': 18,\n",
       " 'graph_completion_with_reasoning': 19}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dict = taxo.toDict(cur_node=taxo.root)\n",
    "with open(os.path.join(args.data_dir, 'enriched.json'), 'w') as fp:\n",
    "    json.dump(updated_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessing/AutoPhrase/data/EN/wiki_quality_orig.txt\", \"r\", encoding='utf-8') as f:\n",
    "    all_phrases = [w.strip() for w in f.readlines() if \" \" in w.strip()]\n",
    "    for a in list(taxo.label2id.keys()) + all_common_phrases:\n",
    "        all_phrases.append(a.strip().replace(\"_\", \" \"))\n",
    "\n",
    "with open(\"preprocessing/AutoPhrase/data/EN/wiki_quality.txt\", \"w\", encoding='utf-8') as f:\n",
    "    for w_id, w in enumerate(all_phrases):\n",
    "        if w_id == (len(all_phrases) - 1):\n",
    "            f.write(f\"{w}\")\n",
    "        else:\n",
    "            f.write(f\"{w}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Corpus Pre-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:01<00:00, 19.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m1.793s\n",
      "user\t0m13.601s\n",
      "sys\t0m0.897s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "No provided expert labels.\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===AutoPhrasing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Enabled\n",
      "Number of threads = 10\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 279831\n",
      "max word token id = 21668\n",
      "# of documents = 34\n",
      "# of distinct POS tags = 55\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 21673\n",
      "# of frequent phrases = 24075\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 48\n",
      "\tThe size of the negative pool = 22632\n",
      "# truth patterns = 21914\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m2.603s\n",
      "user\t0m5.680s\n",
      "sys\t0m0.239s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Saving Model and Results===\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m1.706s\n",
      "user\t0m12.811s\n",
      "sys\t0m0.490s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===Phrasal Segmentation===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Segmentation Model Path = models/NEW/segmentation.model\n",
      "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
      "\tQ(multi-word phrases) >= 0.700000\n",
      "\tQ(single-word phrases) >= 1.000000\n",
      "=======\n",
      "POS guided model loaded.\n",
      "# of loaded patterns = 4519\n",
      "# of loaded truth patterns = 21962\n",
      "POS transition matrix loaded\n",
      "Phrasal segmentation finished.\n",
      "   # of total highlighted quality phrases = 7737\n",
      "   # of total processed sentences = 48201\n",
      "   avg highlights per sentence = 0.160515\n",
      "\n",
      "real\t0m0.396s\n",
      "user\t0m0.370s\n",
      "sys\t0m0.016s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Segmented Corpus Post-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:00, 878.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase segmented corpus written to ../datasets/gen_kgc/ner_event_kgc//phrase_ner_event_kgc.txt\n",
      "\u001b[32m===Corpus Pre-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8150/8150 [00:09<00:00, 879.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Compilation===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m2.441s\n",
      "user\t0m20.037s\n",
      "sys\t0m2.114s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "Current step: Tokenizing wikipedia phrases...\u001b[0K\n",
      "No provided expert labels.\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===AutoPhrasing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Iterations = 2\n",
      "Minimum Support Threshold = 10\n",
      "Maximum Length Threshold = 6\n",
      "POS-Tagging Mode Enabled\n",
      "Number of threads = 10\n",
      "Labeling Method = DPDN\n",
      "\tAuto labels from knowledge bases\n",
      "\tMax Positive Samples = -1\n",
      "=======\n",
      "Loading data...\n",
      "# of total tokens = 1664688\n",
      "max word token id = 33740\n",
      "# of documents = 8150\n",
      "# of distinct POS tags = 56\n",
      "Mining frequent phrases...\n",
      "selected MAGIC = 33749\n",
      "# of frequent phrases = 53488\n",
      "Extracting features...\n",
      "Constructing label pools...\n",
      "\tThe size of the positive pool = 267\n",
      "\tThe size of the negative pool = 50034\n",
      "# truth patterns = 35475\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Rectifying features...\n",
      "Estimating Phrase Quality...\n",
      "Segmenting...\n",
      "Dumping results...\n",
      "Done.\n",
      "\n",
      "real\t0m5.656s\n",
      "user\t0m24.707s\n",
      "sys\t0m1.380s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Saving Model and Results===\u001b[m\n",
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Tokenization===\u001b[m\n",
      "Current step: Tokenizing input file...\u001b[0K\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m1.919s\n",
      "user\t0m13.018s\n",
      "sys\t0m1.075s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: EN\u001b[0K\n",
      "\u001b[32m===Part-Of-Speech Tagging===\u001b[m\n",
      "Current step: Merging...\u001b[0Ks...\u001b[0K\n",
      "\u001b[32m===Phrasal Segmentation===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== Current Settings ===\n",
      "Segmentation Model Path = models/NEW/segmentation.model\n",
      "After the phrasal segmentation, only following phrases will be highlighted with <phrase> and </phrase>\n",
      "\tQ(multi-word phrases) >= 0.700000\n",
      "\tQ(single-word phrases) >= 1.000000\n",
      "=======\n",
      "POS guided model loaded.\n",
      "# of loaded patterns = 24504\n",
      "# of loaded truth patterns = 35742\n",
      "POS transition matrix loaded\n",
      "Phrasal segmentation finished.\n",
      "   # of total highlighted quality phrases = 102610\n",
      "   # of total processed sentences = 189417\n",
      "   avg highlights per sentence = 0.541715\n",
      "\n",
      "real\t0m2.422s\n",
      "user\t0m2.368s\n",
      "sys\t0m0.040s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Generating Output===\u001b[m\n",
      "\u001b[32m===Segmented Corpus Post-processing===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4809it [00:00, 24386.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase segmented corpus written to ../datasets/gen_kgc/ner_event_kgc//phrase_ner_event_kgc_external.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8150it [00:00, 22665.37it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.override:\n",
    "    # pre-process\n",
    "    os.chdir(\"./preprocessing\")\n",
    "    subprocess.check_call(['./auto_phrase.sh', args.data_dir, args.internal])\n",
    "    subprocess.check_call(['./auto_phrase.sh', args.data_dir, args.external])\n",
    "    os.chdir(\"../\")\n",
    "else:\n",
    "    print(\"already pre-processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxo.static_emb = {}\n",
    "# taxo.token_lens = {}\n",
    "# taxo.collection = []\n",
    "# taxo.external_collection = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Collections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8184/8184 [00:05<00:00, 1418.06it/s]\n"
     ]
    }
   ],
   "source": [
    "collection, external_collection = taxo.createCollections(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**External & Internal Term Enrichment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding sentences...\n",
      "encoding phrases...\n"
     ]
    }
   ],
   "source": [
    "print(\"encoding sentences...\")\n",
    "external_sentences = list(set([sentence for paper in external_collection for sentence in paper.sent_tokenize]))\n",
    "internal_sentences = list(set([sentence for paper in collection for sentence in paper.sent_tokenize]))\n",
    "taxo.updateVocab(external_sentences + internal_sentences, 'sentences')\n",
    "# sent2emb = {sent:idx for idx, sent in enumerate(external_sentences)}\n",
    "# external_sent_emb = {idx:emb for idx, emb in  enumerate(sentence_model.encode(external_sentences))}\n",
    "\n",
    "print(\"encoding phrases...\")\n",
    "external_phrases = list(set([phrase for paper in external_collection for sentence in paper.phrase_tokenize for phrase in sentence]))\n",
    "internal_phrases = list(set([phrase for paper in collection for sentence in paper.phrase_tokenize for phrase in sentence]))\n",
    "taxo.updateVocab(external_phrases + internal_phrases, 'phrases')\n",
    "# phrase2emb = {phrase:idx for idx, phrase in enumerate(external_phrases)}\n",
    "# external_phrase_emb = {idx:emb for idx, emb in  enumerate(sentence_model.encode(external_phrases))}\n",
    "\n",
    "taxo.graph.external['phrases'] = external_phrases\n",
    "taxo.graph.external['sentences'] = external_sentences\n",
    "taxo.graph.internal['phrases'] = internal_phrases\n",
    "taxo.graph.internal['sentences'] = internal_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_pool = external_phrases # list(taxo.vocab['phrases'].keys())\n",
    "pool_emb = np.array([taxo.vocab['phrases'][w] for w in phrase_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.29it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 24105.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7789915966386554\n",
      "F1-Macro Score: 0.6585470085470085\n",
      "F1-Micro Score: 0.773913043478261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "node_external_phrase_ranks, gt, preds = expandDiscriminative(taxo, phrase_pool, pool_emb, internal=False)\n",
    "f1_scores(gt, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8184/8184 [00:15<00:00, 511.90it/s] \n"
     ]
    }
   ],
   "source": [
    "term_to_idx, td_matrix, co_matrix = constructTermDocMatrix(taxo, collection + external_collection)\n",
    "co_avg = np.true_divide(co_matrix.sum(),(co_matrix!=0).sum())\n",
    "phrase_pool = internal_phrases # list(taxo.vocab['phrases'].keys())\n",
    "pool_emb = np.array([taxo.vocab['phrases'][w] for w in phrase_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_score = computeBM25Cog(co_matrix, co_avg, k=1.2, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8910364145658264\n",
      "F1-Macro Score: 0.6210752688172043\n",
      "F1-Micro Score: 0.8783068783068783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "node_internal_phrase_ranks, gt, preds = expandInternal(taxo, phrase_pool, pool_emb, term_to_idx, bm_score)\n",
    "f1_scores(gt, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Internal Sentence Enrichment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.81s/it]\n",
      "100%|██████████| 3/3 [00:39<00:00, 13.25s/it]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_pool, full_sent_ranks = expandSentences(taxo, term_to_idx, bm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 27.39it/s]\n"
     ]
    }
   ],
   "source": [
    "class_embs = []\n",
    "for node_id in tqdm(taxo.id2label):\n",
    "    curr_node = taxo.root.findChild(node_id)\n",
    "    curr_node.emb['sentence'] = average_with_harmonic_series(np.stack([taxo.vocab['sentences'][w] \n",
    "                                    for w in curr_node.getAllTerms(granularity='sentences', children=True)], axis=0), axis=0)\n",
    "    class_embs.append(curr_node.emb['sentence'])\n",
    "    # child.papers = {}\n",
    "    # child.paper_scores = {}\n",
    "    # child.ranked = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8184/8184 [00:03<00:00, 2371.91it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = collection + external_collection\n",
    "preds = [set() for paper in collection + external_collection]\n",
    "class_map = {i:[] for i in taxo.id2label}\n",
    "core_classes = {}\n",
    "all_sent_ranks = []\n",
    "all_class_weights = []\n",
    "document_embs = []\n",
    "\n",
    "\n",
    "for idx, paper in tqdm(enumerate(collection + external_collection), total=len(collection) + len(external_collection)):\n",
    "    sent_reprs, sent_avg_ranks, sent_dis_ranks = paper.rankSentences(class_embs, phrases=False)\n",
    "    sent_class_sim = cosine_similarity_embeddings(sent_reprs, class_embs) # .argmax(axis=1) S x C\n",
    "    weights = np.array(weights_from_ranking([sent_dis_ranks])) # S x 1\n",
    "    weights /= weights.sum()\n",
    "    document_embs.append(np.average(sent_reprs, axis=0, weights=weights))\n",
    "    all_sent_ranks.append(sent_dis_ranks)\n",
    "    all_class_weights.append(np.average(sent_class_sim, axis=0, weights=weights))\n",
    "    class_weights = sent_class_sim.T @ weights # C x S @ S x 1\n",
    "    norm_class_weights = (class_weights - class_weights.min())/(class_weights.max() - class_weights.min())\n",
    "\n",
    "    # class_weights = np.bincount(sent_to_class, weights=weights, minlength=len(class_embs))\n",
    "    paper_labels = np.argsort(norm_class_weights)[(np.argmax(np.diff(np.sort(norm_class_weights))) + 1):]\n",
    "    for i in paper_labels:\n",
    "        preds[paper.id].add(str(i))\n",
    "        preds[paper.id] = preds[paper.id].union([p.node_id for p in taxo.root.findChild(str(i), parent=True, node=True)[1]])\n",
    "        class_map[str(i)].append((paper.id, class_weights[i]))\n",
    "\n",
    "for k, v in class_map.items():\n",
    "    score_thresh = np.percentile([s[1] for s in v], 80) if len(v) > 0 else 0\n",
    "    class_map[k] = sorted([s for s in v if (s[1] >= score_thresh) or (s[0] < len(collection))], key=lambda x: x[1], reverse=True)\n",
    "    for p in class_map[k]:\n",
    "        if p[0] in core_classes:\n",
    "            core_classes[p[0]].append(k)\n",
    "        else:\n",
    "            core_classes[p[0]] = [k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522408963585434\n",
      "0.7058823529411765\n",
      "0.6911764705882353\n",
      "0.7058823529411764\n",
      "F1-Macro Score: 0.723681592039801\n",
      "F1-Micro Score: 0.8541666666666666\n"
     ]
    }
   ],
   "source": [
    "print(example_f1(gt, preds[:len(collection)]))\n",
    "print(precision_at_k(list(map(list, preds[:len(collection)])), gt, k=1))\n",
    "print(precision_at_k(list(map(list, preds[:len(collection)])), gt, k=2))\n",
    "print(precision_at_k(list(map(list, preds[:len(collection)])), gt, k=3))\n",
    "f1_scores(gt, preds[:len(collection)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8756302521008403\n",
      "0.9411764705882353\n",
      "0.8970588235294118\n",
      "0.7745098039215688\n",
      "F1-Macro Score: 0.7437895437895439\n",
      "F1-Micro Score: 0.8817204301075269\n"
     ]
    }
   ],
   "source": [
    "print(example_f1(gt, preds[:len(collection)]))\n",
    "print(precision_at_k(list(map(list, preds[:len(collection)])), gt, k=1))\n",
    "print(precision_at_k(list(map(list, preds[:len(collection)])), gt, k=2))\n",
    "print(precision_at_k(list(map(list, preds[:len(collection)])), gt, k=3))\n",
    "f1_scores(gt, preds[:len(collection)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expansion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, T5Tokenizer, T5Config, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/pk36/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "T5_PATH = 't5-base' \n",
    "DEVICE = torch.device('cuda:' + str(1))\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\n",
    "t5_config = T5Config.from_pretrained(T5_PATH)\n",
    "t5_mlm = T5ForConditionalGeneration.from_pretrained(T5_PATH, config=t5_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_definitions import constructPrompt, promptLlamaVLLM\n",
    "from prompts import SiblingSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def widthExpansion(node, unmapped_papers):\n",
    "    prompts = []\n",
    "\n",
    "    for p_id in unmapped_papers:\n",
    "        # get common-sense & corpus-specific options\n",
    "        init_prompt = 'You are an assistant that performs width expansion of taxonomies. Width expansion in taxonomies adds more categories at the same level, increasing options without adding depth. For example, expanding a taxonomy of NLP tasks from [\\\"text_classification\\\" and \\\"named_entity_recognition\\\"] to include \\\"sentiment_analysis\\\", \\\"machine_translation\\\", and \\\"question_answering\\\" would be a width expansion.'\n",
    "        # The content of this paper is \"{collection[p_id].content}\"\\n\\n\n",
    "        main_prompt = f'The title of the paper is \"{collection[p_id].title}\".\\n\\nThe abstract of this paper is \"{collection[p_id].abstract}\"\\n\\nParents/Ancestor Topics: {node.path[1:]}\\n\\nSibling Topics: {\", \".join(map(str, node.parents[0].children))}.\\n\\nBased on the given paper, can you expand the set of siblings with several options? Output your answer as a Python list only containing only the new siblings.\\n\\nOutput Format:\\n\\n{{\"new_siblings\": <list of strings where values are the new siblings (1-3 words) based on the given paper and existing siblings>}}'\n",
    "\n",
    "        prompts.append(constructPrompt(init_prompt, main_prompt, api=False))\n",
    "\n",
    "    output_dict = promptLlamaVLLM(prompts, schema=SiblingSchema, max_new_tokens=500)\n",
    "    output = [json.loads(clean_json_string(c)) if \"```json\" in c else json.loads(c.strip()) for c in output_dict]\n",
    "\n",
    "    all_candidates = []\n",
    "    for c in output:\n",
    "        all_candidates.extend(c['new_siblings'])\n",
    "\n",
    "    return output, all_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 8/8 [00:02<00:00,  2.81it/s, est. speed input: 1111.62 toks/s, output: 100.83 toks/s]\n"
     ]
    }
   ],
   "source": [
    "w_output, w_candidates = widthExpansion(taxo.root.children[0], unmapped_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 7, 12, 13, 18, 22, 24, 31]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmapped_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_freq = sorted(dict(Counter(w_candidates)).items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['discriminative_methods',\n",
       " 'label_semantics',\n",
       " 'few_shot_tasks',\n",
       " 'high_resource_tasks',\n",
       " 'low_resource_tasks',\n",
       " 'bert_baseline',\n",
       " 'snips_dataset',\n",
       " 'text_to_structure',\n",
       " 'schema_based',\n",
       " 'large_scale',\n",
       " 'pretrained_model',\n",
       " 'state_of_the_art',\n",
       " 'unified_framework',\n",
       " 'universal_model',\n",
       " 'adaptive_generation',\n",
       " 'collaborative_learning',\n",
       " 'general_abilities',\n",
       " 'information_extraction',\n",
       " 'sequence_generation',\n",
       " 'knowledge_graph_construction',\n",
       " 'natural_language_processing',\n",
       " 'triplet_contrastive_training',\n",
       " 'batch_wise_dynamic_attention_masking',\n",
       " 'triple_wise_calibration',\n",
       " 'encoder_decoder_based_generation',\n",
       " 'generative_transformer',\n",
       " 'contrastive_information_extraction',\n",
       " 'contrastive_learning',\n",
       " 'triplet_extraction',\n",
       " 'knowledge_graph',\n",
       " 'text_generation',\n",
       " 'language_translation',\n",
       " 'machine_reading',\n",
       " 'event_centric_reasoning',\n",
       " 'correlation_aware_transformer',\n",
       " 'event_correlation_encoding',\n",
       " 'zero_shot_cross_lingual_transfer',\n",
       " 'cross_lingual_event_argument_extraction',\n",
       " 'cross_lingual_event_extraction',\n",
       " 'graph_generation',\n",
       " 'graph_construction',\n",
       " 'graph_learning']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('information_extraction', 4),\n",
       " ('sequence_generation', 3),\n",
       " ('sequence_to_sequence', 2),\n",
       " ('sequence_understanding', 2),\n",
       " ('sequence_representation', 2),\n",
       " ('event_extraction', 2),\n",
       " ('natural_language_processing', 2),\n",
       " ('sequence_labeling', 1),\n",
       " ('sequence_tagging', 1),\n",
       " ('sequence_prediction', 1),\n",
       " ('sequence_classification', 1),\n",
       " ('sequence_learning', 1),\n",
       " ('sequence_modeling', 1),\n",
       " ('text_to_structure', 1),\n",
       " ('universal_model', 1),\n",
       " ('structured_representation', 1),\n",
       " ('schema_based_prompt', 1),\n",
       " ('structured_extraction_language', 1),\n",
       " ('text_to_structure_generation', 1),\n",
       " ('structured_schema_instructor', 1),\n",
       " ('structured_language_model', 1),\n",
       " ('structured_records', 1),\n",
       " ('relation_extraction', 1),\n",
       " ('named_entity_recognition', 1),\n",
       " ('relation_classification', 1),\n",
       " ('knowledge_graph_construction', 1),\n",
       " ('language_understanding', 1),\n",
       " ('language_generation', 1),\n",
       " ('encoder_decoder_architecture', 1),\n",
       " ('natural_language_generation', 1),\n",
       " ('sequence_to_text', 1),\n",
       " ('text_to_text', 1),\n",
       " ('role_filler_extraction', 1),\n",
       " ('event_centric_reasoning', 1),\n",
       " ('discriminative_models', 1),\n",
       " ('pre_training_objectives', 1),\n",
       " ('zero_shot_cross_lingual_transfer', 1),\n",
       " ('cross_lingual_event_extraction', 1),\n",
       " ('cross_lingual_relation_extraction', 1),\n",
       " ('cross_lingual_dependency_parsing', 1),\n",
       " ('cross_lingual_named_entity_recognition', 1),\n",
       " ('cross_lingual_relation_labeling', 1),\n",
       " ('cross_lingual_event_argument_extraction', 1),\n",
       " ('cross_lingual_transfer_learning', 1),\n",
       " ('cross_lingual_language_models', 1),\n",
       " ('cross_lingual_nlp_tasks', 1),\n",
       " ('graph_generation_tasks', 1),\n",
       " ('graph_generation_models', 1),\n",
       " ('graph_representation_learning', 1)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero_shot_cross_lingual_transfer, cross_lingual_event_extraction, cross_lingual_relation_extraction, cross_lingual_dependency_parsing, cross_lingual_named_entity_recognition, cross_lingual_relation_labeling, cross_lingual_event_argument_extraction, cross_lingual_transfer_learning, cross_lingual_language_models, cross_lingual_nlp_tasks'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(w_output[6]['new_siblings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])\n",
    "unmapped_papers = {}\n",
    "expansion_candidates = {}\n",
    "\n",
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    path_ids = {taxo.label2id[i] for i in curr_node.path[1:]} # ignore root\n",
    "\n",
    "    # width expansion (0): if the current node is not a leaf node, get the nodes which are mapped to it, but not any of its children\n",
    "    # depth expansion (1): if the current node is a leaf node, get the nodes which are mapped to it\n",
    "    unmapped_papers[curr_node.node_id] = [p_id for p_id, p in enumerate(collection) if preds[p_id] == path_ids]\n",
    "\n",
    "    if len(curr_node.children):\n",
    "        w_output, w_candidates = widthExpansion(curr_node, unmapped_papers[curr_node.node_id])\n",
    "    else:\n",
    "        depthExpansion(curr_node, unmapped_papers[curr_node.node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_id: 1; preds: ['0', '1']; gt: ['0', '1']; weights: [0.84531204 0.86006769 0.80433236 0.78352513 0.80453067]\n",
      "paper title: augmented_natural_language for generative sequence_labeling\n",
      "paper_id: 7; preds: ['0', '1']; gt: ['0', '1', '2']; weights: [0.82464512 0.84528683 0.7948382  0.7818779  0.79149367]\n",
      "paper title: unified structure_generation for universal information_extraction\n",
      "paper_id: 12; preds: ['0', '1']; gt: ['0', '1', '2']; weights: [0.82684119 0.83132468 0.79769621 0.77207515 0.79802967]\n",
      "paper title: contrastive information_extraction with generative_transformer\n",
      "paper_id: 13; preds: ['0', '1']; gt: ['0', '1', '2']; weights: [0.82035241 0.84272177 0.78811188 0.76300622 0.78246604]\n",
      "paper title: contrastive triple_extraction with generative_transformer\n",
      "paper_id: 18; preds: ['0', '1']; gt: ['0', '1']; weights: [0.82936326 0.83146916 0.79841426 0.79143692 0.79385458]\n",
      "paper title: template filling with generative transformers\n",
      "paper_id: 22; preds: ['0', '1']; gt: ['0', '1', '4']; weights: [0.81135317 0.83404406 0.7839354  0.75801095 0.77749075]\n",
      "paper title: claret : pre_training a correlation_aware context_to_event transformer for event_centric generation and classification\n",
      "paper_id: 24; preds: ['0', '1']; gt: ['0', '1']; weights: [0.81767302 0.84120599 0.7892618  0.76245779 0.77134124]\n",
      "paper title: multilingual generative language_models for zero_shot cross_lingual event_argument_extraction\n",
      "paper_id: 31; preds: ['0', '1']; gt: ['0', '1', '4']; weights: [0.85433424 0.85317809 0.81276485 0.79365519 0.82467276]\n",
      "paper title: explanation_graph generation via pre_trained_language_models : an empirical study with contrastive_learning\n"
     ]
    }
   ],
   "source": [
    "unmapped_papers = []\n",
    "mapped_papers = []\n",
    "\n",
    "for idx, p in enumerate(zip(gt, preds[:len(collection)], all_class_weights)):\n",
    "    if sorted(p[1]) == ['0', '1']:\n",
    "        unmapped_papers.append(idx)\n",
    "        print(f'paper_id: {idx}; preds: {sorted(p[1])}; gt: {p[0]}; weights: {p[2]}')\n",
    "        print(f'paper title: {collection[idx].title}')\n",
    "    else:\n",
    "        mapped_papers.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the paper is \"template filling with generative transformers\". The abstract of this paper is \"template filling is generally tackled by a pipeline of two separate supervised systems one for role_filler extraction and another for template/event recognition . since pipelines consider events in isolation , they can suffer from error_propagation . we introduce a framework based on end_to_end generative transformers for this task ( i.e. , gtt ) . it naturally models the dependence between entities both within a single event and across the multiple events described in a document . experiments demonstrate that this framework substantially outperforms pipeline_based approaches , and other neural end_to_end baselines that do not model between_event dependencies . we further show that our framework specifically improves performance on documents containing multiple events .\"\n",
      "\n",
      " According to the paper, its specific 1-3 word category is \"<extra_id_0>\". Other examples of adjacent, sibling categories to the paper's category are: relation_extraction, entity_linking, knowledge_graph_completion.\n"
     ]
    }
   ],
   "source": [
    "template = f'The title of the paper is \"{collection[unmapped_papers[0]].title}\". The abstract of this paper is \"{collection[unmapped_papers[4]].abstract}\"\\n\\n According to the paper, its specific 1-3 word category is \"<extra_id_0>\". Other examples of adjacent, sibling categories to the paper\\'s category are: {\", \".join(map(str, taxo.root.children[0].children))}.'\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['template_filling',\n",
       " 'template filling',\n",
       " 'template filling',\n",
       " 'documents containing multiple events',\n",
       " 'document_filling']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_res = []\n",
    "end_token='<extra_id_1>'\n",
    "\n",
    "encoded = t5_tokenizer.encode_plus(template, add_special_tokens=True, truncation=True, return_tensors='pt')\n",
    "input_ids = encoded['input_ids'].to(DEVICE)\n",
    "outputs = t5_mlm.generate(input_ids=input_ids, \n",
    "                            num_beams=100, num_return_sequences=10,\n",
    "                            max_length=10)\n",
    "\n",
    "for output in outputs:\n",
    "    _txt = t5_tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    if end_token in _txt:\n",
    "        _end_token_index = _txt.index(end_token)\n",
    "        temp_res.append(_txt[:_end_token_index].strip())\n",
    "temp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f'The title of the paper is \"{collection[1].title}\". The abstract of this paper is \"{collection[1].abstract}\"\\n\\nThis paper has already been classified under the parent topics: {taxo.root.children[0].path[1:]}.\\n\\nBased on the paper\\'s title and abstract, from the adjacent category options, [ {\", \".join(map(str, taxo.root.children[0].children))}, and <extra_id_0> ], this paper falls under category option \" <extra_id_0> \". Fill in the mask, <extra_id_0>. Only output a Python list of 10 options.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the paper is \"augmented_natural_language for generative sequence_labeling\". The abstract of this paper is \"we propose a generative_framework for joint sequence_labeling and sentence_level classification . our model performs multiple sequence_labeling tasks at once using a single , shared natural_language output space . unlike prior discriminative methods , our model naturally incorporates label_semantics and shares knowledge across tasks . our framework is general purpose , performing well on few_shot , low_resource , and high_resource tasks . we demonstrate these advantages on popular named_entity_recognition , slot_labeling , and intent classification benchmarks . we set a new state_of_the_art for few_shot slot_labeling , improving substantially upon the previous 5_shot ( 75.0 % 90.9 % ) and 1_shot ( 70.4 % 81.0 % ) state_of_the_art results . furthermore , our model generates large improvements ( 46.27 % 63.83 % ) in low_resource slot_labeling over a bert baseline by incorporating label_semantics . we also maintain competitive results on high_resource tasks , performing within two points of the state_of_the_art on all tasks and setting a new state_of_the_art on the snips dataset .\"\n",
      "\n",
      "This paper has already been classified under the parent topics: ['generative_knowledge_graph_construction', 'generation_tasks'].\n",
      "\n",
      "Based on the paper's title and abstract, from the adjacent category options, [ relation_extraction, entity_linking, knowledge_graph_completion, and <extra_id_0> ], this paper falls under category option \" <extra_id_0> \". Fill in the mask, <extra_id_0>. Only output a Python list of 10 options.\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "all_res = []\n",
    "end_token='<extra_id_1>'\n",
    "\n",
    "for p_id in unmapped_papers:\n",
    "    # template = f'The title of the paper is \"{collection[p_id].title}\". The abstract of this paper is \"{collection[p_id].abstract}\"\\n\\n According to the paper title and abstract, the specific 1-3 word category of the paper is \" <extra_id_0> \".\\n\\nIn the category taxonomy, the path to \" <extra_id_0> \" is: {\" -> \".join(taxo.root.children[0].path[1:])} -> <extra_id_0>. Other examples of adjacent, sibling categories to the paper\\'s category, <extra_id_0> , are: {\", \".join(map(str, taxo.root.children[0].children))}.'\n",
    "    template = f'The title of the paper is \"{collection[p_id].title}\". The abstract of this paper is \"{collection[p_id].abstract}\"\\n\\nBased on the paper\\'s title and abstract, from the adjacent category options, [ {\", \".join(map(str, taxo.root.children[0].children))}, and <extra_id_0> ], this paper falls under category option \" <extra_id_0> \".'\n",
    "\n",
    "    encoded = t5_tokenizer.encode_plus(template, add_special_tokens=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded['input_ids'].to(DEVICE)\n",
    "    outputs = t5_mlm.generate(input_ids=input_ids, \n",
    "                                num_beams=20, num_return_sequences=10,\n",
    "                                max_length=10)\n",
    "\n",
    "    for output in outputs:\n",
    "        _txt = t5_tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        if end_token in _txt:\n",
    "            _end_token_index = _txt.index(end_token)\n",
    "            res.append(_txt[:_end_token_index].strip().lower().replace(' ', '_'))\n",
    "            all_res.append(_txt[:_end_token_index].strip().lower().replace(' ', '_'))\n",
    "        else:\n",
    "            all_res.append(_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'augmented_natural_language',\n",
       " 'augmented_natural_language.',\n",
       " 'claret',\n",
       " 'comparison_learning',\n",
       " 'contrast_learning',\n",
       " 'contrastive_learning',\n",
       " 'entity_extraction',\n",
       " 'entity_linking',\n",
       " 'event_extraction',\n",
       " 'generative_language_models',\n",
       " 'generative_transformer',\n",
       " 'generative_transformer.',\n",
       " 'generative_transformers',\n",
       " 'graph_generation',\n",
       " 'ie_extraction',\n",
       " 'information_extraction',\n",
       " 'information_extraction.',\n",
       " 'knowledge_graph_building',\n",
       " 'knowledge_graph_completion',\n",
       " 'knowledge_graph_construction',\n",
       " 'relation_extraction',\n",
       " 'semantic_extraction',\n",
       " 'semantics_extraction',\n",
       " 'template_filling',\n",
       " 'template_filling.',\n",
       " 'text_to_structure',\n",
       " 'unified_structure_generation'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'Template filling with generative transformers',\n",
       " 'augmented_natural_language',\n",
       " 'augmented_natural_language for',\n",
       " 'augmented_natural_language.',\n",
       " 'claret',\n",
       " 'comparison_learning',\n",
       " 'contrast_learning',\n",
       " 'contrastive_learning',\n",
       " 'correlation_aware context_to_',\n",
       " 'entity_extraction',\n",
       " 'entity_linking',\n",
       " 'event_extraction',\n",
       " 'generative language_models for zero',\n",
       " 'generative_language_models',\n",
       " 'generative_transformer',\n",
       " 'generative_transformer.',\n",
       " 'generative_transformers',\n",
       " 'graph_generation',\n",
       " 'graph_generation : an empirical study',\n",
       " 'ie_extraction',\n",
       " 'information_extraction',\n",
       " 'information_extraction with generative_',\n",
       " 'information_extraction.',\n",
       " 'knowledge_graph_building',\n",
       " 'knowledge_graph_completion',\n",
       " 'knowledge_graph_construction',\n",
       " 'multilingual generative language_models',\n",
       " 'relation_extraction',\n",
       " 'semantic_extraction',\n",
       " 'semantics_extraction',\n",
       " 'template filling with generative transformer',\n",
       " 'template_filling',\n",
       " 'template_filling with generative',\n",
       " 'template_filling.',\n",
       " 'text_to_structure',\n",
       " 'triple_extraction with generative_',\n",
       " 'unified structure_generation for universal information',\n",
       " 'unified_structure_generation'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Prompt for extraction ?',\n",
       " 'claret',\n",
       " 'data_extraction',\n",
       " 'deepstruct',\n",
       " 'document_filling',\n",
       " 'documents containing multiple events',\n",
       " 'dynamic prefix_tuning',\n",
       " 'education_extraction',\n",
       " 'event_centric',\n",
       " 'event_centric generation and classification',\n",
       " 'event_extraction',\n",
       " 'explanation_graph',\n",
       " 'explanation_graph_generation',\n",
       " 'expression_extraction',\n",
       " 'extraction',\n",
       " 'generative event_extraction',\n",
       " 'generic event_extraction',\n",
       " 'human_annotated graphs',\n",
       " 'information_extraction',\n",
       " 'information_extraction .',\n",
       " 'information_extraction.',\n",
       " 'intellectual_extraction',\n",
       " 'intelligent_extraction',\n",
       " 'intent_recognition',\n",
       " 'knowledge_graph_completion',\n",
       " 'learning_graph_completion',\n",
       " 'no_shot cross_lingual',\n",
       " 'object_extraction',\n",
       " 'prompt for extraction',\n",
       " 'prompt for extraction ?',\n",
       " 'prompt for extractive objectives',\n",
       " 'relation_extraction',\n",
       " 'sequence_extraction',\n",
       " 'structure_prediction',\n",
       " 'template filling',\n",
       " 'template_filling'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the phrases in unmapped papers that are:\n",
    "## how freq in \n",
    "unmapped_phrase_pool = {}\n",
    "for p_id in unmapped_papers:\n",
    "    collection[p_id].vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each paper in a corpus is either assigned to a cluster (mapped) or not (unmapped). Given the following existing clusters and their respective mapped papers below (tag 'existing_clusters') and the list of unmapped papers (tag 'unmapped_papers'), first determine the unmapped papers that SHOULD NOT BE MAPPED to an existing_cluster. From this list of unmapped and irrelevant to existing clusters papers, determine HOW MANY clusters are formed from the list, and what the cluster topic(s) should be. The constraints for cluster topic titles are that (1) they must be a subtopic of parent topic generation_tasks, (2) easily be replaced with (a sibling to) any of the existing cluster titles in the statement, \"[cluster title] is a subtopic of generation_tasks\", and (3) at least 2 papers must be mapped to the cluster. If no clusters should be formed (all unmapped papers should truly be mapped to an existing cluster or less than two papers is mapped to the potential cluster), then simply output 0 for 'number_of_clusters'.\n",
      "\n",
      "existing_clusters:\n",
      "Cluster 0: relation_extraction (Extraction of relationships between entities in text)\n",
      "Cluster 1: entity_linking (Entity linking is the task of linking named entities mentioned in text to their corresponding entries in a knowledge base or database.)\n",
      "Cluster 2: knowledge_graph_completion (Completing missing or inaccurate information in a knowledge graph.)\n",
      "\n",
      "unmapped_papers:\n",
      "- PAPER ID: 1; augmented_natural_language for generative sequence_labeling: we propose a generative framework for joint sequence_labeling and sentence_level classification . our model performs multiple sequence_labeling tasks at once using a single , shared natural_language output space . unlike prior discriminative methods , our model naturally incorporates label_semantics and shares knowledge across tasks . our framework is general purpose , performing well on few_shot , low_resource , and high_resource tasks . we demonstrate these advantages on popular named_entity_recognition , slot_labeling , and intent classification benchmarks . we set a new state_of_the_art for few_shot slot_labeling , improving substantially upon the previous 5_shot ( 75.0 % 90.9 % ) and 1_shot ( 70.4 % 81.0 % ) state_of_the_art results . furthermore , our model generates large improvements ( 46.27 % 63.83 % ) in low_resource slot_labeling over a bert baseline by incorporating label_semantics . we also maintain competitive results on high_resource tasks , performing within two points of the state_of_the_art on all tasks and setting a new state_of_the_art on the snips dataset .\n",
      "- PAPER ID: 7; unified structure generation for universal information_extraction: information_extraction suffers from its varying targets , heterogeneous structures , and demand_specific schemas . in this paper , we propose a unified text_to_structure generation framework , namely uie , which can universally model different ie tasks , adaptively generate targeted structures , and collaboratively learn general ie abilities from different knowledge sources . specifically , uie uniformly encodes different extraction structures via a structured extraction language , adaptively generates target extractions via a schema_based prompt mechanism structural_schema_instructor , and captures the common ie abilities via a large_scale pretrained text_to_structure model . experiments show that uie achieved the state_of_the_art performance on 4 ie tasks , 13 datasets , and on all supervised , low_resource , and few_shot settings for a wide range of entity , relation , event and sentiment extraction tasks and their unification . these results verified the effectiveness , universality , and transferability of uie .\n",
      "- PAPER ID: 13; contrastive triple_extraction with generative_transformer: triple_extraction is an essential task in information_extraction for natural_language processing and knowledge_graph construction . in this paper , we revisit the end_to_end triple_extraction task for sequence_generation . since generative triple_extraction may struggle to capture long_term_dependencies and generate unfaithful triples , we introduce a novel model , contrastive triple_extraction with a generative_transformer . specifically , we introduce a single shared transformer module for encoder_decoder_based generation . to generate faithful results , we propose a novel triplet contrastive training object . moreover , we introduce two mechanisms to further improve model performance ( i.e. , batch_wise_dynamic attention_masking and triple_wise calibration ) . experimental_results on three datasets ( i.e. , nyt , webnlg , and mie ) show that our approach achieves better performance than that of baselines .\n",
      "- PAPER ID: 24; multilingual generative language_models for zero_shot cross_lingual event_argument_extraction: we present a study on leveraging multilingual pre_trained_generative_language models for zero_shot cross_lingual event_argument_extraction ( eae ) . by formulating eae as a language_generation task , our method effectively encodes event structures and captures the dependencies between arguments . we design language_agnostic templates to represent the event argument structures , which are compatible with any language , hence facilitating the cross_lingual_transfer . our proposed model finetunes multilingual pre_trained_generative_language models to generate sentences that fill in the language_agnostic template with arguments extracted from the input passage . the model is trained on source languages and is then directly applied to target languages for event_argument_extraction . experiments demonstrate that the proposed model outperforms the current_state_of_the_art models on zero_shot cross_lingual eae . comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language_models for zero_shot cross_lingual_transfer eae .\n",
      "\n",
      "Output your answer in the following JSON format: \n",
      "      {\n",
      "      \"number_of_clusters\": <integer: number of new k clusters with at least 2 papers mapped to each>,\n",
      "      \"new_cluster_1\": {\n",
      "            \"title\": <string value of cluster_1 title; 2-3 words, lowercase, underscore instead of space>\n",
      "            \"mapped_papers\": <list of at least 2 paper ids which should be mapped to this new cluster_1>\n",
      "        }\n",
      "      ...\n",
      "      \"new_cluster_k\": {\n",
      "            \"title\": <string value of cluster_k title; 2-3 words, lowercase, underscore instead of space>\n",
      "            \"mapped_papers\": <list of at least 2 paper ids which should be mapped to this new cluster_k>\n",
      "        }\n",
      "      }\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# other clusters\n",
    "node_to_expand = '1'\n",
    "top_k = 3\n",
    "parent_node = taxo.root.findChild(node_to_expand)\n",
    "cluster_nodes = parent_node.children\n",
    "\n",
    "print(f\"Each paper in a corpus is either assigned to a cluster (mapped) or not (unmapped). Given the following existing clusters and their respective mapped papers below (tag 'existing_clusters') and the list of unmapped papers (tag 'unmapped_papers'), first determine the unmapped papers that SHOULD NOT BE MAPPED to an existing_cluster. From this list of unmapped and irrelevant to existing clusters papers, determine HOW MANY clusters are formed from the list, and what the cluster topic(s) should be. The constraints for cluster topic titles are that (1) they must be a subtopic of parent topic {parent_node.label}, (2) easily be replaced with (a sibling to) any of the existing cluster titles in the statement, \\\"[cluster title] is a subtopic of {parent_node.label}\\\", and (3) at least 2 papers must be mapped to the cluster. If no clusters should be formed (all unmapped papers should truly be mapped to an existing cluster or less than two papers is mapped to the potential cluster), then simply output 0 for 'number_of_clusters'.\")\n",
    "\n",
    "print(\"\\nexisting_clusters:\")\n",
    "for c_id, c in enumerate(cluster_nodes):\n",
    "    print(f'Cluster {c_id}: {c.label} ({c.description})')\n",
    "    # cluster_papers = class_map[c.node_id][:top_k]\n",
    "    # for p in cluster_papers:\n",
    "    #     print(f'- PAPER ID: {corpus[p[0]].id}; {corpus[p[0]].title}: {corpus[p[0]].abstract}')\n",
    "    # print(\"\\n\")\n",
    "\n",
    "print(\"\\nunmapped_papers:\")\n",
    "for p in no_label_papers:\n",
    "    print(f'- PAPER ID: {corpus[p].id}; {corpus[p].title}: {corpus[p].abstract}')\n",
    "\n",
    "print(\"\"\"\\nOutput your answer in the following JSON format: \n",
    "      {\n",
    "      \"number_of_clusters\": <integer: number of new k clusters with at least 2 papers mapped to each>,\n",
    "      \"new_cluster_1\": {\n",
    "            \"title\": <string value of cluster_1 title; 2-3 words, lowercase, underscore instead of space>\n",
    "            \"mapped_papers\": <list of at least 2 paper ids which should be mapped to this new cluster_1>\n",
    "        }\n",
    "      ...\n",
    "      \"new_cluster_k\": {\n",
    "            \"title\": <string value of cluster_k title; 2-3 words, lowercase, underscore instead of space>\n",
    "            \"mapped_papers\": <list of at least 2 paper ids which should be mapped to this new cluster_k>\n",
    "        }\n",
    "      }\n",
    "      \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'generative_knowledge_graph_construction',\n",
       " '1': 'generation_tasks',\n",
       " '2': 'relation_extraction',\n",
       " '3': 'entity_linking',\n",
       " '4': 'knowledge_graph_completion'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'0', '1', '3'}\n",
      "2 {'0', '1'}\n",
      "3 {'0', '1', '2'}\n",
      "4 {'0', '1', '2'}\n",
      "5 {'0', '1', '3'}\n",
      "6 {'0'}\n",
      "7 {'0', '1', '3', '2'}\n",
      "8 {'0', '1'}\n",
      "9 {'0', '1', '2'}\n",
      "10 {'0', '1', '2'}\n",
      "11 {'0', '1', '2'}\n",
      "12 {'0', '1', '3', '2'}\n",
      "13 {'0', '1', '2'}\n",
      "14 {'0', '1'}\n",
      "15 {'0', '1', '3', '2'}\n",
      "16 {'0', '1', '3', '2'}\n",
      "17 {'0', '1', '3', '2'}\n",
      "18 {'0', '1', '3', '2'}\n",
      "19 {'0', '1', '2'}\n",
      "20 {'0', '1', '3'}\n",
      "21 {'0', '1', '2'}\n",
      "22 {'0', '1', '2'}\n",
      "23 {'0', '1', '2'}\n",
      "24 {'0', '1', '2'}\n",
      "25 {'0', '1'}\n",
      "26 {'0', '1', '2'}\n",
      "27 {'0', '1', '3'}\n",
      "28 {'0', '1', '3'}\n",
      "29 {'0', '1', '2', '4'}\n",
      "30 {'0', '1', '4'}\n",
      "31 {'0', '1', '4'}\n",
      "32 {'0', '1', '2', '4'}\n",
      "33 {'0', '1', '4'}\n",
      "34 {'0', '1', '4'}\n"
     ]
    }
   ],
   "source": [
    "for idx, p in enumerate(preds[:len(collection)]):\n",
    "    print(idx+1, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_definitions import promptGPT, constructPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_title(parent, paper_list, related_titles_list):\n",
    "    # Prompt engineering to provide context and desired output\n",
    "    prompt = f\"\"\"You are given a group of paper titles/abstracts and a list of sibling topics. Your task is to generate a topic keyword that summarizes the cluster of the given papers.\n",
    "    The topic should be a subtopic of {parent} and at the same topic granularity as the provided list of sibling topic titles. Here are the papers in the cluster:\"\"\"\n",
    "\n",
    "    prompt += \"\\n\\n\".join([f\"- {abstract}\" for abstract in paper_list])\n",
    "    prompt += \"\\n\\nHere are the sibling topic titles:\\n\"\n",
    "    prompt += \"\\n\".join([f\"- {title}\" for title in related_titles_list])\n",
    "    prompt += \"\\n\\nBased on these papers and siblings, please first consider 5 candidate topics that represents ALL of the papers within the cluster. Then generate a topic title that summarizes these 5 topics and output it in the same format as the sibling topics (lowercase, underscores instead of spaces).\"\n",
    "\n",
    "    prompts = [constructPrompt(\"You are an expert summarizer and topic generator for scientific research.\", prompt, api=True)]\n",
    "    output = promptGPT(prompts, schema=None, max_new_tokens=200, json_mode=False)\n",
    "    \n",
    "    # Extract and return the title from the response\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_titles = generate_summary_title(\n",
    "    parent=taxo.root.children[0].label,\n",
    "    paper_list=[f\"{collection[i].title}: {collection[i].abstract}\" for i in no_label_papers],\n",
    "    related_titles_list=[c.label for c in taxo.root.children[0].children]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate topics:\n",
      "- generative_sequence_labeling\n",
      "- information_extraction_generation\n",
      "- event_centric_reasoning\n",
      "- template_based_event_extraction\n",
      "- cross_lingual_event_argument_extraction\n",
      "\n",
      "Summary topic title: event_generation_and_extraction\n"
     ]
    }
   ],
   "source": [
    "print(generated_titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are given a group of paper titles/abstracts and a list of sibling topics. Your task is to generate a topic keyword that summarizes the cluster of the given papers.\n",
    "    The topic should be a subtopic of {parent} and at the same topic granularity as the provided list of sibling topic titles. Here are the papers in the cluster:\"\"\"\n",
    "\n",
    "prompt += \"\\n\\n\".join([f\"- {abstract}\" for abstract in paper_list])\n",
    "prompt += \"\\n\\nHere are the sibling topic titles:\\n\"\n",
    "prompt += \"\\n\".join([f\"- {title}\" for title in related_titles_list])\n",
    "prompt += \"\\n\\nBased on these papers and siblings, please first consider 5 candidate topics that represents ALL of the papers within the cluster. Then generate a topic title that summarizes these 5 topics and output it in the same format as the sibling topics (lowercase, underscores instead of spaces).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 paper_title : comet : commonsense transformers for automatic knowledge_graph construction ; paper_abstract : we present the first comprehensive study on automatic knowledge_base construction for two prevalent commonsense knowledge graphs : atomic ( sap et al. , 2019 ) and conceptnet ( speer et al. , 2017 )\n",
      "1 127 contrary to many conventional kbs that store knowledge with canonical templates , commonsense kbs only store loosely structured open_text descriptions of knowledge\n",
      "2 92 we posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge , and propose commonsense transformers ( comet ) that learn to generate rich and diverse commonsense descriptions in natural_language\n",
      "3 105 despite the challenges of commonsense modeling , our investigation reveals promising results when implicit knowledge from deep pre_trained_language_models is transferred to generate explicit_knowledge in commonsense knowledge graphs\n",
      "4 14 empirical results demonstrate that comet is able to generate novel knowledge that humans rate as high quality , with up to 77.5 % ( atomic ) and 91.7 % ( conceptnet ) precision at top 1 , which approaches human performance for these resources\n",
      "5 107 our findings suggest that using generative commonsense models for automatic commonsense kb completion could soon be a plausible alternative to extractive methods\n",
      "6 26 ; paper_content : proceedings of the 57th annual meeting of the association_for_computational_linguistics , pages 47624779 florence , italy , july 28 _ august 2 , 2019. c2019 association for computational linguistics4762comet : commonsense transformers for automatic knowledge_graph construction antoine bosseluthannah rashkinmaarten sapchaitanya malaviya asli celikyilmazyejin choi allen institute for articial_intelligence , seattle , wa , usa paul g. allen school of computer science & engineering , seattle , wa , usa microsoft_research , redmond , wa , usa abstract we present the rst comprehensive study on automatic knowledge_base construction for two prevalent commonsense knowledge graphs : a tomic ( sap et al. , 2019 ) and con_ ceptnet ( speer et al. , 2017 )\n",
      "7 83 contrary to many conventional kbs that store knowledge with canonical templates , commonsense kbs only store loosely structured open_text de_ scriptions of knowledge\n",
      "8 101 we posit that an important step toward automatic common__sense completion is the development of gen_ erative models of commonsense knowledge , and propose com monsensetransformers ( comet ) that learn to generate rich and diverse commonsense descriptions in natural_language\n",
      "9 118 despite the challenges of com_ monsense modeling , our investigation reveals promising results when implicit knowledge from deep pre_trained_language_models is transferred to generate explicit_knowledge in commonsense knowledge graphs\n",
      "10 20 empirical results demonstrate that comet is able to generate novel knowledge that humans rate as high quality , with up to 77.5 % ( a tomic ) and 91.7 % ( conceptnet ) precision at top 1 , which approaches human performance for these re_ sources\n",
      "11 99 our ndings suggest that using gen_ erative commonsense models for automatic commonsense kb completion could soon be a plausible alternative to extractive methods\n",
      "12 38 1 introduction when reading text , humans make commonsense inferences that frame their understanding of the narrative being presented\n",
      "13 100 for machines to achieve this capability , they must be able to acquire rele_ vant and correct commonsense for an unbounded set of situations\n",
      "14 191 in this work , we cast common__sense acquisition as knowledge_base construction and investigate whether large_scale language mod__els can effectively learn to generate the knowledge personx puts their arms around personyloving towards persony to comfort personycaringpersonx goes to the storebring a walletfeels lovedcommonsense knowledge_bases ( seen events ) automatic kb completionxattrxattr xintentoreactxneed unseen eventspersonx buys lunchto get foodxintentxneed naphaving a restdozing off hassubeventhassubeventgoing to a moviehaving funusedforenergycausesatomic conceptnetthrowing a partycausesfigure 1 : comet learns from an existing knowledge_base ( solid lines ) to be able to generate novel nodes and edges ( dashed lines )\n",
      "15 162 necessary to automatically construct a common__sense knowledge_base ( kb )\n",
      "16 133 automatic kb construction is a long_standing goal of articial_intelligence research due to the difculty of achieving high_concept coverage in high_precision curated kbs ( lenat , 1995 ; miller , 1995 )\n",
      "17 173 previous work has developed models capa_ ble of reading and extracting semi_structured text ( suchanek et al. , 2007 ; hoffart et al. , 2013 ; auer et al. , 2007 ; bollacker et al. , 2008 ) and unstruc_ tured text ( dong et al. , 2014 ; carlson et al. , 2010 ; nakashole et al. , 2011 , 2012 ; niu , 2012 ) into re_ lational schemas that can be queried for down_ stream applications\n",
      "18 97 a common thread of these approaches , however , is the focus on encyclope_ dic knowledge , which lends itself to a well_dened space of entities and relations that can be modeled\n",
      "19 80 commonsense knowledge , however , does not cleanly t into a schema comparing two entities with a known relation , leading current approaches4763 commonsense transformer ( comet ) multi_headed attention transformer block w k 1w v 1w q 1w k bw q bkvqw v battention head 1attention head bmulti_headed attention+ , , { } layer normalizationlayer normalization+feedforward networkblockblockblockblockblockblockblockblockblockblocke0 p0e1 p1e |s| p |s|+++++personxsails < xneed > sailboatboat < end > vocabvocabvocabvocabvocab [ mask ] [ mask ] haveconcatenationlinear projectiong~ht ( a ) ( b ) ( c ) htht_1h0l _ 1l _ 1l _ 1l l figure 2 : model diagram\n",
      "20 177 ( a ) in the multi_headed attention module , the key , value , and query all pass through a head_specic projection before a scaled dot_product attention is computed between them\n",
      "21 51 the outputs of the heads are concatenated and projected\n",
      "22 110 ( b ) inside the transformer block , the outputs of all the previous layer blocks from earlier time steps are input to the multi_headed attention with the preceding block for the current time step as the query\n",
      "23 188 ( c ) each token is an input to a rst_layer block along with all preceding tokens\n",
      "24 125 dotted lines indicate outputs to all future blocks in the next layer and inputs from all preceding blocks in the previous layer\n",
      "25 17 to model entities '' as natural_language phrases and relations as any concept that can link them ( li et al. , 2016 ; sap et al. , 2019 )\n",
      "26 124 openie ap_ proaches display this property of open text enti__ties and relations ( etzioni et al. , 2011 ; fader et al. , 2011 ; mausam et al. , 2012 ) , but being extrac_ tive , they only capture knowledge that is explic_ itly mentioned in text , limiting their applicability for capturing commonsense knowledge , which is often implicit ( gordon and van durme , 2013 )\n",
      "27 55 meanwhile , recent progress in training deep contextualized language_models ( peters et al. , 2018 ; radford et al. , 2018 ; devlin et al. , 2018 ) provides an opportunity to explore beyond extrac_ tive methods as an avenue for commonsense kb construction\n",
      "28 198 these large_scale language_models display impressive performance when their under_ lying representations are tuned to solve end tasks , achieving state_of_the_art results on a variety of complex problems\n",
      "29 143 in this work , we dene the com monsensetransformer ( comet ) , which constructs commonsense kbs by using existing tuples as a seed set of knowledge on which to train\n",
      "30 48 using this seed set , a pre_trained_language_model learns to adapt its learned representations to knowledge generation , and produces novel tuples that are high quality\n",
      "31 37 we summarize our contributions in this work as follows\n",
      "32 121 first , we develop a generative approach to knowledge_base construction\n",
      "33 182 a model must learn to produce new nodes and identify edges be_tween existing nodes by generating phrases that coherently complete an existing seed phrase and relation type1\n",
      "34 157 second , we develop a framework for using large_scale transformer language_models to learn to produce commonsense knowledge tu_ ples2\n",
      "35 165 finally , we perform an empirical study on the quality , novelty , and diversity of the common__sense knowledge produced by our approach for two domains , a tomic and conceptnet , as well as an efciency study on the number of seed tuples needed to learn an effective knowledge model\n",
      "36 120 the results indicate that comet is able to pro_ duce high quality tuples as human judges nd that 77.5 % of generated tuples for a tomic events and 91.7 % of generated tuples for conceptnet rela__tions are correct\n",
      "37 184 2 learning to generate commonsense comet is an adaptation framework for construct_ ing commonsense knowledge_bases from language_models by training the language_model on a seed set of knowledge tuples\n",
      "38 89 these tuples provide comet with the kb structure and relations that must be learned , and comet learns to adapt the language_model representations learned from pre__training to add novel nodes and edges to the seed knowledge_graph\n",
      "39 19 1demo is available at https : //mosaickg.apps\n",
      "40 62 allenai.org/ 2code is available at https : //github.com/ atcbosselut/comet_commonsense47642.1 task more specically , the problem assumes comet is given a training knowledge_base of natural_lan__guage tuples in { s , r , o } format , where sis the phrase subject of the tuple , ris the relation of the tuple , and ois the phrase object of the tuple\n",
      "41 98 for example , a conceptnet tuple relating to taking a nap '' would be : ( s=take a nap '' , r=causes , o=have energy '' )\n",
      "42 22 the task is to generate ogiven sandras inputs\n",
      "43 122 notation we dene xs= { xs 0 , ... , xs |s| } as the tokens that make up the subject of the relation , xr= { xr 0 , ... , xr |r| } as the tokens that make up the relation of the tuple , and xo= { xo 0 , ... , xo |o| } as the tokens that make up the object of the tuple\n",
      "44 86 the embedding for any word xis denoted as e. 2.2 transformer language_model while comet is agnostic to the language_model with which it is initialized , in this work , we use the transformer language_model architecture in_ troduced in radford et al\n",
      "45 106 ( 2018 ) ( gpt ) , which uses multiple transformer blocks of multi_headed scaled dot_product attention and fully connected layers to encode input text ( vaswani et al. , 2017 )\n",
      "46 131 figure 2 depicts different components of the gpt architecture and we dene each component in more depth below\n",
      "47 138 transformer block as shown in figure 2 ( b ) , each transformer layer lcontains an architecturally identical transformer block ( though with unique trainable parameters ) that applies the following transformations to the input to the block : gl=multi attn ( hl1 ) ( 1 ) gl=layer norm ( gl+hl1 ) ( 2 ) hl=ffn ( gl ) ( 3 ) hl=layer norm ( hl+gl ) ( 4 ) where m ulti attn is a multi_headed self_ attention_mechanism ( dened below ) , ffn is a two_layer feed_forward network , and l ayer _ norm represents a layer normalization ( ba et al. , 2016 ) operation that is applied to the output of the self_attention and the feedforward network\n",
      "48 66 note that the inputs to the l ayer norm opera_ tions contain a residual connection that sums the output of and input to the previous operation.multi_headed attention the multi_headed at_ tention module of each transformer block , shown in figure 2 ( a ) , is identical to the one originally de_ ned by vaswani et al\n",
      "49 33 ( 2017 )\n",
      "50 136 the attention func_ tion receives three inputs , a query q , key k , and value v. the attention is made of multiple heads that each compute a unique scaled dot_product at_ tention distribution over vusing qandk : attention ( q , k , v ) =softmax ( qkt dk ) v ( 5 ) where dkis the dimensionality of the input vectors representing the query , key and value\n",
      "51 160 for each of the heads , q , k , andvare uniquely projected prior to the attention being computed : hi=attention ( qwq i , kwk i , v wv i ) ( 6 ) where hiis the output of a single attention head andwq i , wk i , andwv iare head_specic projec_ tions for q , k , and v , respectively\n",
      "52 179 the outputs of the attention heads hiare then concatenated : multi h ( q , k , v ) = [ h1 ; ... ; hb ] wo ( 7 ) where wois an output projection of the concate_ nated outputs of the attention heads\n",
      "53 28 as shown in figure 2 ( c ) , we follow radford et al\n",
      "54 159 ( 2018 ) and use the output of the previous layers transformer block as the query input for the multi_headed at_ tention of the next block\n",
      "55 140 the keys and values are outputs of the previous layers block for all pre_ ceding time steps : multi attn ( hl1 t ) =multi h ( hl1 t , hl1 t , hl1 t ) ( 8 ) where hl1 t= { hl1 } < tis the set of previous layer transformer block outputs for time steps pre_ ceding t. input encoder as input to the model , we repre_ sent a knowledge tuple { s , r , o } as a concatenated sequence of the words of each item of the tuple : x= { xs , xr , xo } ( 9 ) since the transformer ( a self_attention model ) has no concept of ordering of tokens , a position em_ bedding ptis initialized for each absolute position in the sequence ( vaswani et al. , 2017 )\n",
      "56 180 for any input word xtx , our encoding of the input is4765 s tokensr token mask tokenso tokens s tokensr tokens mask tokenso tokens mask tokensatomic input template and conceptnet relation_only input template conceptnet relation to language input template personx goes to the mall [ mask ] < xintent > to buy clothesgo to mall [ mask ] [ mask ] has prerequisite [ mask ] have moneyfigure 3 : input token setup for training congurations\n",
      "57 130 for the a tomic dataset , the tokens of the subject , xs ( e.g. , personx goes to the mall ) are followed by mask_ ing tokens , which is followed by a single relation token xr ( e.g. , xintent ) , and then the object tokens xo ( e.g. , to buy clothes )\n",
      "58 85 the model receives the same in_ put for conceptnet , except that a second set of mask_ ing tokens separate xrandxobecause xrcan have a variable number of tokens for conceptnet ( 5.2 ) the sum of its word embedding , etwith a position embedding encoding its absolute position in the sequence x : h0 t=et+pt ( 10 ) where ptis the position embedding for time step t , andh0is the input to the rst transformer layer\n",
      "59 102 3 training comet comet is trained to learn to produce the phrase object oof a knowledge tuple given the tuples phrase subject sand relation r. more specically , given the concatenation of the tokens of sandr : [ xs , xr ] as input , the model must learn to gener__ate the tokens of o : xo ( see 2.1 for denitions of these variables )\n",
      "60 146 loss_function to achieve this goal , comet is trained to maximize the conditional loglikelihood of predicting the phrase object tokens , xo : l=|s|+|r|+|o| t=|s|+|r|logp ( xt|x < t ) ( 11 ) where|s| , |r| , and|o|are the number of tokens in the subject phrase , relation , and object phrase , respectively\n",
      "61 145 figure 3 outlines how the tokens in s , r , andoare organized for different training tasks\n",
      "62 90 datasets comet relies on a seed set of knowl__edge tuples from an existing kb to learn to pro_ duce commonsense knowledge\n",
      "63 25 in this work , we use a tomic and conceptnet as knowledge seed sets , but other commonsense knowledge re_ sources could have been used as well as comet is domain_agnostic.initialization parameters are initialized to the _ nal language_model weights from radford et al\n",
      "64 52 ( 2018 )\n",
      "65 103 additional special_tokens that are added to the vocabulary for ne tuning ( e.g. , relation em_ beddings such as oreact for a tomic andisa for conceptnet ) are initialized by sampling from the standard_normal_distribution\n",
      "66 8 hyperparameters following radford et al\n",
      "67 144 ( 2018 ) s design of the gpt model , we initialize comet with 12 layers , 768_dimensional hidden states , and 12 attention heads\n",
      "68 65 we use a dropout rate of 0.1 and use gelu ( hendrycks and gimpel , 2016 ) units as activation functions\n",
      "69 7 during train__ing , our batch_size is 64\n",
      "70 13 other dataset_specic hyperparameters are provided in appendix a.1\n",
      "71 73 4 a tomic experiments the a tomic dataset3 , released by sap et al\n",
      "72 168 ( 2019 ) , contains 877k tuples covering a variety of social commonsense knowledge around specic event prompts ( e.g. , x goes to the store )\n",
      "73 181 specif_ ically , a tomic distills its commonsense in nine dimensions , covering the events causes ( e.g. , x needs to drive there ) , its effects on the agent ( e.g. , to get food ) and its effect on other direct ( or implied ) participants ( e.g. , others will be fed )\n",
      "74 155 more details about a tomic can be found in ap_ pendix d. for our experiments , a tomic events ( e.g. , x goes to the store ) are phrase subjects , s , the dimension ( e.g. , xintent ) is the phrase rela__tion , r , and the causes/effects ( e.g. , to get food ) are phrase objects , o\n",
      "75 23 we use the training splits from sap et al\n",
      "76 15 ( 2019 ) , resulting in 710k training , 80k development , and 87k test tuples respectively\n",
      "77 9 4.1 setup metrics following sap et al\n",
      "78 79 ( 2019 ) , we eval_ uate our method using bleu_2 as an automatic evaluation metric\n",
      "79 61 we also report the perplexity of the model on its gold generations\n",
      "80 167 the remain_ ing automatic metrics in table 1 measure the pro_ portion of generated tuples and generated objects which are not in the training_set\n",
      "81 153 we report the proportion of all generated tuples that are novel ( % n/t sro ) and that have a novel object ( % n/t o ) 4\n",
      "82 116 to show that these novel objects are diverse ( i.e. , the same novel object is not the only one be_ ing generated ) , we also report the number of novel 3https : //homes.cs.washington.edu/ ~msap/atomic/ 4a new orepresents a new node in the knowledge graph4766model ppl5bleu_2 n/t sro6n/ton/uo 9enc9dec ( sap et al. , 2019 ) _ 10.01 100.00 8.61 40.77 nearestneighbor ( sap et al. , 2019 ) _ 6.61 _____ event2 ( i n ) volun ( sap et al. , 2019 ) _ 9.67 100.00 9.52 45.06 event2p erson x/y ( sap et al. , 2019 ) _ 9.24 100.00 8.22 41.66 event2p re/post ( sap et al. , 2019 ) _ 9.93 100.00 7.38 41.99 comet ( _ pretrain ) 15.42 13.88 100.00 7.25 45.71 comet 11.14 15.10 100.00 9.71 51.20 table 1 : automatic evaluations of quality and novelty for generations of a tomic commonsense\n",
      "83 39 no novelty scores are reported for the nearestneighbor baseline because all retrieved sequences are in the training_set\n",
      "84 166 model oeffect oreact owant xattr xeffect xintent xneed xreact xwant avg 9enc9dec ( sap et al. , 2019 ) 22.92 32.92 35.50 52.20 47.52 51.70 48.74 63.57 51.56 45.32 event2 ( in ) voluntary ( sap et al. , 2019 ) 26.46 36.04 34.70 52.58 46.76 61.32 49.82 71.22 52.44 47.93 event2personx/y ( sap et al. , 2019 ) 24.72 33.80 35.08 52.98 48.86 53.93 54.05 66.42 54.04 46.41 event2pre/post ( sap et al. , 2019 ) 26.26 34.48 35.78 52.20 46.78 57.77 47.94 72.22 47.94 46.76 comet ( _ pretrain ) 25.90 35.40 40.76 48.04 47.20 58.88 59.16 64.52 65.66 49.50 comet 29.02 37.68 44.48 57.48 55.50 68.32 64.24 76.18 75.16 56.45 table 2 : human score of generations of a tomic commonsense\n",
      "85 63 we present comparisons to the baselines from sap et al\n",
      "86 45 ( 2019 )\n",
      "87 35 underlined results are those where comet is not signicantly better at p < 0.05 objects as a function of the set of unique objects produced for all test_set events ( % n/u o )\n",
      "88 40 finally , we perform a human evaluation using workers from amazon_mechanical_turk ( amt )\n",
      "89 82 workers are asked to identify whether a model generation of a tomic commonsense adequately completes a plausible tuple of phrase subject , rela__tion , and phrase object\n",
      "90 36 following the setup of sap et al\n",
      "91 16 ( 2019 ) , we evaluate 100 randomly selected events from the test_set\n",
      "92 70 for each event and rela__tion type , 10 candidates are generated using beam_search and the full beam is evaluated by ve differ__ent workers\n",
      "93 194 overall , n=5000 ratings are produced per relation ( 100 events 5 workers10 candi_ dates )\n",
      "94 57 the reported avg in table 2 is an aver_ age of these scores , yielding n=45000 total ratings for each model\n",
      "95 29 we use pitmans test ( noreen , 1989 ) with 100k permutations to test for statis_ tical signicance\n",
      "96 71 because 50 different hypothe_ ses are tested ( 9 relations + the total ) , the holm_ bonferroni method ( holm , 1979 ) is used to correct signicance thresholds\n",
      "97 54 example events from the development_set and their generated phrase objects are available in table 5\n",
      "98 5 baselines we report the performance of our method against the models trained in sap et al\n",
      "99 132 ( 2019 ) that use lstm sequence_to_sequence mod__els ( sutskever et al. , 2014 ) to encode the input sub_ ject and relation and produce an output object.ablations to evaluate how pre_training on a large corpus helps the model learn to produce knowledge , we train a version of comet that is not initialized with pre_trained weights ( comet ( _ pretrain ) )\n",
      "100 6 we also evaluate the data efciency of our method by training models on different pro_ portions of the training_data\n",
      "101 4 finally , because the ultimate goal of our method is to be able to perform high_quality , diverse knowledge_base construction , we explore how various decoding schemes affect the quality of candidate knowledge tuples\n",
      "102 150 we present the effect of the following gen__eration strategies : argmax greedy_decoding , beam_search with beam sizes , b=2 , 5 , 10 , and top_ ksam_ pling with k = 5 , 10\n",
      "103 96 for each decoding method , we conduct the human evaluation on the number of nal candidates produced by each method\n",
      "104 53 4.2 results overall performance the bleu_2 results in table 1 indicate that comet exceeds the perfor__mance of all baselines , achieving a 51 % relative improvement over the top performing model of sap et al\n",
      "105 44 ( 2019 )\n",
      "106 59 more interesting , however , is the result of the human evaluation , where comet re_ ported a statistically signicant relative avg per_ formance increase of 18 % over the top baseline , 5sap et al\n",
      "107 24 ( 2019 ) s models were trained with a different vocabulary so a direct perplexity comparison is not possible\n",
      "108 141 6all test_set sdo not appear in the training_set so all full tuples must be novel.4767comet decoding method oeffect oreact owant xattr xeffect xintent xneed xreact xwant avg top_5 random sampling ( n=2500 per relation ) 34.60 44.04 35.56 64.56 55.68 58.84 46.68 80.96 58.52 53.27 top_10 random sampling ( n=5000 per relation ) 25.20 37.42 27.34 49.20 47.34 47.06 38.24 72.60 48.10 43.61 beam_search _ 2 beams ( n=1000 per relation ) 43.70 54.20 47.60 84.00 51.10 73.80 50.70 85.80 78.70 63.29 beam_search _ 5 beams ( n=2500 per relation ) 37.12 45.36 42.04 63.64 61.76 63.60 57.60 78.64 68.40 57.57 beam_search _ 10 beams ( n=5000 per relation ) 29.02 37.68 44.48 57.48 55.50 68.32 64.24 76.18 75.16 56.45 greedy_decoding ( n=500 per relation ) 61.20 69.80 80.00 77.00 53.00 89.60 85.60 92.20 89.40 77.53 human validation of gold a tomic 84.62 86.13 83.12 78.44 83.92 91.37 81.98 95.18 90.90 86.18 table 3 : human evaluation testing effect of different decoding schemes on candidate tuple quality\n",
      "109 187 the number of ratings made per relation for each decoding method is provided in the rst column\n",
      "110 137 % train data ppl bleu_2 n/t on/uo 1 % train 23.81 5.08 7.24 49.36 10 % train 13.74 12.72 9.54 58.34 50 % train 11.82 13.97 9.32 50.37 full ( _ pretrain ) 15.18 13.22 7.14 44.55 full train 11.13 14.34 9.51 50.05 table 4 : effect of amount of training_data on automatic evaluation of commonsense generations event2i n ( volun )\n",
      "111 21 this performance increase is consistent , as well , with an improvement being observed across every relation_type\n",
      "112 154 in addition to the quality improvements , table 1 shows that comet produces more novel tuple objects than the baselines , as well\n",
      "113 134 learning knowledge from language signi_ cant differences were also observed between the performance of the model whose weights were ini_ tialized with the pre_trained parameters from the gpt model of radford et al\n",
      "114 3 ( 2018 ) and a model with the same architecture that was trained from random initialization\n",
      "115 46 this 14 % relative improve_ ment in overall human performance conrms that the language representations learned by the gpt model are transferable to generating natural_lan__guage commonsense knowledge\n",
      "116 30 effect of decoding algorithm in table 3 , we show the effect of different generation policies on knowledge quality\n",
      "117 148 the most interesting result is that using greedy_decoding to produce knowl__edge tuples only results in a 10 % relative perfor__mance gap compared to a human evaluation of the a tomic test_set , showing that the knowledge produced by the model approaches human perfor__mance\n",
      "118 186 while producing more total candidates does lower overall performance , quality assess_seed concept relation generated plausible x holds out xs hand to y xattr helpful x meets y eyes xattr intense x watches y every ___ xattr observant x eats red_meat xeffect gets fat x makes crafts xeffect gets dirty x turns xs phone xeffect gets a text x pours ___ over ys head oeffect gets hurt x takes ys head off oeffect bleeds x pisses on ys bonre oeffect gets burned x spoils somebody rotten xintent to be mean x gives y some pills xintent to help x provides for ys needs xintent to be helpful x explains ys reasons xneed to know y x fulls xs needs xneed to have a plan x gives y everything xneed to buy something x eats pancakes xreact satised x makes ___ at work xreact proud x moves house xreact happy x gives birth to the y oreact happy x gives ys friend ___ oreact grateful x goes ___ with friends oreact happy x gets all the supplies xwant to make a list x murders ys wife xwant to hide the body x starts shopping xwant to go home x develops y theory owant to thank x x offer y a position owant to accept the job x takes ___ out for dinner owant to eat table 5 : generations that were randomly selected from a subset of novel generations from the a tomic development_set\n",
      "119 50 a novel generation is a srotuple not found in the training_set\n",
      "120 192 manual evaluation of each tu_ ple indicates whether the tuple is considered plausible by a human annotator\n",
      "121 87 ments still hover around 55 % 7for a beam_size of 10\n",
      "122 170 this result suggests that comet could be ef_ fective with human evaluators in the loop to con_ rm the correctness of generated tuples\n",
      "123 197 efciency of learning from seed tuples be_ cause not all domains will have large available commonsense kbs on which to train , we explore how varying the amount of training_data avail_ able for learning affects the quality and novelty of the knowledge that is produced\n",
      "124 12 our results in table 4 indicate that even with only 10 % of the available training_data , the model is still able to 7this number is partially low due to the many none '' ref_ erences in the oeffect , oreact , owant categories\n",
      "125 94 in any set of 10 candidates , none '' can only be predicted once , which causes most candidates in the beam to be incorrect if none '' is the appropriate answer.4768produce generations that are coherent , adequate , and novel\n",
      "126 161 using only 1 % of the training_data clearly diminishes the quality of the produced gen_ erations , with signicantly lower observed results across both quality and novelty metrics\n",
      "127 115 interest_ ingly , we note that training the model without pre__trained weights performs comparably to training with 10 % of the seed tuples , quantifying the im_ pact of using pre_trained language representations\n",
      "128 172 5 conceptnet experiments the conceptnet dataset8 , provided by li et al\n",
      "129 117 ( 2016 ) , consists of tuples obtained from the open mind common sense ( omcs ) entries in concept_ net_5 ( speer et al. , 2017 )\n",
      "130 64 tuples are in the stan_ dardsroform ( e.g. , take a nap , causes , have energy )\n",
      "131 58 the most condent 1200 tuples were used to create the test_set , while the next 1200 tuples were used to create two development sets , which we combine in this work\n",
      "132 139 the 100k version of the training_set was used to train models , which contains 34 relation_types\n",
      "133 151 5.1 setup metrics we evaluate our models that generate conceptnet relations using the following metrics\n",
      "134 32 first , we report the perplexity of the gold relations in the test_set ( ppl )\n",
      "135 31 to evaluate the quality of gen_ erated knowledge , we also report the number of generated positive examples in the test_set that are scored as correct by the pre_trained bilinear a vg model developed by li et al\n",
      "136 114 ( 2016 ) .9for a given srotuple , this model produces a probability for whether the tuple is correct\n",
      "137 11 we threshold scores at 50 % probability to identify positive predictions\n",
      "138 189 on the completion task originally proposed in li et al\n",
      "139 88 ( 2016 ) , this model achieved 92.5 % accuracy on the test_set , indicating that it is a strong proxy for automatically evaluating whether a generated tuple is correct\n",
      "140 111 finally , we report the same nov_ elty metrics as for a tomic : n/tsroandn/to\n",
      "141 34 baselines as a baseline , we re_implement the bilstm model proposed by saito et al\n",
      "142 41 ( 2018 ) with minor modications outlined in ap_ pendix a.2\n",
      "143 43 this model is trained to learn to en_ code knowledge in both directions : sroand 8https : //ttic.uchicago.edu/~kgimpel/ commonsense.html 9a pre_trained model can be found at https : //ttic.uchicago.edu/~kgimpel/comsense_ resources/ckbc_demo.tar.gzmodel ppl score n/t sro n/tohuman lstm _ s _ 60.83 86.25 7.83 63.86 ckbg ( saito et al. , 2018 ) _ 57.17 86.25 8.67 53.95 comet ( _ pretrain ) 8.05 89.25 36.17 6.00 83.49 comet _ reltok 4.39 95.17 56.42 2.62 92.11 comet 4.32 95.25 59.25 3.75 91.69 table 6 : conceptnet generation results orsto help augment a knowledge_base com_ pletion model\n",
      "144 68 it is only evaluated on the sro tuple generation_task , however\n",
      "145 175 for posterity , we also include the result from a lstm model that is only trained on the srotask ( lstm _ s )\n",
      "146 49 ablations we include the following ablations of our full model\n",
      "147 56 first , we evaluate how pre__training on a large_scale corpus ( radford et al. , 2018 ) helps performance by training a comparison model from scratch , denoted comet ( _ pretrain ) in table 6\n",
      "148 42 second , in our main model , we map relation names to natural_language ( e.g. , isa is a ; hassubeventhas subevent ) so the model can learn to represent these concepts with language , as opposed to learning a special embed__ding from scratch for each relation ( levy et al. , 2017 )\n",
      "149 2 as an ablation , we train a model with_ out converting relation tokens to natural_language ( e.g. , isais a ) , which we denote comet _ reltok\n",
      "150 190 5.2 results quality our results indicate that high_quality knowledge can be generated by the model : the low perplexity scores in table 6 indicate high model condence in its predictions , while the high clas_ sier score ( 95.25 % ) indicates that the kb com_ pletion model of li et al\n",
      "151 113 ( 2016 ) scores the gener_ ated tuples as correct in most of the cases\n",
      "152 18 while adversarial generations could be responsible for this high score , a human evaluation ( following the same design as for a tomic ) scores 91.7 % of greedily decoded tuples as correct\n",
      "153 171 randomly se_ lected examples provided in table 7 also point to the quality of knowledge produced by the model\n",
      "154 152 novelty in addition to being high quality , the generated tuples from comet are also novel , with 59.25 % of the tuples not being present in the train__ing set , showing that the model is capable of gen_ erating new edges between nodes , and even cre_ ating new nodes 3.75 % of onodes are novel to extend the size of the knowledge_graph\n",
      "155 193 one shortcoming , however , is that novel generations4769 classier accuracy0.000.250.500.751.00 % of tuples with edit_distance > = x0 % 25 % 50 % 75 % 100 % edit distance0.00.330.50.671.0 % of novel tuples accuracyfigure 4 : the percentage of novel conceptnet de_ velopment set tuples per minimum edit_distance from training tuples\n",
      "156 0 in green : classier_scored accuracy of each subset\n",
      "157 60 are sometimes simplied forms of tuples from the training_set\n",
      "158 91 in table 7 , for example , the tuple doctor capableof save life is not present in the training_set , but doctor capableof save person life is\n",
      "159 169 many tuples , however , are com_ pletely novel , such as bird bone hasproperty fragile and driftwood atlocation beach , which have no related tuples in the training_set\n",
      "160 119 to explore further , we investigate by how much novel tuples from the development_set differ from training_set phrase objects for the same s , rusing minimum edit_distance of phrase objects\n",
      "161 75 we mea_ sure the edit_distance of phrase object odevin the tuple ( s , r , o dev ) to the otrnfrom the nearest train__ing tuple ( s , r , o trn )\n",
      "162 126 edit_distance is measured us_ ing word tokens ( excluding stop words ) and nor_ malized by the maximum number of words in odev orotrn\n",
      "163 185 the maximum edit_distance is one ( i.e. , entirely different word sequences ) and the mini_ mum edit_distance is zero ( i.e. , the same sequence excluding stopwords )\n",
      "164 104 figure 4 shows the percent_ age of novel development_set tuples that have an edit_distance from the closest training_set tuple of at least the value on the x_axis\n",
      "165 108 over 75 % of the novel tuples have objects that are a normalized edit_distance of > = 0.5from the training phrase ob_ jects , indicating that most of the novel phrase ob_ jects have signicantly different word sequences from their closest analogues in the training_set\n",
      "166 78 learning knowledge from language simi_ larly to a tomic , we explore how pre_training comet on a large language corpus affects its ability to generalize commonsense\n",
      "167 95 this effect is apparent in table 6 , with a clear improve_ ment on automatic and human evaluations by the pretrained comet over the randomly initializedseed relation completion plausible piece partof machine bread isa food oldsmobile isa car happiness isa feel math isa subject mango isa fruit maine isa state planet atlocation space dust atlocation fridge puzzle atlocation your mind college atlocation town dental chair atlocation dentist nger atlocation your nger sing causes you feel good doctor capableof save life post ofce capableof receive letter dove symbolof purity sun hasproperty big_bird bone hasproperty fragile earth hasa many plant yard usedfor play game get pay hasprerequisite work print on printer hasprerequisite get printer play game hasprerequisite have game live haslastsubevent die swim hassubevent get wet sit down motivatedbygoal you be tire all paper receivesaction recycle chair madeof wood earth definedas planet table 7 : randomly selected and novel generations from the conceptnet development_set\n",
      "168 156 novel genera_ tions are srotuples not found in the training_set\n",
      "169 128 man_ ual evaluation of each tuple indicates whether the tuple is considered plausible by a human annotator model\n",
      "170 129 qualitatively , we observe this effect in ta_ ble 7 with the generated example tuple mango isa fruit '' , which is not present in the training_set\n",
      "171 93 the only tuple containing the mango '' entity in the training_set is mango usedfor salsa '' , which is not informative enough\n",
      "172 142 as conrmation , we observe that the output from comet ( _ pretrain ) is mango isa spice , which could be a reasonable inference given the information about mango '' in the seed set of knowledge\n",
      "173 135 representing relations with language while the automatic metrics point to insignicant differ_ ences when comparing models with symbol re_ lations and those with natural_language relations ( table 6 ) , examples can provide qualitative in_ sights into the benets of representing relations as language\n",
      "174 76 while the only non_ornithological ref_ erence to a dove '' in the conceptnet training_set is dove capableof y , our model learns to generalize to produce the tuple dove symbolof purity\n",
      "175 174 the model that uses symbol relation em_ beddings only manages to produce the relation dove symbolof submarine , which seems to relate submarine '' to a more nautical ( and unre_ lated ) word sense of dove '' .47706 related work knowledge_base construction previous work has looked at constructing knowledge_bases as re_ lational schemas using expert knowledge ( lenat , 1995 ; bodenreider , 2004 ; miller , 1995 ) , semi_ structured text extraction ( suchanek et al. , 2007 ; hoffart et al. , 2013 ; auer et al. , 2007 ; bol_ lacker et al. , 2008 ) and unstructured text extraction ( dong et al. , 2014 ; carlson et al. , 2010 ; nakashole et al. , 2011 , 2012 ; niu , 2012 )\n",
      "176 164 in our work , we fo_ cus on construction of commonsense knowledge_bases which require the use of open_text events rather than a well_dened relational schema struc__ture\n",
      "177 10 other work in information_extraction can also be applied to knowledge_base construction with open_text entities ( soderland et al. , 2010 ; et_ zioni et al. , 2011 ; fader et al. , 2011 ; mausam et al. , 2012 ; fan et al. , 2010 ; cui et al. , 2018 ) , but these methods typically extract explicitly stated text re_ lations\n",
      "178 196 conversely , our approach generates new knowledge that is often unstated in text , as com_ monsense information typically is ( gordon and van durme , 2013 )\n",
      "179 77 commonsense knowledge_base completion existing work on generation of novel common__sense knowledge has also used conceptnet and atomic as underlying kbs\n",
      "180 67 specically , li et al\n",
      "181 149 ( 2016 ) proposed a set of neural_network models for scoring tuples in conceptnet\n",
      "182 69 our work differs from this approach as their models evaluate full tu_ ples rather than learning to generate the phrases to make new nodes in the knowledge_graph\n",
      "183 84 saito et al\n",
      "184 163 ( 2018 ) builds upon this work by proposing a joint model for completion and generation of com_ monsense tuples\n",
      "185 123 their work , however , focuses on using tuple generation to augment their kb com_ pletion model , rather than to increase coverage in commonsense kb construction\n",
      "186 72 finally , sap et al\n",
      "187 178 ( 2019 ) use lstm encoder_decoder models to gen_ erate commonsense knowledge about social situa_ tions\n",
      "188 158 we use transformers and investigate the ef_ fect of using pre_trained language representations ( radford et al. , 2018 ) to initialize them\n",
      "189 112 transformers and pre_training finally , our work builds on previous work on adapting pre__trained language_models for various sequence la_ beling , classication , and nli end tasks ( rad_ ford et al. , 2018 ; peters et al. , 2018 ; devlin et al. , 2018 )\n",
      "190 74 our research investigates how pre_trained_language_models can be used for large_scale com_monsense kb construction by generating new graph nodes and edges between nodes\n",
      "191 195 7 conclusion we introduce commonsense transformers ( comet ) for automatic construction of common__sense knowledge_bases\n",
      "192 109 comet is a framework for adapting the weights of language_models to learn to produce novel and diverse common__sense knowledge tuples\n",
      "193 147 empirical results on two commonsense knowledge_bases , a tomic and conceptnet , show that comet frequently produces novel commonsense knowledge that human evaluators deem to be correct\n",
      "194 27 these positive results point to future work in extend_ ing the approach to a variety of other types of knowledge_bases , as well as investigating whether comet can learn to produce openie_style knowledge tuples for arbitrary knowledge seeds\n",
      "195 183 acknowledgments we thank thomas wolf , ari holtzman , chandra bhagavatula , peter clark , rob dalton , ronan le bras , rowan zellers and scott yih for helpful dis_ cussions over the course of this project , as well as the anonymous reviewers for their insightful com_ ments\n",
      "196 47 this research was supported in part by nsf ( iis_1524371 , iis_1714566 , nri_1525251 ) , darpa under the cwc program through the aro ( w911nf_15_1_0543 ) , and samsung research\n",
      "197 176 this material is based , in part , upon work sup_ ported by the national_science_foundation gradu_ ate research fellowship program under grant no\n",
      "198 81 dge_1256082\n"
     ]
    }
   ],
   "source": [
    "focus_paper = 28\n",
    "ordered_ranks = np.array([all_sent_ranks[focus_paper][i] for i in np.arange(len(all_sent_ranks[focus_paper]))])\n",
    "sent_rank = {}\n",
    "# for s_id, sent in enumerate(external_collection[focus_paper - len(collection)].sent_tokenize):\n",
    "for s_id, sent in enumerate(collection[focus_paper].sent_tokenize):\n",
    "    print(s_id, all_sent_ranks[focus_paper][s_id], sent)\n",
    "\n",
    "# print('title + abstract:', ordered_ranks[:13].mean())\n",
    "# print('introduction:', ordered_ranks[13:24].mean())\n",
    "# print('related works:', ordered_ranks[24:31].mean())\n",
    "# print('datasets:', ordered_ranks[31:43].mean())\n",
    "# print('methods:', ordered_ranks[43:67].mean())\n",
    "# print('results:', ordered_ranks[67:80].mean())\n",
    "# print('conclusion:', ordered_ranks[80:83].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_classes = {}\n",
    "gt_labels = []\n",
    "\n",
    "for idx, paper in enumerate(collection):\n",
    "    # core_classes[paper.id] = preds[idx]\n",
    "    gt_labels.append(list(map(int, paper.gold)))\n",
    "\n",
    "# for idx, paper in enumerate(external_collection):\n",
    "#     core_classes[paper.id] = preds[idx]\n",
    "\n",
    "if not os.path.exists(os.path.join(args.data_dir, 'train')):\n",
    "    os.makedirs(os.path.join(args.data_dir, 'train'))\n",
    "if not os.path.exists(os.path.join(args.data_dir, 'test')):\n",
    "    os.makedirs(os.path.join(args.data_dir, 'test'))\n",
    "\n",
    "# json.dump(core_classes, open(os.path.join(args.data_dir, 'train/refined_core_classes.json'), 'w'), indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_definitions import bert_model, bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epoch = 5\n",
    "args.batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts into tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving encoded texts into datasets/gen_kgc/ner_event_kgc/train/training_data.pt\n"
     ]
    }
   ],
   "source": [
    "doc2parents, doc2child = prepare_data(args, taxo, core_classes, corpus, bert_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/classifier_training.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_dict = torch.load(os.path.join(args.data_dir, 'train/training_data.pt'))\n",
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/classifier_training.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_emb = torch.load(os.path.join(args.data_dir, 'class_emb.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42 [00:00<?, ?it/s]/home/pk36/Comparative-Summarization/taxoadapt/./lbm/model.py:132: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "100%|██████████| 42/42 [00:48<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 82.41540959986244\n",
      "Training epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:49<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 20.174095572494878\n",
      "Training epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:49<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 14.28550218954319\n",
      "Training epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:49<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 8.051641978141738\n",
      "Training epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:49<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 6.948110898093479\n"
     ]
    }
   ],
   "source": [
    "train_classifier(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/eval_classifier.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_emb = torch.load(os.path.join(args.data_dir, 'class_emb.pt'))\n",
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/eval_classifier.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_model.load_state_dict(torch.load(model_pth, map_location='cuda'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts into tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.8235294117647058\n",
      "Precision@2: 0.8676470588235294\n",
      "Precision@3: 0.843137254901961\n",
      "MRR: 0.3542592592592593\n",
      "Example F1 (TOP 3): 0.8901960784313725\n",
      "Example F1 (SELECTED): 0.865266106442577\n",
      "Example F1 (ALL): 0.6869747899159663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "predictions, top_classes, all_classes, selected_classes = evaluate_cls(args, collection, bert_tokenizer, os.path.join(args.data_dir, f'train/model.pt'), gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/eval_classifier.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_emb = torch.load(os.path.join(args.data_dir, 'class_emb.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts into tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/eval_classifier.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_model.load_state_dict(torch.load(model_pth, map_location='cuda'))\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 1.0\n",
      "Precision@2: 1.0\n",
      "Precision@3: 0.9215686274509803\n",
      "MRR: 0.3379889455782313\n",
      "Example F1 (TOP 3): 0.890406162464986\n",
      "Example F1 (SELECTED): 0.8169340463458111\n",
      "Example F1 (ALL): 0.6355614973262034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "predictions, top_classes, all_classes, selected_classes = evaluate_cls(args, collection, bert_tokenizer, os.path.join(args.data_dir, f'train/model.pt'), gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Macro Score: 0.5488696488696488\n",
      "F1-Micro Score: 0.6355140186915887\n"
     ]
    }
   ],
   "source": [
    "f1_scores(gt_labels, top_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/eval_classifier.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_emb = torch.load(os.path.join(args.data_dir, 'class_emb.pt'))\n",
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/eval_classifier.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_model.load_state_dict(torch.load(model_pth, map_location='cuda'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts into tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 1.0\n",
      "Precision@2: 1.0\n",
      "Precision@3: 0.823529411764706\n",
      "MRR: 0.3535471331389699\n",
      "Example F1 (TOP 3): 0.8394957983193276\n",
      "Example F1 (SELECTED): 0.8745098039215686\n",
      "Example F1 (ALL): 0.6431372549019608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "predictions, top_classes, all_classes, selected_classes = evaluate_cls(args, collection, bert_tokenizer, os.path.join(args.data_dir, f'train/model.pt'), gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts into tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 1.0\n",
      "Precision@2: 0.9705882352941176\n",
      "Precision@3: 0.8529411764705884\n",
      "MRR: 0.35604956268221577\n",
      "Example F1 (TOP 3): 0.8675070028011204\n",
      "Example F1 (SELECTED): 0.8389589169000933\n",
      "Example F1 (ALL): 0.6431372549019608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# rRECENT\n",
    "predictions, top_classes, all_classes, selected_classes = evaluate_cls(args, collection, bert_tokenizer, os.path.join(args.data_dir, f'train/model.pt'), gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/eval_classifier.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_emb = torch.load(os.path.join(args.data_dir, 'class_emb.pt'))\n",
      "/home/pk36/Comparative-Summarization/taxoadapt/./lbm/eval_classifier.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  class_model.load_state_dict(torch.load(model_pth, map_location='cuda:2'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts into tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 1.0\n",
      "Precision@2: 1.0\n",
      "Precision@3: 0.7941176470588235\n",
      "MRR: 0.3519193391642372\n",
      "Example F1 (TOP 3): 0.8128851540616245\n",
      "Example F1 (SELECTED): 0.6431372549019608\n",
      "Example F1 (ALL): 0.807282913165266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, top_classes, all_classes, selected_classes = evaluate_cls(args, collection, bert_tokenizer, os.path.join(args.data_dir, f'train/model.pt'), gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [0, 1, 2, 4]; gt: ['0', '1', '2']; weights: [10.774901  11.105757   7.7327485  4.756489 ]\n",
      "preds: [0, 1, 2]; gt: ['0', '1', '2']; weights: [12.038707 11.597884 10.183065]\n",
      "preds: [0, 1, 2, 3, 4]; gt: ['0', '1', '2', '3']; weights: [9.584637  9.684874  7.575132  5.1706114 6.488943 ]\n",
      "preds: [0, 1]; gt: ['0', '1', '2', '3']; weights: [9.953376  9.8585205]\n",
      "preds: [0, 1, 2, 4]; gt: ['0', '1', '2']; weights: [10.590563 11.031084  8.630755  5.828255]\n",
      "preds: [0, 1, 2]; gt: ['0', '1', '2', '3']; weights: [11.434895 11.057161  8.91286 ]\n",
      "preds: [0, 1, 2]; gt: ['0', '1', '2']; weights: [11.662855 11.989753 10.723114]\n",
      "preds: [0, 1, 2]; gt: ['0', '1', '2', '3']; weights: [10.98599  10.875311  9.590318]\n",
      "preds: [0, 1, 3]; gt: ['0', '1', '3']; weights: [10.4102   10.507949  6.939367]\n",
      "preds: [0, 1, 3]; gt: ['0', '1', '3']; weights: [10.551313 11.315516 12.889475]\n",
      "preds: [0, 1, 3]; gt: ['0', '1', '3']; weights: [ 9.139808  9.618616 11.839673]\n",
      "preds: [0, 1]; gt: ['0', '1', '3']; weights: [11.577571 11.632612]\n",
      "preds: [0, 1, 2]; gt: ['0', '1', '3']; weights: [12.961905 12.599291  8.752327]\n",
      "preds: [0, 1, 2]; gt: ['0', '1', '3']; weights: [13.0732975 12.572938   7.7978406]\n",
      "preds: [0, 1, 2, 3, 4]; gt: ['0', '1', '3']; weights: [9.496619  9.779821  6.287308  6.3110785 8.143707 ]\n",
      "preds: [0, 1, 3]; gt: ['0', '1', '3']; weights: [ 7.4411187  8.1978445 10.599615 ]\n",
      "preds: [0, 1]; gt: ['0', '1', '3']; weights: [10.139517 10.176433]\n",
      "preds: [0, 1]; gt: ['0', '1']; weights: [10.609672  10.9193535]\n",
      "preds: [0, 1, 2]; gt: ['0', '1']; weights: [12.174138 11.862522 10.447066]\n",
      "preds: [0, 1, 4]; gt: ['0', '1']; weights: [10.360387 10.593278 13.611156]\n",
      "preds: [0, 1, 2]; gt: ['0', '1']; weights: [10.97698  10.47447   7.669224]\n",
      "preds: [0, 1, 2]; gt: ['0', '1']; weights: [11.49986   11.4718075  6.452901 ]\n",
      "preds: [0, 1, 2]; gt: ['0', '1', '5']; weights: [12.064374  11.289147   8.9240885]\n",
      "preds: [0, 1, 2]; gt: ['0', '1']; weights: [12.342972 12.111267 10.23003 ]\n",
      "preds: [0, 1, 2]; gt: ['0', '1']; weights: [11.023628 10.829454  9.725907]\n",
      "preds: [0, 1, 2]; gt: ['0', '1']; weights: [12.844022 12.309454  9.386454]\n",
      "preds: [0, 1, 4]; gt: ['0', '1', '4']; weights: [10.533223 10.786087 11.022344]\n",
      "preds: [0, 1, 4]; gt: ['0', '1', '4']; weights: [11.04044  10.951839 13.468451]\n",
      "preds: [0, 1, 2, 5]; gt: ['0', '1', '5']; weights: [12.004909 10.439292  7.430411 11.406679]\n",
      "preds: [0, 1, 2, 5]; gt: ['0', '1', '5']; weights: [13.325089  11.46791    6.5418487 10.900388 ]\n",
      "preds: [0, 1, 5]; gt: ['0', '1', '5']; weights: [14.445733 11.595956 16.337286]\n",
      "preds: [0, 1, 2, 5]; gt: ['0', '1', '5']; weights: [14.82181  12.969658  6.43172   3.206631]\n",
      "preds: [0, 1, 5]; gt: ['0', '1', '5']; weights: [13.553878  10.671056  15.3738785]\n",
      "preds: [0, 1, 5]; gt: ['0', '1', '5']; weights: [14.793738 11.953671 14.700975]\n"
     ]
    }
   ],
   "source": [
    "for p in zip(gt, selected_classes, predictions):\n",
    "    print(f'preds: {sorted(p[1])}; gt: {p[0]}; weights: {p[2][sorted(p[1])]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering of Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "from hdbscan.flat import (HDBSCAN_flat,\n",
    "                          approximate_predict_flat,\n",
    "                          membership_vector_flat,\n",
    "                          all_points_membership_vectors_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embs = np.array(document_embs)\n",
    "selected_document_embs = document_embs[no_label_papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN(min_cluster_size=2)\n",
    "clusterer.fit(document_embs[:len(collection)])\n",
    "cluster_labels = clusterer.labels_\n",
    "proba = clusterer.probabilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [] 0\n",
      "1 [] 0\n",
      "2 ['2'] 0\n",
      "3 ['2'] 0\n",
      "4 [] 0\n",
      "5 ['2'] 0\n",
      "6 [] -1\n",
      "7 ['2'] -1\n",
      "8 ['2'] 0\n",
      "9 ['2'] 0\n",
      "10 ['2'] 0\n",
      "11 ['2'] 0\n",
      "12 ['2'] 0\n",
      "13 ['2'] 0\n",
      "14 ['2'] 0\n",
      "15 ['2'] 0\n",
      "16 ['2'] 0\n",
      "17 [] -1\n",
      "18 [] 0\n",
      "19 [] 0\n",
      "20 [] 0\n",
      "21 [] 0\n",
      "22 ['4'] 0\n",
      "23 [] 0\n",
      "24 [] 0\n",
      "25 [] 0\n",
      "26 ['3'] 0\n",
      "27 ['3'] -1\n",
      "28 ['4'] 0\n",
      "29 ['4'] 1\n",
      "30 ['4'] 1\n",
      "31 ['4'] 0\n",
      "32 ['4'] 1\n",
      "33 ['4'] 1\n"
     ]
    }
   ],
   "source": [
    "for i, (g, c) in enumerate(zip(taxo.gold_labels, cluster_labels)):\n",
    "    print(i, g[2:], c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN_flat(document_embs,\n",
    "                         cluster_selection_method='eom',\n",
    "                         n_clusters=4, min_cluster_size=2)\n",
    "cluster_labels = clusterer.labels_\n",
    "proba = clusterer.probabilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'generative_knowledge_graph_construction',\n",
       " '1': 'generation_tasks',\n",
       " '2': 'named_entity_recognition',\n",
       " '3': 'relation_extraction',\n",
       " '4': 'entity_linking',\n",
       " '5': 'knowledge_graph_completion'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psuedo_labels = {} # node_id: paper\n",
    "thresh = 0.5\n",
    "\n",
    "queue = deque([taxo.root])\n",
    "\n",
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    class_embs = []\n",
    "    for child in tqdm(curr_node.children):\n",
    "        child.emb['sentence'] = average_with_harmonic_series(np.stack([taxo.vocab['sentences'][w] \n",
    "                                        for w in child.getAllTerms(granularity='sentences', children=False)], axis=0), axis=0)\n",
    "        class_embs.append(child.emb['sentence'])\n",
    "        child.papers = {}\n",
    "        child.paper_scores = {}\n",
    "        child.ranked = []\n",
    "    \n",
    "    if len(class_embs):\n",
    "        # rank sentences based on semantic sim\n",
    "        # compute paper embedding based discriminative sentence weights\n",
    "        for paper in collection:\n",
    "            sent_reprs, sent_avg_ranks, sent_dis_ranks = paper.rankSentences(class_embs, phrases=False)\n",
    "            sent_to_class = cosine_similarity_embeddings(sent_reprs, class_embs).argmax(axis=1)\n",
    "            weights = weights_from_ranking([sent_dis_ranks])\n",
    "\n",
    "            class_weights = np.bincount(sent_to_class, weights=weights, minlength=len(class_embs))\n",
    "            \n",
    "            \n",
    "\n",
    "        # take top 50% of assigned papers as pseudo-labels\n",
    "        for child in curr_node.children:\n",
    "            print(len(child.ranked))\n",
    "            num_papers = int(len(child.ranked) * thresh)\n",
    "            psuedo_labels[child.node_id] = [collection[p_id] for p_id in child.ranked[:num_papers]]\n",
    "\n",
    "            queue.append(child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 43.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 81.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "4\n",
      "11\n",
      "1\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "psuedo_labels = {} # node_id: paper\n",
    "thresh = 0.5\n",
    "\n",
    "queue = deque([taxo.root])\n",
    "\n",
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    class_embs = []\n",
    "    for child in tqdm(curr_node.children):\n",
    "        child.emb['sentence'] = average_with_harmonic_series(np.stack([taxo.vocab['sentences'][w] \n",
    "                                        for w in child.getAllTerms(granularity='sentences', children=False)], axis=0), axis=0)\n",
    "        class_embs.append(child.emb['sentence'])\n",
    "        child.papers = {}\n",
    "        child.paper_scores = {}\n",
    "        child.ranked = []\n",
    "    \n",
    "    if len(class_embs):\n",
    "        # rank sentences based on semantic sim\n",
    "        # compute paper embedding based discriminative sentence weights\n",
    "        paper_embs = np.array([paper.computePaperEmb(class_embs, phrase=False) for paper in collection])\n",
    "\n",
    "        # for each node, assign papers based on similarity gap\n",
    "        class_labels, class_paper_scores, mapping = taxo.mapPapers(collection, paper_embs, curr_node.children, class_embs)\n",
    "\n",
    "        # take top 50% of assigned papers as pseudo-labels\n",
    "        for child in curr_node.children:\n",
    "            print(len(child.ranked))\n",
    "            num_papers = int(len(child.ranked) * thresh)\n",
    "            psuedo_labels[child.node_id] = [collection[p_id] for p_id in child.ranked[:num_papers]]\n",
    "\n",
    "            queue.append(child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/gen_kgc/'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '6', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2', '3'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '6', '4', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2', '3'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '4'],\n",
       " ['0', '1', '4'],\n",
       " ['0', '1', '2', '4'],\n",
       " ['0', '1', '4'],\n",
       " ['0', '1', '4'],\n",
       " ['0', '1', '4'],\n",
       " ['0', '1', '4'],\n",
       " ['0', '1', '2', '4'],\n",
       " ['0', '1', '2', '4'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '5'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '6'],\n",
       " ['0', '1', '6'],\n",
       " ['0', '1', '6'],\n",
       " ['0', '1', '2'],\n",
       " ['0', '1', '2', '6']]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Macro Score: 0.6974358974358974\n",
      "F1-Micro Score: 0.8340807174887893\n"
     ]
    }
   ],
   "source": [
    "f1_scores(gt, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 0]), 4)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(class_weights), np.argmax(np.diff(np.sort(class_weights))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 ['0', '1', '2']\n",
      "1 0 ['0', '1', '2']\n",
      "2 0 ['0', '1', '2', '3', '4']\n",
      "3 0 ['0', '1', '2', '3']\n",
      "4 0 ['0', '1', '2']\n",
      "5 0 ['0', '1', '2', '3', '4']\n",
      "6 0 ['0', '1', '2']\n",
      "7 0 ['0', '1', '2', '3', '4']\n",
      "8 0 ['0', '1', '3']\n",
      "9 1 ['0', '1', '3']\n",
      "10 0 ['0', '1', '3']\n",
      "11 0 ['0', '1', '3']\n",
      "12 0 ['0', '1', '3', '4']\n",
      "13 0 ['0', '1', '3', '4']\n",
      "14 0 ['0', '1', '3']\n",
      "15 1 ['0', '1', '3']\n",
      "16 0 ['0', '1', '3']\n",
      "17 2 ['0', '1', '4']\n",
      "18 2 ['0', '1', '4']\n",
      "19 2 ['0', '1', '4']\n",
      "20 2 ['0', '1', '4']\n",
      "21 2 ['0', '1', '4']\n",
      "22 2 ['0', '1', '4', '6']\n",
      "23 2 ['0', '1', '4']\n",
      "24 2 ['0', '1', '4']\n",
      "25 2 ['0', '1', '4']\n",
      "26 0 ['0', '1', '5']\n",
      "27 3 ['0', '1', '5']\n",
      "28 0 ['0', '1', '6']\n",
      "29 4 ['0', '1', '6']\n",
      "30 4 ['0', '1', '6']\n",
      "31 4 ['0', '1', '6']\n",
      "32 0 ['0', '1', '6']\n",
      "33 4 ['0', '1', '6']\n"
     ]
    }
   ],
   "source": [
    "for id, label in enumerate(cosine_similarity_embeddings(paper_embs, class_embs).argmax(axis=1)):\n",
    "    print(id, label, collection[id].gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generative_knowledge_graph_construction': '0',\n",
       " 'generation_tasks': '1',\n",
       " 'named_entity_recognition': '2',\n",
       " 'relation_extraction': '3',\n",
       " 'event_extraction': '4',\n",
       " 'entity_linking': '5',\n",
       " 'knowledge_graph_completion': '6'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection[0].gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "2 [False, True, False, True, False, False, True, False, False, True]\n",
      "3 [True, True]\n",
      "4 [True, True, True, True, True]\n",
      "5 []\n",
      "6 [True, True, True]\n"
     ]
    }
   ],
   "source": [
    "for k, v in psuedo_labels.items():\n",
    "    print(k, [k in p.gold for p in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankSentences(taxo, sents, sent_phrases, phrase_embs, sent_embs, term_to_idx, bm_score, thresh=3, min_freq=3, percentile=0.999, classify=True):\n",
    "    \n",
    "    node_text_ranks = []\n",
    "\n",
    "    for node_id in tqdm(np.arange(0, len(taxo.label2id))):\n",
    "        # gather node and its siblings\n",
    "        focus_node = taxo.root.findChild(str(node_id))\n",
    "        sibling_nodes = taxo.get_sib(focus_node.node_id, granularity='emb')\n",
    "\n",
    "        # get phrases of node and its siblings\n",
    "        focus_phrase = focus_node.getAllTerms(granularity='phrases', children=False)\n",
    "        focus_phrase_embs = np.stack([taxo.vocab['phrases'][w] for w in focus_phrase], axis=0) # P x dim\n",
    "        sibling_phrase = [sib.getAllTerms(granularity='phrases', children=False) for sib in sibling_nodes] # list of list of sibling node terms\n",
    "        sib_phrase_embs = [np.stack([taxo.vocab['phrases'][p] for p in t], axis=0) for t in sibling_phrase] # [(T x dim), ... ()] \n",
    "        # get sentences of node and its siblings\n",
    "        focus_sent = focus_node.getAllTerms(granularity='sentences', children=False)\n",
    "        focus_sent_embs = np.stack([taxo.vocab['sentences'][s] for s in focus_sent], axis=0) # sentences x dim\n",
    "        sibling_sent = [sib.getAllTerms(granularity='sentences', children=False) for sib in sibling_nodes] # list [(list of sibling node i terms), ...]\n",
    "        sib_sent_embs = [np.stack([taxo.vocab['sentences'][s] for s in t], axis=0) for t in sibling_sent] # list [(# of sib sents x dim)]\n",
    "\n",
    "        # compute target phrase/sentence semantic similarity\n",
    "        focus_phrase_sim = np.stack([cosine_similarity_embeddings(s, focus_phrase_embs).max(axis=0) for s in phrase_embs], axis=0) # S x [P x N] -> S x N\n",
    "        avg_focus_phrase_sim = average_with_harmonic_series(focus_phrase_sim, axis=1)  # S x 1\n",
    "\n",
    "        focus_sent_sim = cosine_similarity_embeddings(sent_embs, focus_sent_embs) # S x N\n",
    "        avg_focus_sent_sim = average_with_harmonic_series(focus_sent_sim, axis=1)  # S x 1\n",
    "\n",
    "        # compute sibling sentence semantic dissimilarity\n",
    "        sib_phrase_sims = [np.stack([cosine_similarity_embeddings(sent_phrase_emb, s_emb).max(axis=0) for sent_phrase_emb in phrase_embs], axis=0) for s_emb in sib_phrase_embs] # siblings x sentences x P x N -> sib x sentences x N\n",
    "        sib_sent_sims = [cosine_similarity_embeddings(sent_embs, s_emb) for s_emb in sib_sent_embs] # siblings x sentences x sib_sents\n",
    "        if len(sibling_nodes):\n",
    "            avg_sib_phrase_sim = np.stack([average_with_harmonic_series(sib_sim, axis=1) for sib_sim in sib_phrase_sims], axis=-1).max(axis=1) # sentences x 1\n",
    "            avg_sib_sent_sim = np.stack([average_with_harmonic_series(sib_sim, axis=1) for sib_sim in sib_sent_sims], axis=-1).max(axis=1) # sentences x 1\n",
    "        else:\n",
    "            avg_sib_phrase_sim = np.zeros_like(avg_focus_phrase_sim)\n",
    "            avg_sib_sent_sim = np.zeros_like(avg_focus_sent_sim)\n",
    "\n",
    "        # compute semantic rank\n",
    "        target_sim_phrase_rank = {idx:rank for rank, idx in enumerate((avg_focus_phrase_sim-avg_sib_phrase_sim).argsort()[::-1])}\n",
    "        target_sim_sent_rank = {idx:rank for rank, idx in enumerate((avg_focus_sent_sim-avg_sib_sent_sim).argsort()[::-1])}\n",
    "\n",
    "        # compute target co-occurrence\n",
    "        target_co_occurrence = np.array([average_with_harmonic_series(getBM25(sent, focus_phrase, term_to_idx, bm_score).mean(axis=0)) for sent in sent_phrases]) # S x 1\n",
    "        \n",
    "        # compute sibling co-occurrence\n",
    "        if len(sibling_nodes):\n",
    "            sib_co_occurrence = np.array([max([average_with_harmonic_series(getBM25(sent, sib_terms, term_to_idx, bm_score).mean(axis=0)) for sib_terms in sibling_phrase]) for sent in sent_phrases]) # S x 1\n",
    "        else:\n",
    "            sib_co_occurrence = np.zeros_like(target_co_occurrence)\n",
    "        \n",
    "        # compute co-occurrence rank\n",
    "        target_co_rank = {idx:rank for rank, idx in enumerate((target_co_occurrence-sib_co_occurrence).argsort()[::-1])}\n",
    "\n",
    "        joint_rank = compute_joint_ranking([target_sim_phrase_rank, target_sim_sent_rank, target_co_rank]) # idx: rank\n",
    "        sorted_ranks = sorted(joint_rank.items(), key=lambda x: x[1])\n",
    "\n",
    "        final_ranks = {}\n",
    "        for idx, rank in sorted_ranks:\n",
    "            # if rank > (1-percentile)*len(sents):\n",
    "            #     break\n",
    "            if (len(sent_phrases[idx]) > 5): # and (avg_focus_phrase_sim[idx] > avg_sib_phrase_sim[idx]) and (avg_focus_sent_sim[idx] > avg_sib_sent_sim[idx]) and (target_co_occurrence[idx] > sib_co_occurrence[idx]):\n",
    "                final_ranks[rank] = (sents[idx], avg_focus_phrase_sim[idx], avg_focus_sent_sim[idx], target_co_occurrence[idx])\n",
    "                # focus_node.internal['sentences'].append(text[idx])\n",
    "        \n",
    "        node_text_ranks.append(final_ranks)\n",
    "\n",
    "    return node_text_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(event_extraction,\n",
       " ['event_level',\n",
       "  'event_',\n",
       "  'event_specific',\n",
       "  'event_type',\n",
       "  'event_role',\n",
       "  'event_generation',\n",
       "  'event_types',\n",
       "  'event_extraction',\n",
       "  'event_arguments',\n",
       "  'multi_event',\n",
       "  'event_detection',\n",
       "  'event_argument_extraction',\n",
       "  'event_record',\n",
       "  'event_related',\n",
       "  'event_masked',\n",
       "  'event_records',\n",
       "  'event_relevant',\n",
       "  'event_correlation'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_id = 4\n",
    "taxo.root.findChild(str(node_id)), taxo.root.findChild(str(node_id)).external['phrases']\n",
    "# taxo.root.findChild(str(node_id)).getAllTerms(granularity='phrases', children=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])\n",
    "taxo.root.papers = {paper.id:paper for paper in collection}\n",
    "\n",
    "while queue:\n",
    "    focus_node = queue.popleft()\n",
    "    sibling_nodes = taxo.get_sib(focus_node.node_id, granularity='nodes')\n",
    "\n",
    "    # get phrases of node and its siblings\n",
    "    focus_phrase = focus_node.getAllTerms(granularity='phrases', children=False)\n",
    "    focus_phrase_embs = np.stack([taxo.vocab['phrases'][w] for w in focus_phrase], axis=0) # P x dim\n",
    "    sibling_phrase = [sib.getAllTerms(granularity='phrases', children=False) for sib in sibling_nodes] # list of list of sibling node terms\n",
    "    sib_phrase_embs = [np.stack([taxo.vocab['phrases'][p] for p in t], axis=0) for t in sibling_phrase] # [(T x dim), ... ()]\n",
    "    \n",
    "    # get sentences of node and its siblings\n",
    "    focus_sent = focus_node.getAllTerms(granularity='sentences', children=False)\n",
    "    focus_sent_embs = np.stack([taxo.vocab['sentences'][s] for s in focus_sent], axis=0) # sentences x dim\n",
    "    sibling_sent = [sib.getAllTerms(granularity='sentences', children=False) for sib in sibling_nodes] # list [(list of sibling node i terms), ...]\n",
    "    sib_sent_embs = [np.stack([taxo.vocab['sentences'][s] for s in t], axis=0) for t in sibling_sent] # list [(# of sib sents x dim)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in tqdm(collection):\n",
    "    # for each sentence in paper\n",
    "    ## weigh each phrase in sentence based on how discriminative it is\n",
    "    for sent in paper.phrase_tokenize:\n",
    "        \n",
    "        phrase_sim = cosine_similarity_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: ['0', '1', '2']; gt: ['0', '1', '2']\n",
      "preds: ['0', '1', '2']; gt: ['0', '1', '2']\n",
      "preds: ['0', '1', '2', '3', '4']; gt: ['0', '1', '2', '3', '4']\n",
      "preds: ['0', '1', '2', '3', '4']; gt: ['0', '1', '2', '3']\n",
      "preds: ['0', '1', '2']; gt: ['0', '1', '2']\n",
      "preds: ['0', '1', '2', '3', '4']; gt: ['0', '1', '2', '3', '4']\n",
      "preds: ['0', '1', '2']; gt: ['0', '1', '2']\n",
      "preds: ['0', '1', '3', '4']; gt: ['0', '1', '2', '3', '4']\n",
      "preds: ['0', '1', '3', '4']; gt: ['0', '1', '3']\n",
      "preds: ['0', '1', '3']; gt: ['0', '1', '3']\n",
      "preds: ['0', '1', '2', '3']; gt: ['0', '1', '3']\n",
      "preds: ['0', '1', '3']; gt: ['0', '1', '3']\n",
      "preds: ['0', '1', '2', '3', '4']; gt: ['0', '1', '3', '4']\n",
      "preds: ['0', '1', '2', '3']; gt: ['0', '1', '3', '4']\n",
      "preds: ['0', '1', '2', '3', '4']; gt: ['0', '1', '3']\n",
      "preds: ['0', '1', '2', '3']; gt: ['0', '1', '3']\n",
      "preds: ['0', '1', '2', '3']; gt: ['0', '1', '3']\n",
      "preds: ['0', '1', '3', '4']; gt: ['0', '1', '4']\n",
      "preds: ['0', '1', '4']; gt: ['0', '1', '4']\n",
      "preds: ['0', '1', '4']; gt: ['0', '1', '4']\n",
      "preds: ['0', '1', '3', '4']; gt: ['0', '1', '4']\n",
      "preds: ['0', '1', '4']; gt: ['0', '1', '4']\n",
      "preds: ['0', '1', '3', '4']; gt: ['0', '1', '4', '6']\n",
      "preds: ['0', '1', '4']; gt: ['0', '1', '4']\n",
      "preds: ['0', '1', '2', '3', '4']; gt: ['0', '1', '4']\n",
      "preds: ['0', '1', '2', '3', '4', '6']; gt: ['0', '1', '4']\n",
      "preds: ['0', '1', '2', '3', '5']; gt: ['0', '1', '5']\n",
      "preds: ['0', '1', '2']; gt: ['0', '1', '5']\n",
      "preds: ['0', '1']; gt: ['0', '1', '6']\n",
      "preds: ['0', '1', '3']; gt: ['0', '1', '6']\n",
      "preds: ['0', '1', '3']; gt: ['0', '1', '6']\n",
      "preds: ['0', '1']; gt: ['0', '1', '6']\n",
      "preds: ['0', '1', '2', '3', '6']; gt: ['0', '1', '6']\n",
      "preds: ['0', '1', '2', '3', '6']; gt: ['0', '1', '6']\n"
     ]
    }
   ],
   "source": [
    "for p in zip(gt, preds):\n",
    "    print(f'preds: {sorted(p[1])}; gt: {sorted(p[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_pool = internal_sentences\n",
    "# sentence_pool_emb = np.array([taxo.vocab['sentences'][s] for s in sentence_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:28<00:00, 21.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# node_external_sent_ranks = expandDiscriminative(taxo, sentence_pool, sentence_pool_emb, granularity='sentences', classify=False, internal=-1)\n",
    "node_internal_sent_ranks = rankSentences(taxo, sentence_pool, sentence_phrase_pool, phrase_pool_emb, sentence_pool_emb, term_to_idx, bm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generative_knowledge_graph_construction': '0',\n",
       " 'generation_tasks': '1',\n",
       " 'named_entity_recognition': '2',\n",
       " 'relation_extraction': '3',\n",
       " 'event_extraction': '4',\n",
       " 'entity_linking': '5',\n",
       " 'knowledge_graph_completion': '6'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ('[ 8 ] propose a dynamic multi_pooling convolutional neural_network ( dm_ cnn ) , which uses a dynamic multi_pooling layer according to event triggers and arguments for event_extraction', 0.913306785813715, 0.7770894100358857, 0.0031051727799206123)\n",
      "3 ('the argument part is specic to event_type ei', 0.914617693454433, 0.7869675802903486, 0.002727315162633439)\n",
      "5 ('the second group contains generation_based event_extraction methods', 0.9080140282650968, 0.8621444665243786, 0.0024849446481576497)\n",
      "6 ('paper_title : text2event : controllable sequence_to_structure generation for end_to_end event_extraction ; paper_abstract : event_extraction is challenging due to the complex structure of event_records and the semantic gap between text and event', 0.9132553905853479, 0.8347001638762626, 0.0025721657225129344)\n",
      "7 (', text2event : controllable sequence_to_structure generation for end_to_end event_extraction , in proc', 0.9081088541185993, 0.814699993799282, 0.0028579141235849856)\n",
      "8 ('2 t ext2event : end_to_end event_extraction as controllable generation given the token sequence x=x1 , ... , x|x|of the input text , text2event directly generate the event structures e=e1 , ... , e|e|via an encoder_ decoder architecture', 0.9095463804400866, 0.8000682296750994, 0.0021511240786456207)\n",
      "9 ('text2event : controllable sequence_to_ structure generation for end_to_end event_extraction', 0.9081088541185993, 0.8188087881184775, 0.002124171519638873)\n",
      "11 ('degree : a data_efcient generative event_extraction model', 0.907955224561632, 0.8036124302255253, 0.0031794988976541192)\n",
      "13 ('2 method the event_extraction task consists of two subtasks : trigger extraction and argument_extraction', 0.9239054852200754, 0.8567668091893891, 0.0031419418719634148)\n",
      "15 ('event_extraction via dynamic multi_ pooling convolutional neural_networks', 0.9077871849216517, 0.7722936247022086, 0.001926571269909672)\n",
      "16 ('[ 59 ] propose a document_level neural event_argument_extraction model by formulating the task as conditional_generation following event templates', 0.902240807740788, 0.8131498619803018, 0.0028884839442398762)\n",
      "17 ('finally , degree is designed for end_to_end event_extraction and can solve event_detection and event_argument_extraction at the same time', 0.9290012679669113, 0.8373583991294018, 0.0033552936461578703)\n",
      "18 ('3.we release the rst end_to_end zero_shot event_extraction framework by combining our argu__ment extraction model with a zero_shot event trigger classication model', 0.9105894934629265, 0.7919984403914383, 0.003185213019733041)\n",
      "20 ('event_extraction most of the event_extraction works focus on ace sentence level event task [ 44 ] , which rstly detects the event triggers and then extracts arguments from a single sentence', 0.913306785813715, 0.8732341935585839, 0.0034739087583764024)\n",
      "21 ('output text event trigger is detonated', 0.8709893970355488, 0.7387311134036683, 0.0032529783799901553)\n",
      "22 ('2.1 event_extraction as structure generation this section describes how to linearize event struc_ ture so that events can be generated in an end_to_ end manner', 0.9098821985850989, 0.8436178036004834, 0.0029523419116738933)\n",
      "23 ('we propose a document_level neural event_argument_extraction model by formulating the task as conditional_generation following event templates', 0.902240807740788, 0.8049274674277209, 0.0035417553194172827)\n",
      "25 ('speci_ cally , instead of decomposing event structure pre__diction into different subtasks and predicting la_ bels , we uniformly model the whole event extrac__tion process in a neural network_based sequence_to_ structure architecture , and all triggers , arguments , and their labels are universally generated as natural_language words', 0.8791272083345907, 0.8320189076877678, 0.0028282042441546093)\n",
      "27 ('by pairing up our argument_extraction model with a keyword_based zero_shot trigger extraction model , we enable zero_shot transfer for new event_types', 0.919364962784233, 0.7847646097797089, 0.0033398412604288877)\n",
      "29 ('we use the pattern event_type is [ mask ]', 0.9018607935905151, 0.7769493729162791, 0.0027923661360725474)\n",
      "30 ('[ 59 ] propose a document_level neural event_argument_extraction model by formulating the task as conditional genera__tion following event templates', 0.902240807740788, 0.814710567555055, 0.002755220051514587)\n",
      "31 ('in summary , the contri_ butions are as follows : 1.we propose a new paradigm for event ex__traction _ sequence_to_structure generation , which can directly extract events from the text in an end_to_end manner', 0.8717532293238106, 0.8519612068633144, 0.003085993481838518)\n",
      "32 ('only arguments for the bold_ faced event triggers are shown', 0.8785698712025463, 0.7844492968804241, 0.003094041324951503)\n",
      "33 ('dmcnn is an dynamic multi_pooling convolutional neural_network for event_extraction [ 8 ]', 0.9081088541185993, 0.7654555606060341, 0.002754870432284822)\n",
      "34 ('to achieve this , we propose three novel event_centric objectives , i.e. , whole event recovering , contrastive event_ correlation encoding and prompt_based_event_locating , which highlight event_level correla_ tions with effective training', 0.9012215011670016, 0.7901327220377051, 0.0028250715478275344)\n",
      "35 ('cleve : contrastive pre_training for event_extraction', 0.9081088541185993, 0.8113967418799862, 0.0024990448464484215)\n",
      "36 ('the rst group is about classication_based event_extraction methods', 0.9079404809679008, 0.8697441387525371, 0.003049686878428781)\n",
      "37 ('we propose a document_level neu_ ral event_argument_extraction model by for_ mulating the task as conditional_generation following event templates', 0.902240807740788, 0.8343302140373506, 0.0030716267250997264)\n",
      "38 ('6 conclusion & future work in this paper , we advocate document_level event_extraction and propose the rst document_level neural event_argument_extraction model', 0.9214296714556163, 0.8233632098173257, 0.00347371379853543)\n",
      "39 ('figure 2 : examples of three event representations', 0.8709620313044352, 0.8021757239055672, 0.002279949147068614)\n",
      "40 ('unlike previous_works ( yang et al. , 2019a ; li et al. , 2021 ) , which separate event_extraction into two pipelined tasks ( event_detection and event_argument_extraction ) , degree is designed for the end_to_end event ex__traction and predict event triggers and arguments at the same time', 0.9311785695175676, 0.8328360580054261, 0.0028579606632248666)\n",
      "41 ('each sample is a 5_sentence document , with trigger word indicating pre_defined event_type and its argument scattering among the whole document', 0.9157736976493498, 0.8303595781084032, 0.0023864324115603227)\n",
      "42 ('how_ ever , their methods can only be applied to event_detection , which differs from our main focus on studying end_to_end event_extraction', 0.9155522085071713, 0.8439063673952282, 0.0034172929878350795)\n",
      "45 ('paper_title : dynamic prefix_tuning for generative template_based event_extraction ; paper_abstract : we consider event_extraction in a generative manner with template_based conditional_generation', 0.908186455671156, 0.7666378712495836, 0.002450182589911819)\n",
      "46 ('process. , virtual event , 2021 , pp', 0.8671050783487865, 0.7567034440866864, 0.002879570857822175)\n",
      "47 ('ere contains 458 english documents , 38 event_types , and 21 argument_roles', 0.916950188156943, 0.831258113291218, 0.001972173688482467)\n",
      "48 ('document_level event_extraction via parallel prediction networks', 0.9077510003839229, 0.8275087422075158, 0.002118664492503427)\n",
      "49 ('5 related work fully supervised event_extraction', 0.9078425180917051, 0.8535813195581698, 0.0021267809554667565)\n",
      "50 ('there is a rising trend of casting the task of event_extraction as a sequence_generation problem by ap_ plying special decoding strategies ( paolini et al. , 2021 ; lu et al. , 2021 ) or steering pretrained lan_ guage models to output conditional_generation se_ quences with discrete prompts ( li et al. , 2021 ; hsu et al. , 2021 )', 0.9083064900793725, 0.8019391836520537, 0.0021885675292214594)\n",
      "51 ('one template for each event_type is usually pre_dened in the ontology.4 we rst introduce our document_level argument_extraction model in section 2.1 and then intro__duce our zero_shot keyword_based trigger extrac__tion model in section 2.2', 0.9278944154173356, 0.8037659736316615, 0.0028454132930497427)\n",
      "52 ('table 6 demonstrates how different compo_model10 % data 100 % data arg_i_arg_c arg_i_arg_c full d egree ( eae ) 63.3 57.3 76.0 73.5 _ w/o event_type denition 60.3 54.4 74.5 71.1 _ w/o eae template 57.0 51.9 73.8 70.4 _ w/o query trigger 55.2 49.9 71.4 69.0 _ only query trigger 51.9 48.1 71.2 69.4 _ only eae template 51.2 46.9 71.4 68.6 _ only event_type denition 46.7 42.3 71.4 68.2 table 6 : ablation_study for the components in the prompt on event_argument_extraction with ace05_e. nents in prompts affect the performance of event_argument_extraction on ace05_e', 0.9300811121383417, 0.7745876719741347, 0.0011882300469185808)\n",
      "53 ('both contain 33 event_types and 22 argument_roles', 0.9170797502600023, 0.7794537228420972, 0.0018985949421024442)\n",
      "54 ('we propose degree ( data_efcient generation_based event_extraction ) , a generation_based model that takes a passage and a manually designed prompt as thearxiv:2108.12724v3 [ cs.cl ] 4 may 2022prompt event_type description the event is related to conflict and some violent physical act', 0.9210074289496087, 0.837672157721086, 0.0028441511558738174)\n",
      "55 ('intell. , virtual event , z._h. zhou , ed. , montreal , canada , 2021 , pp', 0.8715511152020107, 0.7426400666652923, 0.0026519322539374926)\n",
      "56 ('exploring sen__tence community for document_level event_extraction', 0.9081088537534663, 0.8405824651192915, 0.00247910756217326)\n",
      "57 ('[ 8 ] y. chen , l. xu , k. liu , d. zeng , and j. zhao , event_extraction via dynamic multi_pooling convolutional neural_networks , in proc', 0.9078803895522138, 0.7900015109439091, 0.002212821775381321)\n",
      "59 ('7 conclusion in this paper , we studied event_extraction in the template_based conditional_generation manner', 0.9081795414268191, 0.7999270733513011, 0.0038370728788797086)\n",
      "60 ('compared to the inefficient event_backfilling and contextu_ alizing paradigm in eventbert , our model can explicitly and effectively learn event_level corre_ lations between contexts and events by our novel contrastive and prompt_based objectives', 0.8969768648876721, 0.8179844610252737, 0.0028008614443362505)\n",
      "61 ('controllable generationevent schema arrest _jail person crime agent timetransport destination origin artifact vehicle time event_type arrest _jail trigger capture person the man time tuesday agent bounty huntersevent type transport trigger returned artifact the man destination los_angeles origin mexicosequence _to_structure network constraintfigure 1 : the framework of text2event', 0.9075597594394997, 0.7594490938676985, 0.0009082172553058212)\n",
      "62 ('im_ proving cross_lingual_transfer for event argument ex__traction with language_universal sentence structures', 0.8826984193130776, 0.743856496675892, 0.0023095302515662715)\n",
      "63 ('process. , virtual event , 2020 , pp', 0.8671050783487865, 0.7553694756950665, 0.0028020497791523187)\n",
      "64 ('[ 55 ] propose a sequence_ to_structure network with a constrained_decoding algorithm for event_extraction', 0.9081088541185993, 0.8236417258969654, 0.0032376430630384205)\n",
      "65 ('gets easily distracted by the additional con_ text and does not know which event to focus on', 0.8707863108869363, 0.7841877256640087, 0.00286084542213253)\n",
      "66 ('jrnn is a joint event_extraction framework with bidirectional recurrent neural_networks [ 48 ]', 0.9082342263791255, 0.8277944012832357, 0.002596405936698864)\n",
      "67 ('degree ( pipe ) consists of two models : ( 1 ) degree ( ed ) , which aims to exact event triggers for the given event_type , and ( 2 ) de_ gree ( eae ) , which identies argument_roles for the given event_type and the corresponding trig_ ger.degree ( ed ) anddegree ( eae ) are similar todegree but with different prompts and output formats', 0.9276132409562071, 0.7824960686200809, 0.002836757742933819)\n",
      "68 ('language model prim_ ing for cross_lingual event_extraction', 0.9081445549420861, 0.8127210797486372, 0.0034315035898305024)\n",
      "69 ('we consider three ways to include the event_type information : english tokens', 0.906607728564205, 0.8331536726647317, 0.00234532018783458)\n",
      "70 ('previous work ( yang et al. , 2019a ; fincke et al. , 2021 ) usually divides ee into two sub_ tasks : ( 1 ) event_detection , which identies event triggers and their types , and ( 2 ) event_argument_extraction , which extracts the arguments and their roles for given event triggers', 0.9251216243384556, 0.8333796189739331, 0.0028801709017309335)\n",
      "71 ('paper_title : degree : a data_efficient generation_based event_extraction model ; paper_abstract : event_extraction requires high_quality expert human annotations , which are usually expensive', 0.9080916313430695, 0.8517630023372285, 0.0021793487067832676)\n",
      "72 ('4.1 language_agnostic template we create one language_agnostic template tefor each event_type e , in which we list all possible as_ sociated roles3and form a unique html_tag_style template for that event_type e. for example , in figure 2 , the life : die event is associated with four roles : agent , victim , instrument , and place', 0.9105304710957659, 0.7913863010206871, 0.002401969877534855)\n",
      "73 ('the pre_ vious approach like [ 46 ] leverage hand_designed features for event_extraction', 0.9081088541185993, 0.8573474483654082, 0.002384507469003328)\n",
      "75 ('static t ype information event_type arrest_jail', 0.9006985869181815, 0.7012598332456257, 0.0010982154442557108)\n",
      "76 ('to demonstrate the portability of our model , we also create the first end_to_end zero_shot event_extraction framework and achieve 97 % of fully supervised models trigger extraction performance and 82 % of the argument_extraction performance given only access to 10 out of the 33 types on ace', 0.923884707415641, 0.8020103121898858, 0.0030668902690579895)\n",
      "77 ('describes a conict : attack event', 0.8684608050504434, 0.6764948118299896, 0.0021605462703399637)\n",
      "78 ('event_extraction as machine reading com_ prehension', 0.9078499616394528, 0.824355102436537, 0.0021638528335626976)\n",
      "79 ('although there is a rising trend of casting the task of event_extraction as a sequence_generation problem with prompts , these generation_based methods have two significant challenges , including using suboptimal prompts and static event_type information', 0.9200942876833647, 0.7875722610858411, 0.003067572171172632)\n",
      "80 ('to alleviate the above two challenges , we pro__pose a generative template_based event_extraction method with dynamic prexes , denoted as gtee_ dynpref', 0.9082124322266591, 0.7895790705140119, 0.0029343966093777337)\n",
      "81 ('compared with degree , the event_extraction method using xed templates , and text2event , the generative event_extraction method without prompts , gtee_d_ynpref outperforms them in all the datasets , showing the effectiveness of the trainable dynamic prex with prompts', 0.9081499937852545, 0.8387615118807143, 0.002901623732712154)\n",
      "82 ('document_level event_extraction via heteroge_ neous graph_based interaction model with a tracker', 0.9079516814678257, 0.7871989470690411, 0.002986947765020817)\n",
      "84 ('as shown in figure 2 , it contains the following components : event_type denition describes the denition for the given event_type.1for example , the event is related to conict and some violent phys_ ical act', 0.9056189836035563, 0.764163875697735, 0.002341948538114311)\n",
      "85 ('although there is a rising trend of casting the task of event_extraction as a se__quence generation problem with prompts , these generation_based methods have two signi_ cant challenges , including using suboptimal prompts and static event_type information', 0.9200942876833647, 0.7768653661735401, 0.0028984115533047337)\n",
      "86 ('as demonstrated in figure 1 , we follow5216our methodgeneration_based method generation_based event extraction1 ) extracting t ransport event 2 ) extracting arrest_jail event trigger person time agent capture the man tuesday bounty huntersevent type arrest_jailevent type t ransport trigger artifact destination originreturned the man los_angeles mexico generation_based event_extraction the man returned to los_angeles from mexico following his capture t uesday by bounty_hunters', 0.9105145222760875, 0.7880571679910047, 0.0018554690091455214)\n",
      "87 ('ace 2005 dataset has 599 annotated english documents , 33 event_types , and 22 argument_roles', 0.9168843887495036, 0.819079672792807, 0.002340075363581485)\n",
      "88 ('the nal event prediction is then decoded from the generated output', 0.8692537525040491, 0.7458250335990663, 0.002788670225128013)\n",
      "89 ('6 conclusion & future work in this paper , we present degree , a data_efcient generation_based event_extraction model', 0.9080751609488227, 0.8313314385521365, 0.003725076680663677)\n",
      "90 ('5.3 baseline methods we compare gtee_d_ynpref with two groups of event_extraction work', 0.9082065489956803, 0.8641297498251298, 0.003330456230957871)\n",
      "91 ('compared to ace05_en , ace05_en+and ere_ en further consider pronoun roles and multi_token event triggers', 0.87419548282618, 0.7316509369685551, 0.002112869055976232)\n",
      "92 ('the observed distributions of event_types and argument_roles are shown in figure 3', 0.9170020401543192, 0.796716659021578, 0.002709598126943966)\n",
      "93 ('more recent works leverage pre_trained lan_ guage models [ 51 ] or machine reading_comprehension mech_ anisms [ 52 ] [ 54 ] for event_extraction', 0.9081263679820728, 0.8364862460961527, 0.0014994036870165897)\n",
      "94 ('reference ofthegold event [ e ] : ibought thelatest model ofthe phone iwanted , and showed ittomyfriends', 0.86967694402769, 0.7660772821360885, 0.0016773853283360736)\n",
      "95 ('( 2020 ) , which keeps 38 event_types and 21 argument_roles', 0.9168480260964369, 0.7888633172987857, 0.002767403266063719)\n",
      "96 ('we put the english version of the event_type in the prompt even if we are training or testing on non_english languages , for example , using attack for the event_type attack', 0.904636911329468, 0.6956025231031524, 0.0028977689665266996)\n",
      "97 ('tandrindicate the label name of event_type and ar_ gument role.sindicates the text_span in the raw_text , which is the event trigger or argument mention of the extracted event', 0.9166515321036544, 0.8095738020487195, 0.002953259791337609)\n",
      "98 ('alternative end_ to_end event_extraction models , even those incorpo_ rating pretrained lm representations , only model events in isolation ( wadden et al. , 2019 ; du and cardie , 2020 ) , and are mainly evaluated on ace_ style ( doddington et al. , 2004 ) event_extraction from single sentences ( yang and mitchell , 2016 ; lin et al. , 2020 )', 0.909573602963591, 0.8319109433260314, 0.0024573828643846348)\n",
      "99 ('0 10 20 30 40 50 event types0200400600frequency 0 20 40 argument types0200400600 figure 3 : distribution of event_types and argument types in the w ikievents dataset', 0.9072156266087915, 0.7993770320714932, 0.0015750129472183669)\n",
      "100 ('paper_title : document_level event_argument_extraction by conditional_generation ; paper_abstract : event_extraction has long been treated as a sentence_level task in the ie community', 0.9213351467142507, 0.8392874233978841, 0.0022369044561472754)\n",
      "101 ('in this paper , we propose a sequence_to_ structure generation paradigm for event_extraction text2event , which can directly extract events from the text in an end_to_end manner', 0.909573602963591, 0.8636251396158009, 0.004163319460390191)\n"
     ]
    }
   ],
   "source": [
    "for k,v in node_internal_sent_ranks[4].items():\n",
    "    print(k, v)\n",
    "    if k > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxo.resetTaxo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING generative_knowledge_graph_construction; remaining in queue: deque([])\n",
      "before: ['graph_construction', 'knowledge_graph_building', 'generative_knowledge_graph', 'graph_generation', 'graph_building', 'graph_construction_tasks', 'generative_knowledge', 'graph_construction_techniques', 'knowledge_graph_construction', 'graph_building_tasks', 'graph_construction_methods', 'generative_knowledge_graph_construction_techniques', 'graph_construction_approaches', 'knowledge_graph_generation']\n",
      "after: ['generative_knowledge_graph_construction', 'generative_knowledge_graph_construction_techniques', 'generative_knowledge_graph', 'generative_knowledge', 'knowledge_graph_generation', 'knowledge_graph_construction', 'knowledge_graph_building', 'graph_generation', 'graph_construction_tasks', 'graph_construction_approaches', 'graph_construction', 'graph_construction_techniques', 'graph_construction_methods', 'graph_building', 'graph_building_tasks']\n",
      "PROCESSING generation tasks; remaining in queue: deque([])\n",
      "before: ['text_generation', 'knowledge_graph_construction', 'language_generation', 'text_synthesis', 'graph_generation', 'text_to_graph', 'graph_to_text', 'generation_models', 'text_generation_models', 'graph_generation_models', 'text_synthesis_models', 'graph_construction_models', 'knowledge_graph_generation', 'text_generation_techniques', 'graph_generation_techniques', 'generation_methods', 'graph_methods', 'text_construction_methods', 'graph_construction_methods', 'text_generation_approaches']\n",
      "after: ['generation tasks', 'text_generation_models', 'generation_methods', 'text_generation', 'generation_models', 'text_generation_approaches', 'language_generation', 'text_generation_techniques', 'graph_generation', 'graph_generation_models', 'graph_generation_techniques', 'text_construction_methods', 'knowledge_graph_generation', 'graph_construction_models', 'graph_to_text', 'graph_construction_methods', 'text_to_graph', 'graph_methods', 'knowledge_graph_construction', 'text_synthesis_models', 'text_synthesis']\n",
      "PROCESSING named_entity_recognition; remaining in queue: deque([relation_extraction, event_extraction, entity_linking, knowledge_graph_completion])\n",
      "before: ['named_entity', 'entity_recognition', 'named_entity_extraction', 'entity_identification', 'text_classification', 'named_entities', 'entity_categorization', 'named_entity_detection', 'entity_tagging', 'entity_annotation', 'named_entity_tagging', 'entity_extraction', 'named_entity_categorization', 'entity_detection', 'named_entity_identification', 'entity_classification', 'named_entity_annotation']\n",
      "after: ['named_entity_recognition', 'entity_recognition', 'named_entity_categorization', 'named_entity_detection', 'entity_categorization', 'named_entity_extraction', 'named_entity_tagging', 'named_entity_identification', 'entity_detection', 'entity_classification', 'named_entity_annotation', 'named_entity']\n",
      "PROCESSING relation_extraction; remaining in queue: deque([event_extraction, entity_linking, knowledge_graph_completion])\n",
      "before: ['relation_extraction_techniques', 'entity_relation_discovery', 'relationship_identification', 'entity_pair_extraction', 'relation_classification', 'relation_extraction_models', 'entity_relation_prediction', 'relation_extraction_methods', 'entity_pair_classification', 'relation_discovery', 'entity_relation_classification', 'relation_extraction_approaches', 'entity_pair_relation', 'relation_classification_techniques', 'entity_relation_extraction', 'relation_extraction_tasks', 'entity_pair_relation_extraction', 'relation_extraction_algorithms']\n",
      "after: ['relation_classification_techniques', 'relation_extraction_algorithms', 'relation_extraction_methods', 'relation_extraction_techniques', 'relation_classification', 'relation_extraction_approaches', 'relation_extraction_models', 'relation_extraction', 'relation_extraction_tasks', 'relation_discovery', 'entity_pair_relation_extraction', 'entity_relation_extraction']\n",
      "PROCESSING event_extraction; remaining in queue: deque([entity_linking, knowledge_graph_completion])\n",
      "before: ['event_extraction', 'event_detection', 'event_recognition', 'event_identification', 'event_categorization', 'event_classification', 'event_tagging', 'event_annotation', 'event_summarization', 'event_description', 'event_extraction_methods', 'event_extraction_techniques', 'event_extraction_algorithms', 'event_extraction_tools', 'event_extraction_systems', 'event_extraction_approaches', 'event_extraction_metrics', 'event_extraction_evaluation']\n",
      "after: ['event_identification', 'event_extraction_algorithms', 'event_extraction_tools', 'event_extraction_techniques', 'event_extraction_methods', 'event_extraction_systems', 'event_description', 'event_extraction_approaches', 'event_extraction', 'event_extraction_metrics', 'event_tagging', 'event_detection', 'event_extraction_evaluation', 'event_summarization', 'event_annotation', 'event_classification', 'event_recognition']\n",
      "PROCESSING entity_linking; remaining in queue: deque([knowledge_graph_completion])\n",
      "before: ['entity_linking', 'entity_disambiguation', 'entity_matching', 'entity_recognition', 'entity_classification', 'entity_resolution', 'entity_alignment', 'entity_correlation', 'entity_co-reference', 'entity_grounding', 'entity_linking_task', 'entity_linking_methods', 'entity_linking_algorithms', 'entity_linking_techniques', 'entity_linking_evaluation', 'entity_linking_metrics', 'entity_linking_dataset', 'entity_linking_benchmark', 'entity_linking_evaluation_metrics', 'entity_linking_performance']\n",
      "after: ['entity_linking', 'entity_linking_methods', 'entity_linking_techniques', 'entity_linking_performance', 'entity_linking_algorithms', 'entity_linking_metrics', 'entity_linking_benchmark', 'entity_linking_evaluation_metrics', 'entity_linking_dataset', 'entity_linking_evaluation', 'entity_linking_task', 'entity_co-reference', 'entity_grounding', 'entity_correlation', 'entity_disambiguation', 'entity_resolution', 'entity_matching', 'entity_alignment']\n",
      "PROCESSING knowledge_graph_completion; remaining in queue: deque([])\n",
      "before: ['entity_linking', 'knowledge_graph_inference', 'missing_entities', 'graph_completion', 'entity_linkage', 'graph_inference', 'missing_relationships', 'entity_discovery', 'link_prediction', 'relation_prediction', 'graph_filling', 'graph_completion_task', 'missing_entities_in_kg', 'entity_linkage_task', 'graph_inference_task', 'entity_discovery_task', 'relation_prediction_task', 'link_prediction_task', 'knowledge_graph_filling']\n",
      "after: ['knowledge_graph_completion', 'graph_completion_task', 'knowledge_graph_filling', 'graph_completion', 'knowledge_graph_inference', 'graph_inference_task', 'link_prediction_task', 'graph_filling']\n"
     ]
    }
   ],
   "source": [
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    print(f\"PROCESSING {curr_node.label}; remaining in queue: {queue}\")\n",
    "\n",
    "    # update vocab with label and description\n",
    "    taxo.updateVocab([curr_node.label] + curr_node.description.split(), 'phrases')\n",
    "    taxo.updateVocab(curr_node.description, 'sentences')\n",
    "    taxo.updateVocab(curr_node.label + \" : \" + curr_node.description, 'sentences')\n",
    "\n",
    "    # get common-sense class embedding\n",
    "    print('before:', curr_node.common_sense['phrases'])\n",
    "    top_common_phrases, common_phrase_node_emb = computeClassEmb(curr_node, taxo, granularity='phrases', out_phrases=True)\n",
    "    top_common_sentences, common_sent_node_emb = computeClassEmb(curr_node, taxo, granularity='sentences', out_phrases=True)\n",
    "\n",
    "    print('after:', top_common_phrases)\n",
    "    # curr_node.common_sense['phrases'] = top_common_phrases\n",
    "    # curr_node.common_sense['sentences'] = top_common_sentences\n",
    "\n",
    "    for child in curr_node.children:\n",
    "        queue.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_map = {n.node_id:[] for n in taxo.root.children[0].children}\n",
    "\n",
    "for p in collection + external_collection:\n",
    "    for n in taxo.root.children[0].children:\n",
    "        if any([term in p.vocabulary for term in n.common_sense['phrases']]):\n",
    "            paper_map[n.node_id].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 235\n",
      "3 56\n",
      "4 27\n",
      "5 115\n",
      "6 119\n"
     ]
    }
   ],
   "source": [
    "for n_id, paper_list in paper_map.items():\n",
    "    print(n_id, len(paper_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: 2; title: structured_prediction as translation between augmented natural languages; abstract: we propose a new framework , translation between augmented natural languages ( tanl ) , to solve many structured_prediction language tasks including joint_entity_and_relation_extraction , nested named_entity recognition , relation_classification , semantic_role_labeling , event_extraction , coreference_resolution , and dialogue_state_tracking . instead of tackling the problem by training task_specific discriminative classifiers , we frame it as a translation task between augmented natural languages , from which the task_relevant information can be easily extracted . our approach can match or outperform task_specific models on all tasks , and in particular , achieves new state_of_the_art results on joint_entity_and_relation_extraction ( conll04 , ade , nyt , and ace2005 datasets ) , relation_classification ( fewrel and tacred ) , and semantic_role_labeling ( conll_2005 and conll_2012 ) . we accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time ( multi_task_learning ) . finally , we show that our framework can also significantly improve the performance in a low_resource regime , thanks to better use of label_semantics .,\n",
       " id: 3; title: hyspa : hybrid span generation for scalable text_to_graph extraction; abstract: text_to_graph extraction aims to automatically extract information graphs consisting of mentions and types from natural_language texts . existing approaches , such as table filling and pairwise scoring , have shown impressive performance on various information_extraction tasks , but they are difficult to scale to datasets with longer input texts because of their second_order space/time complexities with respect to the input length . in this work , we propose a hybrid span generator ( hyspa ) that invertibly maps the information graph to an alternating_sequence of nodes and edge_types , and directly generates such sequences via a hybrid span decoder which can decode both the spans and the types recurrently in linear time and space complexities . extensive experiments on the ace05 dataset show that our approach also significantly outperforms state_of_the_art on the joint_entity_and_relation_extraction task .,\n",
       " id: 5; title: deepstruct : pretraining of language_models for structure_prediction; abstract: we introduce a method for improving the structural_understanding abilities of language_models . unlike previous approaches that finetune the models with task_specific augmentation , we pretrain language_models to generate structures from the text on a collection of task_agnostic corpora . our structure_pretraining enables zero_shot transfer of the learned knowledge that models have about the structure tasks . we study the performance of this approach on 28 datasets , spanning 10 structure_prediction_tasks including open_information_extraction , joint_entity_and_relation_extraction , named_entity recognition , relation_classification , semantic_role_labeling , event_extraction , coreference_resolution , factual probe , intent detection , and dialogue_state_tracking . we further enhance the pretraining with the task_specific training sets . we show that a 10b parameter language model transfers non_trivially to most tasks and obtains state_of_the_art_performance on 21 of 28 datasets that we evaluate . our code and datasets will be made publicly available .,\n",
       " id: 7; title: unified structure_generation for universal information_extraction; abstract: information_extraction suffers from its varying targets , heterogeneous structures , and demand_specific schemas . in this paper , we propose a unified text_to_structure generation framework , namely uie , which can universally model different ie tasks , adaptively generate targeted structures , and collaboratively learn general ie abilities from different knowledge sources . specifically , uie uniformly encodes different extraction structures via a structured extraction language , adaptively generates target extractions via a schema_based prompt mechanism structural_schema_instructor , and captures the common ie abilities via a large_scale pretrained text_to_structure model . experiments show that uie achieved the state_of_the_art_performance on 4 ie tasks , 13 datasets , and on all supervised , low_resource , and few_shot settings for a wide range of entity , relation , event and sentiment extraction tasks and their unification . these results verified the effectiveness , universality , and transferability of uie .,\n",
       " id: 8; title: extracting relational_facts by an end_to_end neural model with copy_mechanism; abstract: the relational_facts in sentences are often complicated . different relational triplets may have overlaps in a sentence . we divided the sentences into three types according to triplet overlap degree , including normal , entitypairoverlap and singleentiyoverlap . existing_methods mainly focus on normal class and fail to extract relational triplets precisely . in this paper , we propose an end_to_end model based on sequence_to_sequence learning with copy_mechanism , which can jointly extract relational_facts from sentences of any of these classes . we adopt two different strategies in decoding process : employing only one united decoder or applying multiple separated decoders . we test our models in two public datasets and our model outperform the baseline method significantly .,\n",
       " id: 12; title: contrastive information_extraction with generative_transformer; abstract: information_extraction tasks such as entity_relation_extraction and event_extraction are of great importance for natural_language processing and knowledge_graph construction . in this paper , we revisit the end_to_end information_extraction task for sequence_generation . since generative information_extraction may struggle to capture long_term_dependencies and generate unfaithful triples , we introduce a novel model , contrastive information_extraction with a generative_transformer . specifically , we introduce a single shared transformer module for an encoder_decoder_based generation . to generate faithful results , we propose a novel triplet contrastive training object . moreover , we introduce two mechanisms to further improve model performance ( i.e. , batch_wise_dynamic attention_masking and triple_wise calibration ) . experimental_results on five datasets ( i.e. , nyt , webnlg , mie , ace_2005 , and muc_4 ) show that our approach achieves better performance than baselines .,\n",
       " id: 14; title: document_level entity_based extraction as template generation; abstract: document_level entity_based extraction ( ee ) , aiming at extracting entity_centric information such as entity roles and entity relations , is key to automatic knowledge acquisition from text corpora for various domains . most document_level ee systems build extractive models , which struggle to model long_term_dependencies among entities at the document_level . to address this issue , we propose a generative_framework for two document_level ee tasks : role_filler entity_extraction ( ree ) and relation_extraction ( re ) . we first formulate them as a template generation_problem , allowing models to efficiently capture cross_entity dependencies , exploit label_semantics , and avoid the exponential computation complexity of identifying n_ary_relations . a novel cross_attention guided copy_mechanism , topk copy , is incorporated into a pre_trained sequence_to_sequence model to enhance the capabilities of identifying key information in the input document . experiments done on the muc_4 and scirex dataset show new state_of_the_art results on ree ( +3.26 % ) , binary re ( +4.8 % ) , and 4_ary re ( +2.7 % ) in f1_score .,\n",
       " id: 17; title: document_level event_argument_extraction by conditional_generation; abstract: event_extraction has long been treated as a sentence_level task in the ie community . we argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results . we propose a document_level neural event_argument_extraction model by formulating the task as conditional_generation following event templates . we also compile a new document_level event_extraction benchmark dataset wikievents which includes complete event and coreference annotation . on the task of argument_extraction , we achieve an absolute gain of 7.6 % f1 and 5.7 % f1 over the next best model on the rams and wikievents dataset respectively . on the more challenging task of informative argument_extraction , which requires implicit coreference reasoning , we achieve a 9.3 % f1 gain over the best baseline . to demonstrate the portability of our model , we also create the first end_to_end zero_shot event_extraction framework and achieve 97 % of fully supervised models trigger extraction performance and 82 % of the argument_extraction performance given only access to 10 out of the 33 types on ace .,\n",
       " id: 18; title: template filling with generative transformers; abstract: template filling is generally tackled by a pipeline of two separate supervised systems one for role_filler extraction and another for template/event_recognition . since pipelines consider events in isolation , they can suffer from error_propagation . we introduce a framework based on end_to_end generative transformers for this task ( i.e. , gtt ) . it naturally models the dependence between entities both within a single event and across the multiple events described in a document . experiments demonstrate that this framework substantially outperforms pipeline_based approaches , and other neural end_to_end baselines that do not model between_event dependencies . we further show that our framework specifically improves performance on documents containing multiple events .,\n",
       " id: 19; title: grit : generative role_filler transformers for document_level event entity_extraction; abstract: we revisit the classic problem of document_level role_filler entity_extraction ( ree ) for template filling . we argue that sentence_level approaches are ill_suited to the task and introduce a generative transformer_based encoder_decoder framework ( grit ) that is designed to model context at the document_level : it can make extraction decisions across sentence boundaries ; is implicitly aware of noun_phrase coreference structure , and has the capacity to respect cross_role dependencies in the template structure . we evaluate our approach on the muc_4 dataset , and show that our model performs substantially better than prior work . we also show that our modeling choices contribute to model performance , e.g. , by implicitly capturing linguistic knowledge such as recognizing coreferent entity_mentions .,\n",
       " id: 20; title: text2event : controllable sequence_to_structure generation for end_to_end event_extraction; abstract: event_extraction is challenging due to the complex structure of event_records and the semantic gap between text and event . traditional methods usually extract event_records by decomposing the complex structure_prediction task into multiple subtasks . in this paper , we propose text2event , a sequence_to_structure generation paradigm that can directly extract events from the text in an end_to_end manner . specifically , we design a sequence_to_structure network for unified event_extraction , a constrained_decoding algorithm for event knowledge injection during inference , and a curriculum_learning algorithm for efficient model learning . experimental_results show that , by uniformly modeling all tasks in a single model and universally predicting different labels , our method can achieve competitive_performance using only record_level annotations in both supervised_learning and transfer_learning settings .,\n",
       " id: 21; title: degree : a data_efficient generation_based event_extraction model; abstract: event_extraction requires high_quality expert human annotations , which are usually expensive . therefore , learning a data_efficient event_extraction model that can be trained with only a few labeled examples has become a crucial challenge . in this paper , we focus on low_resource end_to_end event_extraction and propose degree , a data_efficient model that formulates event_extraction as a conditional_generation problem . given a passage and a manually designed prompt , degree learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern . the final event predictions are then extracted from the generated sentence with a deterministic algorithm . degree has three advantages to learn well with less training data . first , our designed prompts provide semantic guidance for degree to leverage degree and thus better capture the event arguments . moreover , degree is capable of using additional weakly_supervised information , such as the description of events encoded in the prompts . finally , degree learns triggers and arguments jointly in an end_to_end manner , which encourages the model to better utilize the shared knowledge and dependencies among them . our experimental_results demonstrate the strong performance of degree for low_resource event_extraction .,\n",
       " id: 22; title: claret : pre_training a correlation_aware context_to_event transformer for event_centric generation and classification; abstract: generating new events given context with correlated ones plays a crucial role in many event_centric reasoning tasks . existing works either limit their scope to specific scenarios or overlook event_level correlations . in this paper , we propose to pre_train a general correlation_aware context_to_event transformer ( claret ) for event_centric reasoning . to achieve this , we propose three novel event_centric objectives , i.e. , whole event recovering , contrastive event_correlation encoding and prompt_based_event_locating , which highlight event_level correlations with effective training . the proposed claret is applicable to a wide range of event_centric reasoning scenarios , considering its versatility of ( i ) event_correlation types ( e.g. , causal , temporal , contrast ) , ( ii ) application formulations ( i.e. , generation and classification ) , and ( iii ) reasoning types ( e.g. , abductive , counterfactual and ending reasoning ) . empirical fine_tuning results , as well as zero_ and few_shot_learning , on 9 benchmarks ( 5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations ) , verify its effectiveness and generalization ability .,\n",
       " id: 23; title: dynamic prefix_tuning for generative template_based event_extraction; abstract: we consider event_extraction in a generative manner with template_based conditional_generation . although there is a rising trend of casting the task of event_extraction as a sequence_generation problem with prompts , these generation_based methods have two significant challenges , including using suboptimal prompts and static event_type information . in this paper , we propose a generative template_based event_extraction method with dynamic prefix ( gtee_dynpref ) by integrating context information with type_specific prefixes to learn a context_specific prefix for each context . experimental_results show that our model achieves competitive_results with the state_of_the_art classification_based model oneie on ace 2005 and achieves the best performances on ere . additionally , our model is proven to be portable to new types of events effectively .,\n",
       " id: 24; title: multilingual generative language_models for zero_shot cross_lingual event_argument_extraction; abstract: we present a study on leveraging multilingual_pre_trained generative language_models for zero_shot cross_lingual event_argument_extraction ( eae ) . by formulating eae as a language_generation task , our method effectively encodes event structures and captures the dependencies between arguments . we design language_agnostic templates to represent the event argument structures , which are compatible with any language , hence facilitating the cross_lingual_transfer . our proposed model finetunes multilingual_pre_trained generative language_models to generate sentences that fill in the language_agnostic template with arguments extracted from the input_passage . the model is trained on source languages and is then directly applied to target languages for event_argument_extraction . experiments demonstrate that the proposed model outperforms the current_state_of_the_art models on zero_shot cross_lingual eae . comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language_models for zero_shot cross_lingual_transfer eae .,\n",
       " id: 25; title: prompt for extraction ? paie : prompting argument interaction for event_argument_extraction; abstract: in this paper , we propose an effective yet efficient model paie for both sentence_level and document_level event_argument_extraction ( eae ) , which also generalizes well when there is a lack of training data . on the one hand , paie utilizes prompt_tuning for extractive objectives to take the best advantages of pre_trained_language_models ( plms ) . it introduces two span selectors based on the prompt to select start/end tokens among input texts for each role . on the other hand , it captures argument interactions via multi_role prompts and conducts joint optimization with optimal span assignments via a bipartite_matching_loss . also , with a flexible prompt design , paie can extract multiple arguments with the same role instead of conventional heuristic threshold tuning . we have conducted extensive experiments on three benchmarks , including both sentence_ and document_level eae . the results present promising improvements from paie ( 3.5 % and 2.3 % f1 gains in average on three benchmarks , for paie_base and paie_large respectively ) . further analysis demonstrates the efficiency , generalization to few_shot settings , and effectiveness of different extractive prompt_tuning strategies . our code is available at https : //github.com/mayubo2333/paie .,\n",
       " id: 39; title: 6th china conference on knowledge_graph and semantic computing , ccks 2021; abstract: the proceedings contain 28 papers . the special focus in this conference is on knowledge_graph and semantic computing . the topics include : incorporating complete syntactical knowledge for spoken language_understanding ; nsrl : named_entity recognition withnoisy labels via selective reviewlearning ; knowledge enhanced target_aware stance detection on tweets ; towards nested and fine_grained open_information_extraction ; toward a better text data_augmentation via filtering and transforming augmented instances ; a visual analysis method of knowledge_graph based on the elements and structure ; patentminer : patent vacancy mining via context_enhanced and knowledge_guided graph attention ; multi_task feature learning for social recommendation ; multi_stage knowledge propagation network for recommendation ; federated knowledge_graph embeddings with heterogeneous data ; tgkg : new data graph based on game ontology ; csdqa : diagram question_answering in computer science ; mooper : a large_scale dataset ofpractice_oriented online learning ; meed : a multimodal event_extraction dataset ; c_clue : a benchmark of classical_chinese based on a crowdsourcing system for knowledge_graph construction ; rcwi : a dataset for chinese complex word identification ; diakg : an annotated diabetes dataset for medical_knowledge graph_construction ; weibo_mel , wikidata_mel and richpedia_mel : multimodal entity_linking_benchmark datasets ; makg : a mobile application knowledge_graph for the research of cybersecurity ; text_guided legal knowledge_graph reasoning ; on robustness and bias analysis ofbert_based relation_extraction ; ka_ner : knowledge augmented named_entity recognition ; structural dependency self_attention based hierarchical event model for chinese financial event_extraction ; integrating manifold knowledge forglobal entity_linking withheterogeneous graphs ; content_based open knowledge_graph search : a preliminary study withopenkg.cn ; dependency to semantics : structure transformation and syntax_guided attention for neural semantic_parsing .,\n",
       " id: 352; title: construction andapplication ofevent logic graph : a survey; abstract: since being proposed in 2017 , event_logic_graph has attracted more and more researchers attention and has been gradually applied in various fields such as finance , health_care , transportation , information , politics , etc . unlike the traditional_knowledge graph describing static entities and their attributes and relationships , the event_logic_graph describes the evolution rules and patterns between events . the construction of event logic graphs is significant for understanding human behavior patterns and mining event evolution rules . the survey first systematically combs the work of constructing an event_logic_graph , including event_extraction and event relationship_extraction methods . secondly , the typical application of the event_logic_graph is explained . finally , the challenges of event_logic_graph construction are analyzed , and future_research trends are prospected .,\n",
       " id: 380; title: contrastive information_extraction with generative transformer; abstract: information_extraction tasks such as entity_relation_extraction and event_extraction are of great importance for natural_language processing and knowledge_graph construction . in this paper , we revisit the end_to_end information_extraction task for sequence generation . since generative information_extraction may struggle to capture long_term dependencies and generate unfaithful triples , we introduce a novel model , contrastive information_extraction with a generative transformer . specifically , we introduce a single shared transformer module for an encoder_decoder_based generation . to generate faithful results , we propose a novel triplet contrastive training object . moreover , we introduce two mechanisms to further improve model performance ( i.e. , batch_wise dynamic attention_masking and triple_wise calibration ) . experimental_results on five datasets ( i.e. , nyt , webnlg , mie , ace_2005 , and muc_4 ) show that our approach achieves better performance than baselines.1,\n",
       " id: 527; title: event_logic_graph construction for event mining; abstract: event_logic_graph is a directed and cyclic graph , the nodes in the graph represent events , and the edges represent the logical relationship between events . in essence , event_logic_graph is a knowledge_base of event logic , in order to reveal the evolution law and development mode of the event , we research a method of constructing event_logic_graph , which describes the logical structural relationship between events by adopting knowledge_graph structure . at the same time , in order to describe the event more clearly , we also describe the multi_dimensional attributes of the event . we propose an architecture for constructing event_logic_graph , which includes text_corpus collection , event relationship template construction , event_extraction and structured representation , event similarity calculation and fusion , event trigger word extraction and argument extraction model construction , event relationship pair construction , graph_database storage , and the architecture is used to construct serial event_logic_graph , causal event_logic_graph , conditional event_logic_graph , transitional event_logic_graph , and concurrent event_logic_graph .,\n",
       " id: 539; title: exploiting extensive external information for event_detection through semantic networks word representation and attention map; abstract: event_detection is one of the key tasks to construct knowledge_graph and reason graph , also a hot and difficult problem in information_extraction . automatic event_detection from unstructured natural_language text has far_reaching significance for human cognition and intelligent analysis . however , limited by the source and genre , corpora for event_detection can not provide enough information to solve the problems of polysemy , synonym association and lack of information . to solve these problems , this paper proposes a brand new event_detection model based on extensive external information ( edeei ) . the model employs external corpus , semantic_network , part of speech and attention map to extract complete and accurate triggers . experiments on ace 2005 benchmark dataset show that the model effectively uses the external_knowledge to detect events , and is significantly superior to the state_of_the_art event_detection methods .,\n",
       " id: 552; title: extracting events and their relations from texts : a survey on recent research progress and challenges; abstract: event is a common but non_negligible knowledge type . how to identify events from texts , extract their arguments , even analyze the relations between different events are important for many applications . this paper summaries some constructed event_centric knowledge_graphs and the recent typical approaches for event and event relation_extraction , besides task description , widely used evaluation datasets , and challenges . specifically , in the event_extraction task , we mainly focus on three recent important research problems : 1 ) how to learn the textual semantic representations for events in sentence_level event_extraction ; 2 ) how to extract relations across sentences or in a document level ; 3 ) how to acquire or augment labeled instances for model training . in event relation_extraction , we focus on the extraction approaches for three typical event relation_types , including coreference , causal and temporal relations , respectively . finally , we give out our conclusion and potential research issues in the future .,\n",
       " id: 657; title: ieee international conference on data_mining workshops , icdmw; abstract: the proceedings contain 189 papers . the topics discussed include : detecting performance degradation of software_intensive systems in the presence of trends and long_range dependence ; scalable online_offline stream clustering in apache spark ; distributed mining and modeling of dynamic lead_lag relations in evolving entities ; event_detection for urban dynamic data streams ; segmenting sequences of node_labeled graphs ; inference of partial canonical correlation networks with application to stock_market portfolio selection ; overlapping community detection by local decentralized vertex_centered process ; query_based evolutionary graph cuboid outlier detection ; vertex centric asynchronous belief_propagation algorithm for large_scale graphs ; text network exploration via heterogeneous web of topics ; and classification of normal and pathological brain networks based on similarity in graph partitions .,\n",
       " id: 723; title: joint biomedical entity and relation_extraction with knowledge_enhanced collective inference; abstract: compared to the general news domain , information_extraction ( ie ) from biomedical text requires much broader domain_knowledge . however , many previous ie methods do not utilize any external_knowledge during inference . due to the exponential_growth of biomedical publications , models that do not go beyond their fixed set of parameters will likely fall behind . inspired by how humans look up relevant information to comprehend a scientific text , we present a novel framework that utilizes external_knowledge for joint entity and relation_extraction named keci ( knowledge_enhanced collective inference ) . given an input text , keci first constructs an initial span graph representing its initial understanding of the text . it then uses an entity linker to form a knowledge_graph containing relevant background_knowledge for the the entity mentions in the text . to make the final predictions , keci fuses the initial span graph and the knowledge_graph into a more refined graph using an attention_mechanism . keci takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional_networks . our experimental_results show that the framework is highly effective , achieving new state_of_the_art results in two different benchmark_datasets : biorelex ( binding interaction detection ) and ade ( adverse drug event_extraction ) . for example , keci achieves absolute improvements of 4.59 % and 4.91 % in f1 scores over the state_of_the_art on the biorelex entity and relation_extraction_tasks .,\n",
       " id: 894; title: modeling of unsupervised knowledge_graph of events based on mutual_information among neighbor domains and sparse representation; abstract: text event mining , as an indispensable method of text_mining processing , has attracted the extensive attention of researchers . a modeling method for knowledge_graph of events based on mutual_information among neighbor domains and sparse representation is proposed in this paper , i.e . ukge_ms . specifically , ukge_ms can improve the existing text_mining technology 's ability of understanding and discovering high_dimensional unmarked information , and solves the problems of traditional unsupervised feature_selection methods , which only focus on selecting features from a global perspective and ignoring the impact of local connection of samples . firstly , considering the influence of local information of samples in feature correlation evaluation , a feature clustering algorithm based on average neighborhood mutual_information is proposed , and the feature clusters with certain event correlation are obtained ; secondly , an unsupervised feature_selection method based on the high_order correlation of multi_dimensional statistical data is designed by combining the dimension reduction advantage of local linear embedding algorithm and the feature_selection ability of sparse representation , so as to enhance the generalization_ability of the selected feature items . finally , the events knowledge_graph is constructed by means of sparse representation and l1 norm . extensive_experiments are carried out on five real datasets and synthetic datasets , and the ukge_ms are compared with five corresponding algorithms . the experimental_results show that ukge_ms is better than the traditional method in event clustering and feature_selection , and has some advantages over other methods in text event_recognition and discovery .,\n",
       " id: 902; title: multi_information preprocessing event_extraction with bilstm_crf attention for academic knowledge_graph construction; abstract: academic knowledge_graph is an important application of knowledge_graph in the vertical field of academia . at present , the construction of the academic knowledge_graph is mainly completed by extracting published academic papers , authors , publications , and other information from related databases . however , academic information is not just information of published papers . scholars & # x2019 ; academic activities include participation in academic conferences , visiting and making presentation , and so on . however , the above academic information is hidden in natural_language texts and can not be directly stored in academic knowledge_graph . this article proposes an approach named construct_scholat knowledge_graph to construct an academic event knowledge_graph based on academic social_network scholat . the construction framework mainly consists of two parts : data preprocessing and event_extraction . in the data preprocessing , we propose a knowledge_graph embedding method to represent scholars & # x2019 ; academic social feature . in the event_extraction , we concatenate the preprocessed scholar vector with academic we_media blog text into the extraction model based on bilstm_crf fused with attention_mechanism . the extracted events are added to academic knowledge_graph , and a public relationship exists between the event and the scholar . compared to the previous methods , our framework has an excellent performance after experimental verification . to the best of our knowledge , this is the first study to use the scholar academic social information of the scholar who edited the text as the event_extraction input information . in addition , we publish a chinese event_extraction dataset scholat academic event extraction.https : //www.scholat.com/research/opendata the dataset includes academic we_media blog and the social behavior embedding vector of the scholar . all the data in this dataset are derived from the academic social_network scholat .,\n",
       " id: 1161; title: semi_supervised auto_encoder based event_detection in constructing knowledge_graph for social good; abstract: knowledge_graphs have recently been extensively applied in many different areas ( e.g. , disaster management and relief , disease_diagnosis ) . for example , event_centric knowledge_graphs have been developed to improve decision_making in disaster management and relief . this paper focuses on the task of event_detection , which is the precondition of event_extraction for constructing event_centric knowledge_graphs . event_detection identifies trigger words of events in the sentences of a document and further classifies the types of events . it is straightforward that context information is useful for event_detection . therefore , the feature_based methods adopt cross_sentence information . however , they suffer from the complication of human_designed features . on the other hand , the representation_based methods learn document_level embeddings , which , however , contain much noise caused by unsupervised_learning . to overcome these problems , in this paper we propose a new model based on semi_supervised auto_encoder , which learns context information to enhance event_detection , thus called sae_ceed . this model first applies large_scale unlabeled texts to pre_train an auto_encoder , so that the embeddings of segments learned by the encoder contain the semantic and order information of the original text . it then uses the decoder to extract the context embeddings and fine_tunes them to enhance a bidirectional neural_network model to identify event triggers and their types in sentences . through experiments on the benchmark ace_2005 dataset , we demonstrate the effectiveness of the proposed sae_ceed model . in addition , we systematically conduct a series of experiments to verify the impact of different lengths of text segments in the pre_training of the auto_encoder on event_detection .]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_map[\"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import rankPhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_external, class_emb = rankPhrases(external_sentences, taxo.root, taxo, use_class_emb=True, granularity='sentences', out_phrases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING generative_knowledge_graph_construction; remaining in queue: deque([])\n",
      "PROCESSING generation tasks; remaining in queue: deque([])\n",
      "PROCESSING named_entity_recognition; remaining in queue: deque([relation_extraction, event_extraction, entity_linking, knowledge_graph_completion])\n",
      "PROCESSING relation_extraction; remaining in queue: deque([event_extraction, entity_linking, knowledge_graph_completion])\n",
      "PROCESSING event_extraction; remaining in queue: deque([entity_linking, knowledge_graph_completion])\n",
      "PROCESSING entity_linking; remaining in queue: deque([knowledge_graph_completion])\n",
      "PROCESSING knowledge_graph_completion; remaining in queue: deque([])\n"
     ]
    }
   ],
   "source": [
    "while queue:\n",
    "    curr_node = queue.popleft()\n",
    "    parent_node = curr_node.parents[0]\n",
    "\n",
    "    print(f\"PROCESSING {curr_node.label}; remaining in queue: {queue}\")\n",
    "\n",
    "    # EXTERNAL: get partitioned set of external phrases and sentences\n",
    "    curr_node.external['phrases'] = external_phrases # parent_node.external['phrases']\n",
    "    curr_node.external['sentences'] = external_sentences # parent_node.external['sentences']\n",
    "  \n",
    "    # get top external phrases & external sentences\n",
    "    top_phrases, external_phrase_class_emb = computeClassEmb(curr_node, taxo, class_emb=True, granularity='phrases', out_phrases=True)\n",
    "    top_sentences, external_sentence_class_emb = computeClassEmb(curr_node, taxo, class_emb=True, granularity='sentences', out_phrases=True)\n",
    "\n",
    "    top_external_phrases = [phrase for phrase in top_phrases if phrase in curr_node.external['phrases']]\n",
    "    top_external_sentences = [sent for sent in top_sentences if sent in curr_node.external['sentences']]\n",
    "    curr_node.external['phrases'] = top_external_phrases\n",
    "    curr_node.external['sentences'] = top_external_sentences\n",
    "\n",
    "    # INTERNAL: get partitioned set of internal phrases and sentences\n",
    "    curr_node.internal['phrases'] = internal_phrases # parent_node.internal['phrases']\n",
    "    curr_node.internal['sentences'] = internal_sentences # parent_node.internal['sentences']\n",
    "\n",
    "    # get top internal phrases & sentences\n",
    "    top_phrases, internal_phrase_class_emb = computeClassEmb(curr_node, taxo, class_emb=True, granularity='phrases', out_phrases=True)\n",
    "    top_sentences, internal_sentence_class_emb = computeClassEmb(curr_node, taxo, class_emb=True, granularity='sentences', out_phrases=True)\n",
    "\n",
    "    top_internal_phrases = [phrase for phrase in top_phrases if phrase in curr_node.internal['phrases']]\n",
    "    top_internal_sentences = [sent for sent in top_sentences if sent in curr_node.internal['sentences']]\n",
    "    curr_node.internal['phrases'] = top_internal_phrases\n",
    "    curr_node.internal['sentences'] = top_internal_sentences\n",
    "\n",
    "    for child in curr_node.children:\n",
    "        queue.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phrases': ['entity_recognition',\n",
       "  'bag_of_entity',\n",
       "  'bag_of_entities',\n",
       "  'named',\n",
       "  'anti_nose',\n",
       "  'tokens',\n",
       "  'artificial_neural_networks',\n",
       "  'facial_expression',\n",
       "  '//github.com/wjn1996/mathematical_knowledge_entity_recognition',\n",
       "  'artificial_intelligence',\n",
       "  'andrecognize',\n",
       "  'token',\n",
       "  'murder',\n",
       "  'tokenization',\n",
       "  'negated',\n",
       "  'mention_pronoun',\n",
       "  'word_vector',\n",
       "  'weaponry',\n",
       "  'inflated',\n",
       "  'optical_character_recognition',\n",
       "  'token_based',\n",
       "  \"'phoneme\",\n",
       "  'word_character',\n",
       "  'natural_language',\n",
       "  'intent_classifier',\n",
       "  'entity_classification',\n",
       "  'entity_detection',\n",
       "  'mention',\n",
       "  'name',\n",
       "  'non_named',\n",
       "  'word_by_word',\n",
       "  'weapon',\n",
       "  'named_entity_extraction',\n",
       "  'noun',\n",
       "  'sign_language',\n",
       "  'applications_of_artificial_intelligence',\n",
       "  'noun_phrase',\n",
       "  'intelligent_agent',\n",
       "  'pronoun',\n",
       "  'character_vocabulary',\n",
       "  'word_entity',\n",
       "  'support_vector_machine',\n",
       "  'utterances',\n",
       "  'automatic_speech_recognition',\n",
       "  'naming',\n",
       "  'chinese_language',\n",
       "  'uttered',\n",
       "  'controlled_vocabulary',\n",
       "  'inter_mention',\n",
       "  'negate',\n",
       "  'eye_tracking',\n",
       "  'tagger',\n",
       "  'mention_entity',\n",
       "  'automobile',\n",
       "  'false_negative',\n",
       "  'birds_eye',\n",
       "  'cytokine',\n",
       "  'isolate',\n",
       "  'neural_networks',\n",
       "  'aviation',\n",
       "  'vehicle_pedestrian',\n",
       "  'neural_network',\n",
       "  'microchip',\n",
       "  'uyghur_named',\n",
       "  'bots',\n",
       "  'misdiagnosis',\n",
       "  'lexicon',\n",
       "  'nomenclature',\n",
       "  'masked_entity',\n",
       "  'international_classification_of_diseases',\n",
       "  'mention_context',\n",
       "  'word_vectors',\n",
       "  'character_level',\n",
       "  'named_entity',\n",
       "  'recognize',\n",
       "  'bag_of_word',\n",
       "  'microblog',\n",
       "  'detects',\n",
       "  'bag_level',\n",
       "  'call',\n",
       "  'names',\n",
       "  'decent',\n",
       "  'subword/sub_entity',\n",
       "  'handled',\n",
       "  'support_vector_machines',\n",
       "  'pedestrian_vehicle',\n",
       "  'subword',\n",
       "  'president_obama',\n",
       "  'suspects',\n",
       "  'automobiles',\n",
       "  'recognizes',\n",
       "  'entity',\n",
       "  'identification',\n",
       "  '179',\n",
       "  'tag',\n",
       "  'neural_network_based',\n",
       "  'neural_symbolic',\n",
       "  'neural_based',\n",
       "  'detection',\n",
       "  'wrong_label',\n",
       "  'ethnic',\n",
       "  'cognate',\n",
       "  '#',\n",
       "  'machine_translation',\n",
       "  'number',\n",
       "  'sentiment_controllable',\n",
       "  'sentiment_augmented',\n",
       "  'thebrainof',\n",
       "  'intentional',\n",
       "  'convolutional_neural_network',\n",
       "  'identifying',\n",
       "  'animal_husbandry',\n",
       "  'immunology',\n",
       "  'chinese_characters',\n",
       "  'mentions',\n",
       "  'non_native',\n",
       "  'horn',\n",
       "  'aircraft',\n",
       "  'receptive_field',\n",
       "  'multinomial',\n",
       "  'verbalizer',\n",
       "  'malicious',\n",
       "  'autoimmune_disease',\n",
       "  'molecular',\n",
       "  'word_embedding',\n",
       "  'vector_based',\n",
       "  'infectious',\n",
       "  '320_character',\n",
       "  'wordnet',\n",
       "  'visionlanguage',\n",
       "  'chemical_protein',\n",
       "  'chinese_character',\n",
       "  'boosted',\n",
       "  'describe',\n",
       "  'uncontrollable',\n",
       "  'utterance',\n",
       "  'unit',\n",
       "  'nuclear',\n",
       "  'category',\n",
       "  \"'name\",\n",
       "  'negation',\n",
       "  'word_embeddings',\n",
       "  'microblogging',\n",
       "  'convolutional_neural_networks',\n",
       "  'infections',\n",
       "  'vector',\n",
       "  'differentiates',\n",
       "  'termed',\n",
       "  'bag_of_words',\n",
       "  'pathogenicity',\n",
       "  'intent',\n",
       "  'country',\n",
       "  'beings',\n",
       "  'sentence',\n",
       "  'national',\n",
       "  'semiconductor_industry',\n",
       "  'identifier',\n",
       "  'word/sub_word',\n",
       "  'recognition/normalization',\n",
       "  'conviction',\n",
       "  'token_level',\n",
       "  'name_',\n",
       "  'managed',\n",
       "  'tweet',\n",
       "  'virulent',\n",
       "  'lexicon_based',\n",
       "  'veteran',\n",
       "  'taggers',\n",
       "  'concept_to_sentence',\n",
       "  'anti_money',\n",
       "  'affiliation',\n",
       "  'distinguish',\n",
       "  'vehicle',\n",
       "  'character_feature',\n",
       "  'character_based',\n",
       "  'unique',\n",
       "  'anger',\n",
       "  'vectors',\n",
       "  'boolean_circuit',\n",
       "  'misspelled',\n",
       "  'denoted',\n",
       "  'cruises',\n",
       "  'word_segmentation',\n",
       "  'keyword',\n",
       "  'sentence_embedding',\n",
       "  'detecting',\n",
       "  'neural',\n",
       "  'spurious',\n",
       "  'voice',\n",
       "  'distinguishing',\n",
       "  'accuracy_rate',\n",
       "  'combat',\n",
       "  'named_entities',\n",
       "  'boundaries',\n",
       "  'human_object',\n",
       "  'attention_deficit',\n",
       "  'sentiments',\n",
       "  'molecular_structure',\n",
       "  'bert_imseq2seq_crf',\n",
       "  'terrorist',\n",
       "  'barack_obama',\n",
       "  'elected',\n",
       "  '3188',\n",
       "  'ncbi_disease',\n",
       "  'multilayer_perceptron',\n",
       "  'mobile_phone',\n",
       "  'emotion',\n",
       "  'vectorized',\n",
       "  'deceptive',\n",
       "  'called',\n",
       "  'word_embedding_based',\n",
       "  'bot',\n",
       "  'phone',\n",
       "  'sentiment',\n",
       "  'emotions',\n",
       "  'recognition.to',\n",
       "  'militarycorpus',\n",
       "  'whole_sentence',\n",
       "  'mention_level',\n",
       "  'vehicles',\n",
       "  'language',\n",
       "  'unidentified',\n",
       "  'intelligent_systems',\n",
       "  'speech',\n",
       "  'word_level',\n",
       "  'medical_diagnosis',\n",
       "  'overwhelmed',\n",
       "  'deciphering',\n",
       "  'asthmaenvironment',\n",
       "  'purely',\n",
       "  'word',\n",
       "  'herein',\n",
       "  'string_matching',\n",
       "  'activitynet_caption',\n",
       "  'cause',\n",
       "  'vogue',\n",
       "  'region_based',\n",
       "  'weapons',\n",
       "  'easoning',\n",
       "  'pre_trained_language_models',\n",
       "  'institution',\n",
       "  'birds',\n",
       "  'sentence_specific',\n",
       "  'pre_trained_language',\n",
       "  'veterinary',\n",
       "  'sentences',\n",
       "  'tags',\n",
       "  'fluent',\n",
       "  'tagging',\n",
       "  'label_imbalance',\n",
       "  'pulmonary',\n",
       "  'hospital',\n",
       "  'speaking',\n",
       "  'automate',\n",
       "  'head/tail',\n",
       "  'identifiers',\n",
       "  'military_intelligence',\n",
       "  'telecommunication',\n",
       "  'fighting',\n",
       "  'manage',\n",
       "  'region',\n",
       "  'tweets',\n",
       "  'units',\n",
       "  'multi_neural',\n",
       "  'single_vote',\n",
       "  'unmarked',\n",
       "  'drug_discovery',\n",
       "  'abbreviation',\n",
       "  'tagged',\n",
       "  'flagged',\n",
       "  'intra_sentence',\n",
       "  'lung_cancer',\n",
       "  'domestic',\n",
       "  'detect',\n",
       "  'pedestrians',\n",
       "  'autism_spectrum',\n",
       "  'million_level',\n",
       "  'identify',\n",
       "  're_net',\n",
       "  'attack',\n",
       "  'realm',\n",
       "  'neuro_symbolic',\n",
       "  'indonesian_language',\n",
       "  'obama',\n",
       "  'legitimate',\n",
       "  'chinese_medical',\n",
       "  'spleen',\n",
       "  'id_cnn+crf',\n",
       "  'japanese',\n",
       "  'speer',\n",
       "  'de_identified',\n",
       "  'raise',\n",
       "  'characters',\n",
       "  'neglecting',\n",
       "  'identity',\n",
       "  'attracted',\n",
       "  'spoken',\n",
       "  'character_embedding',\n",
       "  'human_machine',\n",
       "  'serve',\n",
       "  'malware',\n",
       "  'comprise',\n",
       "  'combating',\n",
       "  'sentiment_classification',\n",
       "  'chinese_medicine',\n",
       "  'pronouns',\n",
       "  'distant_supervised',\n",
       "  'non_chinese',\n",
       "  'wordnet_based',\n",
       "  'multi_sentence',\n",
       "  'etiology',\n",
       "  'authenticity',\n",
       "  '!',\n",
       "  'twitter',\n",
       "  'vectorize',\n",
       "  'heroic',\n",
       "  'describing',\n",
       "  'node_embedding',\n",
       "  'topic_embedding',\n",
       "  'sentiment_aware',\n",
       "  'recognized',\n",
       "  '317',\n",
       "  'distantly_supervised',\n",
       "  'asthma',\n",
       "  'boosting',\n",
       "  'character',\n",
       "  '1.1https',\n",
       "  'ep_bot',\n",
       "  'grammatical',\n",
       "  'embeddings_based',\n",
       "  'attackers',\n",
       "  'automated',\n",
       "  'mundane',\n",
       "  'target_language',\n",
       "  'entities',\n",
       "  'automatic',\n",
       "  'recognition.in',\n",
       "  'multi_paragraph',\n",
       "  'calling',\n",
       "  'automates',\n",
       "  'perplexity',\n",
       "  'serves',\n",
       "  'attempt',\n",
       "  'segment',\n",
       "  'distinguished',\n",
       "  'machine_learning',\n",
       "  'maximum_likelihood',\n",
       "  'into',\n",
       "  'out_of_vocabulary',\n",
       "  'ornamental',\n",
       "  'nouns',\n",
       "  'characterize',\n",
       "  'abnormal',\n",
       "  '2017',\n",
       "  'siamese',\n",
       "  'medicine_specific',\n",
       "  'chinese_poetry',\n",
       "  'was',\n",
       "  'species',\n",
       "  'chemistries',\n",
       "  'identifiable',\n",
       "  'cell_cytokine',\n",
       "  'laboratory',\n",
       "  'calls',\n",
       "  'hereinafter',\n",
       "  'base_station',\n",
       "  'report',\n",
       "  'truly',\n",
       "  'picked',\n",
       "  'multi_vote',\n",
       "  'term',\n",
       "  'rare_disease',\n",
       "  'agent',\n",
       "  'mistake',\n",
       "  'infor_mation',\n",
       "  'english_chinese',\n",
       "  'cyber_security',\n",
       "  'cruise',\n",
       "  'italian',\n",
       "  'headway',\n",
       "  'residues',\n",
       "  'idrg_cnn',\n",
       "  'verbalizes',\n",
       "  'patent_medicine',\n",
       "  'inflected',\n",
       "  'director',\n",
       "  'automatics',\n",
       "  'anti_terrorism',\n",
       "  'verbalized',\n",
       "  'description_embodied',\n",
       "  'diagnosis/medications',\n",
       "  'short_term_memory',\n",
       "  'wordties',\n",
       "  '_gram',\n",
       "  'spreading',\n",
       "  'language_understanding',\n",
       "  'topic',\n",
       "  'user_agent',\n",
       "  'sion',\n",
       "  'solely',\n",
       "  'category_based',\n",
       "  'host',\n",
       "  'described',\n",
       "  'words',\n",
       "  'native',\n",
       "  'nodule',\n",
       "  'entity_extraction',\n",
       "  'flavor',\n",
       "  'specificity',\n",
       "  'concatenation',\n",
       "  'distinct',\n",
       "  'mere',\n",
       "  'acknowledged',\n",
       "  'correctly',\n",
       "  'culture',\n",
       "  'cyberbullying',\n",
       "  'characteristic',\n",
       "  'high_precision',\n",
       "  'identifies',\n",
       "  'primarily',\n",
       "  'sentenses',\n",
       "  'affiliations',\n",
       "  'illness',\n",
       "  'vision_language',\n",
       "  'totally',\n",
       "  'cnn_based',\n",
       "  'human_recognizable',\n",
       "  'word_feature',\n",
       "  'vectorization',\n",
       "  'inter_sentence',\n",
       "  'allergies',\n",
       "  'vocabulary',\n",
       "  'chat_bots',\n",
       "  'accuracy',\n",
       "  'identically',\n",
       "  'military',\n",
       "  'entries',\n",
       "  'unrest',\n",
       "  'wang',\n",
       "  'drugbank',\n",
       "  'borders',\n",
       "  'entity_identification',\n",
       "  'asthmakgxe',\n",
       "  'verbalization',\n",
       "  'label_lacking',\n",
       "  'cognates',\n",
       "  'thing',\n",
       "  'numbering',\n",
       "  'specific',\n",
       "  'verbalizing',\n",
       "  'tertiary',\n",
       "  '2019',\n",
       "  'convey',\n",
       "  'animal',\n",
       "  'duconv',\n",
       "  'encompass',\n",
       "  'belonging',\n",
       "  'identities',\n",
       "  'collective',\n",
       "  'topic_word',\n",
       "  'detector',\n",
       "  'category_specific',\n",
       "  'single_sentence',\n",
       "  'dialogre',\n",
       "  'chemical_disease',\n",
       "  'keynote',\n",
       "  'affective_computing',\n",
       "  'united_nations',\n",
       "  'net_',\n",
       "  'foreign',\n",
       "  'uniquely',\n",
       "  'boosts',\n",
       "  'neglect',\n",
       "  'disease_diagnosis',\n",
       "  'transferred',\n",
       "  'succinctly',\n",
       "  'ant_colony',\n",
       "  'volume',\n",
       "  'normalized',\n",
       "  'birth',\n",
       "  'human_like',\n",
       "  'probably',\n",
       "  'ned',\n",
       "  'nabu',\n",
       "  'peak',\n",
       "  'predefined',\n",
       "  'comprises',\n",
       "  'label',\n",
       "  'away',\n",
       "  'direction',\n",
       "  'casrel',\n",
       "  'caselaw',\n",
       "  'operating',\n",
       "  'recognised',\n",
       "  'centers',\n",
       "  'describes',\n",
       "  'characterized',\n",
       "  'cell',\n",
       "  'intra_bag',\n",
       "  'language_agnostic',\n",
       "  'incorrectly',\n",
       "  'google_news',\n",
       "  'social_intelligence',\n",
       "  'netizens',\n",
       "  'accommodate',\n",
       "  'reside',\n",
       "  'said',\n",
       "  'deadly',\n",
       "  'handle',\n",
       "  'mixture_model',\n",
       "  'cyber_threat',\n",
       "  'substring',\n",
       "  'long_short_term_memory',\n",
       "  'tracking',\n",
       "  'tweet_date',\n",
       "  'labeled',\n",
       "  'pattern_recognition',\n",
       "  'segmented',\n",
       "  'diagnostic',\n",
       "  'area',\n",
       "  'stem_cell',\n",
       "  'att_cnn_bigru_crf',\n",
       "  'artificially',\n",
       "  'basically',\n",
       "  'unusual',\n",
       "  'recognition',\n",
       "  'typing',\n",
       "  'radar',\n",
       "  'intelligently',\n",
       "  'vector_space',\n",
       "  'immune',\n",
       "  'microchips',\n",
       "  'allergens',\n",
       "  'politician',\n",
       "  'automating',\n",
       "  'were',\n",
       "  'count',\n",
       "  'unrecognized',\n",
       "  'cells',\n",
       "  'multi_vote_based',\n",
       "  'long_sequence',\n",
       "  'object',\n",
       "  'bert_sentence',\n",
       "  'international',\n",
       "  'stay',\n",
       "  'author',\n",
       "  'dalvi',\n",
       "  'attracting',\n",
       "  'circumvent',\n",
       "  'detected',\n",
       "  'miss',\n",
       "  'automation',\n",
       "  'thailand',\n",
       "  'nuclear_technology',\n",
       "  'flamenco',\n",
       "  'electronic_medical_record',\n",
       "  'intended',\n",
       "  'internal_medicine',\n",
       "  'transportation',\n",
       "  'recurrent_unit',\n",
       "  'distinguishable',\n",
       "  'verbalize',\n",
       "  '168',\n",
       "  'misinformation',\n",
       "  'entity_summarization',\n",
       "  'categorization',\n",
       "  'non_automated',\n",
       "  'normally',\n",
       "  'spontaneous',\n",
       "  'billion_scale',\n",
       "  'recognizing',\n",
       "  'diagnoses/medications',\n",
       "  'delineate',\n",
       "  'net',\n",
       "  'drug_like',\n",
       "  're',\n",
       "  'drug_drug',\n",
       "  'characterizing',\n",
       "  'phrase',\n",
       "  'polarized',\n",
       "  '~100',\n",
       "  'viruses',\n",
       "  'nets',\n",
       "  'negating',\n",
       "  'chinese',\n",
       "  'upon',\n",
       "  'these',\n",
       "  'disease',\n",
       "  'refers',\n",
       "  'inharmonious',\n",
       "  'sentimental',\n",
       "  'auto_emotion',\n",
       "  'quantitized',\n",
       "  'spelling',\n",
       "  'terminology',\n",
       "  'persona',\n",
       "  'unconscious',\n",
       "  'billion_level',\n",
       "  'language_generation',\n",
       "  'negations',\n",
       "  'multi_classifier',\n",
       "  'sentiment_analysis',\n",
       "  'encompasses',\n",
       "  'type_description',\n",
       "  'except',\n",
       "  'govern',\n",
       "  'nltk',\n",
       "  'inter_speaker',\n",
       "  'counter_terrorism',\n",
       "  'intend',\n",
       "  'predict',\n",
       "  'hate_speech',\n",
       "  '\\\\rightarrow',\n",
       "  'embedding_based',\n",
       "  'omitted',\n",
       "  'tweetskb',\n",
       "  'boundary',\n",
       "  'pos',\n",
       "  'succinct',\n",
       "  'bosch',\n",
       "  'part_of_speech',\n",
       "  'imported',\n",
       "  'word_formation',\n",
       "  'entity_agnostic',\n",
       "  'intra_entity',\n",
       "  'abbreviations',\n",
       "  'image_caption',\n",
       "  'threat_intelligence',\n",
       "  '//github.com/zjunlp/deepke/tree/main/example/re/multimodal',\n",
       "  'verbalizable',\n",
       "  'patented',\n",
       "  'cluster',\n",
       "  'st_net',\n",
       "  'kinds',\n",
       "  'mislead',\n",
       "  'hashtags',\n",
       "  'big_data',\n",
       "  'persons',\n",
       "  'verb',\n",
       "  'economic_growth',\n",
       "  'bio_medical',\n",
       "  'resemble',\n",
       "  'born',\n",
       "  'dart',\n",
       "  'cyber_attack',\n",
       "  'conve',\n",
       "  'proper_nouns',\n",
       "  'redefined',\n",
       "  'so_called',\n",
       "  'judicious',\n",
       "  'hamming',\n",
       "  'separation',\n",
       "  'text_matching',\n",
       "  'paragraph',\n",
       "  'image_embodied',\n",
       "  'suspected',\n",
       "  'imperceptible',\n",
       "  'intelligence_based',\n",
       "  'incorporate',\n",
       "  'poisoning',\n",
       "  'wrongly',\n",
       "  'accuracies',\n",
       "  'chinese_based',\n",
       "  'inside',\n",
       "  'institute',\n",
       "  'on_topic',\n",
       "  'paragraphs',\n",
       "  'punishment',\n",
       "  'aligner',\n",
       "  'reranking',\n",
       "  'disease_19',\n",
       "  'semantic_entity',\n",
       "  'mental_illness',\n",
       "  '//github.com/yangxi1016/stroke',\n",
       "  'mes',\n",
       "  'polarity',\n",
       "  'concatenate',\n",
       "  'agents',\n",
       "  'characterizes',\n",
       "  'mass',\n",
       "  'mimic_cxr',\n",
       "  '99',\n",
       "  'multibillion_dollar',\n",
       "  'pedestrian',\n",
       "  'numeric',\n",
       "  'naturalistic',\n",
       "  'lies',\n",
       "  'reorganization',\n",
       "  'posture',\n",
       "  'leave',\n",
       "  'locating',\n",
       "  'abnormality',\n",
       "  'itk_net',\n",
       "  'human_computer',\n",
       "  'indonesian',\n",
       "  'lexicons',\n",
       "  'image_captioning',\n",
       "  'cytokines',\n",
       "  'compounds',\n",
       "  'normalize',\n",
       "  'captures',\n",
       "  'eliminates',\n",
       "  'nominal',\n",
       "  'biomedical',\n",
       "  'assamese',\n",
       "  'isolated',\n",
       "  'language_processing',\n",
       "  'manufacturer',\n",
       "  'multi_typed',\n",
       "  'percent',\n",
       "  'anomaly',\n",
       "  'chiefly',\n",
       "  'symptom_in_chinese',\n",
       "  'position_attention',\n",
       "  'implicitness',\n",
       "  'morphome',\n",
       "  'utterance_query',\n",
       "  'node_labeled',\n",
       "  'mentioned',\n",
       "  'expressions',\n",
       "  'refer',\n",
       "  'agreement',\n",
       "  'chittagonian',\n",
       "  'hosted',\n",
       "  'analyzer',\n",
       "  'automatically',\n",
       "  'gluten_related',\n",
       "  'sequence_based',\n",
       "  'room',\n",
       "  'please',\n",
       "  'language_',\n",
       "  'protein_protein',\n",
       "  'n_gram',\n",
       "  'dynamical',\n",
       "  'year',\n",
       "  '3390',\n",
       "  'alarm',\n",
       "  'embeddings',\n",
       "  'directional',\n",
       "  'overwhelming',\n",
       "  'corpora',\n",
       "  'sentence_pair',\n",
       "  'hidden_markov_model',\n",
       "  'intelligence_analysis',\n",
       "  'random_forest',\n",
       "  'quaternion',\n",
       "  'entry',\n",
       "  'human_human',\n",
       "  'unfamiliarity',\n",
       "  'eliminate',\n",
       "  'winds',\n",
       "  'neglects',\n",
       "  'https',\n",
       "  '~',\n",
       "  'combining_character',\n",
       "  'iden_tifying',\n",
       "  'outlier',\n",
       "  'nubot',\n",
       "  'english_language',\n",
       "  'caption',\n",
       "  'properly',\n",
       "  'something',\n",
       "  'involving',\n",
       "  'named_entityrelation_extraction',\n",
       "  'jiangsu',\n",
       "  'cordis',\n",
       "  'overcome',\n",
       "  'arise',\n",
       "  'matter',\n",
       "  'tweet_topic',\n",
       "  'detect_missing',\n",
       "  'specializes',\n",
       "  'endeavor',\n",
       "  'noted',\n",
       "  'nuclear_power',\n",
       "  'fundamen_tal',\n",
       "  'well_recognized',\n",
       "  'utterance.the',\n",
       "  'child',\n",
       "  'insect',\n",
       "  'nrnet',\n",
       "  'put',\n",
       "  'counting',\n",
       "  'drug_target',\n",
       "  'characteristics',\n",
       "  'disambiguated',\n",
       "  'captions',\n",
       "  'song',\n",
       "  'originate',\n",
       "  'crowdsensing',\n",
       "  'tf',\n",
       "  'human_driving',\n",
       "  'military_oriented',\n",
       "  'descriptor',\n",
       "  'topk_precision',\n",
       "  'log_likelihood',\n",
       "  'multilingual',\n",
       "  'positional',\n",
       "  'criminal',\n",
       "  'entirely',\n",
       "  'things',\n",
       "  'host_',\n",
       "  'unlike',\n",
       "  'taxonomic',\n",
       "  'cyberspace',\n",
       "  'terminals',\n",
       "  'naisc',\n",
       "  'arises',\n",
       "  'resembles',\n",
       "  'plenty',\n",
       "  'object/scene',\n",
       "  'pretrained_language_models',\n",
       "  'target_disease',\n",
       "  'synchronous_motor',\n",
       "  'takes',\n",
       "  '2022',\n",
       "  '__',\n",
       "  'everything',\n",
       "  'indicate',\n",
       "  'suicide_knowledge',\n",
       "  'numbers',\n",
       "  'whom',\n",
       "  'quantify',\n",
       "  'suspicious',\n",
       "  'hyperparameters',\n",
       "  'chemical_compounds',\n",
       "  'throw',\n",
       "  'medical_imaging',\n",
       "  'image_text_matching',\n",
       "  'emphasize',\n",
       "  'respective',\n",
       "  'localize',\n",
       "  'lstm_based',\n",
       "  'thai',\n",
       "  'originally',\n",
       "  'kilt',\n",
       "  'represent',\n",
       "  'wireless_communication',\n",
       "  '_',\n",
       "  'fake',\n",
       "  'carbon_capture',\n",
       "  'spatialnet',\n",
       "  'imbalanced',\n",
       "  'toward',\n",
       "  'single_chain',\n",
       "  'entity_annotation',\n",
       "  'behavior',\n",
       "  'liao_dynasty',\n",
       "  'attention_like',\n",
       "  'forensic',\n",
       "  'chemical_industry',\n",
       "  'latter',\n",
       "  'offensive',\n",
       "  'presence',\n",
       "  'destroy',\n",
       "  'aminer',\n",
       "  'uncommon',\n",
       "  'voltage',\n",
       "  'dropping',\n",
       "  'breast_cancer'],\n",
       " 'sentences': ['finally , based on bilstm_crf benchmark model , a model framework suitable for agricultural named_entity recognition was constructed',\n",
       "  'the experimental_results show that ar+bilstm+crf model has excellent performance for named_entity recognition in adf .',\n",
       "  'furthermore , this study uses automatically labeled datasets to train a deep_learning_based named_entity recognition model',\n",
       "  'experiment results demonstrate that our method can deal with biomedical named_entity recognition and obtain significant performances in both english and chinese biomedical datasets .',\n",
       "  'then , a vietnamese named_entity recognition model is proposed based on residual dense block ( rdb ) convolutional_neural_network ( cnn ) .',\n",
       "  'the experimental_results show that compared with the named_entity recognition method based directly on bilstmcrf on the weakly_supervised named_entity recognition in financial field , our proposed method improves f1_ score in the small data training sample set by nearly 9 % , and it has some generalization_ability .',\n",
       "  'therefore , it is very necessary to improve the recognition performance in the process of named_entity recognition , particularly for the sufficient capture of character position , contextual semantic_features , and long_distance dependency information',\n",
       "  'by designing a optimized fine_tuning method , we have realized the named_entity recognition task in the chinese twenty_four histories',\n",
       "  'conclusions : this paper confirms the effectiveness of using an advanced artificial_intelligence method to carry out named_entity recognition tasks on a corpus of a large number of clinical notes ; this application is promising in the medical setting',\n",
       "  'in this work , we present a simple and effective approach for named_entity recognition',\n",
       "  'a series of experiments were performed to construct the corpus of named_entity recognition',\n",
       "  'the results show that the model performs well in hazop named_entity recognition',\n",
       "  'paper_title : adversarial active_learning for named_entity recognition in cybersecurity ; paper_abstract : owing to the continuous barrage of cyber threats , there is a massive amount of cyber_threat intelligence',\n",
       "  'the test results show that the accuracy , recall , and f1 value of the named_entity recognition task in the aquatic medicine using the bert+cabilstm+crf model reached 93.07 % , 92.85 % , and 92.96 % , respectively',\n",
       "  'flat named_entity recognition can use the vector from pre_trained model to obtain the entity from domain_specific text',\n",
       "  'however , there is no publicly available named_entity recognition dataset in the food_safety domain',\n",
       "  'tests show that the recall_rate and the f_score of the bert_bilstm_crf model are 28.48 % and 18.65 % higher than those of a crf_based entity_recognition model , 13.91 % and 8.69 % higher than those of a bilstm_crf_based entity_recognition model , and 7.08 % and 5.15 % higher than those of a cnn ( convolutional_neural_networks ) _bilstm_crf_based model .',\n",
       "  'paper_title : improving chinese named_entity recognition with semantic_information of character multi_position representation ; paper_abstract : named_entity recognition is an important basic task for information_extraction and construction of knowledge_graph , but the recognition rate needs to be further improved , especially in chinese',\n",
       "  'finally , we propose an artificial_intelligence technique for named_entity recognition for mongolian_language resources .',\n",
       "  'the best f1_score of the named_entity recognition task after training using many rheumatoid_arthritis clinical notes was 0.936',\n",
       "  'experiments on the named_entity recognition task and relational classification task demonstrate that the mda can significantly enhance the efficiency of the deep_learning models compared to cases without augmentation .',\n",
       "  'the proposed framework contains three modules , namely domain feature pre_trained model , lstm_based named_entity recognition and the attention_based nested named_entity recognition',\n",
       "  'to evaluate the proposed method , we conduct named_entity recognition experiments using the status reports of complex equipment in nuclear_power plants',\n",
       "  'the model achieves good results , exhibiting the effectiveness in the task of named_entity recognition in the chemical_industry .',\n",
       "  'in this work , we propose a cdiner model to solve the chinese drug information named_entity recognition',\n",
       "  'paper_title : named_entity recognition method in health preserving field based on bert ; paper_abstract : with the aging of the population development , people pay more attention to health preserving',\n",
       "  'paper_title : research on named_entity recognition method based on transfer_learning for small data_sets ; paper_abstract : aiming at the problem of automatic acquisition of important military target entities , a small sample named_entity recognition method combining a lite bidirectional_encoder_representations_from_transformers ( albert ) , bi_gated recurrent_unit ( bigru ) and conditional_random_field ( crf ) was proposed',\n",
       "  'paper_title : improving neural named_entity recognition with gazetteers ; paper_abstract : the goal of this work is to improve the performance of a neural named_entity recognition system by adding input features that indicate a word is part of a name included in a gazetteer',\n",
       "  'the experimental_results show that the bilstm_crf model has a good effect on grape disease entity_recognition , and the f1 value is as high as 95.44 %',\n",
       "  'moreover , bilstm_crf is introduced for named_entity recognition tasks , and the entity_recognition information is utilized by the multi_head selection structure to solve the problem of overlapping relations',\n",
       "  'traditional named_entity recognition ( ner ) methods based on crf model which relies on large_amounts of hand_crafted features , can not extract more effective features and solve the inconsistency of entity_tagging caused by the diversity of entity names',\n",
       "  'paper_title : multi_neural network collaboration for chinese military named_entity recognition ; paper_abstract : web data contains a large amount of high_value military information which has become an important data source for open_source military_intelligence',\n",
       "  'bi_directional long_short_term_memory ( bilstm ) and iterated dilated convolutional_neural_networks ( idcnn ) combined with conditional_random_field ( crf ) form the model bilstm_idcnn_crf , and it is applied to implement named_entity recognition in chinese pesticide data_sets',\n",
       "  'paper_title : pre_trained_language model based medical named_entity recognition ; paper_abstract : medical named_entity recognition is an important part of structuring chinese electronic_medical_records and construction of medical_knowledge graph',\n",
       "  'the experimental_results prove that the model proposed in this paper has achieved excellent results in the task of named_entity recognition in the field of historical culture',\n",
       "  'paper_title : towards bootstrapping biomedical named_entity recognition using reinforcement_learning ; paper_abstract : named_entity recognition is one of the most fundamental problems in knowledge_graph',\n",
       "  'paper_title : chinese named_entity recognition method in history and culture field based on bert ; paper_abstract : with rapid development of the internet , people have undergone tremendous changes in the way they obtain information',\n",
       "  'the experimental_results show that the roberta + flat compared with the traditional method has a better effect of entity_recognition .',\n",
       "  'paper_title : a multi_layer soft lattice based model for chinese clinical named_entity recognition ; paper_abstract : objective : named_entity recognition ( ner ) is a key and fundamental part of many medical and clinical tasks , including the establishment of a medical_knowledge graph , decision_making support , and question_answering systems',\n",
       "  'and a novel deep neural_network model , named ar+bilstm+crf , which combines attention_mechanism , ranger optimizer , bidirectional lstm , and crf , is proposed for named_entity recognition in adf',\n",
       "  'the average precision of entity_recognition reaches ( formula presented',\n",
       "  'finally , a comparative experiment was designed to verify the effectiveness of the proposed recognition',\n",
       "  'to fully utilize domain_specific features , we present a hybrid method consisting of dedicated rules and a machine_learning model for entity_recognition',\n",
       "  'in order to solve the problem of lack of labeled_data , we propose an end_to_end solution that is not based on domain_knowledge , which instead is based on the pre_trained bert_chinese model and integrates the bilstm_crf model for classical_chinese named_entity recognition',\n",
       "  'the accuracy of entity_recognition reached 87.27 %',\n",
       "  'bert deep_learning classification technology is used to build the sample classification model , the named_entity recognition model is trained based on the bi_lstm technology , as well as the cnc equipment fault corpus sample data is trained , recognized , and modeled',\n",
       "  'paper_title : named_entity recognition in human_nutrition and health domain using rule and bert_flat ; paper_abstract : a nutritious and healthy_diet can be widely expected to reduce the incidence of disease , while improving body health after the disease occurs',\n",
       "  'moreover , the body feature is extracted by convolution neural_network ( cnn )',\n",
       "  'higher recognition was achieved than before',\n",
       "  'paper_title : leveraging multi_source knowledge for chinese clinical named_entity recognition via relational graph_convolutional_network ; paper_abstract : objective : external_knowledge , such as lexicon of words in chinese and domain_knowledge graph ( kg ) of concepts , has been recently adopted to improve the performance of machine_learning methods for named_entity recognition ( ner ) as it can provide additional information beyond context',\n",
       "  'the attention_mechanism was proposed to combine auxiliary classification layer with main classification layer to improve the overall performance.finally , it was sent to conditional_random_field to construct an end_to_end deep_learning model framework suitable for veterinary drug name entity recognition.in the experiment , totally 10 643 sentences and 485 711 characters of veterinary drug text were selected to identify four kinds of entities : drug , adverse effect , intake mode , aimal',\n",
       "  'finally , the label with the highest probability is output through the conditional_random_field ( crf ) layer to obtain each characters category',\n",
       "  'paper_title : chinese drug information named_entity recognition based on mscnn and bert_bilstm_crf ; paper_abstract : in the study of drug_drug interactions , knowledge_graph on drugs plays an important role to extract drug features',\n",
       "  'considering the complexity and ambiguity of data , a named_entity recognition method based on bert in the health_preserving field is proposed',\n",
       "  'the entity_recognition part uses the bidirectional_long_short_term memory_convolutional neural networks_conditions random_field ( bilstm_cnn_crf ) model for entity_extraction',\n",
       "  'especially , it achieves up to 11.46 % improvement in mrr for entity prediction with up to 82 times speedup compared to the state_of_the_art baseline .',\n",
       "  'an ensemble model is constructed based on the hidden_markov_model , conditional_random_field ( crf ) algorithm , bidirectional_long_short_term memory ( bi_lstm ) , and bi_lstm_crf deep_learning network , completing the named_entity recognition of the reports',\n",
       "  'paper_title : bern2 : an advanced neural biomedical named_entity recognition and normalization tool ; paper_abstract : in biomedical natural_language processing , named_entity recognition ( ner ) and named_entity normalization ( nen ) are key tasks that enable the automatic extraction of biomedical entities ( e.g',\n",
       "  'the system is publicly available at github \\\\footnote { \\\\url { https : //github.com/wjn1996/mathematical_knowledge_entity_recognition }',\n",
       "  'in this article , we present bern2 ( advanced biomedical entity_recognition and normalization ) , a tool that improves the previous neural network_based ner tool by employing a multi_task ner model and neural network_based nen models to achieve much faster and more accurate inference',\n",
       "  'since the existing entity_recognition methods only focus on the features of character sequence , it isnt easy to achieve excellent recognition results in a professional and complex electrical knowledge corpus',\n",
       "  'paper_title : deep_learning with language_models improves named_entity recognition for pharmaconer ; paper_abstract : background : the recognition of pharmacological substances , compounds and proteins is essential for biomedical relation_extraction , knowledge_graph construction , drug_discovery , as well as medical question_answering',\n",
       "  'in this paper , we conduct our research for the named_entity recognition , which is an important step of knowledge_graph construction in adf',\n",
       "  'whats more , we present a dataset for chinese named_entity recognition , which contains four categories of entities and consists of 864 sentences from status reports',\n",
       "  'the finding can provide an effective entity_recognition in the field of human_nutrition and health .',\n",
       "  'paper_title : chinese named_entity recognition of geological news based on bert model ; paper_abstract : with the ongoing progress of geological survey work and the continuous accumulation of geological data , extracting accurate information from massive geological data has become increasingly difficult',\n",
       "  'current mainstream recognition methods require lots of manpower , which is time_consuming and laborious',\n",
       "  'firstly , the character and vocabulary information were stitched together and pre_trained in the bert model to improve the recognition ability of the model to entity categories',\n",
       "  'paper_title : a chinese named_entity recognition method based on ernie_bilstm_crf for food_safety domain ; paper_abstract : food_safety is closely related to human health',\n",
       "  'finally , a conditional_random_field ( crf ) is used to realize character_level sequence annotation and then realize the named_entity recognition task of chinese rice variety information',\n",
       "  'furthermore , a large_scale annotated dataset for a chinese named_entity recognition ( ner ) task is established , which provides support for research on chinese ner tasks .',\n",
       "  'correspondingly , the named_entity recognition model in the field of nutrition and health using fusion rules and the bert_flat model presented an accuracy_rate of 95.00 % , a recall_rate of 88.88 % , and an f1_score of 91.81 %',\n",
       "  'however , the traditional named_entity recognition method has certain defects , and it is easy to ignore the association between entities',\n",
       "  'paper_title : recent progress of named_entity recognition over the most popular datasets ; paper_abstract : named_entity recognition ( ner ) has been considered as an initial step for many applications and tasks such as information_retrieval and extraction , question_answering , topic modelling , open_information_extraction , knowledge_graph construction , and so forth',\n",
       "  'the model achieved good result in the agricultural corpus , and the recognition precision , recall , and f_score were respectively 93.48 % , 90.60 % and 92.01 %',\n",
       "  'in order to improve the effect of entity_recognition , this paper proposes entity_recognition based on the bilstm_lpt and bilstm_hanlp models',\n",
       "  'this paper proposes a method for chinese named_entity recognition based on character sequence and word sequence to solve this problem',\n",
       "  'the traditional named_entity recognition ( ner ) model mainly includes hmm , crf , bilstm , bilstm_crf , etc',\n",
       "  'due to the scarcity of labeled ane data , some existing open agricultural entity_recognition models rely on manual features , can reduce the accuracy of entity_recognition',\n",
       "  'the albert_bigru_attention model was based on the results of the named_entity recognition model',\n",
       "  'from a quantitative point of view , we evaluate contextminer as a pre_processing step to perform named_entity recognition ( ner )',\n",
       "  'however , the low recognition accuracy of named_entities has posed a great challenge to the diagnosis and treatment of aquatic_animal diseases',\n",
       "  'the experimental_results show that the recognition accuracy of the embert_bilstm_crf model for the four types of entities was 94.97 % , and the f1_score was 95.93 %',\n",
       "  'the neural cnn_blstm_crf model and the chinese word_segmentation model with dictionary knowledge are jointly trained to build an unified named_entity recognition model which shares the entity types and confidence and changes the computing order from serial to parallel to decrease the error accumulation',\n",
       "  'the nested named_entity recognition based on the attention_mechanism and the weight sliding balance strategy can effectively identify entity types with higher nesting rates',\n",
       "  'in this study , a novel chinese named_entity recognition of agriculture was proposed using embert_bilstm_crf model',\n",
       "  'paper_title : named_entity recognition of agricultural based entity_level masking bert and bilstm_crf ; paper_abstract : an intelligent question_answering of agricultural knowledge can be one of the most important parts of information agriculture',\n",
       "  'compared with the existing named_entity identification methods , the precision rate , recall_rate , and f1 value have been significantly improved .',\n",
       "  'bi_gat_crf also solves the problem of word ambiguity and improves the accuracy of named_entity recognition tasks',\n",
       "  'paper_title : bert_based named_entity recognition in chinese twenty_four histories ; paper_abstract : named_entity recognition in classical_chinese plays a fundamental role in improving the ability of information_extraction and constructing knowledge_graphs from classical_chinese',\n",
       "  'the recognition effect of this model is better than that of lstm_crf , bert_lstm_crf , bert_crf and other models , and the f1=93.81 % .',\n",
       "  'based on these methods this paper proposes a bert_based named_entities identification model bert_bilstm_crf and it is outperforming the established methods',\n",
       "  'for the widely distributed entities , in this paper , we propose an end_to_end named_entity extraction framework , which uses popular deep_learning based approach , known as conditional_random_field ( crf ) , bidirectional_long short_term memory ( bi_ lstm+crf ) and bert+bi_lstm+crf for training and testing the named_entities',\n",
       "  'paper_title : deep_learning_based named_entity recognition and knowledge_graph for accidents of commercial_bank ; paper_abstract : with the diversified development of business , the construction of the banking system has become increasingly complex , which is prone to accidents',\n",
       "  'paper_title : an entity_recognition model based on deep_learning fusion of text feature ; paper_abstract : nowadays a large amount of knowledge has been born on the internet and the way of constructing knowledge_graph is not uniform',\n",
       "  'paper_title : a supervised named_entity recognition method based on pattern_matching and semantic verification ; paper_abstract : named_entity recognition is a basic task in the field of natural_language processing and plays a pivotal role in tasks such as information_extraction , machine_translation , and knowledge_graph construction',\n",
       "  'the experimental_results show that the multi_feature embedding algorithm and local features extracted by cnn can effectively_improve the recognition effect of the entity_recognition model .',\n",
       "  \"compared with other comparison models , the recognition effect of the bilstm_crf model is relatively stable , which to a certain extent verifies that the model has better recognition performance in the task of identifying grape diseases and pests ' entities .\",\n",
       "  'based on the knowledge_enhanced representations of tokens , we deploy a conditional_random_field ( crf ) layer for named_entity label prediction',\n",
       "  'we propose an improved model named boundary detection and category prediction ( bdcp ) for nested ner',\n",
       "  'so , this paper proposes a chinese rice variety information named_entity recognition method based on a bidirectional_long_short_term memory network and conditional_random_field ( bilstm_crf ) , which combines radical features , word_segmentation boundary features , and multi_head attention_mechanism',\n",
       "  'paper_title : named_entity recognition of chinese agricultural text based on attention_mechanism ; paper_abstract : agricultural named_entity recognition is a fundamental tasks for natural_language processing in the agricultural field',\n",
       "  'first , a bidirectional_long_short_term memory neural_network ( bilstm ) was used for two_class entity_identification',\n",
       "  'based on the data_set of this evaluation task , pre_trained_language model based entity_recognition approaches are studied first , select the bilstm_crf model based on random initialized word_embedding as the baseline system ; secondly , apply wordlvec to the baseline system ; thirdly , apply elmo to the baseline system',\n",
       "  'this paper proposes a method of weakly supervised_learning to recognize the complex named_entities ( commonly composed of multiple small entity sequences , hereinafter referred to as cnes ) in the corpus , which makes it difficult to determine the boundaries of such entities',\n",
       "  'paper_title : a novel named_entity recognition algorithm for hot strip rolling based on bert_imseq2seq_crf model ; paper_abstract : named_entity recognition is not only the first step of text information_extraction , but also the key process of constructing domain_knowledge graphs',\n",
       "  'after that , the bilstm+crf model was used for the outer entity_recognition to improve the discrimination of outer entities for the accurate recognition of outer entities',\n",
       "  'in order to build a knowledge_graph of health_preserving field , named_entity recognition is required first',\n",
       "  'secondly , large_scale pretraining language_model , bert was used for agriculture named_entity recognition and provided a pretty well initial parameters containing a lot of basic language knowledge',\n",
       "  'named_entity recognition ( ner ) is a fundamental task for building knowledge_graph',\n",
       "  'in view of the large amount of text data , complex process flow and urgent application needs in the hot strip rolling process , a novel named_entity recognition algorithm based on bert_imseq2seq_crf model is proposed in this paper',\n",
       "  'in order to improve the entity_recognition effect and address the problem of polysemy in nuclear_technology knowledge data_set , an improved nuclear_technology entity_recognition method based on the bert_bilstm_crf combination model was proposed by comparative experiments',\n",
       "  'paper_title : an effective deep_learning method with multi_feature and attention_mechanism for recognition of chinese rice variety information ; paper_abstract : in the process of chinese rice variety information named_entity recognition , traditional methods can not extract potential semantic_information from data and can not capture long_distance dependence',\n",
       "  'the experimental_results showed that our model achieved an f1 of 98.31 % with 4.23 % relative improvement compared to the baseline model ( i.e. , word2vec_based bilstm_crf ) on the self_annotated corpus named chinese named_entity recognition dataset for agricultural diseases and pests ( agcner )',\n",
       "  'this study aims to develop a multi_feature entity_recognition model that considers the differences in text features across different fields',\n",
       "  'this research can not only provide arelatively high entity_recognition accuracy for tasks such as agricultural intelligence question_answering , but also offer new ideas for the identification of chinese named_entities in fishery , animal_husbandry , chinese_medical , and biological fields .',\n",
       "  'a multi_neural network collaboration approach is then developed based on a named_entity recognition model',\n",
       "  'the ccks2019 conference organized a medical named_entity recognition evaluation task to extract six types of medical entities from unstructured chinese electronic_medical_records',\n",
       "  'the framework achieves good results in the field of nuclear_power plant maintenance reports , and the methods for domain pre_trained model and lstm_based flat named_entity recognition have been successfully applied to practical tasks .',\n",
       "  'the training goal for gn is to deceive dn to make wrong classification',\n",
       "  'we first train a convolutional_neural_networks _ long short_term_memory networks ( cnn_lstm ) model to generate a template caption based on the input image',\n",
       "  'named_entity recognition is a crucial step to construct a knowledge_graph',\n",
       "  'the modeling is done using a recurrent neural_network or a factorization machine',\n",
       "  'most existing state_of_the_art_methods are primarily based on convolutional_neural_networks ( cnn ) or long_short_term_memory networks ( lstm )',\n",
       "  'to extract useful information from multi_source data , we adopt the pre_trained bert_crf model to conduct named_entity recognition for incidents records',\n",
       "  'the experimental that results show compared with lattice lstm , id_cnn+crf , crf models , the f1_score of the proposed model reaches 86.86 % on tourism datasets and 95.02 % on resume datasets , displaying high recognition accuracy of the model.02 % on resume datasets , displaying high recognition accuracy of the model.02 % on resume datasets , displaying high recognition accuracy of the model .',\n",
       "  'to identify the entities from these datasets , a rule_based and deep_learning hybrid method is proposed',\n",
       "  'paper_title : recognition of animal drug pathogenicity named_entity based on att_aux_bert_bilstm_crf ; paper_abstract : in order to solve the problems that traditional methods of veterinary drug named_entity recognition rely on artificial design features , which is time_consuming and labor_consuming , and the amount of veterinary drug pathogenic corpus data is less in the process of building veterinary drug pathogenic knowledge_graph , a method based on att_aux_bert_bilstm_crf of veterinary drug text named_entity recognition model was proposed , which combined bert_bilstm_crf models by introducing attention_mechanism and auxiliary classification layer.the text was vectorized by the bert preprocessing model , and then connected to bi_directional long_short term memory network.the auxiliary classification mechanism was introduced , the output of the bert layer was used as the auxiliary classification layer , and the output of the bilstm layer was used as the main classification layer',\n",
       "  'in bert_crf architecture , begin , internal and other ( bio ) labeling rule was used to label the sequence , and the concatenation of character vector and position vector was used as inputs',\n",
       "  'to test the effectiveness of our system , we perform both automatic and manual evaluation of our intent_classifier and slot_filling system on three dialog datasets',\n",
       "  'bi_gat_crf aims to solve the problem of unclear entity boundary recognition and word ambiguity in named_entity recognition tasks',\n",
       "  'the deep_learning method is adopted in this paper and is based on the existing hazop data to recognize the named_entity of hazop text',\n",
       "  'paper_title : named_entity recognition for instructions of chinese_medicine based on pre_trained_language model ; paper_abstract : named_entity recognition ( ner ) of chinese_medicine text is a basic task of constructing medical and health knowledge_graph',\n",
       "  'experiments on two publicly available task oriented dialog datasets show that our proposed fg2seq achieves robust performance on generating appropriate system responses and outperforms the baseline systems .',\n",
       "  \"the well_designed memory component can get rid of the pre_training so that the model does n't depend on the given target entity for training\",\n",
       "  'the bert model combined with the word fusion performed the best , compared with that without the bert model , indicating an effective recognition performance',\n",
       "  'named_entity recognition is a key to construct knowledge_graph',\n",
       "  'paper_title : chinese named_entity recognition method based on albert_bgru_crf ; paper_abstract : named_entity recognition ( ner ) is an important basis for upper_level natural_language processing tasks such as knowledge_graph constructionsearch enginesand recommendation systems.chinese ner labels and classifies proper_nouns or specific named_entities in a text sequence.aiming at the problem that the existing chinese ner methods can not effectively extract long_distance semantic_information and solve the problem of polysemythis study proposes a chinese ner method based on albert pre_training language modelbidirectional gated recurrent_unit ( bgru ) and conditional_random_field ( crf ) called albert_bgru_crf model.firstthe albert pre_trained_language model performs word_embedding on the input text to obtain dynamic word vectorswhich can effectively solve the polysemy problem.secondbgru extracts contextual semantic_features to further understand semantics and obtain semantic_features between long_distance words',\n",
       "  'these models are tested on real medical records , and the experimental_results show that the method can effectively identify the entities , and has certain practical value .',\n",
       "  'the recognition accuracy , recall , and f1 value increased by 12.31 , 12.76 , and 12.53 percentage points , respectively',\n",
       "  \"the generated word_vectors were then inputted into the model , which is composed of traditional bidirectional_long_short_term memory neural_networks and conditional_random_field machine_learning algorithms for the named_entity recognition of clinical notes to improve the model 's effectiveness\",\n",
       "  'the recognition accuracy for 202 documents selected by experts was 79 % for the coincidence of names and 37 % for the coincidence of term identifiers',\n",
       "  'the experimental_results verify the effectiveness of the bertala and knowledge distillation in agricultural entity_recognition .',\n",
       "  'moreover , the current end_to_end neural models trained on small crowd_sourcing datasets ( e.g. , 10k dialogs in the redial dataset ) tend to overfit and have poor chit_chat ability',\n",
       "  'it has been verified that the recognition rate of the model basically meets the requirements , and on this basis , a recommended solution for repairing equipment faults is proposed , which realizes the effective use of fault knowledge .',\n",
       "  'paper_title : bdcp : an improved nested named_entity recognition model based on lstm ; paper_abstract : in construction of knowledge_graphs ( kgs ) , named_entity recognition ( ner ) is a sub_task to identify the boundaries of entities with special meaning and predict their categories in texts',\n",
       "  'under the framework of positive unlabeled learning , the algorithm performs entity_extraction through two stages of entity determination and entity_classification',\n",
       "  'these problems make the recognition of chinese named_entities for online texts more challenging',\n",
       "  'in this study , a diagnosis and treatment of aquatic_animal diseases named_entity recognition was proposed using bert+cabilstm+crf ( bidirectional_encoder_representations_from_transformers+cascade_bi_directional long_short_term_memory+conditional_random_field )',\n",
       "  'in addition , this paper utilizes svm ( support_vector_machines ) to compare the effect of question classification , and utilizes bilstm_crf model to compare the effect of feature words recognition',\n",
       "  'excessive experimental_results show that the kaercnn model proposed in this study is significantly better than traditional machine_learning algorithms in terms of classification accuracy , the f1_score , and practical application effects',\n",
       "  'finally , the results of bert model was compared with the convolutional_neural_network and the piecewise convolutional_network model',\n",
       "  'to overcome the scarcity of labeled named agricultural entity data , weakly named_entity recognition label on agricultural texts crawled from the internet was built with the help of agrikg',\n",
       "  'military named_entity recognition faces some unique challenges not seen in searches for named_entities in other domains , such as military named_entity boundaries being vague and difficult to define , lack of standardized military terms in internet media , extensive use of abbreviations , and the lack of a public military_oriented corpus',\n",
       "  'the results_showed that the model can effectively identify the entities in the veterinary drug pathogenic text , and the f1 value of recognition was 96.7 % .',\n",
       "  'then , neural_networks are used for learning multi_position feature vectors and character_based tagging task',\n",
       "  'based on bert_ala model , bidirectional lstm ( bilstm ) and conditional_random_field ( crf ) were coupled to further improve the recognition precision , giving a bert_ala+bilstm+crf model',\n",
       "  'this is due to the advances in neural_network architectures , increase of computing power and the availability of diverse labeled datasets , which deliver pre_trained , highly accurate models',\n",
       "  'the model performs a multi_scale convolution after the character vector are generated by the bert model , and then to be input to the bi_lstm',\n",
       "  'agriculture named_entity recognition plays a key role in automatic q & a system , which helps obtaining information , understanding agriculture questions and providing answer from the knowledge_graph',\n",
       "  'to solve the above problems and tackle the cner_adp task , a novel chinese named_entity recognition method for agricultural diseases and pests via jointly using radical_embedding and self_attention ( rs_adp ) was proposed',\n",
       "  'firstly , the vietnamese person names , location names , and institution names in vietnamese corpus are collected statistically to build a corresponding entity database to assist the vietnamese named_entity recognition',\n",
       "  'the result showed that the bert_crf model was superior to the others and reported a state_of_the_art_performance',\n",
       "  'furthermore , the chinese named_entity recognition can be confined to the location and semantic_information of characters , due to the long length of agricultural entity and complex naming',\n",
       "  'on the experimental side , we found promising results when the aco_kg model has combined with machine_learning classifiers , namely an increase of 3 % in the classification task .',\n",
       "  'due to scarcity of public dataset for important military targets and many types of entities , albert was used as the generation model of distributed character vectors based on transfer_learning',\n",
       "  'we also fine_tune the proposed masked entity dialogue ( med ) model on smaller corpora which contain dialogues focusing only on the covid_19 disease named as the covid dataset',\n",
       "  'in the first part , bert_bigru_crf model is used to complete the named_entity recognition of power information text',\n",
       "  'finally , to perform personality predictions the resulting embedding matrix was fed to four suggested deep_learning models independently , which are based on convolutional_neural_network ( cnn ) , simple recurrent neural_network ( rnn ) , long short_term_memory ( lstm ) and bidirectional long short_term_memory ( bilstm )',\n",
       "  'paper_title : named_entity recognition for smart_grid operation and inspection domain using attention_mechanism ; paper_abstract : with the rapid development of smart power_grid systems , the field of power_grid operation_and_maintenance is in urgent need of intelligent construction',\n",
       "  'possible intelligent applications based on openkg_covid19 for further development are also described',\n",
       "  'however , the general domain named_entity recognition model can not extract the entities in the network_security domain very well',\n",
       "  'regarding entity_recognition , a bilstm model was used to extract relationships between entities',\n",
       "  'in this work , an approach of model distillation was proposed to recognize agricultural named_entity data',\n",
       "  'experiment results show that the proposed novel model achieves remarkable results for stock_market prediction task .',\n",
       "  'it is urgent to improve the efficiency of breast_cancer diagnosis and treatment through artificial_intelligence technology and improve the postoperative health status of breast_cancer patients',\n",
       "  'many of the existing_methods neglected the role and value of other modal data except images and only relies on low_level image features for disease recognition without utilizing high_level domain_knowledge , leading to poor credibility and interpretability of identification results',\n",
       "  'to explore the named_entity recognition ( ner ) in the field of knowledge_graph construction , the vietnamese grammar and word_formation are analysed deeply in this study , aiming to solve the low recognition precision and low network calculation efficiency in vietnamese named_entity recognition',\n",
       "  'breast_cancer is one of the highest incidences of cancer at present',\n",
       "  'paper_title : crop disease identification and interpretation method based on multimodal deep_learning ; paper_abstract : identification methods of crop diseases based on image modality alone have achieved relative success under limited and restricted conditions',\n",
       "  'it is an important branch in the field of artificial_intelligence',\n",
       "  'experiments on the cosmosqa dataset demonstrate that the proposed cegi model_outperforms the current state_ofthe_ art approaches and achieves the highest accuracy ( 83.6 % ) on the leaderboard .',\n",
       "  'using the multi_head attention_mechanism combined with the bidirectional long and short_term memory network and crf , the accuracy of the neural_network model is further improved',\n",
       "  'therefore , named_entity recognition technology is used to extract named_entities related to food_safety , and building a regulatory knowledge_graph in the field of food_safety can help relevant authorities to regulate food_safety issues and mitigate the hazards caused by food_safety problems',\n",
       "  'given the success of deep_learning and pre_trained_language_models ( lms ) , some lm_based methods are proposed for the kgc task',\n",
       "  'experiments on the cosmosqa dataset demonstrate that the proposed cegi model_outperforms the current state_of_the_art approaches and achieves the accuracy ( 83.6 % ) on the leaderboard .',\n",
       "  'then build a multi_task_learning model , and perform question sentence intent recognition and question sentence entity_recognition at the same time , which can learn the relationship between tasks , improve the effect of the two types of tasks , and shorten the training and inference time , which can effectively reduce training cost',\n",
       "  'paper_title : named_entity recognition in xlnet cyberspace security domain based on dictionary embedding ; paper_abstract : with the increase of network_security incidents , network_security analysts need to analyze massive log information',\n",
       "  'in order to mine the entity information of defect records more efficiently and accurately , this paper proposes a named_entity recognition algorithm to solve the current situation of lack of utilization and analysis of defect records of traction power supply equipment and low efficiency of manual information_processing modes',\n",
       "  'the experimental_results show that the proposed method_achieves better classification results on three open datasets .',\n",
       "  'first , a bert pre_trained_language model is used to encode a single character to obtain a vector representation corresponding to each character',\n",
       "  'methods : inspired by the success of deep_learning with language_models , we compare and explore various representative bert models to promote the development of the pharmaconer task',\n",
       "  'although kg_based approaches prove effective , two issues remain to be solved',\n",
       "  'the experimental_results show that our model is more effective than the current state_of_the_art model ( ccm ) .',\n",
       "  'our experiments demonstrate that the proposed poisoning attacks outperform state_of_art baselines on four kge models for two publicly available datasets',\n",
       "  'therefore , the model can be expected to effectively_improve the accuracy of entity_recognition caused by ambiguity and entity nesting in the task of diagnosis and treatment of aquatic_animal diseases named_entity recognition',\n",
       "  'first , saliency_based attention_mechanism is used in our model , the salient visual_objects and visual features are extracted by using faster region_based convolutional_neural_network ( faster r_cnn )',\n",
       "  'pharmaconer is a named_entity recognition challenge to recognize pharmacological entities from spanish texts',\n",
       "  'entity and relation are the cornerstones of skg ; thus , the task of skg_learning is divided into named_entity recognition and relation_extraction',\n",
       "  'paper_title : multi_feature fusion method for chinese pesticide named_entity recognition ; paper_abstract : chinese pesticide named_entity recognition ( ner ) aims to identify named_entities related to pesticide properties from unstructured chinese pesticide information texts',\n",
       "  'moreover , the transfer matrix of the conditional_random_field ( crf ) algorithm is combined to improve the accuracy of sequence labeling',\n",
       "  'with the increasing computational power , current protein language_models pre_trained with millions of diverse sequences can advance the parameter scale from million_level to billion_level and achieve remarkable improvement',\n",
       "  'compared to existing_methods , mikgi attained the most robust performance with accuracy the highest or near the highest across all tasks',\n",
       "  'finally , crf is used to output the predicted tag sequence',\n",
       "  'paper_title : hierarchical transformer model for scientific named_entity recognition ; paper_abstract : the task of named_entity recognition ( ner ) is an important component of many natural_language processing systems , such as relation_extraction and knowledge_graph construction',\n",
       "  'experimental_results on the dialogre dataset prove that our proposed model bert_mg outperforms the sota baselines .',\n",
       "  'firstly , use the bilstm+crf model of neural_network to realize ner in military field',\n",
       "  'experimental_results have proved that the proposed classifier performs better compared to existing systems .',\n",
       "  'to address this issue , we present a named_entity recognition based framework that accurately extracts covid_19 related information from clinical test results articles , and generates an efficient and interactive visual knowledge_graph',\n",
       "  'compared with traditional machine_learning methods , e.g',\n",
       "  'considering that the task of agriculture named_entity recognition relied heavily on low_end semantic_features but slightly on high_end semantic_features , an attention_based layer aggregation mechanism for bert ( bert_ala ) was designed in this research',\n",
       "  'compared with the four traditional machine_learning algorithms , the accuracy_rate is increased by about 14 % on average , and the f1_score is increased by about 13 %',\n",
       "  'paper_title : semi_supervised geological disasters named_entity recognition using few labeled_data ; paper_abstract : the geological disasters named_entity recognition ( ner ) method aims to recognize entities reflecting disaster event information in unstructured texts to construct a geohazard knowledge_graph that can provide a reference for disaster emergency response',\n",
       "  'the experimental_results show that our model is able to achieve a superior_performance than these existing_methods .',\n",
       "  'to tackle it , in this study , the authors aim to predict a new entity given few reference instances , even only one training instance',\n",
       "  'experiments show that the intelligent question_answering system with the multi_question classification is effective and highly accurate in answering questions in the field of weapon .',\n",
       "  'experimental_results were validated using the hit @ 10 rate entity prediction task',\n",
       "  'then , the bidirectional_long_short_term memory ( bilstm ) and bidirectional_encoder_representations_from_transformers ( bert ) algorithms are adopted to build the named_entity recognition ( ner ) model',\n",
       "  'experiments on forest disease attribute text_corpus show that the precison is 93.15 % , which can be effectively applied to the named_entity recognition task of forest disease text',\n",
       "  'in this paper , our goal is to be able to infer the correct entity given a few training instances , or even only one training instance is available',\n",
       "  'bi_gat_crf can effectively determine entity boundaries and recognize named_entity categories',\n",
       "  'we verify the novelty and accuracy of ep_bot through the experiments .',\n",
       "  'more specifically , we first construct representations for all images of an entity with a neural image encoder',\n",
       "  'paper_title : agricultural named_entity recognition based on semantic aggregation and model distillation ; paper_abstract : with the development of smart agriculture , automatic question and answer ( q & a ) of agricultural knowledge is needed to improve the efficiency of agricultural information acquisition',\n",
       "  'among them , named_entity recognition has been a key technology for intelligent question_answering and knowledge_graph construction in the fields of agricultural domain',\n",
       "  \"bi_lstm improved bert 's insufficient learning ability of the relative position feature , while conditional_random_field models the dependencies of entity_recognition label\",\n",
       "  'paper_title : named_entity recognition in electric_power metering domain based on attention_mechanism ; paper_abstract : named_entity recognition ( ner ) is one key step for constructing power domain_knowledge graph which is increasingly urgent in building smart_grid',\n",
       "  'the methodology is used to create accurately labeled training and test datasets , which are then used to train models for custom entity labeling tasks , centered on the pharmaceutical domain',\n",
       "  'our results show that zsl_kg improves over existing wordnet_based methods on five out of six zero_shot benchmark_datasets in language and vision .',\n",
       "  'but in view of the outstanding nested structure of entities , the model performed better to identify the nested named_entities , such as the clinical symptoms using the named_entity recognition model integrating the bert and cabilstm designed by the hierarchical idea',\n",
       "  'ner ( named_entity recognition ) is the upstream task of knowledge_graph construction , and the quality of the ner model determines the quality of the knowledge_graph to a certain extent',\n",
       "  'paper_title : named_entity recognition in aircraft design field based on deep_learning ; paper_abstract : aircraft design is a kind of knowledge_intensive work involving multi_disciplinary integration , which needs the support of a large amount of knowledge on aircraft design field ( adf )',\n",
       "  'conventional approaches to kgr have achieved promising performance but still have some drawbacks',\n",
       "  'the question analysis module includes named_entity recognition , similarity calculation and question classification',\n",
       "  'we design a branch architecture consisting of the main branch for hoi detection and a supplementary branch for scene recognition',\n",
       "  'research into using kgs for intelligent applications has increased significantly',\n",
       "  'the source_code will be available at https : //github.com/zcy_cqut/macr .',\n",
       "  'paper_title : chinese named_entity recognition in electric_power metering domain based on neural joint learning ; paper_abstract : while the business of electric_power metering is expanding , it is urgent to build an electric_power metering knowledge_graph composed of business information , technical knowledge , industry standards and their internal connections to provide more comprehensive and effective support for the decision_making and development of power_grid',\n",
       "  'the preliminary results suggest that the method_achieves high performance and can help tcm doctors make better diagnosis decisions in practice',\n",
       "  'the accuracy_rate of the proposed approach reaches 95.54 % , and the f1_score reaches 0.901',\n",
       "  'experimental_results_demonstrate that our proposed model_outperforms state_of_the_art models .',\n",
       "  'second , adversarial_training was also introduced to enhance the generalization and robustness in terms of identifying the rare entities',\n",
       "  'furthermore , some promising applications of medsim in drug substitution and drug abuse prevention are presented in case study .',\n",
       "  'however , it remains largely an open_problem how to effectively utilize large and noisy biomedical kg for ddi detection',\n",
       "  'at the same time , compared with the control experiment using only character sequence fea_tures , the f1_score is increased by 2.96 % , and the precision is increased by 4.65 % , which proves the effectiveness of the method proposed in this paper .',\n",
       "  'most of the existing_approaches ignore kgs altogether',\n",
       "  'to alleviate the over_smoothing in high_order chebyshev approximation , a multi_vote based cross_attention ( mvcattn ) with linear computation complexity is also proposed',\n",
       "  'deep_learning have intensively promoted computer vision and natural_language processing ( nlp ) both',\n",
       "  'in order to solve the problem of fast and accurate identification of the named_entities which entity types were pre_defined , a bidirectional encoder representations from transformers_conditional random_field ( bert_crf ) architecture was proposed to solve the task of named_entity recognition ( ner ) in the area of fresh egg supply_chain',\n",
       "  'this intelligent system , which has high value for the treatment of internal_medicine diseases , can effectively solve health issues and reduce the cost of the consultation .',\n",
       "  'this paper proposes an entity_recognition method based on a joint learning model which considers the feature of chinese word_segmentation and ideas of multi_task_learning in the electric_power metering domain',\n",
       "  'to assist the health sector in combating this deadly disease , the authors developed a deep_learning strategy for diabetes named_entity extraction based on a fusion of text characteristic and relationship_extraction utilizing text data as the object',\n",
       "  'it significantly_outperforms the existing_methods and achieves the state_of_the_art_performance .',\n",
       "  'experimental_results show that our proposed system can achieve better performance than other models , and possess great interactivity and accuracy',\n",
       "  'to address the above problem , this paper proposes a deep_learning_based ner model ; namely , the deep , multi_branch bigru_crf model , which combines a multi_branch bidirectional gated recurrent_unit ( bigru ) layer and a conditional_random_field ( crf ) model',\n",
       "  'furthermore , to accommodate the features of both the image and text modalities , we employ a step_by_step training strategy to train the proposed neural_network model',\n",
       "  'paper_title : named_entity recognition of fresh egg supply_chain based on bert_crf architecture ; paper_abstract : recognizing named_entities from raw text is the first step to construct a fresh egg supply_chain knowledge_graph and support a variety of downstream natural_language processing tasks',\n",
       "  'recently , deep neural network_based models have attained very good results in ner',\n",
       "  'the experimental_results showed that the proposed model could effectively recognize the named_entities of agricultural pests and diseases without feature engineering',\n",
       "  'the results show that the entity information recognition algorithm proposed in this paper can effectively and accurately mine the defect record information of traction power supply equipment .',\n",
       "  'results : compared with the traditional word2vec word_vector model , the performance of the bert pre_training model to obtain a word_vector as model input was significantly improved',\n",
       "  'the results demonstrate that our approach achieves state_of_the_art_performance and outperforms many of the existing_approaches .',\n",
       "  'experiments show that the ctd_blstm model obtains higher accuracy and recall_rate than blstm in the chinese_medical named_entity recognition and entity_relationship extraction',\n",
       "  'a variety of artificial_intelligence application products continue to appear , playing various important roles in many fields , such as ai + agriculture , ai + medical , ai + autonomous driving , ai + education , etc',\n",
       "  'in the bilstm model , the context feature of the target entity was learned from the bert output',\n",
       "  \"we aim to develop a model that will be able to give a classification of the disease that the patient might be suffering from after inputting the patient 's symptoms in english or their native language using voice commands or text\",\n",
       "  'the nlp model identified eight types of entities with a recognition accuracy of up to 94.22 %',\n",
       "  'paper_title : panner : pos_aware nested named_entity recognition through heterogeneous graph neural_network ; paper_abstract : nested named_entity recognition ( nested ner ) in knowledge_graph ( kg ) aims at obtaining all meaningful entities , including nested entities for sentences in longer text region',\n",
       "  'emotion recognition is part of affective_computing , which aims to recognize how the person feels , such as happy , sad , anger , disgust , fear , surprise',\n",
       "  'the ckgr successfully narrows the gap between humans and machines',\n",
       "  'aiming at the problem of context memory , a bidirectional gru neural_network is used to fuse the input vectors',\n",
       "  'to alleviate the over_smoothing in high_order chebyshev approximation , a multi_vote_based cross_attention ( mvcattn ) with linear computation complexity is also proposed',\n",
       "  'according to the experiment results , the accuracy_rate , recall_rate , and f1_score of the bilstm_idcnn_crf model in the chinese pesticide data_set were 78.59 % , 68.71 % , and 73.32 % , respectively , which are significantly better than other compared models',\n",
       "  'results : the experimental_results show that deep_learning with language_models can effectively_improve model performance on the pharmaconer dataset',\n",
       "  'these reviewed results could be used to further improve the machine_learning models',\n",
       "  'by further fine_tuning with image_report pairs , kgae consistently outperforms the current state_of_the_art models on two datasets .',\n",
       "  'which compared with the models based on bilstm_crf and bert_bilstm_crf , the recognition performance of embert_bilstm_crf is significantly improved , proved that used pre_trained_language model as the a word_embedding layer can represent the characteristics of characters well and the entity_level masking strategy can alleviate the bias caused by incomplete semantics , thereby enhanced the chinese semantic representation ability of the model , so that enabling the model to more accurately identify chinese agricultural named_entities',\n",
       "  'experiments on five benchmarks show that kg_s2s outperforms many competitive baselines , setting new state_of_the_art_performance',\n",
       "  'the experimental_results of the example are great , which show that the identification method proposed in this paper has theoretical value and practical application value .',\n",
       "  'to improve the recognition accuracy , our method masked_bilstm_crf is proposed to separate the context semantic relationship determination from the entity boundary confirmation',\n",
       "  'in recent_years , deep_learning technologies have developed rapidly such as deep neural_network , attention_mechanism , deep reinforcement_learning and so on',\n",
       "  'the performance of selfkg suggests that self_supervised learning offers great potential for entity_alignment in kgs',\n",
       "  'in order to make better use of these data resources , this paper introduces a process and method to build a knowledge_graph of spleen and stomach diseases in tcm , and takes the emr of tcm for spleen and stomach diseases and related literature inscriptions as data_sources , and selects four commonly used named_entity recognition ( ner ) models for comparative experiments of ner',\n",
       "  'paper_title : chinese named_entity recognition method in electricity based on combining_character sequence and word sequence ; paper_abstract : chinese named_entity recognition in the power field is critical in building a high_quality knowledge_graph of power_equipment fault',\n",
       "  'the source_code is available at https : //github.com/thudm/cogkr .',\n",
       "  'traditional entity_recognition methods of diseases and insect pests highly rely on artificial design features',\n",
       "  'our method_achieves better performance of relationship_extraction and entity name recognition , which helps to construct the knowledge_graph more accurately .',\n",
       "  'we trained custom named_entity recognition ( ner ) model and constructed a cyber_security knowledge_graph ( ckg ) to infer the subjective relevance of the cyber_security text to the user and to generate correlation features',\n",
       "  'the higher quality and the more accurate recognition of the model were achieved , as the deep_learning model was trained to learn more data',\n",
       "  'this model uses bert to fine_tuning character embedding through contextual_information , the problem of polysemy is solved and the performance of entity_recognition of chinese_tea text is improved',\n",
       "  'the model is fine_tuned',\n",
       "  'and the answer accuracy of the question_answering system can reach about 81 % .',\n",
       "  'the improved model is used in chinese named_entity recognition and entity_relationship extraction in the chinese_medical field , named co_training double word_embedding conditioned blstm ( ctd_blstm )',\n",
       "  'military named_entity recognition is a basic , key task for information_extraction , question_answering and knowledge_graphs in the military domain',\n",
       "  'to extract entities from a large amount of historical and cultural information more accurately and efficiently , this paper proposes one named_entity recognition model combining bidirectional_encoder_representations_from_transformers and bidirectional_long_short_term memory_conditional random_field ( bert_bilstm_crf )',\n",
       "  'paper_title : fine grained named_entity recognition via seq2seq framework ; paper_abstract : fine_grained named_entity recognition ( ner ) is crucial to natural_language processing ( nlp ) applications like relation_extraction and knowledge_graph construction',\n",
       "  'paper_title : deep_learning_based named_entity recognition and knowledge_graph construction for geological hazards ; paper_abstract : constructing a knowledge_graph of geological hazards literature can facilitate the reuse of geological hazards literature and provide a reference for geological hazard governance',\n",
       "  \"diabetes is a severe disease that affect people 's health\",\n",
       "  'thus , the qa system based on kg is still faced with difficulties',\n",
       "  'the problem is formulated as a token classification task similar to named_entity extraction',\n",
       "  'the radicalsand drug names were mostly composed of chemical_elements , while the disease names were mostly ended with the word disease , indicating a higher recognition accuracy than that in the nested entities',\n",
       "  'the aim of bert_ala was to adaptively aggregate the output of multiple hidden layers of bert',\n",
       "  'the research method of this study is that we conducted a named_entity recognition task and identified the key relation_types to construct a cryptocurrency anti_money laundering knowledge_graph ( kg )',\n",
       "  'the experimental_results demonstrated that our model outperformed the state_of_the_art work , and achieved a 17.1 % improvement on the f1 values .',\n",
       "  'we propose an approach based on computer vision methods to recognize human_object interaction ( hoi )',\n",
       "  'therefore , these lstm_based models that have achieved high accuracy generally require long training times and extensive training_data , which has obstructed the adoption of lstm_based models in clinical scenarios with limited training time',\n",
       "  'the named_entity recognition task is one of the key tasks in the implementation of the knowledge_graph , which is of great significance for extracting entity information from unstructured_data , namely the hazardous chemical accidents records',\n",
       "  'like any other machine_learning model , these classifiers are very dependent on the size and quality of the training dataset',\n",
       "  'our code_base and the datasets used with detailed instructions for reproducibility is publicly hosted 1.1https : //github.com/mandar_sharma/tcube',\n",
       "  'results : our proposed method_achieves the best performance on ccks2017 and ccks2018 in chinese with f1_scores of 91.88 % and 89.91 % , respectively , significantly outperforming existing_methods',\n",
       "  'specifically , we develop the bag_of_entity ( boe ) loss and the infusion loss to better integrate kg with crs for generating more diverse and informative_responses',\n",
       "  'the segmented maximum pool layer of the pcnn model masked the word unit instead of characters when executing the masked_language model ( mlm )',\n",
       "  'it is also a high demand for the accurate identification of named_entities',\n",
       "  'finally , we completed a historical behavior driven question_answering platform to serve query for elderly',\n",
       "  'although considerable efforts have been made to recognize biomedical entities in english texts , to date , only few limited attempts were made to recognize them from biomedical texts in other languages',\n",
       "  'the pre_training language_model ( bert ) was used to obtain the global features of input sequence , and the crf layer was added at the end of the model to introduce hard constraints',\n",
       "  'paper_title : chinese mineral named_entity recognition based on bert model ; paper_abstract : mineral named_entity recognition ( mner ) is the extraction for the specific types of entities from unstructured chinese mineral text , which is a prerequisite for building a mineral knowledge_graph',\n",
       "  'the proposed model is largely divided into two modules which are synchronized during their training',\n",
       "  'on the other hand , traditional kgr methods , broadly categorized as symbolic and neural , are unable to balance both scalability and interpretability',\n",
       "  'extensive_experiments on the dataset redial show that our macr significantly_outperforms previous state_of_the_art approaches',\n",
       "  'its fusion model architecture is bert+bi_lstm+multi_head_self_attention+fc',\n",
       "  'the results show that our method outperforms all the state_of_the_art algorithms by filtering out wrong results and retaining correct ones .',\n",
       "  'our method_achieves higher performance than the baseline based on the exotic dataset .',\n",
       "  'an empirical evaluation demonstrates the effectiveness of dialokg over state_of_the_art_methods on several standard benchmark_datasets .',\n",
       "  'the results demonstrate that our model_outperforms several state_of_the_art baseline methods in terms of capability and accuracy',\n",
       "  'the experimental_results_demonstrate that the proposed model achieves significantly higher performance than previous models .',\n",
       "  'this study aimed to develop an effective method to identify and classify medical entities in the clinical notes relating to ra and use the entity_identification results in subsequent studies',\n",
       "  'our code and kgs will be made publicly available .',\n",
       "  'the focus is the error accumulation problem brought about by the traditional relational extraction_method of named_entity recognition based on rules or sequence labeling',\n",
       "  'furthermore , we show that the different architectures and training strategies lead to different model biases',\n",
       "  'the bert models can obtain competitive performance by using wordpiece to alleviate the out of vocabulary limitation',\n",
       "  'another challenge is that a deep model requires large_scale manually labelled data , which greatly increases manual_labour',\n",
       "  'in particular , our method surpasses the prior state_of_the_art by a large margin on the grailqa leaderboard',\n",
       "  'experimental_results show that the hit score of proposed approach is higher than that of many competitive state_of_the_art_baselines .',\n",
       "  'in recent_years , kgc methods for chinese have made great progress',\n",
       "  'then , we designed a qa system based on a memory_based neural_network and attention_mechanism',\n",
       "  'however , due to the lack of annotated data and the complexity of grammatical rules , named_entity recognition in classical_chinese has made little progress',\n",
       "  'first , the bert model is used to perform pre_training tasks on massive weaponry corpus',\n",
       "  'the former is used to recognize all candidate head entities and tail entities respectively',\n",
       "  'then , we introduce the latest research progress of deep_learning_based topic models in detail , which can be summed up as three different types of models',\n",
       "  'this can be solved by either fine_tuning the pre_trained_models , or by training custom models',\n",
       "  'we also show the effectiveness of each of the question_answering components in detail , including the query intent recognition and the answer generation .',\n",
       "  'the performance of bert on squad dataset shows that the accuracy of bert can be better than human users',\n",
       "  'the resulting dataset can be used for clinical diagnosis and further research on the disease',\n",
       "  'lasagne also includes a novel entity_recognition module which detects , links , and ranks all relevant entities in the question context',\n",
       "  'then , we benchmarked stonkgs against three baseline models trained on either one of the modalities ( i.e',\n",
       "  'to handle this challenge , we present a novel approach to automatically recognize new biomedical entities',\n",
       "  'we proposed an adversarial contextual embeddings_based model named ace_adp for named_entity recognition in chinese agricultural diseases and pests domain ( cner_adp )',\n",
       "  'experimental_results show performance competitive with published models on the hotpotqa dataset .',\n",
       "  'to classify aspect level sentiment , the memory content is constructed by combining the location information and inputting the multi_level gated recurrent_unit for calculating the sentiment characteristics of aspect terms',\n",
       "  'experimental_results_demonstrate that the improvements in each process are effective and our approach achieves better performance than the best team in ccks2019 competition .',\n",
       "  'extensive_experiments on the benchmark dataset demonstrate that our proposed model achieves much better performance than state_of_the_art_methods .',\n",
       "  'the results show that our model_outperforms others',\n",
       "  'paper_title : named_entity recognition for science and technology policy dynamics ; paper_abstract : dynamic text of science and technology policy reflects the latest intelligence in the field of science and technology policy',\n",
       "  'the experimental_results_demonstrate that our proposed approach outperforms state_of_the_art_methods on both versions of the large_scale benchmark new york times dataset',\n",
       "  'the experimental_results show that the proposed method outperforms other state_of_the_art algorithms .',\n",
       "  'many recent studies focus on improving neural model structures',\n",
       "  'then , the bidirectional_long_short_term memory neural_network is used to extract sequence features before predicting the spans of named_entities',\n",
       "  'concretely , the preliminaries , summaries of kgr models , and typical datasets are introduced and discussed consequently',\n",
       "  'to this end , we capture a complete view of the proposed system .',\n",
       "  'the achievement of this paper can offer a new method for disease identification based on multimodal data and domain_knowledge , which might help improve the intelligence level of crop disease identification .',\n",
       "  'this performance is significantly higher than that of existing pattern_based api misused detectors',\n",
       "  'paper_title : grape diseases and pests named_entity recognition based on bilstm_crf ; paper_abstract : named_entity recognition ( ner ) is one of the foundational and key tasks of knowledge_graph construction',\n",
       "  'then , for the unstructured_text in the data_set , the innovative deep_learning method of bi_gated recurrent_unit ( bigru ) and conditional_random_field ( crf ) model is used to identify the named_entities of forest disease names and therapeutic agents',\n",
       "  'experiments show that our model significantly_outperforms current state_of_the_art_methods',\n",
       "  'finally , the conditional_random_field ( crf ) was utilized to identify entity boundaries and category',\n",
       "  'our source_code is available at https : //github.com/mathisall/hdgcn_pytorch .',\n",
       "  'finally , the model uses the above vectors for classification information',\n",
       "  'experiments on three common kgc datasets_demonstrate the superiority of the proposed ftl_lm , e.g. , it achieves 2.1 % and 3.1 % hits @ 10 improvement over the state_of_the_art lm_based model lp_bert in the wn18rr and fb15k_237 , respectively .',\n",
       "  'experiments show that the f1 value of the proposed model improves 4 % compared with the baseline model .',\n",
       "  'experimental_results_demonstrate the superior_performance of the proposed method over several state_of_the_art_methods .',\n",
       "  'specifically , for entity descriptions , we explore continuous bag_of_words and convolutional_neural_networks models to encode the semantics of entity representations',\n",
       "  'our best performing system achieves a feverous score of 0.23 and 53 % label accuracy on the blind test data .',\n",
       "  'in addition , jieba word_segmentation was used to segment the chinese corpus before the random mask segmentation of the bert model',\n",
       "  'deep_learning has promoted the application of artificial_intelligence ( ai ) techniques to a wide variety of social problems',\n",
       "  'the main method is to change the original sample in a way that is almost imperceptible to the user , and cause an obvious error in the result returned by the model',\n",
       "  'besides , our alignment method can find the correlations between vision and language , resulting in better performance',\n",
       "  'these models achieve an accuracy of 99.27 % and a 98.61 % respectively on the dataset',\n",
       "  'named_entity recognition in network_security domain is an important task to construct knowledge_graph',\n",
       "  'for uni_modal scene ( text modality ) , experiments show that the proposed method surpasses current state_of_the_art_methods on emotion recognition , intent classification , and dialogue act identification tasks',\n",
       "  'at testing stage , the mcts is also combined with the rnn to predict the target node with higher accuracy',\n",
       "  'experimental_results_demonstrate that the proposed method_achieves state of the art results .',\n",
       "  'in addition , we propose the implementation of kgr using a novel neural symbolic framework , with regard to both scalability and interpretability',\n",
       "  'another class of problems exists around target identification tasks',\n",
       "  'firstly , named_entity recognition was realized , then relation_extraction was carried out , and finally , data was imported into the neo4j database to realize the visualization of the knowledge_graph',\n",
       "  'a convolutional attention layer combines the local attention_mechanism and cnn to capture the relationship of local context',\n",
       "  'when classifying the entity tags , we choose crf model as it adds more constraints to avoid position logical problem',\n",
       "  'the proposed model achieved an identification accuracy , precision , sensitivity and specificity of 99.63 % , 99 % , 99.07 % and 99.78 % respectively on a dataset composed of image_text pairs',\n",
       "  'for unstructured_data , bilstm_crf is used for named_entity recognition and then the bert model is used for entity_relationship extraction',\n",
       "  'experiments show that the proposed model performs better than the existing_methods',\n",
       "  'distilling key relations that may affect object recognition is crucially important since treating each region separately leads to a big performance drop when facing heavy long_tail data distributions and plenty of confusing categories',\n",
       "  'the code and data are available at https : //github.com/thudm/selfkg .',\n",
       "  'compared with previous_works , our model achieves an average of 1.6 % improvement ( 2.0 % and 1.5 % improvements in cider and rouge_l , respectively )',\n",
       "  'detailed experimentation shows that our proposed perkg architecture can effectively_improve the performance and alleviate the label sparsity problem of personality analysis .',\n",
       "  'in the system , the question_answering function is realized by template matching , which based on the naive bayes algorithm',\n",
       "  'secondly , according to the classification result , the most similar question template is matched',\n",
       "  'we compare the performance of several choices of methodologies for these sub_tasks using homicide investigation chronologies from los_angeles , california',\n",
       "  'we conduct extensive_experiments on four popular referring segmentation benchmarks and achieve new state_of_the_art performances',\n",
       "  \"results : jieba can accurately identify the herbal name in 'tfds\",\n",
       "  'this kind of kgs generally focus on named_entities , e.g',\n",
       "  'many machine learning_based methods have been proposed for this task',\n",
       "  'the experimental_results_demonstrate that our proposed approach outperforms state_of_the_art the methods on both versions of a large_scale benchmark new york times dataset',\n",
       "  'furthermore , it creates a confusion matrix represents that which intents are ambiguously recognized by approach .',\n",
       "  'experiments on the benchmark dataset show that the effectiveness of nrnet by detailed ablation studies and analysis .',\n",
       "  'our model achieves new state_of_the_art accuracy on the krvqr and fvqa datasets',\n",
       "  'paper_title : model_based clinical note entity_recognition for rheumatoid_arthritis using bidirectional_encoder_representation_from_transformers ; paper_abstract : background : rheumatoid_arthritis ( ra ) is a disease of the immune system with a high rate of disability and there are a large amount of valuable disease_diagnosis and treatment information in the clinical note of the electronic_medical_record',\n",
       "  'many scholars have researched the ner task of electronic_medical_records and drug names , while many factors restrict the research of ner tasks for the instructions of chinese_medicine',\n",
       "  'finally , we compare our model with the state_of_the_art_baselines on two benchmark_datasets , the results of extensive comparison experiments validate the effectiveness of the proposed method .',\n",
       "  'once trained , a machine_learning model is barely portable on a different domain',\n",
       "  'on the experiment of the standard dataset , the bbcm model has a significant_improvement ( f1 value reached 0.9544 ) than the baseline model',\n",
       "  'in the 2021 language and intelligence challenge : multi_skill dialog task , our best model ranked 3rd in the automatic evaluation stage and 5th in the human evaluation stage .',\n",
       "  'although the growth and development of children are rapid , the spleen , stomach and kidney have not yet developed completely',\n",
       "  'however , nearly all previous methods suffer from the problem of error accumulation , i.e. , the boundary recognition error of each entity in step ( 1 ) will be accumulated into the final combined triples',\n",
       "  'experimental_results indicated the proposed deep , multi_branch bigru_crf model outperformed state_of_the_art models',\n",
       "  'experimental_results show that our proposed method_achieves good performance on the agriculture dataset',\n",
       "  'it is found that the precision rate , recall_rate and f1 value of the model are better than pcnn_one and pcnn_ave models',\n",
       "  'meanwhile , this model still maintained over 86 % of f1 value on some other difficultly recognized entities such as weed and pathogeny',\n",
       "  'in experiments , we demonstrate that the kglm achieves significantly better performance than a strong baseline language_model',\n",
       "  'we , then , evaluate the impact of our cognate detection mechanism on neural machine_translation ( nmt ) , as a downstream task',\n",
       "  'first , we release new re model architectures that obtain state_of_the_art f1 scores on 5 out of 7 benchmark_datasets',\n",
       "  'to reduce the cost , insurance inspectors tend to build an intelligent system to detect suspicious claims with inappropriate diagnoses/medications automatically',\n",
       "  'experimental_results_demonstrate that our proposed methods outperform traditional neural symbolic models .',\n",
       "  'furthermore , we propose a novel loss_function to alleviate the false_negative problem during training',\n",
       "  'considering the lack of labeled_data , pretraining language_model was introduced , which is fine tuned with existing labeled_data',\n",
       "  'then , a position code was created for the head and tail position of each character and vocabulary , where the entity position was located with the help of a position vector , in order to improve the recognition of entity boundary',\n",
       "  'the hybrid attention is an adaptation scheme to apply the pre_trained_language model to our model and the copy mechanism is a gate mechanism to control generating a word from generic vocabulary or the input knowledge',\n",
       "  'our system is the winner of track 1 of the lm_kbc challenge , based on bert lm ; it achieves 55.0 % f_1 score on the hidden test set of the challenge .',\n",
       "  'secondly , in order to facilitate the query , this paper establishes entity and relationship/attribute mining based on the continuous bag_of_words ( cbow ) encoding model , bidirectional_long_short_term memory_conditional random_field ( bilstm_crf ) named_entity model , and bidirectional gated recurrent neural_network ( bigru ) intent recognition model for chinese kill chain question and answer ; returns the corresponding entity or attribute values in combination with the knowledge_graph triad form ; and finally constructs the answer return',\n",
       "  'in biomedical field , labeling high_quality biomedical entities requires plenty of linguistic knowledge due to abbreviation and specificity',\n",
       "  'for the spatial representation , we not only adopt a slowfast network to learn global action and scene information , but also exploit the unique cues of face , body and dialogue between characters',\n",
       "  'finally , a model based on the combination of character sequences and word sequences is used to identify chinese named_entities',\n",
       "  'we demonstrate the effectiveness of our system on two datasets in comparison with state_of_the_art models .',\n",
       "  'experiments show that the algorithm proposed in this paper achieves 91.47 % , 88.88 % and 90.16 % in accuracy , recall and f_ { 1 } values while the training convergence speed is smaller than that of the baseline model , which is 1.27 % 4.06 % better than the traditional model',\n",
       "  'in this paper , we propose a named_entity recognition algorithm based on text { bert } +text { bilstm } +text { crf } , which combines attention_mechanism and word_character joint embedding vector , for the text features in the field of grid operation and inspection',\n",
       "  'however , in the medical domain , building a large_scale image_report paired dataset is both time_consuming and expensive',\n",
       "  'the framework combines the perceptual capabilities of computer vision with the cognitive capabilities of kg to improve the accuracy and timeliness of kg updates',\n",
       "  'experimental_results_demonstrate that the proposed approach achieves significantly higher performance compared with other state_of_the_arts .',\n",
       "  'the experimental_results show that the method proposed in this paper is superior than the current mainstream method , which proves the leading role and generalization of the model',\n",
       "  'in this paper , the proposed model combines word_feature into character_feature as characters embedding ; uses a joint model of bilstm and self_attention mechanism to encode characters and a bidirectional label distribution transfer model is used to decode the classification',\n",
       "  'the training_data generator ( tdg ) generates the base training_set for setting up the conversation agent',\n",
       "  'experiments on two opencsr datasets show that the proposed model achieves great performance on benchmark opencsr datasets .',\n",
       "  'yet , this task remains a challenging job for data_driven neural_networks , due to the serious visual and textual data biases',\n",
       "  'the experimental_results_demonstrate that the proposed method can effectively_improve the effectiveness of prediction .',\n",
       "  'specifically , adversarial_training is added to the model training as a regularization method to alleviate the influence of noise on the model , while self_attention is added to the bilstm_crf model to capture features that significant impact entity_classification and improve the accuracy of entity_classification',\n",
       "  'then , accident management based on named_entity recognition and knowledge_graph can be developed',\n",
       "  'the results illustrate that the overall accuracy is 66',\n",
       "  'experimental_results have shown that the proposed classifier outperforms the existing systems , with better domain representation .',\n",
       "  'the bidirectional_encoder_representation_from_transformers ( bert ) pre_training language_model is used as the encoding layer of the word_vector , the bidirectional long short_term_memory ( bilstm ) is used as the character label prediction layer , and the conditional_random_field ( crf ) is used to output the global optimal label',\n",
       "  'recently , some works use graph convolutional_networks to obtain the embeddings of unseen entities for prediction tasks',\n",
       "  'paper_title : improve on entity_recognition method based on bilstm_crf model for the nuclear_technology knowledge_graph ; paper_abstract : the accuracy of entity_recognition is particularly important for knowledge_graph construction',\n",
       "  'the experimental_results show that the proposed approach outperforms previous methods and achieves state_of_the_art_performance',\n",
       "  'inspired by the recent progress of self_supervised learning , we explore the extent to which we can get rid of supervision for entity_alignment',\n",
       "  'experimental_results show that ontoprotein can surpass state_of_the_art_methods with pre_trained protein language_models in tape benchmark and yield better performance compared with baselines in protein_protein interaction and protein function prediction',\n",
       "  'the proposed model_outperforms other popular unsupervised models significantly .',\n",
       "  'in practice , we found that ontology reuse is necessary , and neural_network is the best choice',\n",
       "  'experiments on the large_scale redial dataset demonstrate that the proposed system consistently outperforms state_of_the_art_baselines .',\n",
       "  'with fast developments of deep_learning , this area has attracted great research attention',\n",
       "  'the results show that the model can effectively recognize seven mineral entities with an average f1_score of 0.842 .',\n",
       "  'contextualized representations trained on the mimic_iii database were used to capture context_sensitive meanings of words',\n",
       "  'the result of g_coder on the mimic_iii dataset showed that the micro_f1 score is 69.2 % surpassing the state of art',\n",
       "  'then , using the random_forest algorithm , the standardized classification of entities is accomplished , and the multi_dimensional knowledge_graph network is established',\n",
       "  'conclusion : for the bert models on the pharmaconer dataset , biomedical domain_knowledge has a greater impact on model performance than the native language ( i.e. , spanish )',\n",
       "  'the code and data are publicly available at https : //github.com/macho000/t5_for_kgqg .',\n",
       "  'the second type of the model is named neural_network_based topic model , which employs neural_network structure , such as multilayer_perceptron ( mlp ) or rnn , to model the document generation process with introducing latent topic structure',\n",
       "  'experimental_results show that our method outperforms several state_of_the_art models on benchmark_datasets',\n",
       "  'the generalization_ability of our model is significantly improved by repeating the process of these two components in an alternate way',\n",
       "  'traditional method is tedious , and time_consuming , as well as low accuracy',\n",
       "  'we test the performance of the two models on two chinese_medical datasets : cmeie and cemrds',\n",
       "  'the recognized entities are also used to expand the knowledge_graph generated by dbpedia spotlight for a given pharmaceutical text .',\n",
       "  'the dataset and the word_vectors trained by word2vec are available at github ( https : //github.com/a_mumu/agriculture.git ) .',\n",
       "  'recent_years have witnessed the remarkable success of deep_learning techniques in kg',\n",
       "  'it was shown that the developed architecture is particularly suitable for our field of application .',\n",
       "  'using these data for named_entity recognition to support the construction of knowledge_graphs in the power_grid domain and finally realize the decision intelligence in the power_grid operation and inspection domain is the key of this paper',\n",
       "  'based on the geokg , we propose a bilateral lstm_crf ( long short_term memoryconditional random_field ) model to achieve natural_language question_answering for vges and conduct experiments on the method',\n",
       "  'methods often used to encode sentence , like canonical bidirectional recurrent neural_networks ( birnn ) or convolutional_neural_networks ( cnn ) , are difficult to capture enough information from biomedical text',\n",
       "  'livemedqa system is evaluated in the trec 2017 liveqa medical subtask , where it received an average score of 0.356 on a 3 point scale',\n",
       "  'in experiments , we show that the nklm significantly improves the performance while generating a much smaller number of unknown words .',\n",
       "  'the named_entity recognition task is one of the key tasks for the construction of knowledge_graphs',\n",
       "  'our model improved f1_score better than vncorenlp and underthesea models with 12 % and 17 % respectively',\n",
       "  'the experimental_results_demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans',\n",
       "  'by leveraging this discovery , we develop the self_supervised learning objective for entity_alignment',\n",
       "  'secondly , the method based on dictionary technology and aho_corasick ( ac ) automaton is used to realize fast entity_recognition and question word extraction',\n",
       "  'the results show that our method_achieves superior_performance than baselines',\n",
       "  'the experiment results of our models clearly affirm the effectiveness and superiority of our models against baseline .',\n",
       "  \"based on user 's configuration specified , the system can automatically train and test the model , conduct extensive experimental evaluation of the models selected , and report comprehensive findings\",\n",
       "  'we demonstrate the effectiveness of our system on two datasets in comparison with state_of_the_art models1 .',\n",
       "  'we observe an improvement of up to 18 % points , in terms of f_score , for cognate detection',\n",
       "  'we evaluate our approach on three different tasks : ( i ) standard machine_learning tasks , ( ii ) entity and document modeling , and ( iii ) content_based recommender systems',\n",
       "  'pre_trained large language_models ( plm ) have emerged as a crucial type of approach that provides readily available knowledge for a range of ai applications',\n",
       "  'with the boom of pretrained_language_models , various relative methods have achieved significant improvements in acsa',\n",
       "  'medical dialogue_systems that generate medically appropriate and human_like conversations have been developed using various pre_trained_language_models and a large_scale medical_knowledge base based on unified medical language system ( umls )',\n",
       "  'for the first part , an improved named_entity identification and relationship_extraction method is developed , incorporating a vision sensing pre_training algorithm named bert',\n",
       "  'the character level features are learned in the bert ( bidirectional_encoder_representations_from_transformers ) _based chinese_character embedding representation layer with the context features extracted in the bilstm ( bi_directional long_short_term_memory ) neural_network layer to form the feature matrix',\n",
       "  'for image data , our cmpc_i module first employs entity and attribute words to perceive all the related entities that might be considered by the expression',\n",
       "  'however , in some application fields , it is impossible to achieve the satisfactory results only depending on the traditional ai algorithm',\n",
       "  'this algorithm is suitable for entity_extraction tasks with few training entity samples and reduces the corpus size required for the bilstm_based algorithm entity_extraction',\n",
       "  'experiments with different benchmarks demonstrate the high quality of quint .',\n",
       "  'we also release a neural multi_modal retrieval model that can use images or sentences as inputs and retrieves entities in the kg',\n",
       "  'paper_title : medical question_answering for clinical decision_support ; paper_abstract : the goal of modern clinical decision_support ( cds ) systems is to provide physicians with information relevant to their management of patient care',\n",
       "  'code will be released at https : //github.com/cshizhe/hgr_v2t .',\n",
       "  'experiments on the arc challenge set show that our model_outperforms the previous state_of_the_art qa systems .',\n",
       "  'paper_title : named_entity extraction for chinese electronic_medical_records ; paper_abstract : named_entity_extraction task refers to identifying and extracting proper named_entities from natural_language texts',\n",
       "  'furthermore , experiments on four general chinese ner datasets show that the framework of our approach is transferable .',\n",
       "  'these models are trained and tested in the case study using 879 chemical patents in the carbon_capture domain',\n",
       "  'among them , the character_based method lacks the support of word information , and the word_based method is affected by the word_segmentation efficiency',\n",
       "  'the defect records of the traction power supply equipment of a maintenance department from 2016 to 2019 are used as the data_set for case study , and , the comprehensive evaluation index of entity_recognition of the model has reached 94.66 % , saving 98.50 % time compared with manual recognition',\n",
       "  'we show that kgs are constructed with a single forward_pass of the pre_trained_language_models ( without fine_tuning ) over the corpora',\n",
       "  'then conditional_random_field ( crf ) is introduced to decode the information and obtain the optimal label sequence',\n",
       "  'therefore , chinese word_segmentation is a very difficult task',\n",
       "  'the experimental result shows that our proposed method has achieved a better result .',\n",
       "  'as a result , research on unsupervised models has emerged as an active field recently',\n",
       "  'secondly , a cnns layer with different kernel sizes was considered capturing multi_scale local contextual features',\n",
       "  'as a result , the performance of such models decreases significantly',\n",
       "  'code is available at \\\\url { https : //github.com/urchade/hner } .',\n",
       "  'the final answer is obtained by computing similarity between question and answer',\n",
       "  'firstly , the cbow model was used to pre_train character embedding on a large number of unlabeled agricultural corpora , and alleviate the impact of segmentation accuracy on the performance of the model',\n",
       "  'experimental_results show that the proposed model has high recall_rate 88.16 % and precision rate 89.33 % which is better than the state_of_the_art models .',\n",
       "  'in contrast to prior work , our approach includes a reward function that takes the accuracy , diversity , and efficiency into consideration',\n",
       "  'thanks to the rapid development of computer sciences and telecommunication technologies , this has evolved impressively',\n",
       "  'in the construction of intelligent question_answering system , we use bert_textcnn to realize the task of intention recognition and use bilstm_crf to realize the task of entity_recognition',\n",
       "  'this system consists of three parts',\n",
       "  'for knowledge_graph of chinese history and culture , most researchers adopted traditional named_entity recognition methods to extract entity information from unstructured historical text data',\n",
       "  'the code will be executed on a high_performance cluster ( hpc ) and users can receive the results later on',\n",
       "  'each covid_19 kg was evaluated , and the average precision was found to be above 93 %',\n",
       "  'our model_outperforms unimodal baselines significantly with various evaluation_metrics .',\n",
       "  'the experimental_results on three open datasets show that the proposed model can achieve the most advanced performance compared with previous models .',\n",
       "  'furthermore , we propose to evaluate the crs models in an end_to_end manner , which can reflect the overall performance of the entire system rather than the performance of individual modules , compared to the separate evaluations of the two modules used in previous work',\n",
       "  'extensive_experiments show that our model is effective and competitive with many current state_of_the_art_methods , and also performs well in practice .',\n",
       "  'a benign node labeled as malicious by our approach is a false positive and a correctly identified malicious node is a true positive',\n",
       "  'and it obscures the reasons and details behind specific diagnoses',\n",
       "  'compared with textrcnn , the kaercnn model improves accuracy by about 3 %',\n",
       "  'the crf decoder was used to represent the output of the attention layer in the form of sequence tags',\n",
       "  'while named_entity recognition ( ner ) is an important task for the extraction of factual information and the construction of knowledge_graphs , other information such as terminological concepts and relations between entities are of similar importance in the context of knowledge_engineering , knowledge_base enhancement and semantic_search',\n",
       "  'the experimental_results of tkgc task illustrate the significant performance improvements of our model compared with the existing_approaches',\n",
       "  'furthermore , a three_layered gcn with highway gates is adopted to learn better entity representations from the neighboring structure information',\n",
       "  'we also propose two models an attention rnn and a transformer for the same',\n",
       "  'such modular architectures often come with a complicated and unintuitive connection between the modules , leading to inefficient learning and other issues',\n",
       "  'code and datasets are available in https : //github.com/zjunlp/promptkg/tree/main/genkgc .',\n",
       "  'through experiments on the benchmark ace_2005 dataset , we demonstrate the effectiveness of the proposed sae_ceed model',\n",
       "  'deep_learning based nlp especially large language_models ( llms ) such as bert have found broad acceptance and are used extensively for many applications',\n",
       "  'but most previous methods require that all entities should be seen during training , which is impractical for real_world kgs with new entities emerging daily',\n",
       "  'paper_title : who is mona l. ? identifying mentions of artworks in historical archives ; paper_abstract : named_entity recognition ( ner ) plays an important role in many information_retrieval tasks , including automatic knowledge_graph construction',\n",
       "  'experimental_results on large_scale datasets show that sdt achieves a lower mean rank and higher hits @ 10 than the baseline methods .',\n",
       "  '43 % on the test_dev set , where the accuracy of answering yes or no questions is 83',\n",
       "  'our model achieves the competitive performance compared with the state_of_the_art_methods on flickr30k dataset and ms_coco dataset',\n",
       "  'paper_title : named_entity recognition method in network_security domain based on bert_bilstm_crf ; paper_abstract : with the increase of the number of network threats , the knowledge_graph is an effective method to quickly analyze the network threats from the mass of network_security texts',\n",
       "  'constructing the topic model can represent mentions and candidate entities by using topic distributions',\n",
       "  'the dataset is publicly available under cc by_sa 4.0 in github .',\n",
       "  'hence to contain malware , enterprises must prevent their hosts from accessing malicious domains',\n",
       "  'the bert pre_training model was selected to generate the input word_vector',\n",
       "  'dhge outperforms baseline models on dh_kg , according to experimental_results',\n",
       "  'this poses a challenge to the widely adopted codec_based architectures',\n",
       "  'aiming at the problem that key chinese entity information in network_security related text is difficult to identify , a named_entity recognition model in network_security domain based on bert_bilstm_crf is proposed to identify key named_entities in network_security related text',\n",
       "  'biobert and med_bert are language_models pre_trained for the healthcare domain',\n",
       "  'moreover , entity_recognition and relation recognition , which crf and cnn are applied in , are accessible for professionals before manual annotation in order to increase the efficiency',\n",
       "  'the experimental_results show that our model obtains significant performance gains over several state_of_the_art_baselines .',\n",
       "  'we release the source_code and pre_trained_models .',\n",
       "  'the complexity of the character and the image quality plays a key role in the conversion accuracy',\n",
       "  'secondly , the cabilstm model was designed for the nested named_entity recognition using hierarchical thinking',\n",
       "  'the recognition of named_entity information of the defect record is the premise of constructing the knowledge_graph',\n",
       "  'our approach is to preprocess the structured data followed by named_entity recognition with appropriate finance_related tags',\n",
       "  'extensive_experiments have demonstrated that our approach is superior to existing state_of_the_art algorithms in terms of both efficiency and effectiveness .',\n",
       "  'however , the performance of these models relies heavily on the amount of labeled_data',\n",
       "  'in the vast majority of the existing works , the two tasks are considered separately with different models or algorithms',\n",
       "  'we have implemented the proposed novel algorithm and several typical algorithms for each task',\n",
       "  'moreover , the results of the online a/b test on the large_scale meituan waimai ( mtwm ) kg consistently show our method brings benefits to the industry .',\n",
       "  'the knowledge extracted by regular_expression was used to semi_automatically label the corpus , and then trained by deep_learning model , which achieved good results',\n",
       "  'the extensive_experiments conducted on the covid_19 drug kg dataset show promising results and prove the effectiveness and efficiency of our proposed model',\n",
       "  'experimental_results show that our model_outperforms most metrics compared to prior state_of_the_art_baselines across two benchmarks',\n",
       "  'then a problem base is established for problem template matching',\n",
       "  'information on wide_spread diseases like diabetes and cancer is extensive , heterogeneous and rapidly growing',\n",
       "  'bert only achieved an accuracy of 55.9 % on it',\n",
       "  'these models suffer from higher computational_complexity during training while still losing information beyond the relative distance between entities',\n",
       "  'experiments using real_world kgs show that transalign improves the accuracy of entity_alignment significantly compared to state_of_the_art_methods .',\n",
       "  'the source_code and data of this paper can be obtained from https : //github.com/cciiplab/cet .',\n",
       "  'finally , we discuss the challenges remaining for chinese nqg .',\n",
       "  'paper_title : named_entity recognition for the diagnosis and treatment of aquatic_animal diseases using knowledge_graph construction ; paper_abstract : disease_diagnosis and treatment have been an important support for aquatic_animal health in aquaculture',\n",
       "  'however , there is room for further improvement on overlapping triplet problem in the military domain .',\n",
       "  'meanwhile , our new benchmark could facilitate the further study in this research area .',\n",
       "  'the experiments show that the unsupervised kgae generates desirable medical reports without using any image_report training pairs',\n",
       "  'thus , it is important for the model to be aware of such features to improve the performance',\n",
       "  'in order to verify the effectiveness of the method , we compare the improved algorithm in this paper with the existing classical algorithms and literature algorithms',\n",
       "  'the experimental_results show that compared with the traditional hmm model and bilstm_crf model , the f1_value of our proposed method increases by 7.1 % and 6.5 % respectively',\n",
       "  'as growing kgs inevitably contain non_matchable entities , different from previous_works , the proposed method employs bidirectional nearest neighbor matching to find new entity_alignment and update old alignment',\n",
       "  'paper_title : recognition of chinese agricultural diseases and pests named_entity with joint radical_embedding and self_attention mechanism ; paper_abstract : chinese named_entity recognition in agricultural diseases and pests domain ( cner_adp ) plays an important role in agricultural natural_language processing such as relation_extraction , agricultural knowledge_graph construction , and agricultural knowledge question and answering , but it still presents some problems , i.e. , the neglect of inherent semantic_information and local contextual features and the insufficiency of capturing long_distance dependencies , which will lead to low accuracy and robustness',\n",
       "  'given such a fixed_size vocabulary , it is possible to bootstrap an encoding and embedding for any entity , including those unseen during training',\n",
       "  'therefore , the proposed bert+bi_lstm +multi_head_self_attention+ fc fusion model has certain practical value in chinese_character relationship_extraction .',\n",
       "  'code and data are available at \\\\url { https : //github.com/xnliang98/sms } .',\n",
       "  'as a data_driven technology , its performance depends on a large amount of image labeling data',\n",
       "  'we use six benchmark_datasets to evaluate the proposed method',\n",
       "  'the experimental_results showed that the f1_score of the bert_flat model was 88.99 %',\n",
       "  'the experimental_results have shown that the proposed model_outperforms the current methods and can improve the precision/recall ( pr ) curve area by 8 % to 16 % compared to the state_of_the_art models ; the auc of bg2kga can reach 0.468 in the best case .',\n",
       "  'the performance on the bert model can be further improved by constructing a specific vocabulary based on domain_knowledge',\n",
       "  'extensive_experiments on a public medical dialogue dataset show our kr_ds significantly beats state_of_the_art_methods ( by more than 8 % in diagnosis accuracy )',\n",
       "  'compared with the baseline model , it has increased by 0.43 % and 1.17 % respectively .',\n",
       "  'extensive_experiments conducted over widely_used benchmark_datasets demonstrate the effectiveness of the proposed framework .',\n",
       "  'after the segmentation of the question , the aho_corasick automaton algorithm and semantic_similarity are applied to entity_recognition',\n",
       "  'it offers a core set of software for operating robots that can be extended by creating or using existing packages , making it possible to program robotic software that can be reused on different hardware platforms',\n",
       "  'experimental_results show that our method_achieves state_of_the_art_performance and the effectiveness and reliability of skg .',\n",
       "  'hence , one system might outperform other systems in one dataset and fail to do in another one',\n",
       "  'experiments on benchmark_datasets show our proposed method can achieve superior_performance compared to analogous methods .',\n",
       "  'our codes and models can be found at https : //github.com/changzhisun/probr/ .',\n",
       "  'however , the existing chinese named_entity identification methods are mainly for standardized texts',\n",
       "  'this phenomenon stems from the inherent complexity of biological systems and our poor understanding of human diseases',\n",
       "  'experiment results show that binet outperforms state_of_the_art_methods on a wide range of kgqa and kgc benchmark_datasets .',\n",
       "  'finally , we test bilstm and bert_based pre_trained_language_models ( plms ) on our dataset and propose a baseline for the following studies',\n",
       "  'to fully mine and utilize geological data , this study proposes a geological news named_entity recognition ( gnner ) method based on the bidirectional_encoder_representations_from_transformers ( bert ) pre_trained_language model',\n",
       "  'we conducted several experiments on four benchmark_datasets to evaluate the performance of multpax against different state_of_the_art_baselines',\n",
       "  'however , the existing captioning frameworks basically enumerate the objects in the image',\n",
       "  'the practical relevance of the proposed methodology has been proven in the study of 1.1 million unique messages from > 400,000 distinct users related to one of the most popular dietary fads that evolve into a multibillion_dollar industry , i.e. , gluten_free food',\n",
       "  'empirical experiments demonstrated that mokge can significantly improve the diversity while achieving on par performance on accuracy on two gcr benchmarks , based on both automatic and human evaluations .',\n",
       "  'then , the bilstm layer was used to learn long_distance text information , and the crf was applied to obtain the globally optimal labeling sequence , so as to output the crop disease entities',\n",
       "  'later , the network parameters are trained via the expectation_maximization ( em ) algorithm',\n",
       "  'the joint representation of two different representations of an entity is regarded as the final representation',\n",
       "  'if the system could not match an answer for a question , the question would be added to unsolved question list and the system would alert administrator to deal with it',\n",
       "  'furthermore , the proposed kbqg outperforms all baselines in our experiments on two real_world datasets .',\n",
       "  'finally , we extensively evaluate our method on several large_scale real_world benchmark_datasets , obtaining favorable results compared with state_of_the_art_methods .',\n",
       "  'the experimental_results show that compared with the bilstm_crf model and the bert_bilstm_crf model , the f1_score is increased by 23.15 % and 10.62 % , respectively',\n",
       "  'then by using a tagger based on supervised_learning and an instance selector based on reinforcement_learning , we iteratively generate new biomedical entities',\n",
       "  'extensive_experiments show that the proposed model achieves substantial improvements against the state_of_the_art_baselines .',\n",
       "  'experiments on two real_world kgs demonstrate the effectiveness of our method',\n",
       "  'the experiments show that the algorithm in this paper has certain effectiveness and superiority .',\n",
       "  'sequencematcher tool and the deckard similarity algorithm',\n",
       "  'a crf layer obtains the output tag sequences',\n",
       "  'our proposed approach shows competitive performance on the hotpotqa distractor setting benchmark compared to the recent state_of_the_art models .',\n",
       "  'experimental_results show that the proposed approach outperforms state_of_ the_art methods by a considerable gain',\n",
       "  'we make use of intent classification and slot_filling , the two important components of any dialogue agent , exploit their interconnectedness , and finally construct a kg',\n",
       "  'with military weapons as the research direction , an svm question classification method based on chinese_character algorithm is proposed , and a question_answering system over knowledge_graph of weapons is established',\n",
       "  'nevertheless , the recall_rate of the improved model was dropped slightly in the comparison test',\n",
       "  'besides , the label ( code ) distribution is uneven',\n",
       "  'over the course of its development , the label supervision has been considered necessary for accurate alignments',\n",
       "  'the f1_score of the best_performing method was 0.97 , indicating the effectiveness of the proposed approach',\n",
       "  'traditional works about emotion recognition mainly focus on the characteristic of the person itself , such as audio , text , facial_expression , body posture',\n",
       "  'the results of extensive_experiments based on real_world dataset which is collected from the news show that our model generates image captions closer to the corresponding real_world captions .',\n",
       "  'we will make the code publicly available at https : //github.com/bionlplab/report_generation_amia2022 .',\n",
       "  'the experimental_results show that this model can efficiently and accurately predict the physical label of hot strip rolling , and the model performance index is better than other models , with the f1_score reaching 91.47 %',\n",
       "  'crowdsourcing efforts like paperswithcode among others are devoted to the construction of leaderboards predominantly for various subdomains in artificial_intelligence',\n",
       "  'text_based automatic personality prediction ( app ) is the automated forecasting of the personality of individuals based on the generated/exchanged text contents',\n",
       "  'we study the importance of some modeling choices and criteria for designing the model , and we demonstrate that it can be used to label data for a supervised classifier to achieve an even better performance without relying on any humanly_annotated training_data',\n",
       "  \"parkinson 's disease is the second most common neurodegenerative disorder worldwide , affecting approximately 12 percent of the human population older than 65 years\",\n",
       "  'as a solution , researchers build medical question_answering ( qa ) systems',\n",
       "  'according to the test results , the model performance is good .',\n",
       "  'finally , the experimental validation shows that our approach is able to enhance existing kge models and can provide more robust representations of kgs in noisy scenarios .',\n",
       "  'the test_std set based accuracies calculated are 66',\n",
       "  'however , research on kg updates in the industry is scarce , with most current research focusing on text_based kg updates',\n",
       "  'it performs text_classification using state_of_the_art transfer_learning models , and thoroughly integrates the results obtained through a proposed methodology',\n",
       "  'experiments demonstrate that proposed model_outperforms most state_of_the_art_methods on the vqa v2.0 benchmark_datasets .',\n",
       "  'our method_achieves state_of_the_art_performance on the pharmaconer dataset , with a max f1_score of 92.01 %',\n",
       "  'therefore , they are hard to generalize to a broad set of applications and kgs',\n",
       "  'the results show our proposed framework outperforms several state_of_the_art_baselines .',\n",
       "  'it could be used as a basis for further research on other domain_specific named_entity recog_nition .',\n",
       "  'this work introduces a pre_trained bert model and a dilated gated convolutional_neural_network ( dgcnn ) as an encoder to distinguish the long_range semantics representation from the input sequence',\n",
       "  'the source_code of ctrn will be available at https : //github.com/2399240664/ctrn .',\n",
       "  'our code is released at : https : //github.com/declare_lab/dialog_hgat',\n",
       "  'by using deep_learning models to extract entities',\n",
       "  'while a variety of algorithms have been proposed , most of them are built upon different combinations of image and language features as well as multi_modal attention and fusion',\n",
       "  'at the same time , the model uses a pre_trained_language model as the basis to solve the problem of lack of semantic knowledge in traditional models',\n",
       "  'our code is available at https : //github.com/mm_ir/dualvgr_videoqa .',\n",
       "  'we proposed an innovative approach for recognizing knowledge entities , which included sequence tagging , text_classification , and keyword matching',\n",
       "  'our code is available at https : //github.com/spyflying/cmpc_refseg .',\n",
       "  'codes are available at https : //github.com/zjunlp/deepke/tree/main/example/re/multimodal .',\n",
       "  'the experimental_results show that the orem_af presented a 74.22 % accuracy and 75.12 % f1 value on the agricultural product data_set , while the 84.51 % accuracy and 75.43 % f1 value on the common data_set',\n",
       "  'further usage of the platform is already planned or implemented for many of our projects .',\n",
       "  'thus , they can be great supplements to existing pre_trained_language_models',\n",
       "  'at macro_level , the rs_adp model achieved optimal precision , recall , and f1 values of 94.16 % , 94.47 % , and 94.32 % , respectively',\n",
       "  'the url of this tool is https : //cinnqi.github.io/neo4j_d3_vkg/',\n",
       "  'experiments on benchmark_datasets demonstrate the effectiveness of our method .',\n",
       "  'to solve the above problems , we developed bank accident management from the perspective of knowledge support to introduce relevant methods and technologies in the field of artificial_intelligence',\n",
       "  'we won the 5th place at ccks 2022 track 1 rematch stage , which proved the effectiveness of our method .',\n",
       "  'how to make the automatic question_answering system more intelligent is a popular research direction in the field of natural_language processing',\n",
       "  'existing studies pay more attention to the matching between utterances and responses by calculating the matching score based on learned features , leading to insufficient model reasoning ability',\n",
       "  'paper_title : neural entity_summarization with joint encoding and weak supervision ; paper_abstract : in a large_scale knowledge_graph ( kg ) , an entity is often described by a large number of triple_structured facts',\n",
       "  'weshow that this novel form of medical question_answering ( q/a ) produces very promising resultsin ( a ) identifying accurately the answers and ( b ) it improves medical article rankingby40 % .',\n",
       "  'in experiments , we tested our model using the new york times ( nyt ) public dataset',\n",
       "  'experimental_results on opendialkg show that our approach significantly_outperforms state_of_the_art_methods on both automatic and human evaluation by a large margin , especially in hallucination reduction ( 17.54 % in feqa ) .',\n",
       "  'the proposed work can also provide a basis for the context_aware intelligent question and answer .',\n",
       "  'extensive_experiments justify our model_outperforms other state_of_the_art_baselines substantially .',\n",
       "  'self_supervised learning on a large corpus of data automatically generates deep_learning_based language_models',\n",
       "  'we implement recent typical methods for named_entity recognition and relation_extraction as a benchmark to evaluate the proposed dataset thoroughly',\n",
       "  'vogue comprises four modules that are trained simultaneously through multi_task_learning',\n",
       "  \"for the input questions , the system firstly performs entity_recognition , using entity type labeling combined with entity similarity matching to identify entities in the user 's questions\",\n",
       "  'for each category , representative algorithms and newly proposed algorithms are presented',\n",
       "  'our experiments on real_world datasets , comparison with related work and user study demonstrate the superior efficiency , precision and user satisfaction of our approach in multi_entity resolution ( mer ) .',\n",
       "  'the system has an accuracy of 0.822 , a precision of 0.837 , and a recall of 0.9015 for a set of 500 questions and answers .',\n",
       "  'extensive experimental_results on two widely used datasets_demonstrate that the proposed model performs better than the state_of_the_art_baselines .',\n",
       "  'recently many related models and methods were proposed , such as translational methods , deep_learning based methods , multiplicative approaches',\n",
       "  'the precision , recall and f1_score were 91.82 % , 90.44 % and 91.01 % , respectively',\n",
       "  'the performance and adaptability of the algorithm are further verified with different datasets',\n",
       "  'experimental_results on biomedical dataset and general field dataset show that our method is effective .',\n",
       "  'many sub_problems were raised in this regard , and reasonable efforts have been made to solve them',\n",
       "  'a named_entity recognition was also proposed to accurately obtain six types of entities in text : food , nutrients , population , location , disease , and efficacy in the field of human nutritional health , combining rules with bert_flat ( bidirectional encoder representations from transformers_flat lattice transformer ) model',\n",
       "  'besides having a user_friendly interface , it is fast , supports customization , and is fault_tolerant on both client and server side',\n",
       "  'we adopt the deep neural_network of bi_gat_crf , which has an accuracy of 90.75 , a recall_rate of 91.53 , and an f1_score of 91.14 in the hazop chinese text',\n",
       "  'it was found that the precision of the model was 86.56 % , the recall_rate was 91.01 % , and the f1_score was 88.72 % , compared with the model without location information , indicating improved by 1.55 , 0.20 , and 0.32 percentage points',\n",
       "  'the continuous expansion of large electronic clinical records provides an opportunity to learn medical_knowledge by machine_learning',\n",
       "  'it infers that the active_learning capability led to the strong migration',\n",
       "  'we claim that our algorithm solves some bottlenecks in existing work , and demonstrate that it achieves superior accuracy on real_world datasets .',\n",
       "  'the technique stands on aggregating significant contextual features human_object interactions and scene recognition',\n",
       "  'in this paper we propose esa , a neural_network with supervised attention mechanisms for entity_summarization',\n",
       "  'the experimental_results show that our method remarkably improves the performance compared to several state_of_the_art_baselines .',\n",
       "  'moreover , it had certain generalization and outperformed other models .',\n",
       "  'the experimental_results show that our gtlr method outperforms recent state_of_the_art_methods .',\n",
       "  'we then label the nodes with high marginal probability of being malicious as malicious nodes and benign otherwise',\n",
       "  'furthermore , we evaluate the applicability of our method under a transfer_learning setting and show that bioie achieves promising performance in processing medical text from different formats and writing styles .',\n",
       "  'furthermore , to mitigate the class_imbalance problem that most end_to_end crss face , we propose a new negative_sampling method which could make the proposed crs learn better',\n",
       "  'malicious domain accesses , however , account for majority of host infections',\n",
       "  'the model can be deployed on multiple clouds independently , increasing the systems flexibility , robustness , and security',\n",
       "  'extensive simulations are conducted to evaluate our proposed algorithm in comparison to some state_of_the_art schemes .',\n",
       "  'the results indicated a considerable improvements in prediction accuracies in all of the suggested classifiers .',\n",
       "  'extensive_experiments strongly evidence that our proposed model obtains significant performance compared with state_of_the arts .',\n",
       "  'the experimental_results show that this models precision , recall , and f1_score are 95.78 % , 97.07 % , and 96.42 % , respectively',\n",
       "  'our approach can be readily applied to any other small dataset size like hate_speech or abusive language and text_classification problem using any machine_learning model .',\n",
       "  'then it uses a convolutional_neural_network with an attention_mechanism to select valid information in the text and obtain the overall vector representation of the text',\n",
       "  'as kgs grow , previous alignment results face the need to be revisited while new entity_alignment waits to be discovered',\n",
       "  'specifically , in the biomedical domain , the size of the data has increased exponentially in the last decade , and with the advances in the technologies to collect and generate data , a faster growth rate is expected for the next years',\n",
       "  'our released codes are available at https : //github.com/cgcl_codes/dhunet .',\n",
       "  'existing_approaches mainly adopt a supervised manner and heavily rely on coupled image_report pairs',\n",
       "  'beng builds upon the successful benchmarking platform gerbil , is opensource and is publicly available along with the data it contains .',\n",
       "  'therefore , its application in kgqa can further improve the accuracy of answer prediction',\n",
       "  'the novelties of our work are the five layer model structure and the attention_mechanism',\n",
       "  'in our model , sentences are encoded by recurrent convolutional_neural_network ( rcnn ) , which combines the advantages of birnn and cnn flexibly , containing more information of sentence',\n",
       "  'besides , the ablation study and discussion demonstrated that ace_adp could not only effectively extract rare entities but also maintain a powerful ability to predict new entities in new datasets with high accu_racy',\n",
       "  'experimental_results show a superior_performance than other baselines , especially significant improvements on the automated extracted kg .',\n",
       "  'the tag decoding layer leverages conditional_random fields ( crf ) to solve the dependency between the output tags and obtain the global optimal label sequence',\n",
       "  'to push forward the future_research on expert_sensitive task_oriented_dialogue system , we first release a large_scale medical dialogue consultant benchmark ( mdg_c ) with 16 gastrointestinal diseases for evaluating consultant capability and a medical dialogue diagnosis benchmark ( mdg_d ) with 6 diseases for measuring diagnosis capability of models , respectively',\n",
       "  'the decision_making ability of the system can improve by incorporating human knowledge to guide the vision_based algorithms',\n",
       "  'we introduce qampari , an odqa benchmark , where question answers are lists of entities , spread across many paragraphs',\n",
       "  'the evaluation shows the framework achieves remarkable improvement on f1_score',\n",
       "  'experimental_results_demonstrate the better performance of our method',\n",
       "  'the code is available at ( https : //github.com/ruizhang_ai/gcp/ )',\n",
       "  'different from existing kgc systems , gbuilder provides a flexible and user_defined pipeline to embrace the rapid development of ie models',\n",
       "  'extensively experiments on two benchmarks show that our method improves the performance than the state_of_the_art baseline and some cases study also confirmed the explainability of our model .',\n",
       "  'experimental_results significantly outperform baselines by nearly 1.72.0 in f1 on three public datasets , docred , dialogre , and mpdd',\n",
       "  'firstly , entity type and relation type are defined , and then multi_source data are fused , and then entity_recognition of bio annotated data_sets is carried out by using the bert_bilstm_crf model',\n",
       "  'we further show the superiority of our kr_ds on a newly collected medical dialogue system dataset , which is more challenging retaining original self_reports and conversational data between patients and doctors .',\n",
       "  'second , the word_vector is sent to a bidirectional_long_short_term memory model for further training to obtain contextual features',\n",
       "  'this paper conducts experiments on the public dataset weibo ner and the self_built food domain dataset food',\n",
       "  'in the traditional question_answering system , the quality of answers was not high due to incomplete data and distinctive vocabulary',\n",
       "  'the kgs , both with and without type information , are considered',\n",
       "  'first , kg_based approaches ignore the information in the conversational context but only rely on entity relations and bag of words to recommend items',\n",
       "  'experiment results show that mmm achieves 87.13 % accuracy on the 5_shot text_classification benchmark amazon review sentiment_classification ( arsc ) , outperforming other baselines , such as induction networks ( 85.63 % ) and distributional signatures ( 81.16 % )',\n",
       "  'named_entity recognition ( ner ) , as a core technology for constructing a geological hazard knowledge_graph , has to face the challenges that named_entities in geological hazard literature are diverse in form , ambiguous in semantics , and uncertain in context',\n",
       "  '2018 ; li et al',\n",
       "  'compared to mainstream models ssan , gain , and atlop , fedre_kd improved the f1score by 22.07 , 20.06 , and 22.38 , respectively .',\n",
       "  'besides , our proposed framework could be easily adaptive to various kge models and explain the predicted results .',\n",
       "  'it has received much attention not only in academia but also in industry',\n",
       "  'finally , the personality vector of each entity node is learned for prediction by designing a walk strategy on the personality heterogeneous graph',\n",
       "  'extensive_experiments were conducted on the benchmark dataset , and the results demonstrate that our framework outperforms state_of_the_art baseline models regarding effectiveness and efficiency .',\n",
       "  'the experimental_results show the effectiveness of our proposed method',\n",
       "  'we conduct experiments on the conll dataset and tac dataset , and various datasets provided by gerbil platform',\n",
       "  'experimental_results show that the pre_trained_language model is comparable to the best approach of this evaluation task , and the context_related pre_trained_language model performs better .',\n",
       "  'code is available at https : //github.com/spyflying/cmpc_refseg .',\n",
       "  'since these improvements are reported in aggregate , however , little is known about ( i ) how to select the appropriate knowledge for solid performance across tasks , ( ii ) how to combine this knowledge with neural language_models , and ( iii ) how these pairings affect granular task performance',\n",
       "  'to this end , we utilize the tool of boolean_circuit to obtain all the theoretical results given in this paper',\n",
       "  'the existing research methods mainly apply traditional machine_learning or deep_learning algorithms for short_text classification',\n",
       "  'we learn structure_based representations of entities and relations and explore a deep convolutional_neural_network with attention to encode description_based representations of entities',\n",
       "  'paper_title : a neural topic model with word_vectors and entity vectors for short texts ; paper_abstract : traditional topic models are widely used for semantic discovery from long texts',\n",
       "  'kgs , by contrast , contain only positive samples , necessitating that negative samples are generated by replacing the head/tail of predicates with randomly chosen entities',\n",
       "  'chinese word_segmentation results are the basis for computers to understand natural_language',\n",
       "  'however , existing rl_based methods have some problems , such as unstable training and poor reward function',\n",
       "  'at present , there are many adversarial algorithms for computer vision , but there are few for nlp models , and there is almost no algorithm for question answer task',\n",
       "  'in this paper , we propose an unsupervised method to cast the knowledge contained within language_models into kgs',\n",
       "  'however , recent kge models achieve performance improvements by excessively increasing the embedding dimensions , which may cause enormous training costs and require more storage space',\n",
       "  'extensive_experiments were conducted on the benchmark dataset',\n",
       "  'existing_approaches for this task have yielded impressive results when the training and testing data are from the same domain',\n",
       "  'experimental_results show that our review score predictor reaches 71.4 % _100 % accuracy',\n",
       "  '( code is publicly available at https : //github.com/ruiqingding/knowledgeda',\n",
       "  'paper_title : image_captioning with internal and external_knowledge ; paper_abstract : automatically generating a human_like description for a given image is a potential research in artificial_intelligence , which has attracted a great of attention recently',\n",
       "  'first , we use a small number of manually labeled biomedical entities as seeds to label some biomedical texts and learn their features autonomously',\n",
       "  'extensive_experiments have been conducted and the results demonstrate that our algorithm could achieve better video captioning performance than the state_of_the_art algorithms .',\n",
       "  'experiments on benchmarks verify the state_of_the_art_performance of our method .',\n",
       "  'in this article , we take extra knowledge information of medical encyclopedia into account and we associate the original text in the named_entity recognition task with its encyclopedic knowledge to enhance the ability of entity_recognition through the establishment of the connection and interaction of the joint_network',\n",
       "  'experiments confirm the effectiveness and efficiency of our method on several benchmark data_sets .',\n",
       "  'extensive_experiments demonstrate the effectiveness of our model architecture .',\n",
       "  'the prior methods of dre do not meaningfully leverage speaker information_they just prepend the utterances with the respective speaker names',\n",
       "  'paper_title : context_driven image_caption with global semantic_relations of the named_entities ; paper_abstract : automatic image_captioning has achieved a great progress',\n",
       "  'the obtained results are compared to the fine_tuned bert and biobert models trained on the same dataset',\n",
       "  'finally , we empirically evaluate the performance of the proposed approach on real benchmark data',\n",
       "  'finally , crf is used to implement the sequence labeling task',\n",
       "  'in most previous methods , features are usually extracted by the hand_crafted templates',\n",
       "  'paper_title : leveraging concept_enhanced pre_training model and masked_entity language_model for named_entity disambiguation ; paper_abstract : named_entity disambiguation ( ned ) refers to the task of resolving multiple named_entity mentions in an input_text sequence to their correct references in a knowledge_graph',\n",
       "  'named_entity ) approaches to identify mentions of genes , diseases , drugs , chemicals , symptoms , chinese herbs and patent medicines , etc',\n",
       "  'this paper studies and implements chinese coreference_resolution from two aspects : named_entity recognition and coreference_resolution',\n",
       "  'a small number of labelled data are used to pre_train the model , and then , a large number of unlabelled data are used to fine_tune the pre_training model',\n",
       "  'furthermore , most previous_works focus on binary ddi prediction whereas the multi_typed ddi pharmacological effect prediction is a more meaningful but harder task',\n",
       "  'evaluation results on trec web track ad_hoc task demonstrate that all of the four_way interactions in the duet are useful , the attention_mechanism successfully steers the model away from noisy entities , and together they significantly outperform both word_based and entity_based learning to rank systems .',\n",
       "  'in the field of artificial_intelligence , there is a very important sub field _ natural_language processing ( nlp )',\n",
       "  'experiments are conducted to show the effectiveness of our model .',\n",
       "  'our extensive_experiments have shown that the proposed approach consistently achieves the state_of_the_art_performance across all the test datasets in both unsupervised and supervised settings and the improvement margins are considerable .',\n",
       "  'compared with the modern chinese corpus , it is irrecoverable and specially organized , making it difficult to be learned by existing pre_trained_language_models',\n",
       "  'therefore , this paper synthetically uses deep_learning models such as bidirectional lstms , conditional_random fields and pcnn to carry out entity_recognition and relationship_extraction for text data , such as electronic_medical_record and medical community , to construct visual knowledge_graph',\n",
       "  'according to the results , it can conclude that the application of the bert model instead of the word2vec algorithm for word_vector training is helpful to the model recognition , and the exclusive dictionary word_segmentation and part of speech classification of nuclear_technology texts contribute to improving the quality of labeled_data',\n",
       "  'to address these challenges , we propose a novel and effective closed_loop neural_symbolic learning framework enginekg via incorporating our developed kge and rule learning modules',\n",
       "  'for cross_institutional medical code mapping , the top 1 and top 5 accuracy were 91.0 % and 97.5 % when mapping medication codes at va to rxnorm medication codes at mgb ; 59.1 % and 75.8 % when mapping va local laboratory codes to loinc hierarchy',\n",
       "  'compared with the other existing medical question_answering systems , our system adopts several state_of_the_art technologies including medical entity_disambiguation and medical dialogue_generation , which is more friendly to provide medical services to patients',\n",
       "  'our contributions in this paper are twofold',\n",
       "  'however , real_world kgs do not remain static , but rather evolve and grow in tandem with the development of kg applications',\n",
       "  'some known drugs linked to covid_19 in the literature were identified , as well as some candidate drugs that have not yet been studied',\n",
       "  'experiments conducted on widely baselines show that the proposed framework is superior to the state_of_the_art_methods .',\n",
       "  'further , we propose a hybrid model based on neural_network matrix factorization ( nnmf ) that considers multi_source signals simultaneously',\n",
       "  'by introducing boundary detection unit , our model extracts the boundaries of entities and restrict the number of candidate entities',\n",
       "  'our approach can be generalized to other diseases as well as to other clinical questions .',\n",
       "  'we conducted experiments on the datasets flickr30k and mscoco , which have 31,783 and 123,287 images , respectively',\n",
       "  'in this paper , we proposed a named_entity recognition model , albert_ bigru_ crf , and a relationship_extraction model , albert_bigru_attention , which were embedded with albert ( a lite bidirectional_encoder_representation_from_transformers ) pre_training language_model',\n",
       "  'using dictionary is the simplest way for labeling , but it is difficult to obtain a versatile dictionary and usually a dictionary for one corpus is not suitable for another corpus due to bad transferability',\n",
       "  'we also designed a hamming lower_bound label encoding algorithm to encode the label representations in lower dimensions',\n",
       "  'however , most existing_methods concentrate on modern chinese and ignore the classical_chinese due to its complexity , making research in this field relatively lacking',\n",
       "  'the experimental result proves the effectiveness of the proposed method .',\n",
       "  'finally , the similarity between the input image and text is calculated based on knowledge_fused features to complete the matching process',\n",
       "  'evaluation results show that our approach significantly_outperforms the state of the art on two public benchmarks .',\n",
       "  'owing to the enormous rise in the costs of pharmaceutical r & d , several pharmaceutical companies are leveraging repurposing strategies',\n",
       "  'despite all the notable advancements , current kgqa systems only focus on answer generation techniques and not on answer verbalization',\n",
       "  'however , the precise emotions of one conversational bot are crucial to improve user satisfaction',\n",
       "  'an attention_mechanism was added between the segmented maximum pooling layer and the classification layer , further to extract the high_level semantics',\n",
       "  'experiments on three datasets show that the proposed method clearly outperforms state_of_the_art_methods',\n",
       "  'more specifically , our framework ( diva ) is composed of three modules , i.e',\n",
       "  'in terms of specific categories , it achieved f1 values as high as 95.81 % , 97.76 % , and 97.23 % on easily identifiable entities such as crop , disease , and pest',\n",
       "  'the experimental_results_demonstrate the effectiveness of the proposed model on several datasets .',\n",
       "  'paper_title : ace_adp : adversarial contextual embeddings based named_entity recognition for agricultural diseases and pests ; paper_abstract : entity_recognition tasks , which aim to utilize the deep_learning_based models to identify the agricultural diseases and pests_related nouns such as the names of diseases , pests , and drugs from the texts collected on the internet or input by users , are a fundamental component for agricultural knowledge_graph construction and question_answering , which will be implemented as a web_application and provide the general public with solutions for agricultural diseases and pest_control',\n",
       "  'extensive_experiments on both mdg_c and mdg_d benchmarks demonstrate the superiority of our hgr over state_of_the_art knowledge grounded approaches in general fields of medical dialogue system .',\n",
       "  'experiments with msra datasets show that this method outperforms word_based and character_based baselines and achieves a higher recall_rate compared to other methods .',\n",
       "  'paper_title : incorporating domain_knowledge into language_models by using graph convolutional_networks for assessing semantic textual similarity : model development and performance comparison ; paper_abstract : background : although electronic_health_record systems have facilitated clinical documentation in health_care , they have also introduced new challenges , such as the proliferation of redundant information through the use of copy and paste commands or templates',\n",
       "  'these tasks are generally focused on tagging common entities , but domain_specific use_cases require tagging custom entities which are not part of the pre_trained_models',\n",
       "  'the classifier is equipped with 2 components_ontology and machine_learning data_model',\n",
       "  'the proposed model incorporates category knowledge and local knowledge for improved data representation',\n",
       "  'we conduct extensive_experiments to evaluate existing popular methods , and find that they fail to achieve promising performance',\n",
       "  'we introduce its system design , architecture , algorithms , functions , and implementation',\n",
       "  'firstly , during the stage of topic entity_recognition , a deep transition model is constructed to extract topic entities , and an efficient entity_linking strategy is presented , which combines character matching and entity_disambiguation model',\n",
       "  'the experimental_results on real_world data demonstrate the effectiveness of the proposed model .',\n",
       "  'we finalize this survey with open research problems relevant to mmkgs .',\n",
       "  'while these models are undeniably useful , it is a challenge to quantify their performance beyond traditional accuracy metrics',\n",
       "  'additionally , our model consistently outperforms the state_of_the_art model in domain adaptation settings',\n",
       "  'the question and answer system implemented by this method has a quick query response and can answer relatively complex_questions',\n",
       "  'understanding the nature of this disease , when there is no available cure , is vital to encourage accurate clinical diagnosis and drug_discovery prospects',\n",
       "  'we aim to develop this scheme further for real_world robot intelligence in human_robot interaction .',\n",
       "  'recent studies attempt to employ external_knowledge to improve classification performance , but they ignore the correlation between external_knowledge and have poor interpretability',\n",
       "  'extensive_experiments on common odqa benchmark_datasets ( natural question and triviaqa ) demonstrate that kg_fid can improve vanilla fid by up to 1.5 % on answer exact match score and achieve comparable performance with fid with only 40 % of computation cost .',\n",
       "  'experimental_results on a large_scale dataset demonstrate that kecrs outperforms state_of_the_art chit_chat_based crs , in terms of both recommendation accuracy and response_generation quality .',\n",
       "  'artificial intelligence_based methods are promising in their potential to discover new treatment options',\n",
       "  'when the amount of literature available is vast , it is important to represent the disease domain as completely as possible',\n",
       "  'this approach combines the advantages of prior_knowledge and neural_networks',\n",
       "  'the combination of artificial_intelligence and traditional industries the emergence of artificial_intelligence has made these traditional industries shine',\n",
       "  'in detail , we detect the context using region proposal network ( rpn ) to extract nodes as the input of the graph convolution network ( gcn ) , which transfers the convolution operation from euclidean data_structure to noneuclidean data_structure',\n",
       "  'in this tutorial , we provide a comprehensive overview on recent research and development in this direction',\n",
       "  'by grouping entities by types , we are better able to take advantage of the benefits of mil and further denoise the training signal',\n",
       "  'this pipeline was evaluated on a manually crafted gold_standard , yielding competitive results',\n",
       "  'experimental_results on open_domain corpus ( nlpcc2014 ) demonstrate the validity of the proposed method',\n",
       "  'we test our model on fb15k_237 and nell_995 datasets with different tasks',\n",
       "  'however , these models have not been thoroughly compared , and they were only tested on self_created datasets',\n",
       "  'lastly , a disease identification model based on image_text multimodal collaborative representation and knowledge assistance ( itk_net ) was constructed',\n",
       "  'second , we perform multi_task_learning experiments between open_domain qa and task_oriented dialog , and benchmark our model on a popular nlg dataset',\n",
       "  'after experimental comparison , the proposed method performs better on both fb15k and win18 datasets , with a numerical improvement of about 2.6 % compared with the original translation model , which verifies the reasonableness and effectiveness of the method .',\n",
       "  'for crisis cases , however , kara model achieved a much higher recall than the baseline ( 0.870 vs 0.791 )',\n",
       "  'in these models , token sequence of a text is feeded into a neural_network to generate text with the guidance of latent topics',\n",
       "  'recently , kg has been applied in various fields , such as intelligent search , question_answering systems and so on',\n",
       "  'paper_title : sliqa_i : towards cold_start development of end_to_end spoken language interface for question_answering ; paper_abstract : question_answering ( qa ) has become a key capability for voice enabled personal assistants to automatically answer various user questions',\n",
       "  'additional machine_learning methods can be applied to this representation to make predictions within genomic , pharmaceutical , and clinical domains',\n",
       "  'first , the key technologies for constructing knowledge_graph are explained in detail from named_entity recognition , entity_relation_extraction and entity_alignment',\n",
       "  'in this way , we generate the news image_caption with named_entities',\n",
       "  'the experimental_results show that the proposed methods achieve the state_of_the_art_performance in terms of both automatic and human evaluation .',\n",
       "  'the c_statistics of kara and the baseline were 0.815 and 0.760 , respectively',\n",
       "  'the evaluation results demonstrate that our approach significantly_outperforms the state_of_the_art_baselines , with a significance t_test p & lt ; 0.041',\n",
       "  'the model also predicts ddis using multiple labels rather than single or binary labels',\n",
       "  'we further design a reward based on a multiple_choice cloze test to drive the model to better capture entity interactions',\n",
       "  'the performance of the bert_pcnn_att_jieba model was compared with the classical cnn , pcnn model , as well as the cnn , pcnn , pcnn_att , and pcnn_jieba models combined with bert under the same data_set and the consistent experimental parameters',\n",
       "  'extensive_experiments on public datasets indicate that our supervised method significantly_outperforms the previous methods and the unsupervised one has competitive performance .',\n",
       "  'the prefix att means the model is based on attention_mechanism',\n",
       "  'reliable and scalable identification of malicious domains , how_ ever , is challenging',\n",
       "  'in this paper , we propose a novel method named dhu_net for addressing the time variability challenge and the dynamic unseen entity challenge derived from it',\n",
       "  'experiments on the benchmark dataset redial show our recindial model significantly surpasses the state_of_the_art_methods',\n",
       "  \"then the system predicts the user 's question intention and use the trained question classifier to predict the category number\",\n",
       "  'our approach can be applied to the task of chinese word_segmentation in specific domains containing rare terms .',\n",
       "  'besides , the lack of large labeled datasets has been a serious obstacle impending further research',\n",
       "  'compared to the previous methods , our framework has an excellent performance after experimental verification',\n",
       "  'combining cmpc_i or cmpc_v with tgfe can form our image or video version referring segmentation frameworks and our frameworks achieve new state_of_the_art performances on four referring image_segmentation benchmarks and three referring video segmentation benchmarks respectively',\n",
       "  'among the methods , unified_qa reaches the best performance on the bdd_qa dataset with the adaptation of multiple formats of question answers',\n",
       "  'moreover , acp_based models are shown to outperform the baselines .',\n",
       "  'moreover , the character case also has a certain impact on model performance .',\n",
       "  'our source_code and datasets are public available at https : //github.com/dice_group/multpax .',\n",
       "  'our results demonstrate that stonkgs outperforms both baselines , especially on the more challenging tasks with respect to the number of classes , improving upon the f1_score of the best baseline by up to 0.084 ( i.e',\n",
       "  'intelligent support for mobile_phone detection process with the knowledge_graph .',\n",
       "  'we adopt two benchmarks , i.e. , activitynet_caption and charades_sta , to evaluate our model and conduct comprehensive experiments to analyze the effectiveness of each component',\n",
       "  'the study also presents experimental_results obtained by combining the different components of the strategy',\n",
       "  'our approach obtained the highest results of subtask b compared to the other task participants .',\n",
       "  'result our qg_srgr model is trained , validated and tested on the vqa v2',\n",
       "  'extensive_experiments are carried out on five real datasets and synthetic datasets , and the ukge_ms are compared with five corresponding algorithms',\n",
       "  'extensive_experiments on vmr dataset and visr dataset demonstrate the effectiveness of the proposed framework .',\n",
       "  'we conducted extensive_experiments on multiple kgs',\n",
       "  'the saat outputs were compared with the gold_standard , manually developed by two experts',\n",
       "  'using the training corpus , we design a hierarchical svm classifier to realize the entity knowledge_extraction',\n",
       "  'by accessible to all , we mean that the proposed software_framework is freely available on github and zenodo with an open_source license',\n",
       "  'evaluated on mimic_cxr and iu_xray datasets , our method is able to outperform previous state_of_the_art models on these two datasets .',\n",
       "  'as we evaluated , complex chinese_language processing techniques , such as segmentation and parsing , were not necessary for practice and complex architectures were not necessary to build the qa system',\n",
       "  'experimental_results show the effectiveness of the proposed method .',\n",
       "  'this study dives into a more realistic and challenging setting where new entities emerge in multiple batches',\n",
       "  'then , we discuss the design and implementation of this approach within an automatic community_shared software_framework ( a.k.a',\n",
       "  'experimental_results significantly outperform baselines by nearly 1.7 % 2.0 % in f1 on three public datasets , docred , dialogre , and mpdd',\n",
       "  'this is a comparable result with the known approaches to solving this problem',\n",
       "  'among them , the average accuracy of kgipsl on the yago dataset is 14.9 % higher than that of the baseline method .',\n",
       "  'in contrast to adding context to experiments , a considerable amount of training_data is available to support target identification',\n",
       "  'experiments on wikitext_103 , wmt19 , and enwik8 english datasets_demonstrate that our approach produces a better language_model in terms of perplexity and bits per character',\n",
       "  'finally , we conduct extensive_experiments with state_of_the_art kgqa models and compare their performance on crunchqa',\n",
       "  'according to the audit knowledge_graph , a character_level convolutional_neural_network is trained and an intelligent audit question_answering system is built based on the semantic_similarity',\n",
       "  'then the text prediction sequence was obtained from the crf layer',\n",
       "  'this is important in high_risk domains such as healthcare , intelligence , etc',\n",
       "  'at the end of this paper , we introduce the result of the experiment and a korean template generation module developed using srdf .',\n",
       "  'synchronous modulator is a synchronous_motor in a special operating state',\n",
       "  'first of all , the bilstm+crf model was used to identify the inner entities that appeared frequently , and then the dimension reduction of the identified inner entity feature matrix was connected outer entity feature matrix to retain the complete inner entity feature information',\n",
       "  'it solves the problems that it is difficult to recognize entities from insufficient context , and traditional models usually neglect the relevance between sentiment entities and aspect entities',\n",
       "  'this paper summarizes the existing work in the field of dsre , and pays more attention to the methods based on deep_learning',\n",
       "  'the dataset is available at < uri > https : //cricvqa.github.io < /uri > .',\n",
       "  'thirdly , the bayesian classifier is used to classify the questions and the bidirectional_long_short_term memory ( bilstm ) combined with conditional_random fields ( crf ) method is applied to extract contents from the input questions',\n",
       "  'however , in most enterprises , it is challenging to quickly construct kgs with multi_source and heterogeneous data and apply kgs to meet diverse business demands',\n",
       "  'with back_propagation from ranking labels , the model learns simultaneously how to demote noisy entities and how to rank documents with the word_entity duet',\n",
       "  'however , large_scale and high_quality datasets are rare due to the high complexity and huge workforce cost of making such a dataset',\n",
       "  'in addition , the experimental_results of comparison with deep_learning algorithms also show that the proposed model has better performance in classification of short_text from other fields',\n",
       "  'we also obtain better or comparable performance compared to systems that are fine_tuned from large pretrained_language_models',\n",
       "  'our algorithms accuracy was evaluated against gensim , largely improving its accuracy',\n",
       "  'in medical domain , vqa systems are still in their infancy as the datasets are limited by scale and application scenarios',\n",
       "  'before training , the cnes in the corpus will be masked , and then use the masked corpus training the semantic model through bilstm_crf , which can verify whether the context semantics of the corresponding location entities are correct',\n",
       "  'however , it is not an easy task regarding the huge number of existing applications that are available for use',\n",
       "  'in summary , semnet 2.0 is a comprehensive open_source software for significantly faster , more effective , and user_friendly means of automated biomedical lbd',\n",
       "  'the proposed model is an encoder_decoder architecture',\n",
       "  'the framework is further extended with the capability to extract attribute value across multiple product categories with a single model , by training the decoder to predict both product category and attribute value and conditioning its output on product category',\n",
       "  'the experimental_results show that our proposed method has very good performance on both cross_kg and cross_language data_sets',\n",
       "  'to this end , we propose a novel context_aware graph ( cag ) neural_network',\n",
       "  'comprehensive experimental_results on two real_world crs datasets ( including both english and chinese with about 10,000 dialogues ) show the superiority of our proposed method',\n",
       "  'hence , vector_based similarity representations are learned from multiple perspectives to model multi_level alignments comprehensively',\n",
       "  'experimental_results show that the proposed framework achieves the best performance compared with the state_of_the_art_methods on large_scale kgs .',\n",
       "  'experiments show that kidrr achieves improvements of nearly 2.2 % and 1.6 % relative to recall @ 1 on flicr30k and mscoco , respectively , compared to the current state_of_the_art_baselines .',\n",
       "  'existing works mainly use a sequence_based deep neural encoder to process questions',\n",
       "  'standard rbms , however , operate on bag_of_words assumption , ignoring the inherent underlying relational structures among words',\n",
       "  'to evaluate the proposed approach , we use salzburgerland kg , a real kg describing touristic entities of the region of salzburg , austria',\n",
       "  'then , the multi_head attention_mechanism is introduced to assist the bidirectional_long_short_term memory network ( bilstm ) in acquiring long_distance context_dependence',\n",
       "  'the source_code and datasets of this paper are available at https : //github.com/ngl567/lcge .',\n",
       "  'system testing shew that the q & a system has a high accuracy_rate in response of tree hole rescue questions',\n",
       "  'experiments show that the method is feasible and accurate .',\n",
       "  \"existing language_models such as word2vec , elmo , and bert have various disadvantages , e.g. , they ca n't solve the problem of polysemous words and have poor ability of context fusion and computational efficiency\",\n",
       "  'compared with other models , such as lstm model , lstm_crf model and bilstm_crf model , att_bilstm_crf had obvious advantages in different size corpus , and it can effectively identify entities for agricultural texts .',\n",
       "  'named_entity recognition has achieved an f1_score of 89.37 % , and relation_extraction has achieved an f1_score of 72.64 %',\n",
       "  'experiments with several machine_learning methods and deep_learning methods show that the proposed method has a certain improvement in f_score .',\n",
       "  'the results show that the method proposed in this paper is effective and has certain scalability and popularization .',\n",
       "  'in this paper , we explore multilingual kg completion , which leverages limited seed alignment as a bridge , to embrace the collective knowledge from multiple languages',\n",
       "  'our approach introduces the use of context from a knowledge_graph to generate improved feature representations for cognate detection',\n",
       "  'research related to covid_19 , biomedicine , and many other communities can benefit from openkg_covid19',\n",
       "  'this multi_modal retrieval model can be integrated into any ( neural_network ) model pipeline',\n",
       "  'the application is language and corpus agnostic , but can be tuned for special needs of a specific language or a corpus',\n",
       "  'we find that such resources provide insignificant gains to the performance of fine_tuned language_models',\n",
       "  'when extracting entities from electronic_health_records ( ehrs ) , ner models mostly apply long_short_term_memory ( lstm ) and have surprising performance in clinical ner',\n",
       "  'secondly , the labeled text sequence was vectorized at character level by word_vector technology',\n",
       "  'experiments conducted on widely adopted benchmarks show that the proposed model is superior to the latest kbqa method .',\n",
       "  'data have become an important source of competitive_intelligence',\n",
       "  'the code and datasets are available at https : //github.com/pkusjh/lass .',\n",
       "  'despite the short time on the market , it seems to be quickly well_noticed with 4.6 thousand stars on github at the moment',\n",
       "  'to overcome mvkgf , entity_alignment is the most studied',\n",
       "  '( code is released at https : //github.com/alexa/dstqa )',\n",
       "  'we ranked 9th out of 31 teams in the competition',\n",
       "  \"it fitted the bert_ala+bilstm+crf model 's output of bilstm layer and crf layer\",\n",
       "  'through analysis of extensive experiment results , our method could effectively discover more abnormal data without big loss of time costs , has strong generalization , and correspondingly improves the performance for abnormal detection .',\n",
       "  'extensive evaluations over both an open_released english dataset and our chinese dataset demonstrate that our approach conkadi outperforms the state_of_the_art approach ccm , in most experiments .',\n",
       "  'it enjoys further performance boost when employing a pre_trained bert encoder , outperforming the strongest baseline by 17.5 and 30.2 absolute gain in f1_score on two public datasets nyt and webnlg , respectively',\n",
       "  'our approach is based on a versatile probabilistic formulation _ the restricted boltzmann machine ( rbm ) _ where the underlying graphical_model is an undirected bipartite_graph',\n",
       "  'with thousands of packages available per stable_distribution , encapsulating algorithms , sensor drivers , etc. , it is the de facto middleware for robotics',\n",
       "  'the decoder part consists of a multi_relation classifier for the relation_classification task , and an improved long_short_term_memory for the entity_recognition task',\n",
       "  'boe loss provides an additional supervision signal to guide crs to learn from both human_written utterances and kg',\n",
       "  'in addition , the application status of medical_knowledge maps in clinical decision_support , medical intelligence semantic retrieval , medical question_answering system and other medical services are introduced',\n",
       "  'extensive evaluations showed that our model_outperforms the state_of_the_art models on the opendialkg dataset on multiple metrics .',\n",
       "  'finally , we employ a capsule network to extract different linguistic units ( word and phrase ) from the relations , and dynamically predict the optimal option based on the extracted units',\n",
       "  'we develop an open_source library including 12 representative embedding_based entity_alignment approaches , and extensively evaluate these approaches , to understand their strengths and limitations',\n",
       "  'then one bidirectional_long_short_term memory ( bilstm ) layer is applied to semantically encode the input text',\n",
       "  'experimental_results show the best performance compared to the state_of_art on complex_questions .',\n",
       "  'without training on large_scale labeled_data , current ner methods based on deep_learning models can not identify specific geological disaster entities from geological disaster situation reports',\n",
       "  'as one of the most import research areas , entity_relation_extraction applies usual recurrent neural_networks ( rnns ) and convolutional_neural_networks ( cnns ) and has achieved good results',\n",
       "  'comparative evaluations of instance matching systems can inform us about the performance of such systems regarding artificial benchmarks or real_world data challenges',\n",
       "  'this method uses the weighted graph_convolutional_network to model the semantic dependency_parsing of the entity description and construct the semantic dependency_parsing graph_embedding',\n",
       "  'experimental_results_demonstrate the effectiveness of our method',\n",
       "  'meanwhile , it validated the precision in nuclear_technology knowledge entity_recognition for collected data , which could be used in pre_procedure in nuclear_technology knowledge_graph construction .',\n",
       "  'extensive_experiments demonstrate the effectiveness of our model .',\n",
       "  'paper_title : review of deep_learning_based topic model ; paper_abstract : as a research hotspot for more than twenty years , topic model plays an important role in semantic analysis of multi_documents.the topic model is adept in extracting groups of keywords from documents to represent their core idea , and thus provides crucial support for document_classification , information_retrieval , automatic summarization of multi_documents , sentiment_analysis and so on',\n",
       "  'to encourage reproducible results , we make our code and mwp dataset public available at \\\\url { https : //github.com/tal_ai/make_emnlp2021 } .',\n",
       "  'the experimental_results show that our model achieves a sota performance of 72.64 % and a good performance of 69.68 % for f1 values on the public and self_built datasets , respectively',\n",
       "  'evaluation on the hotpotqa dataset in the distractor setting shows that our method outperforms the published sota entity_based method in five out of six metrics .',\n",
       "  'experimental_results show that the model can achieve good results on test data .',\n",
       "  'the special word attention emphasizes on word importance when focusing on different regions of the input image , and makes full use of the internal annotation knowledge to assist the calculation of visual attention',\n",
       "  'however , there are many named_entities in news text , and existing_approaches are unable to directly generate named_entities in the news image_caption',\n",
       "  'empirically , extensive_experiments on three real_world tkgs demonstrate the superiority of metatkgr over state_of_the_art_baselines by a large margin .',\n",
       "  'we show that our model significantly_outperforms other baselines',\n",
       "  'we conducted fine_tuning training on chinesedailynercorpus',\n",
       "  'our experimental_results also demonstrate the effectiveness and robustness of zs_ska .',\n",
       "  'the datasets including cm3kg at https : //github.com/wengsyx/cm3kg and cmcqa at https : //github.com/wengsyx/cmcqa are also released to further promote future_research .',\n",
       "  'extensive_experiments are conducted on public real_world datasets to show the superior_performance of the proposed method',\n",
       "  'for each module , we discuss the details of our building methods',\n",
       "  'this framework specifically consist of two pre_trained_language_models to provide the embed dings and a sequence labeling model to tag the entity labels',\n",
       "  'moreover , chinese has a very complex grammar , and the word_segmentation criteria are varied with the contexts',\n",
       "  'existing_approaches to solve this problem take a two_step approach',\n",
       "  'at last , the fully connected layer was used to decode the information and output the crop disease entity_relation triples',\n",
       "  'this method usually results in many instances with incorrect labels',\n",
       "  'the experimental_results show that it has a significant performance gain over several different state_of_the_art_baselines .',\n",
       "  'paper_title : entity_aware image_caption generation ; paper_abstract : current image_captioning approaches generate descriptions which lack specific information , such as named_entities that are involved in the images',\n",
       "  'however , these two models inherit the bag_of_word assumption of the standard lda model , which disable the exploitation of more distinguishable n_gram features',\n",
       "  ...],\n",
       " 'examples': []}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root.children[0].children[0].external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_papers = taxo.rankPapers([curr_node.emb['sentences'] for curr_node in taxo.root.children], 2, internal=True, phrase=False, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(66, 0),\n",
       " (65, 1),\n",
       " (71, 2),\n",
       " (69, 3),\n",
       " (51, 4),\n",
       " (47, 5),\n",
       " (55, 6),\n",
       " (50, 7),\n",
       " (48, 8),\n",
       " (70, 9),\n",
       " (24, 10),\n",
       " (54, 11),\n",
       " (4, 12),\n",
       " (62, 13),\n",
       " (25, 14),\n",
       " (11, 15),\n",
       " (22, 16),\n",
       " (53, 17),\n",
       " (67, 18),\n",
       " (59, 19)]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_embs = {idx: paper_emb for idx, paper_emb in enumerate(sentence_model.encode([paper.title for paper in collection]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING using_llms_on_graphs; remaining in queue: deque([])\n",
      "['1', '2', '3']\n",
      "next!\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "label_free node_classification on graphs with large_language_models ( llms )\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "text2mol : cross_modal molecule retrieval with natural_language queries\n",
      "functional output regression for machine_learning in materials_science\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "graph_aware language_model pre_training on a large graph corpus can help multiple graph applications\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "gimlet : a unified graph_text model for instruction_based molecule zero_shot learning\n",
      "next!\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "graph_neural prompting with large_language_models\n",
      "patton : language_model pretraining on text_rich_networks\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "graph_aware language_model pre_training on a large graph corpus can help multiple graph applications\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "disentangled_representation_learning_with_large language_models for text_attributed_graphs\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "next!\n",
      "git_mol : a multi_modal large_language_model for molecular_science with graph , image , and text\n",
      "molca : molecular graph_language modeling with cross_modal projector and uni_modal_adapter\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "interactive molecular discovery with natural_language\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "translation between molecules and natural_language\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "multi_modal molecule structure_text model for text_based retrieval and editing\n",
      "neighborhood contrastive_learning for scientific document representations with citation embeddings\n",
      "unifying molecular and textual_representations via multi_task language_modelling\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "graphtext : graph_reasoning in text_space\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "molfm : a multimodal molecular foundation_model\n",
      "galactica : a large_language_model for science\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING pure_graphs; remaining in queue: deque([text_rich_graphs, text_paired_graphs])\n",
      "['4']\n",
      "PROCESSING text_rich_graphs; remaining in queue: deque([text_paired_graphs])\n",
      "['8', '14', '20']\n",
      "next!\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "neighborhood contrastive_learning for scientific document representations with citation embeddings\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "label_free node_classification on graphs with large_language_models ( llms )\n",
      "text2mol : cross_modal molecule retrieval with natural_language queries\n",
      "a deep_learning system bridging molecule structure and biomedical text with comprehension comparable to human_professionals\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "enhancing activity_prediction models in drug_discovery with the ability to understand human language\n",
      "translation between molecules and natural_language\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "next!\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "gimlet : a unified graph_text model for instruction_based molecule zero_shot learning\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "graphllm : boosting_graph_reasoning_ability_of_large language_model\n",
      "graph_neural prompting with large_language_models\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "language is all a graph needs\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "next!\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "graph_neural prompting with large_language_models\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "gimlet : a unified graph_text model for instruction_based molecule zero_shot learning\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "graphllm : boosting_graph_reasoning_ability_of_large language_model\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "language is all a graph needs\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "interactive molecular discovery with natural_language\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING text_paired_graphs; remaining in queue: deque([llm_as_predictor, llm_as_encoder, llm_as_aligner])\n",
      "['23', '26']\n",
      "next!\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "functional output regression for machine_learning in materials_science\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "graphllm : boosting_graph_reasoning_ability_of_large language_model\n",
      "translation between molecules and natural_language\n",
      "graph_aware language_model pre_training on a large graph corpus can help multiple graph applications\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "next!\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "graph_neural prompting with large_language_models\n",
      "molfm : a multimodal molecular foundation_model\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "molca : molecular graph_language modeling with cross_modal projector and uni_modal_adapter\n",
      "train your own gnn teacher : graph_aware distillation on textual_graphs\n",
      "galactica : a large_language_model for science\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "graphtext : graph_reasoning in text_space\n",
      "interactive molecular discovery with natural_language\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "text2mol : cross_modal molecule retrieval with natural_language queries\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_encoder, llm_as_aligner, llm_as_predictor, llm_as_aligner])\n",
      "['9', '12', '13']\n",
      "next!\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "translation between molecules and natural_language\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "functional output regression for machine_learning in materials_science\n",
      "a deep_learning system bridging molecule structure and biomedical text with comprehension comparable to human_professionals\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "unifying molecular and textual_representations via multi_task language_modelling\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "enhancing activity_prediction models in drug_discovery with the ability to understand human language\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "git_mol : a multi_modal large_language_model for molecular_science with graph , image , and text\n",
      "galactica : a large_language_model for science\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "next!\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "graphtext : graph_reasoning in text_space\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "graph_neural prompting with large_language_models\n",
      "train your own gnn teacher : graph_aware distillation on textual_graphs\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "disentangled_representation_learning_with_large language_models for text_attributed_graphs\n",
      "gimlet : a unified graph_text model for instruction_based molecule zero_shot learning\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "patton : language_model pretraining on text_rich_networks\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "next!\n",
      "language is all a graph needs\n",
      "graph_aware language_model pre_training on a large graph corpus can help multiple graph applications\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_encoder; remaining in queue: deque([llm_as_aligner, llm_as_predictor, llm_as_aligner, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning])\n",
      "['15', '18', '19']\n",
      "next!\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "functional output regression for machine_learning in materials_science\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "translation between molecules and natural_language\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "galactica : a large_language_model for science\n",
      "interactive molecular discovery with natural_language\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "multi_modal molecule structure_text model for text_based retrieval and editing\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "next!\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "graph_neural prompting with large_language_models\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "language is all a graph needs\n",
      "next!\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "graph_neural prompting with large_language_models\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "language is all a graph needs\n",
      "empower text_attributed_graphs learning with large_language_models ( llms )\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "molxpt : wrapping molecules with text for generative pre_training\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_aligner; remaining in queue: deque([llm_as_predictor, llm_as_aligner, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation])\n",
      "['21', '22']\n",
      "next!\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "graphllm : boosting_graph_reasoning_ability_of_large language_model\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "interactive molecular discovery with natural_language\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "graphtext : graph_reasoning in text_space\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "galactica : a large_language_model for science\n",
      "next!\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "minimally_supervised structure_rich_text_categorization via learning on text_rich_networks\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "neighborhood contrastive_learning for scientific document representations with citation embeddings\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "git_mol : a multi_modal large_language_model for molecular_science with graph , image , and text\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "text2mol : cross_modal molecule retrieval with natural_language queries\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "adsgnn : behavior_graph augmented relevance_modeling in sponsored_search\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_aligner, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment])\n",
      "['24', '25']\n",
      "next!\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "functional output regression for machine_learning in materials_science\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "translation between molecules and natural_language\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "a deep_learning system bridging molecule structure and biomedical text with comprehension comparable to human_professionals\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "label_free node_classification on graphs with large_language_models ( llms )\n",
      "unifying molecular and textual_representations via multi_task language_modelling\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "next!\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "molfm : a multimodal molecular foundation_model\n",
      "graph_neural prompting with large_language_models\n",
      "greaselm : graph_reasoning enhanced language_models for question_answering\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "graphtext : graph_reasoning in text_space\n",
      "language is all a graph needs\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "harnessing explanations : llm_to_lm interpreter for enhanced text_attributed graph_representation learning\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "can language_models solve_graph_problems in natural_language ?\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING llm_as_aligner; remaining in queue: deque([graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm])\n",
      "['27']\n",
      "PROCESSING graph_as_sequence; remaining in queue: deque([graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm])\n",
      "['10', '11']\n",
      "next!\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "evaluating large_language_models on graphs : performance insights and comparative_analysis\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "galactica : a large_language_model for science\n",
      "functional output regression for machine_learning in materials_science\n",
      "relm : leveraging language_models for enhanced chemical_reaction prediction\n",
      "can llms effectively leverage graph structural_information through prompts , and why ?\n",
      "interactive molecular discovery with natural_language\n",
      "a deep_learning system bridging molecule structure and biomedical text with comprehension comparable to human_professionals\n",
      "empowering molecule_discovery for molecule_caption_translation with large_language_models : a chatgpt perspective\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "multi_modal molecule structure_text model for text_based retrieval and editing\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "translation between molecules and natural_language\n",
      "unifying molecular and textual_representations via multi_task language_modelling\n",
      "talk like a graph : encoding graphs for large_language_models\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "next!\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "edgeformers : graph_empowered transformers for representation_learning on textual_edge networks\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "e2eg : end_to_end node_classification using graph_topology and text_based node_attributes\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "gnn_lm : language_modeling based on global contexts via gnn\n",
      "learning on large_scale text_attributed_graphs via variational inference\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "touchup_g : improving_feature_representation_through_graph_centric_finetuning\n",
      "disentangled_representation_learning_with_large language_models for text_attributed_graphs\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "patton : language_model pretraining on text_rich_networks\n",
      "textgnn : improving text_encoder via graph_neural_network in sponsored_search\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING graph_empowered_llm; remaining in queue: deque([graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based])\n",
      "[]\n",
      "PROCESSING graph_aware_llm_finetuning; remaining in queue: deque([optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based])\n",
      "[]\n",
      "PROCESSING optimization; remaining in queue: deque([data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based])\n",
      "['16', '17']\n",
      "next!\n",
      "translation between molecules and natural_language\n",
      "graphgpt : graph instruction_tuning for large_language_models\n",
      "llm4dyg : can large_language_models solve spatial_temporal problems on dynamic_graphs ?\n",
      "augmenting low_resource text_classification with graph_grounded pre_training and prompting\n",
      "regression transformer enables concurrent sequence regression and generation for molecular language_modelling\n",
      "functional output regression for machine_learning in materials_science\n",
      "catalyst property_prediction with catberta : unveiling feature exploration strategies through large_language_models\n",
      "mol_instructions : a large_scale biomolecular instruction dataset for large_language_models\n",
      "prot2text : multimodal protein 's function generation with gnns and transformers\n",
      "think_on_graph : deep and responsible reasoning of large_language_model on knowledge_graph\n",
      "what can large_language_models do in chemistry ? a comprehensive benchmark on eight tasks\n",
      "metadata_induced_contrastive learning for zero_shot multi_label text_classification\n",
      "label_free node_classification on graphs with large_language_models ( llms )\n",
      "galactica : a large_language_model for science\n",
      "smiles_bert : large scale unsupervised pre_training for molecular_property_prediction\n",
      "heterformer : transformer_based deep node_representation_learning_on_heterogeneous text_rich_networks\n",
      "twhin_bert : a socially_enriched pre_trained language_model for multilingual tweet representations at twitter\n",
      "simteg : a frustratingly simple approach improves textual graph_learning\n",
      "node feature_extraction by self_supervised multi_scale neighborhood_prediction\n",
      "chemformer : a pre_trained transformer for computational_chemistry\n",
      "next!\n",
      "gpt4graph : can large_language_models understand graph structured_data ? an empirical evaluation and benchmarking\n",
      "structgpt : a general framework for large_language_model to reason over structured_data\n",
      "congrat : self_supervised contrastive pretraining for joint graph and text_embeddings\n",
      "grenade : graph_centric language_model for self_supervised representation_learning on text_attributed_graphs\n",
      "deep bidirectional language_knowledge graph pretraining\n",
      "graph_toolformer : to empower llms with graph_reasoning ability via prompt augmented by chatgpt\n",
      "language is all a graph needs\n",
      "pretraining language_models with text_attributed heterogeneous graphs\n",
      "specter : document_level representation_learning using citation_informed transformers\n",
      "extracting molecular_properties from natural_language with multimodal contrastive_learning\n",
      "patton : language_model pretraining on text_rich_networks\n",
      "reasoning on graphs : faithful and interpretable large_language_model reasoning\n",
      "molfm : a multimodal molecular foundation_model\n",
      "disentangled_representation_learning_with_large language_models for text_attributed_graphs\n",
      "learning_multiplex_representations_on_text_attributed graphs with one language_model encoder\n",
      "a molecular multimodal foundation_model associating molecule graphs with natural_language\n",
      "efficient and effective training of language and graph_neural_network models\n",
      "graphformers : gnn_nested_transformers for representation_learning on textual graph\n",
      "train your own gnn teacher : graph_aware distillation on textual_graphs\n",
      "can large_language_models empower molecular_property_prediction ?\n",
      "ranking-based classification 0.05555555555555555\n",
      "PROCESSING data_augmentation; remaining in queue: deque([knowledge_distillation, prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING knowledge_distillation; remaining in queue: deque([prediction_alignment, latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING prediction_alignment; remaining in queue: deque([latent_space_alignment, graph_as_sequence, graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING latent_space_alignment; remaining in queue: deque([graph_as_sequence, graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING graph_as_sequence; remaining in queue: deque([graph_empowered_llm, rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING graph_empowered_llm; remaining in queue: deque([rule_based, gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING rule_based; remaining in queue: deque([gnn_based, one_step, two_step])\n",
      "[]\n",
      "PROCESSING gnn_based; remaining in queue: deque([one_step, two_step])\n",
      "[]\n",
      "PROCESSING one_step; remaining in queue: deque([two_step])\n",
      "[]\n",
      "PROCESSING two_step; remaining in queue: deque([])\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "class_labels = {}\n",
    "class_mappings = {}\n",
    "paper_scores = {}\n",
    "\n",
    "while queue:\n",
    "    target_node = queue.popleft()\n",
    "    print(f\"PROCESSING {target_node.label}; remaining in queue: {queue}\")\n",
    "\n",
    "    class_ids = [child.node_id for child in target_node.children]\n",
    "    print(class_ids)\n",
    "\n",
    "    if len(target_node.children) <= 1:\n",
    "        continue\n",
    "    \n",
    "    ranked_papers = []\n",
    "    for idx in np.arange(len(class_ids)):\n",
    "        ranked_papers.append(taxo.rankPapers([curr_node.emb['sentences'] for curr_node in target_node.children], idx, internal=True, phrase=False, top_k=20))\n",
    "    \n",
    "    for p in ranked_papers:\n",
    "        print(\"next!\")\n",
    "        for c in p:\n",
    "            print(collection[c[0]].title)\n",
    "\n",
    "    ranked_paper_embs = [average_with_harmonic_series([paper_embs[p[0]] for p in c]) for c in ranked_papers]\n",
    "\n",
    "\n",
    "    node_labels, node_paper_scores, node_mapping = taxo.mapPapers(np.array(list(paper_embs.values())), target_node.children, ranked_paper_embs)\n",
    "\n",
    "    class_labels[target_node.node_id] = node_labels\n",
    "    paper_scores[target_node.node_id] = node_paper_scores\n",
    "    class_mappings[target_node.node_id] = node_mapping\n",
    "\n",
    "\n",
    "    # full_text_sim = cosine_similarity_embeddings(list(paper_embs.values()), ranked_paper_embs)\n",
    "    # full_text_preds = full_text_sim.argmax(axis=1)\n",
    "    # full_text_scores = [p_id in target_node.children[pred].gold for p_id, pred in enumerate(full_text_preds)]\n",
    "\n",
    "    # for idx, paper_id in enumerate(target_node.papers):\n",
    "    #     target_node.children[winner_idxs[idx]].papers[paper_id] = target_node.papers[paper_id]\n",
    "\n",
    "    print(\"ranking-based classification\", sum(full_text_scores)/len(full_text_scores))\n",
    "\n",
    "    for child in target_node.children:\n",
    "        queue.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection[0].rankSentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': [[1],\n",
       "  [0, 1],\n",
       "  [1, 0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [0],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1, 0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [2],\n",
       "  [2],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [2]],\n",
       " '2': [[1, 2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [1, 0],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [2, 1],\n",
       "  [0],\n",
       "  [2, 1],\n",
       "  [1],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [2],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [2],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [1, 0],\n",
       "  [2],\n",
       "  [1, 0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [2],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0, 1],\n",
       "  [0],\n",
       "  [1]],\n",
       " '3': [[1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0]],\n",
       " '8': [[2],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [1],\n",
       "  [1, 2],\n",
       "  [1, 2],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [2, 0],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [0],\n",
       "  [1, 2],\n",
       "  [2, 0],\n",
       "  [1, 2],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [1, 2],\n",
       "  [2, 0],\n",
       "  [2, 1],\n",
       "  [2],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [0, 2],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [2, 0]],\n",
       " '14': [[2],\n",
       "  [2],\n",
       "  [0, 2],\n",
       "  [0],\n",
       "  [2],\n",
       "  [2],\n",
       "  [2],\n",
       "  [0, 2],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [1, 2],\n",
       "  [0, 2],\n",
       "  [1, 2],\n",
       "  [2, 1],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [1, 2],\n",
       "  [2],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [2, 1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [2],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [2, 1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [2, 0],\n",
       "  [],\n",
       "  [],\n",
       "  [0, 1],\n",
       "  [1],\n",
       "  [2, 0]],\n",
       " '20': [[0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0]],\n",
       " '23': [[0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0]],\n",
       " '9': [[1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0]],\n",
       " '15': [[1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [1],\n",
       "  [],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [1],\n",
       "  [0],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [],\n",
       "  [0],\n",
       "  [],\n",
       "  [0]]}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rank_by_class_discriminative_significance, filter_by_class_discriminative_significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m phrase_reprs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([taxo\u001b[38;5;241m.\u001b[39mstatic_emb[w]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m internal_phrases], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ranks \u001b[38;5;241m=\u001b[39m rank_by_class_discriminative_significance(phrase_reprs, [average_with_harmonic_series([taxo\u001b[38;5;241m.\u001b[39mstatic_emb[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m c\u001b[38;5;241m.\u001b[39mexternal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrases\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m focus_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren], focus_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mindex(focus_node))\n\u001b[1;32m      3\u001b[0m ranked_tok \u001b[38;5;241m=\u001b[39m {internal_phrases[idx]:rank \u001b[38;5;28;01mfor\u001b[39;00m idx, rank \u001b[38;5;129;01min\u001b[39;00m ranks\u001b[38;5;241m.\u001b[39mitems()}\n",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m phrase_reprs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([taxo\u001b[38;5;241m.\u001b[39mstatic_emb[w]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m internal_phrases], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ranks \u001b[38;5;241m=\u001b[39m rank_by_class_discriminative_significance(phrase_reprs, [\u001b[43maverage_with_harmonic_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtaxo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphrases\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m focus_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren], focus_node\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mindex(focus_node))\n\u001b[1;32m      3\u001b[0m ranked_tok \u001b[38;5;241m=\u001b[39m {internal_phrases[idx]:rank \u001b[38;5;28;01mfor\u001b[39;00m idx, rank \u001b[38;5;129;01min\u001b[39;00m ranks\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/utils.py:162\u001b[0m, in \u001b[0;36maverage_with_harmonic_series\u001b[0;34m(representations, axis)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim):\n\u001b[1;32m    161\u001b[0m     weights[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/numpy/lib/function_base.py:548\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    546\u001b[0m     scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw)\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m--> 548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    549\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    551\u001b[0m     avg \u001b[38;5;241m=\u001b[39m avg_as_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(a, wgt,\n\u001b[1;32m    552\u001b[0m                       dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\u001b[38;5;241m.\u001b[39msum(axis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw) \u001b[38;5;241m/\u001b[39m scl\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "phrase_reprs = np.concatenate([taxo.static_emb[w].reshape((-1, 768)) for w in internal_phrases], axis=0)\n",
    "ranks = rank_by_class_discriminative_significance(phrase_reprs, [average_with_harmonic_series([taxo.static_emb[w] for w in c.external['phrases']]) for c in focus_node.parents[0].children], focus_node.parents[0].children.index(focus_node))\n",
    "ranked_tok = {internal_phrases[idx]:rank for idx, rank in ranks.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning': 0,\n",
       " 'reasoning': 1,\n",
       " 'classification': 2,\n",
       " 'retrieval': 3,\n",
       " 'inference': 4,\n",
       " 'smoothing': 5,\n",
       " 'graph': 6,\n",
       " 'model': 7,\n",
       " 'knowledge': 8,\n",
       " 'representations': 9,\n",
       " 'policy': 10,\n",
       " 'representation': 11,\n",
       " 'based': 12,\n",
       " 'understanding': 13,\n",
       " 'cross': 14,\n",
       " 'analysis': 15,\n",
       " 'models': 16,\n",
       " 'representation_learning': 17,\n",
       " 'taxonomy': 18,\n",
       " 'routing': 19,\n",
       " 'function': 20,\n",
       " 'generation': 21,\n",
       " 'grouping': 22,\n",
       " 'encoding': 23,\n",
       " 'network': 24,\n",
       " 'trajectory': 25,\n",
       " 'dialogue': 26,\n",
       " 'dictionary': 27,\n",
       " 'using': 28,\n",
       " 'discourse': 29,\n",
       " 'algorithm': 30,\n",
       " 'derivation': 31,\n",
       " 'search': 32,\n",
       " 'with': 33,\n",
       " 'estimates': 34,\n",
       " 'modeling': 35,\n",
       " 'attention': 36,\n",
       " 'prompting': 37,\n",
       " 'targets': 38,\n",
       " 'molecular': 39,\n",
       " 'rating': 40,\n",
       " 'programming': 41,\n",
       " 'scene': 42,\n",
       " 'transmission': 43,\n",
       " 'feature': 44,\n",
       " 'information': 45,\n",
       " 'for': 46,\n",
       " 'task': 47,\n",
       " 'semantic': 48,\n",
       " 'collaboration': 49,\n",
       " 'hierarchy': 50,\n",
       " 'conditioned': 51,\n",
       " 'reward': 52,\n",
       " 'action': 53,\n",
       " 'correction': 54,\n",
       " 'searches': 55,\n",
       " 'structure': 56,\n",
       " 'ordering': 57,\n",
       " 'via': 58,\n",
       " 'entity': 59,\n",
       " 'reading': 60,\n",
       " 'is': 61,\n",
       " 'graph_learning': 62,\n",
       " 'prompt': 63,\n",
       " 'training': 64,\n",
       " 'text': 65,\n",
       " 'style': 66,\n",
       " 'resolution': 67,\n",
       " 'through': 68,\n",
       " 'label': 69,\n",
       " 'machine': 70,\n",
       " 'data': 71,\n",
       " 'kernel': 72,\n",
       " 'referring': 73,\n",
       " 'mode': 74,\n",
       " 'variation': 75,\n",
       " 'fragment': 76,\n",
       " 'in': 77,\n",
       " 'passage': 78,\n",
       " 'track': 79,\n",
       " 'acoustic': 80,\n",
       " 'tensor': 81,\n",
       " '.': 82,\n",
       " 'relation': 83,\n",
       " 'assignment': 84,\n",
       " 'matching': 85,\n",
       " 'proximity': 86,\n",
       " 'descriptions': 87,\n",
       " 'proof': 88,\n",
       " 'textual': 89,\n",
       " 'trace': 90,\n",
       " 'consensus': 91,\n",
       " 'definitions': 92,\n",
       " 'directional': 93,\n",
       " 'joint': 94,\n",
       " 'methods': 95,\n",
       " 'tasks': 96,\n",
       " 'and': 97,\n",
       " 'formula': 98,\n",
       " 'route': 99,\n",
       " 'mention': 100,\n",
       " 'shape': 101,\n",
       " 'graph_processing': 102,\n",
       " 'story': 103,\n",
       " 'operating': 104,\n",
       " 'analyses': 105,\n",
       " 'assistance': 106,\n",
       " 'naming': 107,\n",
       " 'attack': 108,\n",
       " 'semantics': 109,\n",
       " 'can': 110,\n",
       " 'to': 111,\n",
       " 'over': 112,\n",
       " 'description': 113,\n",
       " 'interaction': 114,\n",
       " 'message': 115,\n",
       " 'discrete': 116,\n",
       " 'image': 117,\n",
       " 'guided': 118,\n",
       " 'policies': 119,\n",
       " 'domain': 120,\n",
       " 'indicator': 121,\n",
       " 'reinforcement': 122,\n",
       " 'graph_reasoning': 123,\n",
       " 'context': 124,\n",
       " 'gradient': 125,\n",
       " 'graphs': 126,\n",
       " 'optimization': 127,\n",
       " 'coupling': 128,\n",
       " 'by': 129,\n",
       " 'relational': 130,\n",
       " 'separation': 131,\n",
       " 'framework': 132,\n",
       " 'preference': 133,\n",
       " 'collective': 134,\n",
       " 'machine_learning': 135,\n",
       " 'ranks': 136,\n",
       " 'logic': 137,\n",
       " 'event': 138,\n",
       " 'analytics': 139,\n",
       " 'hypothesis': 140,\n",
       " 'link_prediction': 141,\n",
       " 'temporal': 142,\n",
       " 'recommendation': 143,\n",
       " 'classifications': 144,\n",
       " 'tables': 145,\n",
       " 'measurements': 146,\n",
       " 'membership': 147,\n",
       " 'traffic': 148,\n",
       " 'design': 149,\n",
       " 'indicators': 150,\n",
       " 'skills': 151,\n",
       " 'generators': 152,\n",
       " 'synonym': 153,\n",
       " 'sequence': 154,\n",
       " 'genetic': 155,\n",
       " 'forms': 156,\n",
       " 'features': 157,\n",
       " 'anchor': 158,\n",
       " 'instruction': 159,\n",
       " 'correlation': 160,\n",
       " 'comparisons': 161,\n",
       " 'engagement': 162,\n",
       " 'boundary': 163,\n",
       " 'classification_task': 164,\n",
       " 'networks': 165,\n",
       " 'determination': 166,\n",
       " 'catalyst': 167,\n",
       " 'structured': 168,\n",
       " 'process': 169,\n",
       " 'of': 170,\n",
       " 'from': 171,\n",
       " 'morphological': 172,\n",
       " 'driven': 173,\n",
       " 'exploration': 174,\n",
       " 'data_management': 175,\n",
       " 'specification': 176,\n",
       " 'masks': 177,\n",
       " 'automation': 178,\n",
       " 'trigger': 179,\n",
       " 'similarity': 180,\n",
       " 'back': 181,\n",
       " 'multi': 182,\n",
       " 'sentences': 183,\n",
       " 'set': 184,\n",
       " 'patent': 185,\n",
       " 'locally': 186,\n",
       " 'or': 187,\n",
       " '(': 188,\n",
       " 'has': 189,\n",
       " 'exchange': 190,\n",
       " 'node_classification': 191,\n",
       " 'coordination': 192,\n",
       " 'security': 193,\n",
       " 'architectural': 194,\n",
       " 'variations': 195,\n",
       " 'complex': 196,\n",
       " 'tracking': 197,\n",
       " 'oracle': 198,\n",
       " 'free': 199,\n",
       " 'accounts': 200,\n",
       " 'data_integration': 201,\n",
       " 'geographical': 202,\n",
       " 'definition': 203,\n",
       " 'business': 204,\n",
       " 'marketing': 205,\n",
       " 'tracing': 206,\n",
       " 'educational': 207,\n",
       " 'graph_generation': 208,\n",
       " 'node': 209,\n",
       " 'boosting_graph_reasoning_ability_of_large': 210,\n",
       " 'plan': 211,\n",
       " 'distributed': 212,\n",
       " 'indices': 213,\n",
       " 'strongly': 214,\n",
       " 'theme': 215,\n",
       " 'throughout': 216,\n",
       " 'learning_multiplex_representations_on_text_attributed': 217,\n",
       " 'the': 218,\n",
       " 'interdisciplinary': 219,\n",
       " 'transformations': 220,\n",
       " ',': 221,\n",
       " 'switching': 222,\n",
       " 'filtering': 223,\n",
       " 'defect': 224,\n",
       " 'labels': 225,\n",
       " 'convergence': 226,\n",
       " 'aids': 227,\n",
       " 'on': 228,\n",
       " 'traveling': 229,\n",
       " 'probability': 230,\n",
       " 'tag': 231,\n",
       " 'dynamics': 232,\n",
       " 'claims': 233,\n",
       " 'beam': 234,\n",
       " 'trading': 235,\n",
       " 'authorship': 236,\n",
       " 'insertion': 237,\n",
       " 'feature_selection': 238,\n",
       " 'architecture': 239,\n",
       " 'losses': 240,\n",
       " 'evaluation': 241,\n",
       " 'strategic': 242,\n",
       " 'genre': 243,\n",
       " 'concerning': 244,\n",
       " 'prediction': 245,\n",
       " 'homogeneous': 246,\n",
       " 'regular': 247,\n",
       " 'orientation': 248,\n",
       " 'pairing': 249,\n",
       " 'conditioning': 250,\n",
       " 'learned': 251,\n",
       " 'system': 252,\n",
       " 'word': 253,\n",
       " 'dependence': 254,\n",
       " 'examination': 255,\n",
       " 'chemical': 256,\n",
       " 'language_modeling': 257,\n",
       " 'radical': 258,\n",
       " 'pyramid': 259,\n",
       " 'creation': 260,\n",
       " 'filters': 261,\n",
       " 'formulas': 262,\n",
       " 'extreme_multi_label': 263,\n",
       " 'interfaces': 264,\n",
       " 'graph_based': 265,\n",
       " 'label_propagation': 266,\n",
       " 'prefix': 267,\n",
       " 'predictions': 268,\n",
       " 'node_classication': 269,\n",
       " 'surveys': 270,\n",
       " 'data_fusion': 271,\n",
       " 'model_selection': 272,\n",
       " 'in_context_learning': 273,\n",
       " 'reaches': 274,\n",
       " 'enterprise': 275,\n",
       " 'relations': 276,\n",
       " 'weakly': 277,\n",
       " 'orthogonal': 278,\n",
       " 'health': 279,\n",
       " 'biomedical': 280,\n",
       " 'loss': 281,\n",
       " 'regulation': 282,\n",
       " 'expertise': 283,\n",
       " 'research': 284,\n",
       " 'graphics': 285,\n",
       " 'module': 286,\n",
       " 'sequential': 287,\n",
       " 'sparse': 288,\n",
       " 'ne': 289,\n",
       " 'binary_classification': 290,\n",
       " 'assisted': 291,\n",
       " 'fine': 292,\n",
       " 'portal': 293,\n",
       " 'pre_training': 294,\n",
       " 'projections': 295,\n",
       " 'trends': 296,\n",
       " 'ensembles': 297,\n",
       " 'gold': 298,\n",
       " 'language': 299,\n",
       " '?': 300,\n",
       " 'approximation': 301,\n",
       " 'protocol': 302,\n",
       " 'graph_representations': 303,\n",
       " 'compilation': 304,\n",
       " 'perspective': 305,\n",
       " 'positioning': 306,\n",
       " 'environmental': 307,\n",
       " 'ad': 308,\n",
       " 'are': 309,\n",
       " 'deep': 310,\n",
       " 'stems': 311,\n",
       " 'transformers': 312,\n",
       " 'tool': 313,\n",
       " 'social': 314,\n",
       " 'seeds': 315,\n",
       " 'document_classification': 316,\n",
       " 'probe': 317,\n",
       " 'relationship': 318,\n",
       " 'neighborhood': 319,\n",
       " 'without': 320,\n",
       " 'stemming': 321,\n",
       " 'propagation': 322,\n",
       " 'urban': 323,\n",
       " 'instructions': 324,\n",
       " 'signed': 325,\n",
       " 'variance': 326,\n",
       " 'counterpart': 327,\n",
       " 'term': 328,\n",
       " 'data_model': 329,\n",
       " 'generations': 330,\n",
       " 'searching': 331,\n",
       " 'coming': 332,\n",
       " 'tends': 333,\n",
       " 'text_graph': 334,\n",
       " 'across': 335,\n",
       " 'topic': 336,\n",
       " 'into': 337,\n",
       " 'a': 338,\n",
       " 'that': 339,\n",
       " 'patterns': 340,\n",
       " 'hits': 341,\n",
       " 'both': 342,\n",
       " 'spoken': 343,\n",
       " 'translations': 344,\n",
       " 'text_related': 345,\n",
       " 'recovery': 346,\n",
       " 'at': 347,\n",
       " 'data_structure': 348,\n",
       " 'logical': 349,\n",
       " 'visually': 350,\n",
       " 'yield_prediction': 351,\n",
       " 'nonlinear': 352,\n",
       " 'chart': 353,\n",
       " 'working': 354,\n",
       " 'bug': 355,\n",
       " 'approach': 356,\n",
       " 'pattern': 357,\n",
       " 'storage': 358,\n",
       " 'tools': 359,\n",
       " 'graph_related': 360,\n",
       " 'ea': 361,\n",
       " 'knowledge_graph': 362,\n",
       " 'plans': 363,\n",
       " 'query': 364,\n",
       " 'prediction_task': 365,\n",
       " 'principal': 366,\n",
       " 'career': 367,\n",
       " 'simulations': 368,\n",
       " 'linguistics': 369,\n",
       " 'image_based': 370,\n",
       " 'artificial': 371,\n",
       " 'performance': 372,\n",
       " 'content_based': 373,\n",
       " 'walks': 374,\n",
       " 'display': 375,\n",
       " 'global': 376,\n",
       " 'canonical': 377,\n",
       " 'customer': 378,\n",
       " 'coarse': 379,\n",
       " 'data_quality': 380,\n",
       " 'multi_classification': 381,\n",
       " 'plots': 382,\n",
       " 'structure_free': 383,\n",
       " 'audience': 384,\n",
       " 'data_processing': 385,\n",
       " 'equations': 386,\n",
       " 'sensing': 387,\n",
       " 'substantially': 388,\n",
       " 'symmetric': 389,\n",
       " 'competition': 390,\n",
       " 'techniques': 391,\n",
       " 'clean': 392,\n",
       " 'assessments': 393,\n",
       " 'regulatory': 394,\n",
       " 'text_generation': 395,\n",
       " 'sensitivity': 396,\n",
       " 'tolerance': 397,\n",
       " 'revisited': 398,\n",
       " 'sequences': 399,\n",
       " 'gets': 400,\n",
       " 'graph_free': 401,\n",
       " 'preparation': 402,\n",
       " 'precursor': 403,\n",
       " 'graph_text': 404,\n",
       " 'overlap': 405,\n",
       " ';': 406,\n",
       " 'students': 407,\n",
       " 'toward': 408,\n",
       " 'formats': 409,\n",
       " 'star': 410,\n",
       " 'metadata': 411,\n",
       " 'systems': 412,\n",
       " 'problems': 413,\n",
       " 'uniformly': 414,\n",
       " 'non': 415,\n",
       " 'fragments': 416,\n",
       " 'reviewer': 417,\n",
       " 'structural': 418,\n",
       " 'onto': 419,\n",
       " 'relaxation': 420,\n",
       " 'chunk': 421,\n",
       " 'scholar': 422,\n",
       " 'text_molecule': 423,\n",
       " 'information_extraction': 424,\n",
       " 'situation': 425,\n",
       " 'classification_performance': 426,\n",
       " 'biased': 427,\n",
       " 'level': 428,\n",
       " 'impression': 429,\n",
       " 'stack': 430,\n",
       " 'gains': 431,\n",
       " 'subjective': 432,\n",
       " 'text_based': 433,\n",
       " 'transfers': 434,\n",
       " 'medication': 435,\n",
       " 'text_conditional': 436,\n",
       " 'realistic': 437,\n",
       " 'text_representations': 438,\n",
       " 'problem_solving': 439,\n",
       " 'neural': 440,\n",
       " 'translation': 441,\n",
       " 'database': 442,\n",
       " 'selective': 443,\n",
       " 'embodied': 444,\n",
       " 'sequence_based': 445,\n",
       " 'command': 446,\n",
       " 'fitness': 447,\n",
       " 'messages': 448,\n",
       " 'demographic': 449,\n",
       " 'bar': 450,\n",
       " 'venue': 451,\n",
       " 'masked_language': 452,\n",
       " 'meta_learning': 453,\n",
       " 'rational': 454,\n",
       " 'exceeds': 455,\n",
       " 'text_guided': 456,\n",
       " 'progression': 457,\n",
       " 'abilities': 458,\n",
       " 'quantum': 459,\n",
       " 'cell': 460,\n",
       " 'turkish': 461,\n",
       " 'was': 462,\n",
       " 'network_analysis': 463,\n",
       " 'thinking': 464,\n",
       " 'indexed': 465,\n",
       " 'conflicts': 466,\n",
       " 'cooking': 467,\n",
       " 'knowledge_aware': 468,\n",
       " 'signatures': 469,\n",
       " 'acceptance': 470,\n",
       " 'graph_matching': 471,\n",
       " 'web': 472,\n",
       " 'bank': 473,\n",
       " 'evolving': 474,\n",
       " 'type': 475,\n",
       " 'lines': 476,\n",
       " 'themes': 477,\n",
       " 'mining': 478,\n",
       " 'expectation': 479,\n",
       " 'check': 480,\n",
       " 'texts': 481,\n",
       " 'healthcare': 482,\n",
       " 'macro': 483,\n",
       " 'multiple': 484,\n",
       " 'transferring': 485,\n",
       " 'graph_aware': 486,\n",
       " 'runs': 487,\n",
       " 'human': 488,\n",
       " 'user': 489,\n",
       " 'approaches': 490,\n",
       " 'sensitive': 491,\n",
       " 'ranking': 492,\n",
       " 'node_level': 493,\n",
       " 'aggregation': 494,\n",
       " 'format': 495,\n",
       " 'related': 496,\n",
       " 're': 497,\n",
       " 'functions': 498,\n",
       " 'post_processing': 499,\n",
       " 'reached': 500,\n",
       " 'responding': 501,\n",
       " 'learnt': 502,\n",
       " 'offensive': 503,\n",
       " 'such': 504,\n",
       " 'computational': 505,\n",
       " 'knowledge_base': 506,\n",
       " 'documents': 507,\n",
       " 'identification': 508,\n",
       " 'equivalence': 509,\n",
       " 'knowledgebase': 510,\n",
       " 'large_language_model': 511,\n",
       " 'closure': 512,\n",
       " 'edge_classication': 513,\n",
       " 'stacks': 514,\n",
       " 'graph_level': 515,\n",
       " 'relation_extraction': 516,\n",
       " 'subject': 517,\n",
       " 'supply': 518,\n",
       " 'directly': 519,\n",
       " 'calculation': 520,\n",
       " 'improving_feature_representation_through_graph_centric_finetuning': 521,\n",
       " 'extension': 522,\n",
       " 'text_information': 523,\n",
       " 'compact': 524,\n",
       " 'mono_information': 525,\n",
       " 'lecture': 526,\n",
       " 'kg': 527,\n",
       " 'protocols': 528,\n",
       " 'varies': 529,\n",
       " 'examples': 530,\n",
       " 'text_descriptions': 531,\n",
       " 'content': 532,\n",
       " 'construction': 533,\n",
       " 'hints': 534,\n",
       " 'structure_understanding': 535,\n",
       " 'trust': 536,\n",
       " 'stories': 537,\n",
       " 'representational': 538,\n",
       " 'plot': 539,\n",
       " 'stacked': 540,\n",
       " 'closed': 541,\n",
       " 'augmented': 542,\n",
       " 'inspection': 543,\n",
       " 'cache': 544,\n",
       " 'conceptualization': 545,\n",
       " 'catalog': 546,\n",
       " 'arrays': 547,\n",
       " 'conversations': 548,\n",
       " 'generated': 549,\n",
       " 'decreases': 550,\n",
       " 'text_structure': 551,\n",
       " 'guidelines': 552,\n",
       " 'supervision_signals': 553,\n",
       " 'character_level': 554,\n",
       " 'graph_enhanced': 555,\n",
       " 'assumptions': 556,\n",
       " 'size': 557,\n",
       " 'sliding': 558,\n",
       " 'classification_based': 559,\n",
       " 'cyclic': 560,\n",
       " 'feature_based': 561,\n",
       " 'almost': 562,\n",
       " 'agreement': 563,\n",
       " 'sports': 564,\n",
       " 'phenomena': 565,\n",
       " 'column': 566,\n",
       " 'delay': 567,\n",
       " 'effectively': 568,\n",
       " 'local': 569,\n",
       " 'all': 570,\n",
       " 'sentiment': 571,\n",
       " 'character_based': 572,\n",
       " 'depicting': 573,\n",
       " 'metric': 574,\n",
       " 'graph_augmented': 575,\n",
       " 'language_model': 576,\n",
       " 'interacting': 577,\n",
       " 'restrictions': 578,\n",
       " 'isolated': 579,\n",
       " 'cumulative': 580,\n",
       " 'experiences': 581,\n",
       " 'distribution': 582,\n",
       " 'infomax': 583,\n",
       " 'molecule': 584,\n",
       " 'xml': 585,\n",
       " 'micro': 586,\n",
       " 'sentence_bert': 587,\n",
       " 'scaled': 588,\n",
       " 'exhibited': 589,\n",
       " 'control': 590,\n",
       " 'composed': 591,\n",
       " 'steiner': 592,\n",
       " 'semantic_level': 593,\n",
       " 'zero_': 594,\n",
       " 'populated': 595,\n",
       " 'rewards': 596,\n",
       " 'cue': 597,\n",
       " 'qualities': 598,\n",
       " 'text_only': 599,\n",
       " 'anomaly_detection': 600,\n",
       " 'scripts': 601,\n",
       " 'may': 602,\n",
       " 'property': 603,\n",
       " 'ai': 604,\n",
       " 'mesh': 605,\n",
       " 'private': 606,\n",
       " 'counts': 607,\n",
       " 'inside': 608,\n",
       " 'graph_data': 609,\n",
       " 'ol': 610,\n",
       " 'circular': 611,\n",
       " 'an': 612,\n",
       " 'textural': 613,\n",
       " 'entity_level': 614,\n",
       " 'keeps': 615,\n",
       " 'objective': 616,\n",
       " 'smiles': 617,\n",
       " 'chinese': 618,\n",
       " 'textual_representation': 619,\n",
       " 'lessons': 620,\n",
       " 'parameters': 621,\n",
       " 'spatial': 622,\n",
       " 'settings': 623,\n",
       " 'robot': 624,\n",
       " 'patient': 625,\n",
       " 'document_document': 626,\n",
       " 'conflict': 627,\n",
       " 'files': 628,\n",
       " 'entities': 629,\n",
       " 'collecting': 630,\n",
       " 'trade': 631,\n",
       " 'entirely': 632,\n",
       " 'dedicated': 633,\n",
       " 'message_passing': 634,\n",
       " 'unique': 635,\n",
       " 'textgraph': 636,\n",
       " 'result': 637,\n",
       " 'attributes': 638,\n",
       " 'retrieval_reasoning': 639,\n",
       " 'sketch': 640,\n",
       " 'mined': 641,\n",
       " 'euclidean': 642,\n",
       " 'generalisation': 643,\n",
       " 'graph_structure': 644,\n",
       " 'text_level': 645,\n",
       " 'editors': 646,\n",
       " 'entity_aware': 647,\n",
       " 'properly': 648,\n",
       " 'deep_learning': 649,\n",
       " 'relation_paths': 650,\n",
       " 'uncertainty': 651,\n",
       " 'reaction': 652,\n",
       " 'personalized_pagerank': 653,\n",
       " 'diffusion': 654,\n",
       " 'visual_language': 655,\n",
       " 'situations': 656,\n",
       " 'cluster_analysis': 657,\n",
       " 'advertising': 658,\n",
       " 'text_aware': 659,\n",
       " 'knowledge_intensive': 660,\n",
       " 'wi': 661,\n",
       " 'topology': 662,\n",
       " 'guarantees': 663,\n",
       " 'serving': 664,\n",
       " 'entropy': 665,\n",
       " 'more': 666,\n",
       " 'localized': 667,\n",
       " 'machines': 668,\n",
       " 'narratives': 669,\n",
       " 'customized': 670,\n",
       " 'dialogues': 671,\n",
       " 'constructions': 672,\n",
       " 'arguments': 673,\n",
       " 'remains': 674,\n",
       " 'database_systems': 675,\n",
       " 'versatile_reading': 676,\n",
       " 'excel': 677,\n",
       " 'text_attributed_graphs': 678,\n",
       " 'co_reference': 679,\n",
       " 'em': 680,\n",
       " 'as': 681,\n",
       " 'when': 682,\n",
       " 'new': 683,\n",
       " 'strategy': 684,\n",
       " 'automatic': 685,\n",
       " 'auto_encoding': 686,\n",
       " 'vector': 687,\n",
       " 'associated': 688,\n",
       " 'language_models': 689,\n",
       " 'ensemble': 690,\n",
       " 'brand': 691,\n",
       " 'graph_coloring': 692,\n",
       " 'vocabulary': 693,\n",
       " 'multihop': 694,\n",
       " 'multi_scale': 695,\n",
       " 'reasoning_process': 696,\n",
       " 'form': 697,\n",
       " 'reader': 698,\n",
       " 'representation.in': 699,\n",
       " 'balanced': 700,\n",
       " 'beam_search': 701,\n",
       " 'transforms': 702,\n",
       " 'averaging': 703,\n",
       " 'meta_path': 704,\n",
       " 'weight': 705,\n",
       " 'several': 706,\n",
       " 'developers': 707,\n",
       " 'expressed': 708,\n",
       " 'kg_augmented': 709,\n",
       " 'relationships': 710,\n",
       " 'comparison': 711,\n",
       " 'shared': 712,\n",
       " 'consists': 713,\n",
       " 'graph_neural': 714,\n",
       " 'base': 715,\n",
       " 'daily': 716,\n",
       " 'text_summary': 717,\n",
       " 'graph_neural_networks': 718,\n",
       " 'signs': 719,\n",
       " 'satisfying': 720,\n",
       " 'extraction': 721,\n",
       " 'continuously': 722,\n",
       " 'node_representation_learning_on_heterogeneous': 723,\n",
       " 'node_representations': 724,\n",
       " 'physics': 725,\n",
       " 'rep': 726,\n",
       " 'have': 727,\n",
       " 'ke': 728,\n",
       " 'response': 729,\n",
       " 'returns': 730,\n",
       " 'text_classification': 731,\n",
       " 'influences': 732,\n",
       " 'learningbased': 733,\n",
       " 'prospective': 734,\n",
       " 'clip': 735,\n",
       " 'realization': 736,\n",
       " 'parameter': 737,\n",
       " 'loop': 738,\n",
       " 'replacement': 739,\n",
       " 'labelling': 740,\n",
       " 'meta_paths': 741,\n",
       " 'semantic_based': 742,\n",
       " 'instructional': 743,\n",
       " 'visual_linguistic': 744,\n",
       " 'union': 745,\n",
       " 'penalty': 746,\n",
       " 'relevance_modeling': 747,\n",
       " 'concept': 748,\n",
       " 'query_based': 749,\n",
       " 'hit': 750,\n",
       " 'transferable': 751,\n",
       " 'horizontal': 752,\n",
       " 'graph_syntax': 753,\n",
       " 'citation_based': 754,\n",
       " 'multi_relation': 755,\n",
       " 'rc': 756,\n",
       " 'became': 757,\n",
       " 'notions': 758,\n",
       " 'text_to_graph': 759,\n",
       " 'jointly': 760,\n",
       " 'scheme': 761,\n",
       " 'ned': 762,\n",
       " 'feature_extraction': 763,\n",
       " 'cross_information': 764,\n",
       " 'method': 765,\n",
       " 'care': 766,\n",
       " 'multi_relational': 767,\n",
       " 'relevance': 768,\n",
       " 'intent': 769,\n",
       " 'solid': 770,\n",
       " 'its': 771,\n",
       " 'brain': 772,\n",
       " 'network_enhanced': 773,\n",
       " 'contemporary': 774,\n",
       " 'cid': 775,\n",
       " 'apt': 776,\n",
       " 'ability': 777,\n",
       " 'text_rich': 778,\n",
       " 'assignments': 779,\n",
       " 'graph_isomorphism': 780,\n",
       " 'fingerprint': 781,\n",
       " 'properties': 782,\n",
       " 'facilitates': 783,\n",
       " 'concrete': 784,\n",
       " 'knowledge_alignment': 785,\n",
       " 'network_topology': 786,\n",
       " 'ordered': 787,\n",
       " 'graph_to_text': 788,\n",
       " 'author': 789,\n",
       " 'pairs': 790,\n",
       " 'multi_typed': 791,\n",
       " 'machine_generated': 792,\n",
       " 'graph_representation': 793,\n",
       " 'really': 794,\n",
       " 'data_mining': 795,\n",
       " 'structural_information': 796,\n",
       " 'questions': 797,\n",
       " 'indicative': 798,\n",
       " 'representing': 799,\n",
       " 'partially': 800,\n",
       " 'accuracy': 801,\n",
       " 'helped': 802,\n",
       " 'replication': 803,\n",
       " 'hindi': 804,\n",
       " 'text_oriented': 805,\n",
       " 'gnn_architectures': 806,\n",
       " 'controlling': 807,\n",
       " 'prompting_methods': 808,\n",
       " 'edge_detection': 809,\n",
       " 'knowledge_bases': 810,\n",
       " 'computations': 811,\n",
       " 'arrangement': 812,\n",
       " 'persistent': 813,\n",
       " 'contrastive_learning': 814,\n",
       " 'concepts': 815,\n",
       " 'initially': 816,\n",
       " 'thread': 817,\n",
       " 'paper/6703_inductive_representation_learning_on_large_graphs.pdf': 818,\n",
       " 'visual': 819,\n",
       " 'enjoys': 820,\n",
       " 'statistical_model': 821,\n",
       " 'skeleton_based': 822,\n",
       " 'selections': 823,\n",
       " 'dynamic': 824,\n",
       " 'downstream_performance': 825,\n",
       " 'conveyed': 826,\n",
       " 'node_features': 827,\n",
       " 'graph_like': 828,\n",
       " 'reasonably': 829,\n",
       " 'narrows': 830,\n",
       " 'nodes': 831,\n",
       " 'literal': 832,\n",
       " 'mathematical': 833,\n",
       " 'some': 834,\n",
       " 'label_based': 835,\n",
       " 'gathered': 836,\n",
       " 'train': 837,\n",
       " 'graph_extraction': 838,\n",
       " 'chance': 839,\n",
       " 'tree_structured': 840,\n",
       " 'energy': 841,\n",
       " 'tries': 842,\n",
       " 'different': 843,\n",
       " 'machine_learned': 844,\n",
       " 'results': 845,\n",
       " 'japanese': 846,\n",
       " 'inter': 847,\n",
       " 'ns': 848,\n",
       " 'these': 849,\n",
       " 'layers': 850,\n",
       " 'returned': 851,\n",
       " 'explicit_reasoning': 852,\n",
       " 'applications': 853,\n",
       " 'structured_data': 854,\n",
       " 'accurately': 855,\n",
       " 'decoration': 856,\n",
       " 'retrieval_augmented': 857,\n",
       " 'would': 858,\n",
       " 'within': 859,\n",
       " 'class_level': 860,\n",
       " 'accumulation': 861,\n",
       " 'data_analysis': 862,\n",
       " 'sizes': 863,\n",
       " 'versus': 864,\n",
       " 'invoking_linearization_generation': 865,\n",
       " 'string_based': 866,\n",
       " 'traits': 867,\n",
       " 'algorithms': 868,\n",
       " 'figures': 869,\n",
       " 'gan': 870,\n",
       " 'text_to_graph_molecule_generation': 871,\n",
       " 'delivery': 872,\n",
       " 'truth': 873,\n",
       " 'measure': 874,\n",
       " 'text_image': 875,\n",
       " 'failures': 876,\n",
       " 'covers': 877,\n",
       " 'serialized': 878,\n",
       " 'managed': 879,\n",
       " 'economic': 880,\n",
       " 'corpus_level': 881,\n",
       " 'thai': 882,\n",
       " 'advice': 883,\n",
       " 'post_filtering': 884,\n",
       " 'designs': 885,\n",
       " 'neighboring': 886,\n",
       " 'multi_hop': 887,\n",
       " 'zeroshot': 888,\n",
       " 'introductions': 889,\n",
       " 'model_agnostic': 890,\n",
       " 'exploitation': 891,\n",
       " 'person': 892,\n",
       " 'precisely': 893,\n",
       " 'under': 894,\n",
       " 'tb': 895,\n",
       " 'repeatedly': 896,\n",
       " 'repetition': 897,\n",
       " 'questioning': 898,\n",
       " 'their': 899,\n",
       " 'ow': 900,\n",
       " 'multi_round': 901,\n",
       " 'custom': 902,\n",
       " 'materials': 903,\n",
       " 'infrastructure': 904,\n",
       " 'target': 905,\n",
       " 'knowledge_graphs': 906,\n",
       " 'graph_syntax_tree': 907,\n",
       " 'join': 908,\n",
       " 'classifying': 909,\n",
       " 'grows': 910,\n",
       " 'strategically': 911,\n",
       " 'matched': 912,\n",
       " 'root': 913,\n",
       " 'grounding': 914,\n",
       " 'smile': 915,\n",
       " 'receives': 916,\n",
       " 'one': 917,\n",
       " 'domain_knowledge': 918,\n",
       " 'indirectly': 919,\n",
       " 'advanced': 920,\n",
       " 'reasoning_tasks': 921,\n",
       " 'visible': 922,\n",
       " \"'\": 923,\n",
       " 'stores': 924,\n",
       " 'very': 925,\n",
       " 'visualizations': 926,\n",
       " 'specific': 927,\n",
       " 'dice': 928,\n",
       " 'substitution': 929,\n",
       " 'semantic_information': 930,\n",
       " 'document': 931,\n",
       " 'classic': 932,\n",
       " 'successful': 933,\n",
       " 'modeling_language': 934,\n",
       " 'transductive': 935,\n",
       " 'diabetes': 936,\n",
       " 'simulator': 937,\n",
       " 'effortlessly': 938,\n",
       " 'between': 939,\n",
       " 'dataless': 940,\n",
       " 'assigns': 941,\n",
       " 'reading_comprehension': 942,\n",
       " 'parent': 943,\n",
       " 'due': 944,\n",
       " 'walk_based': 945,\n",
       " 'graph_structured_data': 946,\n",
       " 'predictive_text': 947,\n",
       " 'meta_data': 948,\n",
       " 'translated': 949,\n",
       " 'gnn_nested_transformers': 950,\n",
       " 'zeroshot_entail': 951,\n",
       " 'community_detection': 952,\n",
       " 'exactly': 953,\n",
       " 'resulted': 954,\n",
       " 'priority': 955,\n",
       " 'patents': 956,\n",
       " 'graph_theory': 957,\n",
       " 'websites': 958,\n",
       " 'repeated': 959,\n",
       " 'visual_semantic': 960,\n",
       " 'stopping': 961,\n",
       " 'components': 962,\n",
       " 'guides': 963,\n",
       " 'bio': 964,\n",
       " 'textual_descriptions': 965,\n",
       " 'information_retrieval': 966,\n",
       " 'crawl': 967,\n",
       " 'te': 968,\n",
       " 'retention': 969,\n",
       " 'facts': 970,\n",
       " 'polymer': 971,\n",
       " 'tree_based': 972,\n",
       " 'intention': 973,\n",
       " 'defines': 974,\n",
       " 'perfect': 975,\n",
       " 'human_machine': 976,\n",
       " 'pipeline': 977,\n",
       " 'various': 978,\n",
       " 'en': 979,\n",
       " 'multi_agent': 980,\n",
       " 'constantly': 981,\n",
       " 'reranking': 982,\n",
       " 'metabolic': 983,\n",
       " 'node_summary': 984,\n",
       " 'page_rank': 985,\n",
       " 'clustered': 986,\n",
       " 'recorded': 987,\n",
       " 'marks': 988,\n",
       " 'paragraph_level': 989,\n",
       " 'should': 990,\n",
       " 'trained': 991,\n",
       " 'multi_objective': 992,\n",
       " 'text_gcn': 993,\n",
       " 'yielded': 994,\n",
       " 'context_based': 995,\n",
       " 'pseudo_label': 996,\n",
       " 'discovered': 997,\n",
       " 'test': 998,\n",
       " 'sensible': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_augmentation'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_node = taxo.root.findChild(\"18\")\n",
    "focus_node.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['answer_set',\n",
       " 'answer_prediction',\n",
       " 'answer_generation',\n",
       " 'invoices',\n",
       " 'answering_systems',\n",
       " 'image_question',\n",
       " 'answer_candidates',\n",
       " 'answer_answer',\n",
       " 'disengagement',\n",
       " 'conversation_generation',\n",
       " 'answer_contained',\n",
       " 'chatting',\n",
       " 'non_mediated',\n",
       " 'present_participle',\n",
       " 'past_participle',\n",
       " 'soliciting',\n",
       " 'dialogue_generation',\n",
       " 'intolerant',\n",
       " 'donations',\n",
       " 'candidate_answers',\n",
       " 'hate_speech',\n",
       " 'self_distillation',\n",
       " 'dialogue_interaction',\n",
       " 'teacher_forcing',\n",
       " 'know_why',\n",
       " 'exclamation',\n",
       " 'sub_questions',\n",
       " 'canencourage',\n",
       " 'question_comment',\n",
       " 'complex_questions',\n",
       " 'never_before_seen',\n",
       " 'question2subgraph',\n",
       " 'redirected',\n",
       " 'help_seekers',\n",
       " 'query_answering',\n",
       " 'unanswerable',\n",
       " 'frustratingly',\n",
       " 'quests',\n",
       " 'hate_keyword',\n",
       " 'dialogue_management',\n",
       " 'multi_teacher',\n",
       " 'aroused',\n",
       " 'non_directly',\n",
       " 'few/zero_shot',\n",
       " 'weakly_annotated',\n",
       " 'arouse',\n",
       " 'answering_complex',\n",
       " 'dining',\n",
       " 'helpdesks',\n",
       " 'missing_links',\n",
       " 'redirection',\n",
       " 'hop_attention',\n",
       " 'audiences',\n",
       " 'delegate',\n",
       " 'conversational_agent',\n",
       " 'human_authored',\n",
       " 'well_understood',\n",
       " 'dialogue_systems',\n",
       " 'human_given',\n",
       " 'embodying',\n",
       " 'background_answer',\n",
       " 'speaker_centric',\n",
       " 'deaf_community',\n",
       " 'super_sentential',\n",
       " 'helper',\n",
       " 'received_increasing_attention',\n",
       " 'correct_answer',\n",
       " 'dialog_systems',\n",
       " 'persuasive',\n",
       " 'relieving',\n",
       " 'conversational_agents',\n",
       " 'discourage',\n",
       " 'sad',\n",
       " 'emotion',\n",
       " 'projectivity',\n",
       " 'zero_shot',\n",
       " 'speaker_level',\n",
       " 'fake_news',\n",
       " 'quest',\n",
       " 'natural_sounding',\n",
       " 'traveller',\n",
       " 'do_it_yourself',\n",
       " 'human_understandable',\n",
       " 'who_doing_what',\n",
       " 'explainer',\n",
       " 'hesitant',\n",
       " 'overwhelm',\n",
       " 'explain',\n",
       " 'citizen',\n",
       " 'interaction_understanding',\n",
       " 'embody',\n",
       " 'miscommunication',\n",
       " 'polish_language',\n",
       " 'correct_answers',\n",
       " 'arousing',\n",
       " 'fickleness',\n",
       " 'unknowable',\n",
       " 'crowds',\n",
       " 'gluten_free',\n",
       " 'open_ended',\n",
       " 'user_interested',\n",
       " 'invoke',\n",
       " 'anaphors',\n",
       " 'advisor',\n",
       " 'ques_',\n",
       " 'sought_after',\n",
       " 'propagators',\n",
       " 'curators',\n",
       " 'chat_rooms',\n",
       " 'know_what',\n",
       " 'deceiving',\n",
       " 'boring',\n",
       " 'self_supplementation',\n",
       " 'semi_supervisions',\n",
       " 'invisible',\n",
       " '\\\\rho',\n",
       " 'unnoticeable',\n",
       " 'nontext',\n",
       " 'delegates',\n",
       " 'transfers',\n",
       " 'empathy',\n",
       " 'drastic',\n",
       " 'wrongly_linked',\n",
       " 'fake_news_detection',\n",
       " 'chatt_ing',\n",
       " 'delved',\n",
       " 'non_skilled',\n",
       " 'relieves',\n",
       " 'triggering',\n",
       " 'embodiment',\n",
       " 'sender',\n",
       " 'anaphoras',\n",
       " 'desire',\n",
       " 'auto_suggest',\n",
       " 'indivisible',\n",
       " 'activate',\n",
       " 'hyper_',\n",
       " 'user_to_user',\n",
       " 'holder_target_opinion',\n",
       " 'lovers',\n",
       " 'clients',\n",
       " 'voter',\n",
       " 'vision_languageunderstanding',\n",
       " 'gated_attention',\n",
       " 'guess',\n",
       " 'positive_feedback',\n",
       " 'hme_videoqa',\n",
       " 'hate_it',\n",
       " 'whys',\n",
       " 'self_directed',\n",
       " 'hesitancy',\n",
       " 'multi_emotion',\n",
       " 'hypernymy',\n",
       " 'ok_vqa',\n",
       " 'uploads',\n",
       " 'afden',\n",
       " 'excitement',\n",
       " 'chat',\n",
       " 'curating',\n",
       " 'distort',\n",
       " 'relieve',\n",
       " 'scranviz_a',\n",
       " 'information_missing',\n",
       " 'church_rosser',\n",
       " 'conducive',\n",
       " 'adversative',\n",
       " 'video_grounded',\n",
       " 'moral',\n",
       " 'aided_des',\n",
       " 'self_attending',\n",
       " 'brainstorming',\n",
       " 'anti_trustrank',\n",
       " 'near_human',\n",
       " 'hate',\n",
       " 'purchases',\n",
       " 'bachelor',\n",
       " 'semi_supervision',\n",
       " 'buying',\n",
       " 'bi_attention',\n",
       " 'speaker_layer',\n",
       " 'user_experience',\n",
       " 'user_contributed',\n",
       " 'news_comment',\n",
       " 'instilling',\n",
       " 'disobey',\n",
       " 'deluge',\n",
       " 'acception',\n",
       " 'sarcasm_detection',\n",
       " 'utterance_level',\n",
       " 'uploader',\n",
       " \"'opinion_holder_target\",\n",
       " 'backlog',\n",
       " 'relocation',\n",
       " 'multi_head_attention',\n",
       " \"'citizen\",\n",
       " 'knowhrl',\n",
       " 'donation',\n",
       " 'bi_lstm+multi_head_self_attention',\n",
       " 'anaphora',\n",
       " 'dykgchat',\n",
       " 'unhuman',\n",
       " 'hand_authored',\n",
       " 'homophily',\n",
       " 'curing',\n",
       " 'neighbor_attention',\n",
       " 'condemnation',\n",
       " 'interview_based',\n",
       " '//github.com/madaan/thinkaboutit',\n",
       " 'emotion_driven',\n",
       " 'semi_nave',\n",
       " 'learnable',\n",
       " 'grass',\n",
       " 'hypernymic',\n",
       " 'strongly_',\n",
       " 'endusers',\n",
       " 'speaker_independent',\n",
       " 'inbound_directional',\n",
       " 'non_native',\n",
       " 'buyer_generated',\n",
       " 'muslim',\n",
       " 'enjoys',\n",
       " 'indirection',\n",
       " 'laughter',\n",
       " 'illocutionary',\n",
       " 'dialogue_state',\n",
       " 'reintegrates',\n",
       " 'emotion_centered',\n",
       " '//github.com/urchade/hner',\n",
       " 'human_comprehensible',\n",
       " 'expert_annotated',\n",
       " 'deprive',\n",
       " 'absorbing',\n",
       " 'weakly_',\n",
       " 'crowd_sourced',\n",
       " 'cyberbullying',\n",
       " 'immigrants',\n",
       " 'propaganda',\n",
       " 'counselors',\n",
       " 'generated/exchanged',\n",
       " 'whats',\n",
       " 'non_receipt',\n",
       " 'human_to_human',\n",
       " 'easily_ignored',\n",
       " 'injector',\n",
       " 'injective',\n",
       " 'emotions',\n",
       " 'broking',\n",
       " 'amr_enhanced',\n",
       " 'disable',\n",
       " 'sydney_captions',\n",
       " 'smith',\n",
       " '//github.com/dreaminvoker/sire',\n",
       " 'kullbackleibler',\n",
       " 'gaze+',\n",
       " 'emotionally',\n",
       " 'vaccination',\n",
       " 'crowd',\n",
       " 'hurdle_trust',\n",
       " 'unattainable',\n",
       " 'attributive',\n",
       " 'multi_head_self_attention',\n",
       " 'human_ai',\n",
       " 'orem_af',\n",
       " 'insurmountable',\n",
       " 'emotionless',\n",
       " 'hackforums',\n",
       " 'interquestion',\n",
       " '//github.com/14dtj/cocoqa/video',\n",
       " 'emotion_probiotic',\n",
       " 'storied',\n",
       " 'far_reaching',\n",
       " 'connectionists',\n",
       " 'wonderland',\n",
       " 'emotionality',\n",
       " 'anger',\n",
       " 'crisis',\n",
       " 'enlightenment',\n",
       " 'ike_xai',\n",
       " 'inverse',\n",
       " 'few_shot',\n",
       " 'user_expected',\n",
       " 'dialogue_states',\n",
       " 'ai_supported',\n",
       " 'far_away',\n",
       " 'squashed',\n",
       " 'ad_dition',\n",
       " 're_curate',\n",
       " 'cag_distill',\n",
       " 'gone',\n",
       " 'playground',\n",
       " 'hypo_hypernym',\n",
       " 'emotion_specific',\n",
       " 'co_attention',\n",
       " 'aho_corasick',\n",
       " 'hacker',\n",
       " '//github.com/dreaminvoker/gain',\n",
       " 'desires',\n",
       " 'money_laundering',\n",
       " 'inter_play',\n",
       " 'search/browsing',\n",
       " 'autologistic',\n",
       " 'forgetting',\n",
       " 'overcame',\n",
       " 'input_driven',\n",
       " 'unrestricted_hop',\n",
       " '+multi_head_self_attention+',\n",
       " 'empowering',\n",
       " 'gradcam',\n",
       " 'recourse',\n",
       " 'homekeepers',\n",
       " 'competence',\n",
       " 'oke_cnn',\n",
       " 'fromtext',\n",
       " 'debatepedia',\n",
       " 'hateful',\n",
       " 'transfer',\n",
       " 'rejection',\n",
       " 'attention_like',\n",
       " 'response_selection',\n",
       " 'act_r_called',\n",
       " 'achieved_great_success',\n",
       " 'emotion_level',\n",
       " 'resolve',\n",
       " 'embarrassing',\n",
       " 'affective',\n",
       " 'hashtag_generated',\n",
       " 'attention_assisted',\n",
       " 'embarrassingly',\n",
       " 'affect',\n",
       " 'negating',\n",
       " 'inject',\n",
       " 'target_voter_oriented',\n",
       " 'dream',\n",
       " 'gauss_jordan',\n",
       " 'e_participation',\n",
       " 'motivate',\n",
       " 'selfish',\n",
       " 'root_seeking',\n",
       " 'debilitating',\n",
       " 'hatred',\n",
       " 'polarity_sentiment',\n",
       " 'wechat',\n",
       " 'crowd_workers',\n",
       " 'hadith',\n",
       " 'breaking_news',\n",
       " 'yahoo_yahoo',\n",
       " 'purchase',\n",
       " 'spill',\n",
       " 'motivation',\n",
       " 'customer_satisfaction',\n",
       " 'participatory',\n",
       " 'heartburn.we',\n",
       " 'task_agnostic',\n",
       " 'luke',\n",
       " 'delegated',\n",
       " 'ulti_label',\n",
       " 'disene',\n",
       " 'forced',\n",
       " 'guessing',\n",
       " 'fact_augmentation',\n",
       " 'outbound_directional',\n",
       " 'non_opinion',\n",
       " 'explaining',\n",
       " 'never_ending',\n",
       " 'literal_level',\n",
       " 'speech_acts',\n",
       " 'open_chat',\n",
       " 'prestige',\n",
       " 'attributives',\n",
       " 'hadoop',\n",
       " 'ir_qa',\n",
       " 'foreseeable',\n",
       " 'attention_gated',\n",
       " 'one_shot',\n",
       " 'self_bleu',\n",
       " 'pushouts',\n",
       " 'unfaithful',\n",
       " 'cif_ireland',\n",
       " 'crisis/disaster',\n",
       " 'cross_attention',\n",
       " 'unnoticed',\n",
       " 'web_native',\n",
       " 'good_turing',\n",
       " 'subject_verb_object',\n",
       " 'unlinked',\n",
       " 'eat',\n",
       " 'alienation',\n",
       " 'informal',\n",
       " 'enthusiasts',\n",
       " 'floods',\n",
       " 'user_generated',\n",
       " 'injects',\n",
       " 'reskilling',\n",
       " 'citizen_centred',\n",
       " 'forward_pass',\n",
       " 'emotion_sentiment',\n",
       " 'embodiments',\n",
       " 'on_site',\n",
       " 'invigilator',\n",
       " 'undeveloped',\n",
       " 'knowledge_sharing',\n",
       " 'demecarium',\n",
       " 'impede',\n",
       " 'doesnt',\n",
       " 'exertion',\n",
       " 'speech_act',\n",
       " 'creativity',\n",
       " 'hackforum',\n",
       " 'church',\n",
       " 'transfer_enhanced',\n",
       " 'eats',\n",
       " 'backlink',\n",
       " 'hownet',\n",
       " 'bijective',\n",
       " 'yes_no',\n",
       " 'explanationlp',\n",
       " '//turkunlp.org',\n",
       " 'translation_understanding',\n",
       " 'possessives',\n",
       " 'kullback_leibler',\n",
       " 'client/user',\n",
       " 'like_minded',\n",
       " 'movie_data',\n",
       " 'genuinely',\n",
       " 'self_annotated',\n",
       " 'inquiry',\n",
       " 'fiction',\n",
       " 'surfacing',\n",
       " 'inquestion',\n",
       " 'participation',\n",
       " 'okvqa',\n",
       " 're_enhances',\n",
       " 'wrong_labeled',\n",
       " 'eaten',\n",
       " 'sociable',\n",
       " 'achilles_heel',\n",
       " 'crowdsourcing_based',\n",
       " 'murder',\n",
       " 'response_generation',\n",
       " 'impeding',\n",
       " 'feed_forward',\n",
       " 'fact_tampering',\n",
       " 'deceptively',\n",
       " 'endorsements',\n",
       " 'customer_support',\n",
       " 'non_annotated',\n",
       " 'client',\n",
       " 'affection',\n",
       " 'utterance_layer',\n",
       " 'andemotions',\n",
       " 'pullnet',\n",
       " 'inspirational',\n",
       " 'empowerment',\n",
       " 'valence',\n",
       " 'brother',\n",
       " 'attentions',\n",
       " 'actant_relationship',\n",
       " 'laban_bartenieff',\n",
       " 'opinion_role',\n",
       " 'unwritten',\n",
       " 'sadga',\n",
       " 'non_crisis',\n",
       " 'objection',\n",
       " 'indisputable',\n",
       " 'emotion_detection',\n",
       " 'non_recommendation',\n",
       " 'onesidedness',\n",
       " 'a_la_carte',\n",
       " 'detect_missing',\n",
       " 'willingness',\n",
       " 'pretrain',\n",
       " 'emotion_cause',\n",
       " 'mass_media',\n",
       " 'unnatural',\n",
       " 'people',\n",
       " 'non_superordinate',\n",
       " 'user/client',\n",
       " 'victim',\n",
       " 'online_dating',\n",
       " 'exaustive',\n",
       " 'multiliteracies',\n",
       " 'disconnect',\n",
       " 'harassment',\n",
       " 'acceptablity',\n",
       " 'worker',\n",
       " 'reader/viewers',\n",
       " 'feel',\n",
       " 'user_focussed',\n",
       " 'great_success',\n",
       " 'comprehension_analysis',\n",
       " 'feelings',\n",
       " 'know_how',\n",
       " 'sellers',\n",
       " 'explainable',\n",
       " 'enquiry',\n",
       " 'gauges',\n",
       " 'scrutable',\n",
       " 'play',\n",
       " 'opinion_level',\n",
       " 'musdst',\n",
       " 'exquestions11https',\n",
       " 'aristocratic',\n",
       " 'inspiring',\n",
       " 'involvement',\n",
       " 'objectionable',\n",
       " 'word_play',\n",
       " '20_newsgroups',\n",
       " 'profoundness',\n",
       " 'emotional',\n",
       " 'uncanny',\n",
       " 'empowers',\n",
       " 'mention/entity',\n",
       " 'soulmate',\n",
       " 'understanding_based',\n",
       " 'ultimate_voter',\n",
       " 'language_transfer',\n",
       " 'predicting_missing',\n",
       " 'flourishing',\n",
       " 'out_links',\n",
       " 'distiller',\n",
       " 'human_object_interaction',\n",
       " 'crowd_sourcing',\n",
       " 'apollo_13',\n",
       " 'murmur',\n",
       " 'empowered',\n",
       " 'fostering',\n",
       " 'livemedqa',\n",
       " 'pop_up',\n",
       " 'fictions',\n",
       " 'human_interpretable',\n",
       " 'outwit',\n",
       " 'indonesian_language',\n",
       " 'disability',\n",
       " 'engaged',\n",
       " 'visualizer',\n",
       " 'hackers',\n",
       " 'back_translation',\n",
       " 'image_sharing',\n",
       " 'weakly_supervised',\n",
       " 'director_actor_critic',\n",
       " 'adhd_inattentive',\n",
       " 'domain_adversarial',\n",
       " 'followers/followees',\n",
       " 'motivational',\n",
       " 'deprives',\n",
       " 'call_centre',\n",
       " 'podcast',\n",
       " 'delirium',\n",
       " 'okb',\n",
       " 'mutual_distillation',\n",
       " \"'video+subtitles\",\n",
       " 'dordrecht',\n",
       " 'arguable',\n",
       " 'euphemism',\n",
       " 'preventable',\n",
       " 'okvqa_s3',\n",
       " 'inrs_teacute/leacute/com',\n",
       " '//github.com/noahs_ark/neurboparser',\n",
       " 'llocation',\n",
       " 'great_expectations',\n",
       " 'user_selected',\n",
       " 'prelearned',\n",
       " 'prosperity',\n",
       " 'suicide',\n",
       " 'sagm',\n",
       " 'successor',\n",
       " 'poverty',\n",
       " 'hacking',\n",
       " 'nominations',\n",
       " 'ontonotes_5.0',\n",
       " 'struggle',\n",
       " 'teach',\n",
       " 'offense',\n",
       " 'felt',\n",
       " 'sagdre',\n",
       " 'originator',\n",
       " 'sarcasm',\n",
       " 'adhd_200',\n",
       " 'internet_forum',\n",
       " 'continuations',\n",
       " 'untruthful',\n",
       " 'contributor',\n",
       " 'collectibles',\n",
       " 'fakedetector',\n",
       " 'love_it',\n",
       " 'delaunay',\n",
       " '//github.com/lancopku/well_classified_examples_are_underestimated',\n",
       " 'subordinate',\n",
       " 'cooking',\n",
       " 'inviable',\n",
       " 'autoencoder',\n",
       " 'sheet_fed',\n",
       " 'user_review_object',\n",
       " 'inverseconsultation',\n",
       " 'underreporting',\n",
       " 'non_obvious',\n",
       " 'possessions',\n",
       " 'hyperactivity',\n",
       " 'bullish',\n",
       " 'query_log',\n",
       " 'inflection_point',\n",
       " 'morally_disengaged',\n",
       " 'resolved',\n",
       " 't_cross_attention',\n",
       " 'voorhees',\n",
       " 'emotion_analysis',\n",
       " 'www.people.com.cn',\n",
       " 'rumor',\n",
       " 'explainit',\n",
       " 'hourglass',\n",
       " 'hyponymy',\n",
       " 'struggled',\n",
       " 'mutual_reinforcement_based',\n",
       " 'adamic_adar',\n",
       " 'deeming',\n",
       " 'rendering',\n",
       " 'kannada_language',\n",
       " 'injectivity',\n",
       " 'decisive',\n",
       " 'mutual_reinforcement',\n",
       " 'tensorflow2',\n",
       " 'exploring_and_distilling',\n",
       " 'fully_informed',\n",
       " 'expert_labeled',\n",
       " 'norwegian',\n",
       " 'choice_choice',\n",
       " 'walks_with_backward',\n",
       " 'hypernym',\n",
       " 'self_supervision',\n",
       " 'deski',\n",
       " 'attention_heads',\n",
       " 'underrating',\n",
       " 'hater',\n",
       " 'opponent',\n",
       " 'competences',\n",
       " 'autogenerated',\n",
       " 'impulse',\n",
       " 'imagined',\n",
       " 'homicide',\n",
       " 'group_recommendation',\n",
       " 'underestimate',\n",
       " 'actorcritic',\n",
       " 'contributing',\n",
       " 'plays',\n",
       " 'autopruner',\n",
       " 'folksonomy',\n",
       " 'dialokg',\n",
       " 'domain_transferable',\n",
       " 'semiotics',\n",
       " 'gathers',\n",
       " 'export',\n",
       " 'struggles',\n",
       " 'hyper_relations',\n",
       " 'adjudicators',\n",
       " 'argument_roles',\n",
       " 'morality',\n",
       " 'proxies',\n",
       " '\\\\gls',\n",
       " 'non_linguistic',\n",
       " 'graph_conversation',\n",
       " 'frisian_dutch',\n",
       " 'atgir',\n",
       " 'natural_languages',\n",
       " 'multi_shot',\n",
       " 'counter_intuitively',\n",
       " 'natural',\n",
       " 'panner',\n",
       " 'marriage',\n",
       " 'weak_supervision',\n",
       " 'hypernym_hyponym',\n",
       " 'unaware',\n",
       " 'ontonotes',\n",
       " 'non_opinions',\n",
       " 'entity_disambiguation',\n",
       " 'passports',\n",
       " 'unsupervisory',\n",
       " 'unobserved',\n",
       " 'emptiness',\n",
       " 'suicide_related',\n",
       " 'acceptabilities',\n",
       " 'uyghur',\n",
       " 'teacher_model',\n",
       " 'tolkien',\n",
       " '//youtu.be/vqaxi1wydau',\n",
       " 'knowledge_distillation',\n",
       " 'affordable',\n",
       " 'lurking',\n",
       " 'playing',\n",
       " 'non_content',\n",
       " 'anti_money',\n",
       " 'dehrm',\n",
       " 'home_made',\n",
       " 'sadness',\n",
       " 'forward_backward',\n",
       " 'collaborative_writing',\n",
       " 'unobservable',\n",
       " 'prevented',\n",
       " 'query_resolution',\n",
       " 'explained',\n",
       " 'back_propagated',\n",
       " 'imagination',\n",
       " 'autism',\n",
       " 'retailers',\n",
       " 'human_object',\n",
       " 'person',\n",
       " 'humanitarian',\n",
       " 'nonassociative',\n",
       " '//github.com/torchkge_team/torchkge',\n",
       " 'wielding',\n",
       " 'knowledge_transfer',\n",
       " 'task_success',\n",
       " 'at_a_glance',\n",
       " 'missing_facts',\n",
       " 'rewarding',\n",
       " 'semi_automatically',\n",
       " 'laundering',\n",
       " 'her/his',\n",
       " 'ourselves',\n",
       " 'emotion_affective',\n",
       " 'renderings',\n",
       " 'machine_reading_comprehension',\n",
       " 'taught',\n",
       " 'difficulty_controllable',\n",
       " 'possession',\n",
       " 'argument_clusters',\n",
       " 'fication',\n",
       " 'contributors',\n",
       " 'knowledge_injected',\n",
       " 'victoria',\n",
       " 'evtextract',\n",
       " 'chit_chat',\n",
       " 'lection',\n",
       " 'imagine_and_verbalize',\n",
       " 'spanish_to_english',\n",
       " 'thuyg_20',\n",
       " 'suicide_knowledge',\n",
       " 'bird',\n",
       " 'crisis_intervention',\n",
       " 'humanity',\n",
       " 'populate',\n",
       " 'previously_unseen',\n",
       " 'chunyu',\n",
       " 'pho_tos/videos',\n",
       " 'close_or_open',\n",
       " 'misuse',\n",
       " 'influencers',\n",
       " 'bert+bi_lstm+multi_head_self_attention+fc',\n",
       " 'family_enhanced',\n",
       " 'zero/few_shot_learning',\n",
       " 'creative_summ',\n",
       " 'perniciousness',\n",
       " 'hilton_hotels',\n",
       " 'suicidal',\n",
       " 'brothers',\n",
       " 'childhood',\n",
       " 'funds',\n",
       " 'expressibility',\n",
       " 'udt_qa',\n",
       " 'maker',\n",
       " 'role_to_role',\n",
       " 'video_qa',\n",
       " 'global_augmented',\n",
       " 'out_of_domain',\n",
       " 'self_collected',\n",
       " 'exerted',\n",
       " 'attensy_sner',\n",
       " 'emotional_contagion',\n",
       " 'self_adversarial',\n",
       " 'birds_eye',\n",
       " 'emotion_recognition',\n",
       " 'asthmakgxe',\n",
       " '//github.com/wangbing1416/vagr',\n",
       " 'in_links',\n",
       " 'item_level',\n",
       " 'folks',\n",
       " 'decipherable',\n",
       " 'appropri_ate',\n",
       " 'root_oriented',\n",
       " 'outmost',\n",
       " 'homophonic',\n",
       " 'riding',\n",
       " 'american_football',\n",
       " 'informative_responses',\n",
       " 'bottom_up_attention',\n",
       " 'cad/camobject',\n",
       " 'abstactive',\n",
       " 'oppositions',\n",
       " 'citizens',\n",
       " 'actor',\n",
       " 'day_to_day',\n",
       " 'burdensome',\n",
       " 'full_attention',\n",
       " 'subcollection',\n",
       " 'autistic',\n",
       " 'duc_02',\n",
       " 'free_direction',\n",
       " 'forward_connection',\n",
       " 'long_sought',\n",
       " 'drea',\n",
       " 'overburdened',\n",
       " 'choice_dependent',\n",
       " 'beings',\n",
       " 'wh_',\n",
       " 'adhd_',\n",
       " 'creatively',\n",
       " 'semi_structured',\n",
       " 'knowedge',\n",
       " 'facial_expression',\n",
       " 'myhill_nerode',\n",
       " 'attribution',\n",
       " 'motive',\n",
       " 'exert',\n",
       " 'informality',\n",
       " 'creative',\n",
       " 'idssim_wknkn',\n",
       " 'hyponym',\n",
       " 'william',\n",
       " '//github.com/tsafavi/cascader',\n",
       " 'conll_rdf',\n",
       " 'struggling',\n",
       " 'responsibilities',\n",
       " 'feels',\n",
       " 'expert_curated',\n",
       " 'gerrit',\n",
       " 'streaming_media',\n",
       " 'acts',\n",
       " 'activations',\n",
       " 'loses',\n",
       " 'abridged',\n",
       " 'primary_education',\n",
       " 'golfers',\n",
       " 'invoked',\n",
       " 'necker_cube',\n",
       " 'unreachable',\n",
       " 'modal_specific',\n",
       " 'counterproductive',\n",
       " 'root_to_leaf',\n",
       " 'resolves',\n",
       " 'directives',\n",
       " 'drools',\n",
       " 'moods',\n",
       " 'reputable',\n",
       " 'comprehensibility',\n",
       " 'question_reasoning',\n",
       " 'dehumanization',\n",
       " 'hypernyms',\n",
       " 'co_learns',\n",
       " 'suicide_oriented',\n",
       " 'meaningless',\n",
       " 'feeding',\n",
       " 'high_score',\n",
       " 'unmanageable',\n",
       " 'knowledge_demanding',\n",
       " 'surfer',\n",
       " 'imagetext',\n",
       " 'animated',\n",
       " '//asthmakgxe.moreair.info/',\n",
       " 'off_label',\n",
       " 'serverless',\n",
       " 'label_lacking',\n",
       " 'naturalness',\n",
       " 'whitegoods',\n",
       " 'convolve',\n",
       " 'votership',\n",
       " 'ucca_like',\n",
       " 'anglo_american',\n",
       " 'gaming',\n",
       " 'bonuses',\n",
       " 'multi_attention',\n",
       " 'makehuman',\n",
       " 'one_to_one',\n",
       " 'transductive/inductive',\n",
       " 'demonstrable',\n",
       " 'unlink',\n",
       " 'advertisers',\n",
       " 'origin_destination',\n",
       " 'outlying',\n",
       " 'autoencoders',\n",
       " 'virtue',\n",
       " 'sports',\n",
       " 'supervision_based',\n",
       " 'ishizaki',\n",
       " 'success',\n",
       " 'unattributed',\n",
       " 'lack',\n",
       " 'foreverdreaming',\n",
       " 'rewards',\n",
       " 'demonstrator',\n",
       " 'prabhumoye',\n",
       " 'proud',\n",
       " 'multi_user',\n",
       " 'ontonotes4.0',\n",
       " 'ebm',\n",
       " 'endorsement',\n",
       " 'payment',\n",
       " 'goods',\n",
       " 'exams',\n",
       " 'self_service',\n",
       " 'web_directory',\n",
       " 'evoprompting',\n",
       " 'excellent_results',\n",
       " 'uptoyou',\n",
       " 'amber',\n",
       " 'hakia',\n",
       " 'drug_seeking',\n",
       " 'self_unbiased',\n",
       " 'mirflickr_25',\n",
       " 'mturk287',\n",
       " 'speech_perception',\n",
       " 'unveils',\n",
       " 'information_overload',\n",
       " 'umkm',\n",
       " 'intra_cultural',\n",
       " 'michel',\n",
       " 'neutrality',\n",
       " 'semi_',\n",
       " 'non_pretrained',\n",
       " 'lecturers',\n",
       " 'self_made',\n",
       " 'enemies',\n",
       " 'strongly_supervised',\n",
       " 'kermit',\n",
       " 'overlays',\n",
       " 'walk_induced',\n",
       " 'sunnah',\n",
       " 'co_training',\n",
       " 'event_role',\n",
       " '//www.nactem.ac.uk/cord/',\n",
       " 'hard_attention',\n",
       " 'aimlessly',\n",
       " 'literal_aware',\n",
       " 'objectivity',\n",
       " 'non_intrusive',\n",
       " 'suicide_bombing',\n",
       " 'entity_mention',\n",
       " 'customer_service',\n",
       " 'politifact.com',\n",
       " 'ocr_obj',\n",
       " 'person_focused',\n",
       " 'in_house',\n",
       " 'turn_taking',\n",
       " 'charlie_hebdo',\n",
       " 'inbound_direction',\n",
       " 'col_lection',\n",
       " 'observer',\n",
       " 'tatoeba',\n",
       " 'pretrained',\n",
       " 'inputlog',\n",
       " 'lstms',\n",
       " 'krvqr',\n",
       " 'hake',\n",
       " 'hyper_relational',\n",
       " 'bidirectional_encoder_representations_from_transformers',\n",
       " 'semi_gdner',\n",
       " \"'infectious\",\n",
       " 'incor_poration',\n",
       " 'head_qa',\n",
       " 'phishing',\n",
       " 'attention_enhanced',\n",
       " 'human_human',\n",
       " 'connection_attention',\n",
       " \"'naturalness\",\n",
       " 'category_related',\n",
       " 'newcomers',\n",
       " 'happening',\n",
       " 'hawk_r',\n",
       " 'external_information',\n",
       " 'downward',\n",
       " 'advocacy',\n",
       " 'entity_seeking',\n",
       " 're_harnessing',\n",
       " 'illicit',\n",
       " 'climbing',\n",
       " 'activation',\n",
       " 'uke',\n",
       " 'unintuitive',\n",
       " 'text_generation',\n",
       " 'jeopardises',\n",
       " 'non_semantic',\n",
       " 'inspire',\n",
       " 'de_identified',\n",
       " 'sags',\n",
       " 'rumours',\n",
       " 'feder',\n",
       " 'syllabification',\n",
       " 'non_ironic',\n",
       " 'lively',\n",
       " 'ultimate',\n",
       " 'infuses',\n",
       " 'unidentifiable',\n",
       " 'recall_oriented_understudy_for_gisting',\n",
       " 'unidentified',\n",
       " 'video2entities',\n",
       " 'video_level',\n",
       " 'target_centered',\n",
       " 'curation',\n",
       " 'un_annotated',\n",
       " 'strands',\n",
       " 'riders',\n",
       " 'motivations',\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_node.external['phrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_embs = {idx: paper_emb for idx, paper_emb in enumerate(sentence_model.encode([paper.title for paper in collection]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque([taxo.root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING using_llms_on_graphs; remaining in queue: deque([])\n",
      "['1', '2', '3']\n",
      "before ranking-based classification:  0.6111111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 4096.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.7916666666666666\n",
      "PROCESSING pure_graphs; remaining in queue: deque([text_rich_graphs, text_paired_graphs])\n",
      "['4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2270.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.7142857142857143\n",
      "PROCESSING text_rich_graphs; remaining in queue: deque([text_paired_graphs, llm_as_predictor])\n",
      "['8', '14', '20']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3788.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.3\n",
      "PROCESSING text_paired_graphs; remaining in queue: deque([llm_as_predictor, llm_as_predictor, llm_as_encoder, llm_as_aligner])\n",
      "['23', '26']\n",
      "before ranking-based classification:  0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4433.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.0\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_predictor, llm_as_encoder, llm_as_aligner, llm_as_predictor, llm_as_aligner])\n",
      "['5', '6', '7']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.07142857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 4675.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.21428571428571427\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_encoder, llm_as_aligner, llm_as_predictor, llm_as_aligner, direct_answering, heuristic_reasoning, algorithmic_reasoning])\n",
      "['9', '12', '13']\n",
      "before ranking-based classification:  0.06060606060606061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 5617.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.18181818181818182\n",
      "PROCESSING llm_as_encoder; remaining in queue: deque([llm_as_aligner, llm_as_predictor, llm_as_aligner, direct_answering, heuristic_reasoning, algorithmic_reasoning, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning])\n",
      "['15', '18', '19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 5140.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.0\n",
      "PROCESSING llm_as_aligner; remaining in queue: deque([llm_as_predictor, llm_as_aligner, direct_answering, heuristic_reasoning, algorithmic_reasoning, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation])\n",
      "['21', '22']\n",
      "before ranking-based classification:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4718.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.0\n",
      "PROCESSING llm_as_predictor; remaining in queue: deque([llm_as_aligner, direct_answering, heuristic_reasoning, algorithmic_reasoning, graph_as_sequence, graph_empowered_llm, graph_aware_llm_finetuning, optimization, data_augmentation, knowledge_distillation, prediction_alignment, latent_space_alignment])\n",
      "['24', '25']\n",
      "before ranking-based classification:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m full_text_class_embs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c_id \u001b[38;5;129;01min\u001b[39;00m tqdm(class_ids):\n\u001b[0;32m---> 37\u001b[0m     full_text_class_embs\u001b[38;5;241m.\u001b[39mappend(\u001b[43maverage_with_harmonic_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpaper_embs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     39\u001b[0m full_text_sim \u001b[38;5;241m=\u001b[39m cosine_similarity_embeddings(np\u001b[38;5;241m.\u001b[39marray([paper_embs[p_id] \u001b[38;5;28;01mfor\u001b[39;00m p_id \u001b[38;5;129;01min\u001b[39;00m target_node\u001b[38;5;241m.\u001b[39mpapers]), full_text_class_embs)\n\u001b[1;32m     40\u001b[0m full_text_preds \u001b[38;5;241m=\u001b[39m full_text_sim\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/utils.py:136\u001b[0m, in \u001b[0;36maverage_with_harmonic_series\u001b[0;34m(representations, axis)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim):\n\u001b[1;32m    135\u001b[0m     weights[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepresentations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Comparative-Summarization/taxoadapt/env/lib/python3.8/site-packages/numpy/lib/function_base.py:548\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    546\u001b[0m     scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw)\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[0;32m--> 548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    549\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    551\u001b[0m     avg \u001b[38;5;241m=\u001b[39m avg_as_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(a, wgt,\n\u001b[1;32m    552\u001b[0m                       dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\u001b[38;5;241m.\u001b[39msum(axis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeepdims_kw) \u001b[38;5;241m/\u001b[39m scl\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "while queue:\n",
    "    target_node = queue.popleft()\n",
    "    print(f\"PROCESSING {target_node.label}; remaining in queue: {queue}\")\n",
    "\n",
    "    class_ids = [child.node_id for child in target_node.children]\n",
    "    print(class_ids)\n",
    "\n",
    "    phrase_class_embs = [computeClassEmb(curr_node, taxo, granularity='phrases') for curr_node in target_node.children]\n",
    "    sent_class_embs = [computeClassEmb(curr_node, taxo, granularity='sentences') for curr_node in target_node.children]\n",
    "    joint_class_embs = [(p+s)/2 for p, s in zip(phrase_class_embs, sent_class_embs)]\n",
    "\n",
    "    encoded_papers = np.array([paper_embs[p_id] for p_id in target_node.papers])\n",
    "    phrase_sim = cosine_similarity_embeddings(encoded_papers, phrase_class_embs) # P x C\n",
    "    sent_sim = cosine_similarity_embeddings(encoded_papers, sent_class_embs)\n",
    "    joint_sim = cosine_similarity_embeddings(encoded_papers, joint_class_embs)\n",
    "\n",
    "    phrase_winner = phrase_sim.argmax(axis=1) # P x 1\n",
    "    sent_winner = sent_sim.argmax(axis=1)\n",
    "    joint_winner = joint_sim.argmax(axis=1)\n",
    "    winner_idxs = stats.mode(np.stack((phrase_winner, sent_winner, joint_winner), axis=1), axis=1, keepdims=False)[0]\n",
    "    labels = [class_ids[winner_idx] for winner_idx in winner_idxs]\n",
    "    correct = [paper_id in target_node.children[winner_idxs[idx]].gold for idx, paper_id in enumerate(target_node.papers)]\n",
    "    print(\"before ranking-based classification: \", sum(correct)/len(correct))\n",
    "\n",
    "    scores = (phrase_sim[np.arange(len(phrase_sim)), \n",
    "                        winner_idxs] + sent_sim[np.arange(len(sent_sim)), \n",
    "                                                winner_idxs] + joint_sim[np.arange(len(joint_sim)), \n",
    "                                                                        winner_idxs])/3\n",
    "\n",
    "    class_map = {i:[] for i in class_ids}\n",
    "    for idx, paper_id in enumerate(target_node.papers):\n",
    "        class_map[labels[idx]].append((scores[idx], target_node.papers[paper_id]))\n",
    "\n",
    "    class_map = {i:sorted(class_map[i], key=lambda x: -x[0]) for i in class_ids}\n",
    "    full_text_class_embs = []\n",
    "    for c_id in tqdm(class_ids):\n",
    "        full_text_class_embs.append(average_with_harmonic_series(np.array([paper_embs[p[1].id] for p in class_map[c_id]])))\n",
    "\n",
    "    full_text_sim = cosine_similarity_embeddings(np.array([paper_embs[p_id] for p_id in target_node.papers]), full_text_class_embs)\n",
    "    full_text_preds = full_text_sim.argmax(axis=1)\n",
    "    full_text_scores = [p_id in target_node.children[pred].gold for p_id, pred in enumerate(full_text_preds)]\n",
    "\n",
    "    for idx, paper_id in enumerate(target_node.papers):\n",
    "        target_node.children[winner_idxs[idx]].papers[paper_id] = target_node.papers[paper_id]\n",
    "\n",
    "    print(\"ranking-based classification\", sum(full_text_scores)/len(full_text_scores))\n",
    "\n",
    "    for child in target_node.children:\n",
    "        queue.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 7, 3, 59, 49, 47]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p[1].id for p in class_map[c_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_class_embs.append(average_with_harmonic_series(np.array([encoded_papers[p[1].id] for p in class_map[c_id]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8', '14', '20']\n",
      "llm_as_predictor: 57 phrases filtered!\n",
      "llm_as_encoder: 75 phrases filtered!\n",
      "llm_as_aligner: 27 phrases filtered!\n",
      "llm_as_predictor: 46 sentences filtered!\n",
      "llm_as_encoder: 12 sentences filtered!\n",
      "llm_as_aligner: 18 sentences filtered!\n"
     ]
    }
   ],
   "source": [
    "target_node = taxo.root#.children[1]\n",
    "\n",
    "class_ids = [child.node_id for child in target_node.children]\n",
    "print(class_ids)\n",
    "\n",
    "phrase_class_embs = [computeClassEmb(curr_node, taxo, granularity='phrases') for curr_node in target_node.children]\n",
    "sent_class_embs = [computeClassEmb(curr_node, taxo, granularity='sentences') for curr_node in target_node.children]\n",
    "# joint_class_embs = [computeClassEmb(curr_node, taxo, granularity='mixed') for curr_node in target_node.children]\n",
    "joint_class_embs = [(p+s)/2 for p, s in zip(phrase_class_embs, sent_class_embs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_rich_graphs: 6586 phrases filtered!\n"
     ]
    }
   ],
   "source": [
    "external_phrase_embs, sim_diff, keep_phrase = compareClasses(external_phrases, taxo, target_node.children[0], 'phrases')\n",
    "filtered_embs = external_phrase_embs[keep_phrase, :]\n",
    "filtered_external_phrases = list(compress(external_phrases, keep_phrase))\n",
    "filtered_diffs = sim_diff[keep_phrase]\n",
    "print(f'{target_node.label}: {len(external_phrases) - len(filtered_external_phrases)} phrases filtered!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before ranking-based classification:  0.8194444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking-based classification 0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "encoded_papers = sentence_model.encode([collection[p_id].raw_text for p_id in target_node.papers])\n",
    "phrase_sim = cosine_similarity_embeddings(encoded_papers, phrase_class_embs) # P x C\n",
    "sent_sim = cosine_similarity_embeddings(encoded_papers, sent_class_embs)\n",
    "joint_sim = cosine_similarity_embeddings(encoded_papers, joint_class_embs)\n",
    "\n",
    "phrase_winner = phrase_sim.argmax(axis=1) # P x 1\n",
    "sent_winner = sent_sim.argmax(axis=1)\n",
    "joint_winner = joint_sim.argmax(axis=1)\n",
    "winner_idxs = stats.mode(np.stack((phrase_winner, sent_winner, joint_winner), axis=1), axis=1, keepdims=False)[0]\n",
    "labels = [class_ids[winner_idx] for winner_idx in winner_idxs]\n",
    "correct = [paper_id in target_node.children[winner_idxs[idx]].gold for idx, paper_id in enumerate(target_node.papers)]\n",
    "print(\"before ranking-based classification: \", sum(correct)/len(correct))\n",
    "\n",
    "scores = (phrase_sim[np.arange(len(phrase_sim)), \n",
    "                    winner_idxs] + sent_sim[np.arange(len(sent_sim)), \n",
    "                                            winner_idxs] + joint_sim[np.arange(len(joint_sim)), \n",
    "                                                                      winner_idxs])/3\n",
    "\n",
    "class_map = {i:[] for i in class_ids}\n",
    "for idx, paper_id in enumerate(target_node.papers):\n",
    "    class_map[labels[idx]].append((scores[idx], target_node.papers[paper_id]))\n",
    "\n",
    "class_map = {i:sorted(class_map[i], key=lambda x: -x[0]) for i in class_ids}\n",
    "full_text_class_embs = []\n",
    "for c_id in tqdm(class_ids):\n",
    "    full_text_class_embs.append(average_with_harmonic_series(sentence_model.encode([p[1].raw_text for p in class_map[c_id]])))\n",
    "\n",
    "full_text_sim = cosine_similarity_embeddings(sentence_model.encode([collection[p_id].raw_text for p_id in target_node.papers]), full_text_class_embs)\n",
    "full_text_preds = full_text_sim.argmax(axis=1)\n",
    "full_text_scores = [p_id in target_node.children[pred].gold for p_id, pred in enumerate(full_text_preds)]\n",
    "\n",
    "# for idx, paper_id in enumerate(target_node.papers):\n",
    "#     target_node.children[winner_idxs[idx]].papers[paper_id] = target_node.papers[paper_id]\n",
    "\n",
    "print(\"ranking-based classification\", sum(full_text_scores)/len(full_text_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title: 0.24444444444444444\n",
    "title + abstract: 0.26666666666666666\n",
    "raw_text: 0.28888888888888886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5,6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/')\n",
    "model = BertModel.from_pretrained('/home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/', output_hidden_states=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<00:00, 1444.28it/s]\n",
      "100%|██████████| 51136/51136 [00:06<00:00, 7733.38it/s]\n"
     ]
    }
   ],
   "source": [
    "cnt = defaultdict(int)\n",
    "all_raw_phrases = []\n",
    "token_lens = {}\n",
    "\n",
    "for paper in tqdm(collection):\n",
    "\tall_raw_phrases.extend(paper.raw_text.strip().split())\n",
    "\n",
    "for phrase in tqdm(set(all_raw_phrases)):\n",
    "\ttoken_lens[phrase] = len(tokenizer(phrase).input_ids)\n",
    "\n",
    "cnt = Counter(all_raw_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeProp(phrase_sim, sent_sim):\n",
    "    print('phrase avg:', phrase_sim.mean(axis=0))\n",
    "    print('sent avg:', sent_sim.mean(axis=0))\n",
    "    phrase_count = Counter(phrase_sim.argmax(axis=1))\n",
    "    phrase_list = np.array([phrase_count[i] \n",
    "                            if i in phrase_count \n",
    "                            else 0 \n",
    "                            for i in np.arange(len(taxo.root.children))])\n",
    "    sent_count = Counter(sent_sim.argmax(axis=1))\n",
    "    sent_list = np.array([sent_count[i] \n",
    "                          if i in sent_count \n",
    "                          else 0 \n",
    "                          for i in np.arange(len(taxo.root.children))])\n",
    "    \n",
    "    print(f\"Phrase: {phrase_list/sum(phrase_list)}\")\n",
    "    print(f\"Sentence: {sent_list/sum(sent_list)}\")\n",
    "    return phrase_count, sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(text, token_lens, length=512):\n",
    "\tchunks = [[]]\n",
    "\tsplit_text = text.split()\n",
    "\tcount = 0\n",
    "\tfor word in split_text:\n",
    "\t\tnew_count = count + token_lens[word] + 2 # 2 for [CLS] and [SEP]\n",
    "\t\tif new_count > length:\n",
    "\t\t\tchunks.append([word])\n",
    "\t\t\tcount = token_lens[word]\n",
    "\t\telse:\n",
    "\t\t\tchunks[len(chunks) - 1].append(word)\n",
    "\t\t\tcount = new_count\n",
    "\t\n",
    "\tchunks = [\" \".join(chunk) for chunk in chunks]\n",
    "\treturn chunks if len(chunks) > 1 else chunks[0]\n",
    "\n",
    "def encode(w):\n",
    "\tif type(w) == str:\n",
    "\t\tchunks = chunkify(w, token_lens)\n",
    "\telse:\n",
    "\t\tchunks = w\n",
    "\t\n",
    "\tencoded_data = tokenizer(chunks, padding=True, return_tensors=\"pt\").to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(**encoded_data)\n",
    "\t\n",
    "\t# Get all hidden states\n",
    "\tif (type(w) == str) and (len(output.last_hidden_state.shape) == 3):\n",
    "\t\treturn output.last_hidden_state.mean(axis=(0,1)).cpu().numpy()\n",
    "\telse:\n",
    "\t\treturn output.last_hidden_state.mean(axis=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [05:51<00:00,  4.88s/it]\n"
     ]
    }
   ],
   "source": [
    "cnt = defaultdict(int)\n",
    "vocab_emb = {}\n",
    "vocab_sent_emb = {}\n",
    "\n",
    "for paper in tqdm(collection):\n",
    "\tdata = paper.raw_text.strip().split()\n",
    "\tsent_data = paper.raw_text.strip().split(\" . \")\n",
    "\tp_embs = sentence_model.encode(data)\n",
    "\ts_embs = sentence_model.encode(sent_data)\n",
    "\tfor w_id, word in enumerate(data):\n",
    "\t\tcnt[word] += 1\n",
    "\t\tif word not in vocab_emb:\n",
    "\t\t\tvocab_emb[word] = p_embs[w_id]\n",
    "\tfor s_id, sent in enumerate(sent_data):\n",
    "\t\tvocab_sent_emb[sent] = s_embs[s_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 423/423 [00:03<00:00, 135.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for w in tqdm(all_common_phrases):\n",
    "    if w not in vocab_emb:\n",
    "        vocab_emb[w] = sentence_model.encode(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 46604),\n",
       " ('.', 31280),\n",
       " ('the', 30334),\n",
       " ('and', 14675),\n",
       " (')', 14552),\n",
       " ('(', 14263),\n",
       " ('of', 12858),\n",
       " ('to', 10450),\n",
       " ('in', 10030),\n",
       " ('a', 9576),\n",
       " ('for', 6881),\n",
       " (':', 6764),\n",
       " ('we', 6445),\n",
       " ('is', 5707),\n",
       " ('on', 5222),\n",
       " ('as', 4366),\n",
       " ('with', 4331),\n",
       " ('[', 3910),\n",
       " ('graph', 3906),\n",
       " (']', 3898),\n",
       " ('et', 3473),\n",
       " ('that', 3272),\n",
       " ('are', 3188),\n",
       " (';', 2906),\n",
       " ('model', 2896),\n",
       " ('al.', 2762),\n",
       " ('from', 2608),\n",
       " ('this', 2602),\n",
       " ('by', 2589),\n",
       " ('can', 2191),\n",
       " ('1', 1978),\n",
       " ('our', 1948),\n",
       " ('be', 1889),\n",
       " ('%', 1851),\n",
       " ('tasks', 1850),\n",
       " ('models', 1797),\n",
       " ('llms', 1727),\n",
       " ('which', 1720),\n",
       " ('an', 1640),\n",
       " ('text', 1515),\n",
       " ('2', 1514),\n",
       " ('performance', 1494),\n",
       " ('it', 1456),\n",
       " ('reasoning', 1379),\n",
       " ('information', 1368),\n",
       " ('node', 1330),\n",
       " ('molecule', 1328),\n",
       " ('data', 1311),\n",
       " ('nodes', 1249),\n",
       " ('language', 1242),\n",
       " ('task', 1206),\n",
       " ('3', 1181),\n",
       " ('each', 1145),\n",
       " ('or', 1129),\n",
       " ('table', 1107),\n",
       " ('results', 1087),\n",
       " ('have', 1038),\n",
       " ('not', 1023),\n",
       " ('dataset', 1015),\n",
       " ('paper', 1003),\n",
       " ('different', 996),\n",
       " ('these', 993),\n",
       " ('2022', 978),\n",
       " ('more', 959),\n",
       " ('4', 952),\n",
       " ('all', 946),\n",
       " ('such', 926),\n",
       " ('at', 919),\n",
       " ('also', 916),\n",
       " ('two', 910),\n",
       " ('between', 898),\n",
       " ('training', 897),\n",
       " ('2023', 886),\n",
       " ('using', 879),\n",
       " ('input', 861),\n",
       " ('datasets', 857),\n",
       " ('use', 852),\n",
       " ('graphs', 848),\n",
       " ('2021', 830),\n",
       " ('learning', 828),\n",
       " ('figure', 828),\n",
       " ('prediction', 821),\n",
       " ('2020', 820),\n",
       " ('has', 812),\n",
       " ('both', 805),\n",
       " ('knowledge', 795),\n",
       " ('#', 795),\n",
       " ('*', 786),\n",
       " ('into', 771),\n",
       " ('>', 765),\n",
       " ('5', 762),\n",
       " ('2019', 758),\n",
       " ('where', 749),\n",
       " ('their', 729),\n",
       " ('al', 699),\n",
       " ('molecules', 697),\n",
       " ('its', 684),\n",
       " ('based', 661),\n",
       " ('other', 656),\n",
       " ('methods', 649),\n",
       " ('c', 639),\n",
       " ('representations', 638),\n",
       " ('used', 630),\n",
       " ('work', 629),\n",
       " ('only', 625),\n",
       " (\"''\", 615),\n",
       " ('given', 612),\n",
       " ('one', 604),\n",
       " ('0', 589),\n",
       " ('prompt', 586),\n",
       " ('gnn', 583),\n",
       " ('@', 582),\n",
       " ('natural_language', 579),\n",
       " ('llm', 579),\n",
       " ('while', 577),\n",
       " ('<', 571),\n",
       " ('set', 565),\n",
       " ('will', 555),\n",
       " ('``', 547),\n",
       " ('number', 543),\n",
       " ('however', 539),\n",
       " ('when', 535),\n",
       " ('?', 526),\n",
       " ('6', 522),\n",
       " ('molecular', 522),\n",
       " ('generate', 514),\n",
       " ('transformer', 514),\n",
       " ('pre_training', 513),\n",
       " ('structure', 511),\n",
       " ('then', 505),\n",
       " ('method', 501),\n",
       " ('gnns', 500),\n",
       " ('than', 499),\n",
       " ('}', 494),\n",
       " ('bert', 491),\n",
       " ('{', 488),\n",
       " ('framework', 486),\n",
       " ('example', 481),\n",
       " ('generation', 475),\n",
       " ('tokens', 470),\n",
       " ('representation', 469),\n",
       " ('approach', 467),\n",
       " ('shown', 464),\n",
       " ('lm', 462),\n",
       " ('zhang', 456),\n",
       " ('large', 451),\n",
       " ('experiments', 449),\n",
       " ('first', 447),\n",
       " ('three', 443),\n",
       " ('g', 441),\n",
       " ('i', 439),\n",
       " ('accuracy', 438),\n",
       " ('&', 438),\n",
       " ('new', 435),\n",
       " ('embeddings', 432),\n",
       " ('most', 430),\n",
       " ('features', 428),\n",
       " ('various', 425),\n",
       " ('e', 424),\n",
       " ('embedding', 415),\n",
       " ('same', 415),\n",
       " ('but', 412),\n",
       " ('function', 410),\n",
       " ('conference', 408),\n",
       " ('understanding', 408),\n",
       " ('retrieval', 406),\n",
       " ('zero_shot', 404),\n",
       " ('li', 403),\n",
       " ('encoder', 401),\n",
       " ('output', 399),\n",
       " ('lms', 398),\n",
       " ('smiles', 398),\n",
       " ('query', 396),\n",
       " ('following', 395),\n",
       " ('2018', 394),\n",
       " ('label', 394),\n",
       " ('further', 393),\n",
       " ('7', 392),\n",
       " ('edges', 389),\n",
       " ('research', 386),\n",
       " ('wang', 385),\n",
       " ('pretraining', 385),\n",
       " ('over', 382),\n",
       " ('edge', 377),\n",
       " ('n', 376),\n",
       " ('_', 372),\n",
       " ('generated', 371),\n",
       " ('e.g.', 367),\n",
       " ('t', 366),\n",
       " ('section', 366),\n",
       " ('propose', 363),\n",
       " ('better', 363),\n",
       " ('been', 363),\n",
       " ('pre_trained', 362),\n",
       " ('8', 361),\n",
       " ('b', 360),\n",
       " ('fine_tuning', 359),\n",
       " ('10', 358),\n",
       " ('may', 352),\n",
       " ('node_classification', 352),\n",
       " ('r', 351),\n",
       " ('sequence', 350),\n",
       " ('h', 350),\n",
       " ('proposed', 347),\n",
       " ('token', 340),\n",
       " ('time', 340),\n",
       " ('neighbors', 338),\n",
       " ('s', 336),\n",
       " ('through', 335),\n",
       " ('baselines', 334),\n",
       " ('attention', 334),\n",
       " ('protein', 334),\n",
       " ('there', 332),\n",
       " ('trained', 332),\n",
       " ('how', 328),\n",
       " ('prompts', 327),\n",
       " ('without', 327),\n",
       " ('them', 326),\n",
       " ('api', 325),\n",
       " ('including', 324),\n",
       " ('via', 324),\n",
       " ('question', 323),\n",
       " ('answer', 323),\n",
       " ('process', 322),\n",
       " ('liu', 322),\n",
       " ('examples', 320),\n",
       " ('similar', 320),\n",
       " ('property', 319),\n",
       " ('test', 318),\n",
       " ('labels', 318),\n",
       " ('they', 317),\n",
       " ('chemical', 317),\n",
       " ('network', 316),\n",
       " ('papers', 316),\n",
       " ('study', 314),\n",
       " ('semantic', 312),\n",
       " ('large_language_models', 311),\n",
       " ('across', 311),\n",
       " ('d', 310),\n",
       " ('ability', 308),\n",
       " ('encoding', 305),\n",
       " ('some', 303),\n",
       " ('parameters', 303),\n",
       " ('user', 301),\n",
       " ('evaluation', 300),\n",
       " ('learn', 300),\n",
       " ('2017', 299),\n",
       " ('=', 298),\n",
       " ('v', 297),\n",
       " ('i.e.', 297),\n",
       " ('textual', 293),\n",
       " ('p', 291),\n",
       " ('like', 290),\n",
       " ('context', 289),\n",
       " ('+', 289),\n",
       " ('iclr', 286),\n",
       " ('no', 284),\n",
       " ('proceedings', 284),\n",
       " ('best', 282),\n",
       " ('perform', 281),\n",
       " ('layer', 281),\n",
       " ('similarity', 281),\n",
       " ('https', 280),\n",
       " ('provide', 280),\n",
       " ('within', 278),\n",
       " ('compared', 276),\n",
       " ('chen', 276),\n",
       " ('classification', 276),\n",
       " ('relation', 276),\n",
       " ('kg', 275),\n",
       " ('structures', 271),\n",
       " ('modeling', 271),\n",
       " ('...', 270),\n",
       " ('description', 270),\n",
       " ('problem', 269),\n",
       " ('existing', 268),\n",
       " ('texts', 267),\n",
       " ('feature', 267),\n",
       " ('design', 263),\n",
       " ('applications', 262),\n",
       " ('appendix', 262),\n",
       " ('9', 262),\n",
       " ('well', 262),\n",
       " ('corpus', 261),\n",
       " ('published', 261),\n",
       " ('respectively', 260),\n",
       " ('since', 259),\n",
       " ('could', 258),\n",
       " ('analysis', 257),\n",
       " ('score', 256),\n",
       " ('about', 253),\n",
       " ('any', 252),\n",
       " ('domain', 252),\n",
       " ('multiple', 251),\n",
       " ('specific', 251),\n",
       " ('prompting', 250),\n",
       " ('chatgpt', 250),\n",
       " ('show', 249),\n",
       " ('types', 249),\n",
       " ('specifically', 247),\n",
       " ('corresponding', 247),\n",
       " ('train', 246),\n",
       " ('evaluate', 245),\n",
       " ('document', 243),\n",
       " ('discovery', 242),\n",
       " ('machine_learning', 240),\n",
       " ('descriptions', 238),\n",
       " ('link_prediction', 238),\n",
       " ('random', 236),\n",
       " ('o', 236),\n",
       " ('connected', 235),\n",
       " ('potential', 235),\n",
       " ('thus', 234),\n",
       " ('original', 234),\n",
       " ('system', 234),\n",
       " ('entities', 233),\n",
       " ('search', 231),\n",
       " ('related', 231),\n",
       " ('pairs', 230),\n",
       " ('layers', 230),\n",
       " ('architecture', 230),\n",
       " ('do', 229),\n",
       " ('baseline', 229),\n",
       " ('!', 228),\n",
       " ('target', 227),\n",
       " ('predict', 227),\n",
       " ('downstream_tasks', 227),\n",
       " ('neighbor', 226),\n",
       " ('properties', 226),\n",
       " ('pp', 226),\n",
       " ('improve', 224),\n",
       " ('objective', 224),\n",
       " ('demonstrate', 222),\n",
       " ('complex', 221),\n",
       " ('if', 220),\n",
       " ('capture', 219),\n",
       " ('due', 218),\n",
       " ('setting', 218),\n",
       " ('was', 218),\n",
       " ('after', 217),\n",
       " ('j.', 217),\n",
       " ('available', 216),\n",
       " ('general', 216),\n",
       " ('neighborhood', 216),\n",
       " ('conduct', 216),\n",
       " ('several', 214),\n",
       " ('entity', 214),\n",
       " ('result', 213),\n",
       " ('12', 212),\n",
       " ('he', 210),\n",
       " ('directly', 210),\n",
       " ('even', 209),\n",
       " ('networks', 209),\n",
       " ('few_shot', 208),\n",
       " ('20', 208),\n",
       " ('size', 206),\n",
       " ('effectiveness', 206),\n",
       " ('during', 206),\n",
       " ('processing', 205),\n",
       " ('q', 205),\n",
       " ('does', 204),\n",
       " ('representation_learning', 204),\n",
       " ('abstract', 203),\n",
       " ('2024', 203),\n",
       " ('many', 203),\n",
       " ('strategy', 202),\n",
       " ('type', 201),\n",
       " ('approaches', 200),\n",
       " ('relations', 200),\n",
       " ('a.', 200),\n",
       " ('experimental', 198),\n",
       " ('inference', 198),\n",
       " ('chemistry', 198),\n",
       " ('path', 197),\n",
       " ('pretrained', 197),\n",
       " ('science', 196),\n",
       " ('yang', 196),\n",
       " ('average', 195),\n",
       " ('samples', 195),\n",
       " ('l', 193),\n",
       " ('outperforms', 192),\n",
       " ('comparison', 192),\n",
       " ('loss', 192),\n",
       " ('focus', 191),\n",
       " ('settings', 190),\n",
       " ('see', 190),\n",
       " ('human', 189),\n",
       " ('introduce', 189),\n",
       " ('among', 188),\n",
       " ('predictions', 188),\n",
       " ('/', 187),\n",
       " ('shows', 187),\n",
       " ('documents', 187),\n",
       " ('x', 186),\n",
       " ('11', 186),\n",
       " ('drug', 186),\n",
       " ('kgs', 185),\n",
       " ('graph_neural_networks', 184),\n",
       " ('utilize', 184),\n",
       " ('transformers', 184),\n",
       " ('j', 182),\n",
       " ('wu', 182),\n",
       " ('need', 182),\n",
       " ('domains', 181),\n",
       " ('category', 181),\n",
       " ('instruction', 180),\n",
       " ('gpt_4', 179),\n",
       " ('module', 179),\n",
       " ('instructions', 178),\n",
       " ('novel', 178),\n",
       " ('effective', 178),\n",
       " ('reaction', 178),\n",
       " ('significant', 177),\n",
       " ('include', 177),\n",
       " ('space', 177),\n",
       " ('modalities', 177),\n",
       " ('state_of_the_art', 176),\n",
       " ('additional', 176),\n",
       " ('tog', 176),\n",
       " ('topic', 176),\n",
       " ('present', 175),\n",
       " ('significantly', 175),\n",
       " ('therefore', 175),\n",
       " ('knowledge_graph', 175),\n",
       " ('provided', 174),\n",
       " ('capabilities', 173),\n",
       " ('graph_toolformer', 173),\n",
       " ('ogbn_arxiv', 173),\n",
       " ('biomedical', 173),\n",
       " ('y', 172),\n",
       " ('follows', 172),\n",
       " ('here', 172),\n",
       " ('cora', 172),\n",
       " ('because', 170),\n",
       " ('obtain', 170),\n",
       " ('contrastive_learning', 170),\n",
       " ('benchmark', 169),\n",
       " ('what', 169),\n",
       " ('under', 169),\n",
       " ('effectively', 169),\n",
       " ('associated', 168),\n",
       " ('international_conference', 167),\n",
       " ('find', 166),\n",
       " ('structural', 166),\n",
       " ('understand', 166),\n",
       " ('questions', 164),\n",
       " ('role', 164),\n",
       " ('title', 164),\n",
       " ('2016', 164),\n",
       " ('above', 163),\n",
       " ('m', 163),\n",
       " ('leverage', 163),\n",
       " ('relevant', 163),\n",
       " ('generative', 163),\n",
       " ('represent', 163),\n",
       " ('downstream', 163),\n",
       " ('product', 163),\n",
       " ('galactica', 163),\n",
       " ('diverse', 162),\n",
       " ('very', 162),\n",
       " ('gl', 162),\n",
       " ('pubmed', 162),\n",
       " ('so', 160),\n",
       " ('make', 160),\n",
       " ('stage', 160),\n",
       " ('small', 159),\n",
       " ('gr', 159),\n",
       " ('15', 158),\n",
       " ('k', 158),\n",
       " ('mlp', 158),\n",
       " ('node_features', 158),\n",
       " ('overall', 157),\n",
       " ('name', 157),\n",
       " ('large_scale', 157),\n",
       " ('whether', 156),\n",
       " ('note', 156),\n",
       " ('0.00', 156),\n",
       " ('molt5', 156),\n",
       " ('consider', 155),\n",
       " ('atom', 155),\n",
       " ('instance', 154),\n",
       " ('still', 154),\n",
       " ('were', 154),\n",
       " ('scores', 154),\n",
       " ('problems', 153),\n",
       " ('way', 153),\n",
       " ('yu', 153),\n",
       " ('base', 152),\n",
       " ('second', 151),\n",
       " ('achieves', 151),\n",
       " ('improvement', 151),\n",
       " ('30', 151),\n",
       " ('strategies', 151),\n",
       " ('joint', 151),\n",
       " ('positive', 150),\n",
       " ('proteins', 150),\n",
       " ('achieve', 148),\n",
       " ('single', 148),\n",
       " ('contains', 147),\n",
       " ('correct', 147),\n",
       " ('detailed', 147),\n",
       " ('adopt', 146),\n",
       " ('should', 146),\n",
       " ('details', 146),\n",
       " ('comprehensive', 145),\n",
       " ('wei', 145),\n",
       " ('metrics', 145),\n",
       " ('sample', 145),\n",
       " ('compare', 145),\n",
       " ('studies', 145),\n",
       " ('enhance', 144),\n",
       " ('14', 144),\n",
       " ('xu', 144),\n",
       " ('sun', 143),\n",
       " ('four', 143),\n",
       " ('computer', 143),\n",
       " ('case', 143),\n",
       " ('key', 143),\n",
       " ('association_for_computational_linguistics', 143),\n",
       " ('f', 143),\n",
       " ('vector', 143),\n",
       " ('provides', 142),\n",
       " ('high', 141),\n",
       " ('calls', 141),\n",
       " ('instead', 140),\n",
       " ('encoders', 140),\n",
       " ('simple', 139),\n",
       " ('queries', 139),\n",
       " ('center', 139),\n",
       " ('distribution', 139),\n",
       " ('employ', 138),\n",
       " ('found', 138),\n",
       " ('functions', 138),\n",
       " ('answers', 136),\n",
       " ('generating', 136),\n",
       " ('deep', 136),\n",
       " ('select', 136),\n",
       " ('step', 135),\n",
       " ('format', 135),\n",
       " ('powerful', 135),\n",
       " ('usa', 135),\n",
       " ('sequences', 135),\n",
       " ('ablation', 135),\n",
       " ('future', 134),\n",
       " ('furthermore', 134),\n",
       " ('ai', 134),\n",
       " ('`', 134),\n",
       " ('validation', 134),\n",
       " (\"'\", 132),\n",
       " ('hu', 132),\n",
       " ('smiles_strings', 132),\n",
       " ('much', 131),\n",
       " ('16', 131),\n",
       " ('13', 131),\n",
       " ('structural_information', 131),\n",
       " ('form', 131),\n",
       " ('applied', 131),\n",
       " ('c.', 131),\n",
       " ('paradigm', 131),\n",
       " ('matching', 130),\n",
       " ('metric', 130),\n",
       " ('improves', 130),\n",
       " ('observe', 130),\n",
       " ('main', 130),\n",
       " ('negative', 130),\n",
       " ('arxiv', 130),\n",
       " ('neural_network', 130),\n",
       " ('interaction', 130),\n",
       " ('captioning', 130),\n",
       " ('together', 129),\n",
       " ('works', 129),\n",
       " ('length', 129),\n",
       " ('full', 129),\n",
       " ('21', 129),\n",
       " ('limited', 128),\n",
       " ('nlp', 127),\n",
       " ('systems', 127),\n",
       " ('acm', 127),\n",
       " ('ground_truth', 127),\n",
       " ('specter', 127),\n",
       " ('question_answering', 126),\n",
       " ('selected', 126),\n",
       " ('uses', 126),\n",
       " ('community', 126),\n",
       " ('finetuning', 126),\n",
       " ('molecular_graph', 126),\n",
       " ('designed', 125),\n",
       " ('open', 125),\n",
       " ('those', 125),\n",
       " ('experiment', 125),\n",
       " ('jointly', 125),\n",
       " ('end', 124),\n",
       " ('higher', 124),\n",
       " ('take', 124),\n",
       " ('additionally', 124),\n",
       " ('s.', 124),\n",
       " ('supervised', 124),\n",
       " ('order', 123),\n",
       " ('previous', 123),\n",
       " ('class', 123),\n",
       " ('labeled', 123),\n",
       " ('application', 122),\n",
       " ('relationships', 122),\n",
       " ('products', 122),\n",
       " ('cross_modal', 122),\n",
       " ('standard', 121),\n",
       " ('denotes', 121),\n",
       " ('common', 121),\n",
       " ('categories', 121),\n",
       " ('selection', 121),\n",
       " ('call', 121),\n",
       " ('techniques', 120),\n",
       " ('would', 120),\n",
       " ('multimodal', 120),\n",
       " ('according', 119),\n",
       " ('able', 119),\n",
       " ('semantics', 119),\n",
       " ('another', 119),\n",
       " ('less', 119),\n",
       " ('important', 119),\n",
       " ('predicting', 119),\n",
       " ('m.', 119),\n",
       " ('amazon', 119),\n",
       " ('$', 119),\n",
       " ('extensive', 118),\n",
       " ('32', 118),\n",
       " ('values', 118),\n",
       " ('neural_networks', 118),\n",
       " ('often', 118),\n",
       " ('enables', 118),\n",
       " ('either', 118),\n",
       " ('neural', 118),\n",
       " ('annotations', 118),\n",
       " ('sentence', 117),\n",
       " ('out', 117),\n",
       " ('|', 117),\n",
       " ('capability', 116),\n",
       " ('citation', 116),\n",
       " ('report', 116),\n",
       " ('local', 115),\n",
       " ('100', 115),\n",
       " ('cot', 115),\n",
       " ('aim', 115),\n",
       " ('introduced', 115),\n",
       " ('academic', 115),\n",
       " ('outputs', 115),\n",
       " ('current', 115),\n",
       " ('learned', 115),\n",
       " ('quality', 115),\n",
       " ('w', 115),\n",
       " ('url_https', 115),\n",
       " ('multi_modal', 115),\n",
       " ('raw_text', 115),\n",
       " ('=o', 115),\n",
       " ('up', 114),\n",
       " ('indicates', 114),\n",
       " ('you', 114),\n",
       " ('w/o', 114),\n",
       " ('molecular_property_prediction', 114),\n",
       " ('recent', 113),\n",
       " ('users', 113),\n",
       " ('aggregation', 113),\n",
       " ('help', 113),\n",
       " ('roberta', 113),\n",
       " ('tag', 113),\n",
       " ('50', 112),\n",
       " ('zhao', 112),\n",
       " ('recommendation', 112),\n",
       " ('attributes', 112),\n",
       " ('devlin', 112),\n",
       " ('fine_tuned', 112),\n",
       " ('heterogeneous', 112),\n",
       " ('steps', 111),\n",
       " ('obtained', 111),\n",
       " ('encode', 111),\n",
       " ('parameter', 111),\n",
       " ('atoms', 111),\n",
       " ('huang', 110),\n",
       " ('randomly', 110),\n",
       " ('limitations', 110),\n",
       " ('finally', 110),\n",
       " ('predicted', 110),\n",
       " ('text_encoder', 110),\n",
       " ('accurate', 109),\n",
       " ('top', 109),\n",
       " ('explore', 109),\n",
       " ('contrast', 109),\n",
       " ('initial', 109),\n",
       " ('fig', 109),\n",
       " ('galm', 109),\n",
       " ('next', 108),\n",
       " ('classes', 108),\n",
       " ('dynamic', 108),\n",
       " ('list', 107),\n",
       " ('17', 107),\n",
       " ('inputs', 107),\n",
       " ('sampling', 107),\n",
       " ('zhu', 107),\n",
       " ('advances_in_neural_information_processing_systems', 107),\n",
       " ('ours', 107),\n",
       " ('moreover', 107),\n",
       " ('code', 106),\n",
       " ('zhou', 106),\n",
       " ('larger', 106),\n",
       " ('structured_data', 106),\n",
       " ('twitter', 106),\n",
       " ('field', 106),\n",
       " ('t5', 106),\n",
       " ('regression', 106),\n",
       " ('large_language_model', 105),\n",
       " ('addition', 105),\n",
       " ('strong', 105),\n",
       " ('impact', 105),\n",
       " ('allows', 105),\n",
       " ('apply', 105),\n",
       " ('acc', 105),\n",
       " ('augmentation', 105),\n",
       " ('23', 105),\n",
       " ('dragon', 105),\n",
       " ('leveraging', 104),\n",
       " ('online', 104),\n",
       " ('self_supervised', 104),\n",
       " ('total', 103),\n",
       " ('improving', 103),\n",
       " ('gpt_models', 103),\n",
       " ('besides', 103),\n",
       " ('vectors', 103),\n",
       " ('raw', 103),\n",
       " ('momu', 103),\n",
       " ('range', 102),\n",
       " ('lee', 102),\n",
       " ('real_world', 102),\n",
       " ('scientific', 102),\n",
       " ('before', 101),\n",
       " ('requires', 101),\n",
       " ('22', 101),\n",
       " ('24', 101),\n",
       " ('address', 101),\n",
       " ('require', 101),\n",
       " ('tags', 101),\n",
       " ('modality', 101),\n",
       " ('contrastive', 101),\n",
       " ('graphformers', 101),\n",
       " ('ogbn_products', 101),\n",
       " ('algorithm', 100),\n",
       " ('notably', 100),\n",
       " ('represents', 100),\n",
       " ('extract', 100),\n",
       " ('aims', 100),\n",
       " ('jiang', 100),\n",
       " ('2015', 100),\n",
       " ('retrieve', 100),\n",
       " ('us', 100),\n",
       " ('classication', 100),\n",
       " ('rt', 100),\n",
       " ('possible', 99),\n",
       " ('d.', 99),\n",
       " ('long', 99),\n",
       " ('terms', 99),\n",
       " ('19', 99),\n",
       " ('pipeline', 99),\n",
       " ('memory', 99),\n",
       " ('database', 99),\n",
       " ('cost', 99),\n",
       " ('z', 99),\n",
       " ('giant', 99),\n",
       " ('abilities', 98),\n",
       " ('valid', 98),\n",
       " ('construct', 98),\n",
       " ('challenges', 98),\n",
       " ('gcn', 98),\n",
       " ('links', 98),\n",
       " ('split', 98),\n",
       " ('lower', 97),\n",
       " ('instances', 97),\n",
       " ('18', 97),\n",
       " ('incorporate', 97),\n",
       " ('sets', 97),\n",
       " ('social_network', 97),\n",
       " ('denoted', 97),\n",
       " ('names', 97),\n",
       " ('text_classification', 97),\n",
       " ('scientic', 97),\n",
       " ('caption', 97),\n",
       " ('likely', 96),\n",
       " ('rich', 96),\n",
       " ('scibert', 96),\n",
       " ('chemformer', 96),\n",
       " ('solve', 95),\n",
       " ('cases', 95),\n",
       " ('probability', 95),\n",
       " ('explanations', 95),\n",
       " ('generates', 95),\n",
       " ('optimization', 95),\n",
       " ('mean', 95),\n",
       " ('per', 95),\n",
       " ('micol', 95),\n",
       " ('crucial', 94),\n",
       " ('level', 94),\n",
       " ('investigate', 94),\n",
       " ('includes', 94),\n",
       " ('although', 94),\n",
       " ('performs', 94),\n",
       " ('demonstrates', 94),\n",
       " ('instruction_tuning', 94),\n",
       " ('grad', 94),\n",
       " ('40', 93),\n",
       " ('few', 93),\n",
       " ('goal', 93),\n",
       " ('challenging', 93),\n",
       " ('backbone', 93),\n",
       " ('plms', 93),\n",
       " ('e2eg', 93),\n",
       " ('recently', 92),\n",
       " ('scenarios', 92),\n",
       " ('challenge', 92),\n",
       " ('smaller', 92),\n",
       " ('l.', 92),\n",
       " ('training_data', 92),\n",
       " ('training_set', 92),\n",
       " ('rog', 92),\n",
       " ('hence', 92),\n",
       " ('in_context_learning', 91),\n",
       " ('relevance', 91),\n",
       " ('especially', 91),\n",
       " ('augmented', 91),\n",
       " ('dynamic_graph', 91),\n",
       " ('content', 91),\n",
       " ('social', 91),\n",
       " ('k.', 91),\n",
       " ('being', 90),\n",
       " ('statistics', 90),\n",
       " ('presented', 90),\n",
       " ('benchmarks', 90),\n",
       " ('scale', 90),\n",
       " ('reported', 90),\n",
       " ('h.', 90),\n",
       " ('learning_rate', 90),\n",
       " ('cls', 90),\n",
       " ('enhancing', 89),\n",
       " ('reference', 89),\n",
       " ('improved', 89),\n",
       " ('deep_learning', 89),\n",
       " ('raph', 89),\n",
       " ('matrix', 89),\n",
       " ('graphsage', 89),\n",
       " ('unlabeled', 89),\n",
       " ('denote', 88),\n",
       " ('topology', 88),\n",
       " ('fundamental', 88),\n",
       " ('languages', 88),\n",
       " ('consists', 88),\n",
       " ('25', 88),\n",
       " ('image', 88),\n",
       " ('modules', 88),\n",
       " ('translation', 88),\n",
       " ('0.01', 88),\n",
       " ('tweet', 88),\n",
       " ('remains', 87),\n",
       " ('complexity', 87),\n",
       " ('introduction', 87),\n",
       " ('hard', 87),\n",
       " ('distinct', 87),\n",
       " ('maximum', 87),\n",
       " ('indicate', 87),\n",
       " ('value', 87),\n",
       " ('components', 87),\n",
       " ('confidence', 87),\n",
       " ('lead', 87),\n",
       " ('utilizing', 87),\n",
       " ('continuous', 87),\n",
       " ('fine_tune', 87),\n",
       " ('required', 86),\n",
       " ('capacity', 86),\n",
       " ('pair', 86),\n",
       " ('part', 86),\n",
       " ('prior', 86),\n",
       " ('despite', 86),\n",
       " ('considering', 86),\n",
       " ('incorporating', 86),\n",
       " ('text_attributed_graphs', 86),\n",
       " ('volume', 86),\n",
       " ('retrieved', 86),\n",
       " ('g.', 86),\n",
       " ('glem', 86),\n",
       " ('kv_plm', 86),\n",
       " ('max', 85),\n",
       " ('weights', 85),\n",
       " ('source', 85),\n",
       " ('demonstrated', 85),\n",
       " ('towards', 85),\n",
       " ('direct', 85),\n",
       " ('get', 85),\n",
       " ('vanilla', 85),\n",
       " ('described', 85),\n",
       " ('usually', 85),\n",
       " ('r.', 85),\n",
       " ('rst', 85),\n",
       " ('hamilton', 84),\n",
       " ('along', 84),\n",
       " ('making', 84),\n",
       " ('development', 84),\n",
       " ('t.', 84),\n",
       " ('2014', 84),\n",
       " ('node_representations', 84),\n",
       " ('candidate', 84),\n",
       " ('resulting', 83),\n",
       " ('compute', 83),\n",
       " ('text_attributes', 83),\n",
       " ('hyperparameters', 83),\n",
       " ('plm', 83),\n",
       " ('captions', 83),\n",
       " ('structured', 82),\n",
       " ('topological', 82),\n",
       " ('4.2', 82),\n",
       " ('represented', 82),\n",
       " ('importance', 82),\n",
       " ('efficient', 82),\n",
       " ('interactions', 82),\n",
       " ('improvements', 82),\n",
       " ('inductive', 82),\n",
       " ('images', 82),\n",
       " ('decoder', 82),\n",
       " ('0.03', 82),\n",
       " ('objectives', 81),\n",
       " ('widely', 81),\n",
       " ('node_embeddings', 81),\n",
       " ('particularly', 81),\n",
       " ('identify', 81),\n",
       " ('3.2', 81),\n",
       " ('___', 81),\n",
       " ('link', 81),\n",
       " ('icl', 81),\n",
       " ('hand', 80),\n",
       " ('might', 80),\n",
       " ('increase', 80),\n",
       " ('yield', 80),\n",
       " ('ieee', 80),\n",
       " ('follow', 80),\n",
       " ('2023a', 80),\n",
       " ('fully', 80),\n",
       " ('b.', 80),\n",
       " ('certain', 79),\n",
       " ('effect', 79),\n",
       " ('achieved', 79),\n",
       " ('final', 79),\n",
       " ('university', 79),\n",
       " ('pages', 79),\n",
       " ('procedure', 79),\n",
       " ('text_based', 79),\n",
       " ('scaffold', 79),\n",
       " ('advanced', 78),\n",
       " ('citations', 78),\n",
       " ('batch_size', 78),\n",
       " ('graph_based', 77),\n",
       " ('combined', 77),\n",
       " ('takes', 77),\n",
       " ('evaluated', 77),\n",
       " ('position', 77),\n",
       " ('27', 77),\n",
       " ('exploration', 77),\n",
       " ('handle', 77),\n",
       " ('seed', 77),\n",
       " ('llama', 77),\n",
       " ('vision', 77),\n",
       " ('unsupervised', 77),\n",
       " ('eq', 77),\n",
       " ('n.', 77),\n",
       " ('0.05', 77),\n",
       " ('grenade', 77),\n",
       " ('moleculestm', 77),\n",
       " ('last', 76),\n",
       " ('produce', 76),\n",
       " ('patterns', 76),\n",
       " ('particular', 76),\n",
       " ('good', 76),\n",
       " ('enabling', 76),\n",
       " ('difference', 76),\n",
       " ('textual_information', 76),\n",
       " ('unified', 76),\n",
       " ('graphllm', 76),\n",
       " ('nd', 76),\n",
       " ('statement', 76),\n",
       " ('adsgnn', 76),\n",
       " ('text2mol', 76),\n",
       " ('brown', 75),\n",
       " ('version', 75),\n",
       " ('inspired', 75),\n",
       " ('group', 75),\n",
       " ('authors', 75),\n",
       " ('exhibit', 75),\n",
       " ('words', 75),\n",
       " ('35', 75),\n",
       " ('26', 75),\n",
       " ('statements', 75),\n",
       " ('lin', 75),\n",
       " ('energy', 75),\n",
       " ('journal', 75),\n",
       " ('component', 75),\n",
       " ('ppr', 75),\n",
       " ...]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cnt.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_phrases = [w for w in cnt if (w not in stopwords.words('english'))]\n",
    "phrase_reprs = np.concatenate([vocab_emb[w].reshape((-1, 768)) for w in iv_phrases], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(vocab_sent_emb.keys())\n",
    "sent_reprs = np.concatenate([vocab_sent_emb[s].reshape((-1, 768)) for s in sents], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"SeeTopic/{args.dataset}\"):\n",
    "    os.makedirs(f\"SeeTopic/{args.dataset}\")\n",
    "\n",
    "with open(f\"SeeTopic/{args.dataset}/{args.dataset}.txt\", \"w\") as f:\n",
    "    for p in taxo.external_collection:\n",
    "        f.write(f\"paper_title : {p.title} ; paper_abstract : {p.abstract}\\n\")\n",
    "    for p in taxo.collection:\n",
    "        f.write(f\"paper_title : {p.title} ; paper_abstract : {p.abstract}\\n\")\n",
    "\n",
    "children_with_terms = taxo.root.getChildren(terms=True)\n",
    "\n",
    "with open(f\"SeeTopic/{args.dataset}/keywords_0.txt\", \"w\") as f:\n",
    "    for idx, c in enumerate(children_with_terms):\n",
    "        str_c = \",\".join(c[1])\n",
    "        f.write(f\"{idx}:{c[0]},{str_c}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Get PLM Embeddings===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### CONSTRUCTING AND TOKENIZING VOCAB #######\n",
      "####### COMPUTING STATIC EMBEDDINGS #######\n",
      "####### COMPUTING RAW WORD EMBEDDINGS #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:30<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Iter 0: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../llm_graph/llm_graph.txt\n",
      "Reading topics from file llm_graph_1/keywords.txt\n",
      "Vocab size: 14396\n",
      "Words in train file: 1290798\n",
      "Read 3 topics\n",
      "graph_context\trepresentation_learning\tgraph_search\t\n",
      "augmentation_methods\talignment_methods\tgraph_neural_networks\t\n",
      "representation_enhancement\tsequence_graph\tgraph_to_sequence\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000099  Progress: 99.67%  Words/thread/sec: 13.26k  Topic mining results written to file llm_graph_1/res_cate.txt\n",
      "\u001b[32m===Iter 1: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 2: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 2: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../llm_graph/llm_graph.txt\n",
      "Reading topics from file llm_graph_2/keywords.txt\n",
      "Vocab size: 14396\n",
      "Words in train file: 1290798\n",
      "Read 3 topics\n",
      "graph_context\trepresentation_learning\tgraph_search\tgraph_reasoning\tanswer_generation\tgraph_generation\t\n",
      "augmentation_methods\talignment_methods\tgraph_neural_networks\taugmentation_techniques\ttwo_stage\tpredefined_rules\t\n",
      "representation_enhancement\tsequence_graph\tgraph_to_sequence\tgraph_to_sequence_model\ttextual_graphs\tsql_to_text\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000099  Progress: 99.67%  Words/thread/sec: 13.02k  Topic mining results written to file llm_graph_2/res_cate.txt\n",
      "\u001b[32m===Iter 2: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 3: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 3: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../llm_graph/llm_graph.txt\n",
      "Reading topics from file llm_graph_3/keywords.txt\n",
      "Vocab size: 14396\n",
      "Words in train file: 1290798\n",
      "Read 3 topics\n",
      "graph_context\trepresentation_learning\tgraph_search\tgraph_reasoning\tanswer_generation\tgraph_generation\tpath_finding\tgraph_representation\tgtrl\t\n",
      "augmentation_methods\talignment_methods\tgraph_neural_networks\taugmentation_techniques\ttwo_stage\tpredefined_rules\trepresentation_learning\tlogical_rules\tencoder_based\t\n",
      "representation_enhancement\tsequence_graph\tgraph_to_sequence\tgraph_to_sequence_model\ttextual_graphs\tsql_to_text\tword_relation\tgraph_aware\tsyntax_enhanced\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000099  Progress: 100.14%  Words/thread/sec: 13.28k  Topic mining results written to file llm_graph_3/res_cate.txt\n",
      "\u001b[32m===Iter 3: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 4: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 4: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../llm_graph/llm_graph.txt\n",
      "Reading topics from file llm_graph_4/keywords.txt\n",
      "Vocab size: 14396\n",
      "Words in train file: 1290798\n",
      "Read 3 topics\n",
      "graph_context\trepresentation_learning\tgraph_search\tgraph_reasoning\tanswer_generation\tgraph_generation\tpath_finding\tgraph_representation\tgtrl\tgraph_optimization\tqas\tgraph_based\t\n",
      "augmentation_methods\talignment_methods\tgraph_neural_networks\taugmentation_techniques\ttwo_stage\tpredefined_rules\trepresentation_learning\tlogical_rules\tencoder_based\tsocial_networks\tgraph_optimization\tgraph_reasoning\t\n",
      "representation_enhancement\tsequence_graph\tgraph_to_sequence\tgraph_to_sequence_model\ttextual_graphs\tsyntax_enhanced\tsql_to_text\tgraph_aware\tword_relation\tgtae\ttemporal_knowledge_graph_reasoning\tdependency_structures\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Alpha: 0.000073  Progress: 99.91%  Words/thread/sec: 13.19k  Topic mining results written to file llm_graph_4/res_cate.txt\n",
      "\u001b[32m===Iter 4: Ensemble===\u001b[m\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"./SeeTopic\")\n",
    "subprocess.check_call(['./seetopic.sh', args.dataset, str(args.iters), args.model])\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
