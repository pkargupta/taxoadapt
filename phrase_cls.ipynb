{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\"\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e71a01d7634085befd927923842e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import Counter\n",
    "from taxonomy import Taxonomy, Paper\n",
    "from utils import filter_phrases, cosine_similarity_embeddings, average_with_harmonic_series, rank_by_significance, rank_by_discriminative_significance, rank_by_relation\n",
    "from model_definitions import sentence_model\n",
    "import subprocess\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle as pk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.track = \"Text Classification\"\n",
    "        self.dim = \"Methodology\"\n",
    "        self.input_file = \"datasets/sample_1k.txt\"\n",
    "        self.iters = 4\n",
    "        self.model = \"bert_full_ft\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Taxonomy Construction & Reading in Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Types of Methodology Proposed in Text Classification Research Papers': {'description': None, 'children': {'supervised_learning': {'description': 'Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.', 'children': {}}, 'unsupervised_learning': {'description': 'Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.', 'children': {}}, 'semi_supervised_learning': {'description': 'Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.', 'children': {}}, 'deep_learning': {'description': 'Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.', 'children': {}}, 'ensemble_methods': {'description': 'Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.', 'children': {}}}}}\n"
     ]
    }
   ],
   "source": [
    "# input: track, dimension -> get base taxonomy (2 levels) -> Class Tree, Class Node (description, seed words)\n",
    "\n",
    "taxo = Taxonomy(args.track, args.dim, args.input_file)\n",
    "base_taxo = taxo.buildBaseTaxo(levels=1, num_terms=20)\n",
    "\n",
    "print(base_taxo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the input keywords file for seetopic -> get phrases -> filter using LLM\n",
    "dir_name = (args.track + \"_\" + args.dim).lower().replace(\" \", \"_\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}\"):\n",
    "    os.makedirs(f\"SeeTopic/{dir_name}\")\n",
    "\n",
    "with open(f\"SeeTopic/{dir_name}/{dir_name}.txt\", \"w\") as f:\n",
    "    for p in taxo.collection:\n",
    "        f.write(f\"{p.text}\\n\")\n",
    "\n",
    "\n",
    "## get first level of children\n",
    "children_with_terms = taxo.root.getChildren(terms=True)\n",
    "with open(f\"SeeTopic/{dir_name}/keywords_0.txt\", \"w\") as f:\n",
    "    for idx, c in enumerate(children_with_terms):\n",
    "        str_c = \",\".join(c[1])\n",
    "        f.write(f\"{idx}:{c[0]},{str_c}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phrase Mining for Level 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Get PLM Embeddings===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### CONSTRUCTING AND TOKENIZING VOCAB #######\n",
      "####### COMPUTING STATIC EMBEDDINGS #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5349/5349 [00:57<00:00, 93.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Iter 0: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_1/keywords.txt\n",
      "Vocab size: 5313\n",
      "Words in train file: 186360\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\t\n",
      "non_negative_matrix_factorization\tlatent_semantic_analysis\ttopic_modeling\t\n",
      "self_training\tco_training\ttransfer_learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tlong_short_term_memory\t\n",
      "bagging\tstacking\tvoting\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_1/res_cate.txt\n",
      "\u001b[32m===Iter 1: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 2: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 2: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_2/keywords.txt\n",
      "Vocab size: 5313\n",
      "Words in train file: 186360\n",
      "Read 5 topics\n",
      "supervised_learning\tnaive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\t\n",
      "unsupervised_learning\tnon_negative_matrix_factorization\tlatent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tclustering_algorithm\t\n",
      "semi_supervised_learning\tself_training\tco_training\ttransfer_learning\tmulti_task_learning\tfew_shot_learning\t\n",
      "deep_learning\tconvolutional_neural_networks\trecurrent_neural_networks\tlong_short_term_memory\tattention_mechanism\ttransformers\t\n",
      "bagging\tstacking\tvoting\trandom_forest\tgradient_boosting\tcross_validation\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_2/res_cate.txt\n",
      "\u001b[32m===Iter 2: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 3: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 3: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_3/keywords.txt\n",
      "Vocab size: 5313\n",
      "Words in train file: 186360\n",
      "Read 5 topics\n",
      "supervised_learning\tnaive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tgradient_boosting\tneural_networks\t\n",
      "unsupervised_learning\tnon_negative_matrix_factorization\tlatent_semantic_analysis\ttopic_modeling\tclustering_algorithm\tclassic_feature_selection\tfaceted_classification\tterm_weighting\tsimilarity_measure\t\n",
      "semi_supervised_learning\tself_training\tco_training\ttransfer_learning\tmulti_task_learning\tfew_shot_learning\tactive_learning\tcurriculum_learning\tmeta_learning\t\n",
      "deep_learning\tconvolutional_neural_networks\trecurrent_neural_networks\tlong_short_term_memory\tattention_mechanism\ttransformers\tword_embeddings\tlanguage_models\tpre_trained_word_embeddings\t\n",
      "bagging\tstacking\tvoting\trandom_forest\tgradient_boosting\tcross_validation\tbase_learners\trandom_forests\tensemble_learning\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_3/res_cate.txt\n",
      "\u001b[32m===Iter 3: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 4: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 4: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_4/keywords.txt\n",
      "Vocab size: 5313\n",
      "Words in train file: 186360\n",
      "Read 5 topics\n",
      "supervised_learning\tnaive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tgradient_boosting\tneural_networks\tfeature_selection\tfeature_engineering\tdata_augmentation\t\n",
      "unsupervised_learning\tnon_negative_matrix_factorization\tlatent_semantic_analysis\ttopic_modeling\tclustering_algorithm\tsimilarity_measure\tfaceted_classification\tclassic_feature_selection\tterm_weighting\tstring_vectors\tnumerical_vectors\tfeature_transformation\t\n",
      "semi_supervised_learning\tself_training\tco_training\ttransfer_learning\tmulti_task_learning\tfew_shot_learning\tactive_learning\tcurriculum_learning\tmeta_learning\tmeta-learning\tprompt_tuning\ttraining_examples\t\n",
      "deep_learning\tconvolutional_neural_networks\trecurrent_neural_networks\tlong_short_term_memory\tattention_mechanism\ttransformers\tword_embeddings\tlanguage_models\tpre_trained_word_embeddings\tfine_tuning\ttransfer_learning\tmulti_task_learning\t\n",
      "bagging\tstacking\tvoting\trandom_forest\tgradient_boosting\tcross_validation\tbase_learners\trandom_forests\tensemble_learning\tensemble_models\tcluster_based\tensemble_techniques\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_4/res_cate.txt\n",
      "\u001b[32m===Iter 4: Ensemble===\u001b[m\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"./SeeTopic\")\n",
    "subprocess.check_call(['./seetopic.sh', dir_name, str(args.iters), args.model])\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2emb = {}\n",
    "with open(f'./SeeTopic/{dir_name}/embedding_{args.model}.txt') as fin:\n",
    "\tfor line in fin:\n",
    "\t\tdata = line.strip().split()\n",
    "\t\tif len(data) != 769:\n",
    "\t\t\tcontinue\n",
    "\t\tword = data[0]\n",
    "\t\temb = np.array([float(x) for x in data[1:]])\n",
    "\t\temb = emb / np.linalg.norm(emb)\n",
    "\t\tword2emb[word] = emb\n",
    "\n",
    "taxo.word2emb = word2emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(f'SeeTopic/{dir_name}', 'static_emb.pk')):\n",
    "\twith open(os.path.join(f'SeeTopic/{dir_name}', 'static_emb.pk'), \"rb\") as f:\n",
    "\t\tsaved_emb = pk.load(f)\n",
    "\t\tstatic_emb = saved_emb[\"static_emb\"]\n",
    "\t\ttoken_lens = saved_emb[\"token_lens\"]\n",
    "\t\ttokenized_sents = saved_emb[\"tokenized_sents\"]\n",
    "\t\ttokenized_docs = saved_emb[\"tokenized_docs\"]\n",
    "\n",
    "\tfor p_id, paper in enumerate(taxo.collection):\n",
    "\t\tpaper.sentences = tokenized_docs[p_id]\n",
    "\t\tpaper.tokenized = tokenized_sents[p_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[supervised_learning,\n",
       " unsupervised_learning,\n",
       " semi_supervised_learning,\n",
       " deep_learning,\n",
       " ensemble_methods]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.static_emb = static_emb\n",
    "taxo.root.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "supervised_learning_filtering_explanation: 'unsupervised_learning','semi_supervised_learning', 'deep_learning', and 'ensemble_methods' were filtered because they are not subtopics of'supervised_learning', but rather sibling topics or parent topics of'supervised_learning'.\n",
      "supervised_learning_filtered: ['naive_bayes', 'decision_trees', 'random_forest','support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'bagging','stacking', 'voting','sentence_classification', 'feature_space', 'capsule_network', 'projection_method','stance_classification', 'document-level', 'dimensionality_reduction', 'existing_works', 'tc', 'clustering', 'dual-channel', 'feature_representations', 'epat-bert', 'pseudo','mlm', 'arises','statistical_methods','multi-grained','snad', 'attention_network', 'prior', 'aspect-level_sentiment_classification', 'extending', 'fine-grained', 'feature_mapping', 'domain_adaptive', 'associations', 'climate_change_denial', 'classification_algorithms', 'establishing', 'connectives', 'parameterization', 'higher_education', 'department', 'rnode', 'variational', 'feature_weights', 'higher-order', 'difficulty', 'commentary', 'digitalization', 'numerical_features', 'few-shot','maximum_entropy', 'data_mining', '3a', 'feature_weight','specialized', 'feature_vector', 'external','shot', 'optimization_problem', 'dqd', 'bivalency', 'error_propagation', 'catastrophic_forgetting', 'isomer', 'web_content', 'feature_values', 'alleviate', 'chunks', 'user_experience', 'taxonomies', 'incrementally','supervise', 'generalization', 'genre', 'tackling', 'tried','sufficiently', 'elmo', 'top-k','regularization','self', 'algerian', 'lda-based', 'expert_knowledge', 'feature_reduction', 'albert','morphology', 'centered', 'great_progress', 'exclusive','seek', 'administrative','mixup', 'genetic_algorithm', 'egfi', 'experiments_prove']\n",
      "---\n",
      "---\n",
      "unsupervised_learning_filtering_explanation: 'unsupervised_learning', 'unsupervised_learning', 'bee', 'bat', 'accidentology' were filtered because they are irrelevant to 'unsupervised_learning' or can be subtopics of other parent topics.\n",
      "unsupervised_learning_filtered: ['non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling','similarity_measure', 'clustering_algorithm','string_vectors', 'faceted_classification', 'numerical_vectors', 'classic_feature_selection', 'feature_transformation', 'term_weighting', 'dependency_graph','string_vector', 'gibbs_sampling','supervised_topic_model', 'orthogonality','self_organizing_map','sentence_representation', 'case-based_reasoning', 'feature_representation', 'key_concepts','vectorization', 'hownet', 'tokenization', 'paragraphs', 'auto-tagging', 'pattern_matching', '1d-som', 'automatically_constructed','string_matching', 'nonparametric', 'term_distribution', 'transformed', 'cbr', 'rough_set','supervised_term_weighting','smoothing', 'text_representation','maintained', 'feature_vectors', 'calculating', 'feature_extraction', 'word_vectors', 'word_vector','sparseness', 'automatically_classifies','sentence_similarity', 'cross-modal', 'weight_matrix', 'neighboring', 'text_mining', 'local_binary_pattern', 'co-occurrence', 'generalization_ability','simulated_annealing', 'characterizes', 'text_classification', 'term_frequency_-_inverse_document_frequency','multi-layered','readability_assessment','sequential_minimization', 'essence', 'binary_classification', 'preserved', 'ordinal','re', 'enrich','microblog', 'pathways','readability_assessment']\n",
      "---\n",
      "---\n",
      "semi_supervised_learning_filtering_explanation:'semi_supervised_learning','supervised_learning', 'out-of-distribution','shorter', 'deep_learning','reaching','representation_learning', 'jatit_&_lls', 'contrastive_learning', 'desired', 'interactively', 'rule_induction','reaches', 'irm', 'classifier_ensemble', 'rouge', 'transferability', '2009', 'threshold', 'bns', 'journal', 'rights_reserved', 'training_samples', '17', 'weakly_supervised', 'error_reduction', 'considerably', 'budget', 'query_strategies', '93.3','substantially', 'adaptively','similarity-based', 'intent_detection','relevant_articles', 'joint_modeling', 'batch', 'inc', 'textual_representation','simulated', '0.80','sub-optimal','stance_detection', 'uncertainty-based', 'prepare', 'differentiated', 'contact', 'labeling_efforts', 'nl-lda', '0.4', 'pseudo-labeling','related_works', 'ltd.', 'crossover', 'justified', 'learner', 'perceptron', 'cwc', 'trigger_detection', 'ordering','relation_classification', '81', 'rule_based', 'optimizes', 'textfooler', 'assigns', '98', 'functionality', 'otherwise', 'instructing', 'coding','micro_f1', 'voice_recognition', '2006', 'initialization', 'tedious','restricting','metamorphic', 'prompts', 'clustered', 'low_dimensional', 'expanded', 'objective_function', 'dynamically', 'clusters','margin' were filtered because they are not relevant to semi-supervised learning or can be subtopics of other parent topics.\n",
      "semi_supervised_learning_filtered: ['self_training', 'co_training', 'transfer_learning','multi_task_learning', 'few_shot_learning', 'active_learning', 'curriculum_learning','meta_learning', 'training_examples', 'prompt_tuning','meta-learning']\n",
      "---\n",
      "---\n",
      "deep_learning_filtering_explanation:'supervised_learning', 'unsupervised_learning','semi_supervised_learning', 'ensemble_methods', and'machine_learning' were filtered because they are not specific to deep learning, and 'artificial_neural_network', 'neural_network','state-of-art','sota', 'generative_model', 'gcn', 'deep_neural_network', 'pretrained_models', 'deepmoji', 'hisans','statistical_machine_translation', 'deep_neural_networks', 'topfuzz4sa', 'bidirectional_encoder_representations_from_transformers', 'extensive_evaluation', 'low-cost', 'generative_models','sequence-based', 'deep_belief_network', 'low-resource_languages', 'joint_training', 'bi-directional_long_short-term_memory', 'gru', 'artificial_neural_networks', 'projecting', 'document_representation', 'distilbert','semantic_role_labeling', 'leopard', 'german_language','vector_space_model', 'offensive_language', 'topic_modelling', 'ulmfit', 'latent_variable','recurrent', 'losses','resource-poor','mbert', 'transformer-based', 'bigru','recurrent_neural_network','model-agnostic', 'bert', 'lightweight','seq2seq', 'primary_focus', 'topic_models', 'blstm-resnet', 'vanilla', 'topic_model', 'cross-layer', 'pre-trained_language_models', 'primarily','multi-classifier', 'deck', 'bae', 'parsing', 'target-dependent','max_pooling', 'intermediate', 'decoding','machine_reading_comprehension', 'clstm', 'literature-mining', 'plms', 'downstream_task','marbert', 'generative', 'pretraining', 'context-free', 'convolutional' were filtered because they are not specific to deep learning.\n",
      "deep_learning_filtered: ['convolutional_neural_networks','recurrent_neural_networks', 'long_short_term_memory', 'attention_mechanism', 'transformers', 'word_embeddings', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning','multi_task_learning','masked_language_model', 'word_representations', 'gated_recurrent_unit','mlcnn', 'xlnet','stacked', 'bidirectional_long_short-term_memory', 'context-aware','speech-to-intent_classification', 'dmix', 'architectures', 'pre-trained_transformer', 'neural_network', 'contextualized','sota', 'word_representation', 'generative_model', 'deepmoji', 'hisans','statistical_machine_translation', 'deep_neural_networks', 'topfuzz4sa', 'bidirectional_encoder_representations_from_transformers', 'extensive_evaluation', 'low-cost', 'generative_models','sequence-based', 'deep_belief_network', 'low-resource_languages', 'joint_training', 'bi-directional_long_short-term_memory', 'gru', 'artificial_neural_networks', 'projecting', 'document_representation', 'distilbert','semantic_role_labeling', 'leopard', 'german_language','vector_space_model', 'offensive_language', 'topic_modelling', 'ulmfit', 'latent_variable','recurrent', 'losses','resource-poor','mbert', 'transformer-based', 'bigru','recurrent_neural_network','model-agnostic', 'bert', 'lightweight','seq2seq', 'primary_focus', 'topic_models', 'blstm-resnet', 'vanilla', 'topic_model', 'cross-layer', 'pre-trained_language_models', 'primarily','multi-classifier', 'deck', 'bae', 'parsing', 'target-dependent','max_pooling', 'intermediate', 'decoding','machine_reading_comprehension', 'clstm', 'literature-mining', 'plms', 'downstream_task','marbert', 'generative', 'pretraining', 'context-free', 'convolutional']\n",
      "---\n",
      "ensemble_methods_filtering_explanation: 'ensemble_methods', 'ensemble_learning', 'ensemble_models', 'ensemble_techniques', 'ensemble_method','macro-averaged', 'well-established', 'best-performing', 'perturbation','manuscript' were filtered because they are either irrelevant to 'ensemble_methods' or can be subtopics of other parent topics such as'supervised_learning', 'unsupervised_learning','semi_supervised_learning', or 'deep_learning'.\n",
      "ensemble_methods_filtered: ['bagging','stacking', 'voting', 'random_forest', 'gradient_boosting', 'cross_validation', 'base_learners', 'random_forests', 'cluster_based', 'attention_layer', 'base_classifiers', 'k-nearest_neighbour', 'dt', 'radial_basis_function', 'c4.5', 'distance_measure', 'complement', 'labeling_cost', 'capsule_networks', 'rbf', 'handcrafted_feature', 'joint_representation', 'adaboost','substantial_improvement', 'linear_kernel','multilayer_perceptron', 'firebert', 'term_weighting_schemes', 'fuzzy_rule', 'c1', 'feature_combination','mnb', 'bernoulli','mlp-based', 'd1', 'partitioning','resource-constrained', 'pooling', 'c5.0', 'channeling','mrmr','me', 'unigram_and_bigram', 'ensembles', 'adaptive_neuro-fuzzy', 'zoning', 'optimally', 'unbalanced_data', 'non-symmetrical', 'c4.5_algorithm','smote', 'essa-acnn','squares', 'k10', 'intent_classification','symbolic','response_generation', 'cc', 'doc2vec', 'vote', 'collected_tweets','scgru', 'thresholding','multinomial_naive_bayes','morphological_analysis', 'cso-lstmnn', 'hidden_layer', 'gss','memory-based', 'chi', 'bottleneck','multinomial_logistic_regression', 'gaussian_naive_bayes', '±', 'polarity_detection', 'decreasing','mfom', 'nb', 'em_algorithm', 'ac', 'f0', 'document_vectors','master-slaves', 'emotional_states', 'handwritten_character_recognition', '0.90', 'feature-enhanced', 'nn']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "with open(f\"./SeeTopic/{dir_name}/keywords_seetopic.txt\", \"r\") as f:\n",
    "    children_phrases = [i.strip().split(\":\")[1].split(\",\") for i in f.readlines()]\n",
    "    filtered_children_phrases = []\n",
    "    for c_id, c in enumerate(taxo.root.children):\n",
    "        # other parents\n",
    "        other_parents = \"\\n\".join([f\"Sibling topic: {i.label}; Description: {i.desc}\" for i in taxo.root.children if i != c])\n",
    "        other_terms = [p for child in children_phrases[:c_id] + children_phrases[c_id+1:] for p in child]\n",
    "        # filter the child phrases\n",
    "        child_phrases = filter_phrases(c, children_phrases[c_id], word2emb, other_parents, other_terms)\n",
    "        # child_phrases = filter_phrases(c, children_phrases[c_id], other_parents=other_parents)\n",
    "        filtered_children_phrases.append(child_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_id, c in enumerate(taxo.root.children):\n",
    "    c.addTerms(filtered_children_phrases[c_id], mined=True, addToParent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get initial, exact-matching pool of papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised_learning 140\n",
      "unsupervised_learning 132\n",
      "semi_supervised_learning 20\n",
      "deep_learning 122\n",
      "ensemble_methods 58\n"
     ]
    }
   ],
   "source": [
    "for c in taxo.root.children:\n",
    "    print(c.label, len(c.papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node-Oriented Sentence Representations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_phrase_reprs = taxo.getClassReprs(taxo.root.children, phrase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:38<00:00,  7.71s/it]\n"
     ]
    }
   ],
   "source": [
    "for c in tqdm(taxo.root.children):\n",
    "    c.rankPapers(class_phrase_reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reprs = taxo.getClassReprs(taxo.root.children, phrase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_reprs = []\n",
    "for p in taxo.collection:\n",
    "    # paper_reprs.append(p.computePaperEmb(class_reprs, phrase=False))\n",
    "    paper_reprs.append(p.emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels, mapping = taxo.mapPapers(paper_reprs, taxo.root.children, class_reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmapped: 292\n",
      "supervised_learning: 81\n",
      "unsupervised_learning: 58\n",
      "semi_supervised_learning: 191\n",
      "deep_learning: 219\n",
      "ensemble_methods: 437\n"
     ]
    }
   ],
   "source": [
    "for k, v in mapping.items():\n",
    "    if k == -1:\n",
    "        print(f\"unmapped: {len(v)}\")\n",
    "    else:\n",
    "        print(f\"{taxo.root.children[k]}: {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_repr = taxo.root.updateNodeEmb(phrase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_siblings = taxo.siblingExpansion(taxo.root, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian-accented': 0,\n",
       " 'learning': 1,\n",
       " 'phrase': 2,\n",
       " 'ensuing': 3,\n",
       " 'sub-level': 4,\n",
       " 'lakoff': 5,\n",
       " 'graph': 6,\n",
       " 'communications': 7,\n",
       " 'quantified': 8,\n",
       " 'nominal': 9,\n",
       " 'models': 10,\n",
       " 'symptom-specific': 11,\n",
       " 'linear': 12,\n",
       " 'keyword_matching': 13,\n",
       " 'model': 14,\n",
       " 'fusion': 15,\n",
       " 'chat-bot': 16,\n",
       " 'phonological': 17,\n",
       " 'machine': 18,\n",
       " 'snomed-ct': 19,\n",
       " 'fuzzy': 20,\n",
       " 'decentralized': 21,\n",
       " 'hie': 22,\n",
       " 'i-vector': 23,\n",
       " 'trump': 24,\n",
       " 'sampling': 25,\n",
       " 'optimization': 26,\n",
       " 'text_generation': 27,\n",
       " 'git': 28,\n",
       " '0.743': 29,\n",
       " 'network': 30,\n",
       " 'proactive': 31,\n",
       " 'feature_extractor': 32,\n",
       " 'free_software': 33,\n",
       " 'algorithm': 34,\n",
       " 'sequence': 35,\n",
       " 'transcriptions': 36,\n",
       " 'chats': 37,\n",
       " 'suomi24': 38,\n",
       " 'crossfunctional': 39,\n",
       " 'occupations': 40,\n",
       " 'algorithms': 41,\n",
       " 'agency': 42,\n",
       " 'decision_tree': 43,\n",
       " 'naïve_bayesian': 44,\n",
       " 'bag': 45,\n",
       " 'technique': 46,\n",
       " 'bi-languages': 47,\n",
       " 'spatial_information': 48,\n",
       " 'race-': 49,\n",
       " 'classification': 50,\n",
       " 'markers': 51,\n",
       " 'overt': 52,\n",
       " 'data_sparseness': 53,\n",
       " 'responsive': 54,\n",
       " 'representation': 55,\n",
       " 'search': 56,\n",
       " 'hierarchical': 57,\n",
       " 'go-pay': 58,\n",
       " '37120': 59,\n",
       " 'land': 60,\n",
       " 'rgb': 61,\n",
       " 'featured': 62,\n",
       " 'decompose': 63,\n",
       " 'exclusion': 64,\n",
       " 'domain-trained': 65,\n",
       " 'feature': 66,\n",
       " 'segmental': 67,\n",
       " 'label': 68,\n",
       " 'autobiographical': 69,\n",
       " 'rule': 70,\n",
       " 'based': 71,\n",
       " 'archivology': 72,\n",
       " 'representations': 73,\n",
       " 'spanbert': 74,\n",
       " 'justifies': 75,\n",
       " 'sara': 76,\n",
       " 'ranking': 77,\n",
       " 'language': 78,\n",
       " 'self-reinforcing': 79,\n",
       " 'trading': 80,\n",
       " 'algebra': 81,\n",
       " 'neural': 82,\n",
       " 'gestures': 83,\n",
       " 'regression': 84,\n",
       " 'preserves': 85,\n",
       " 'tree': 86,\n",
       " 'intersection': 87,\n",
       " 'sees': 88,\n",
       " 'methods': 89,\n",
       " '1124': 90,\n",
       " 'nle': 91,\n",
       " 'semantic': 92,\n",
       " 'architecture': 93,\n",
       " 'modeling': 94,\n",
       " 'complex_queries': 95,\n",
       " 'multlingual': 96,\n",
       " 'vector': 97,\n",
       " 'fourier': 98,\n",
       " 'hitch': 99,\n",
       " 'imposing': 100,\n",
       " 'academia': 101,\n",
       " 'dings': 102,\n",
       " 'map': 103,\n",
       " 'rules': 104,\n",
       " 'retrieval': 105,\n",
       " 'assisted_living': 106,\n",
       " 'biasing': 107,\n",
       " 'features': 108,\n",
       " 'matching': 109,\n",
       " 'pattern': 110,\n",
       " 'temporal_relation': 111,\n",
       " 'restriction': 112,\n",
       " 'encounter': 113,\n",
       " 'concept': 114,\n",
       " 'forward/backward': 115,\n",
       " 'stemming': 116,\n",
       " 'online_handwritten': 117,\n",
       " 'debt': 118,\n",
       " 'deep': 119,\n",
       " 'non-focus': 120,\n",
       " 'documents-e.g.': 121,\n",
       " 'explicated': 122,\n",
       " 'retrospective': 123,\n",
       " 'transportation': 124,\n",
       " 'strategy': 125,\n",
       " 'joint-entity-sentiment-topic': 126,\n",
       " 'knowledge_engineering': 127,\n",
       " 'dcgan-based': 128,\n",
       " 'version_control': 129,\n",
       " 'videotaped': 130,\n",
       " 'approaches': 131,\n",
       " 'textual_descriptions': 132,\n",
       " 'component': 133,\n",
       " 'das': 134,\n",
       " 'shallow': 135,\n",
       " 'term': 136,\n",
       " 'statistical': 137,\n",
       " 'sentimental_analysis': 138,\n",
       " 'techniques': 139,\n",
       " 'along': 140,\n",
       " 'dialogue': 141,\n",
       " 'emotion': 142,\n",
       " 'smart': 143,\n",
       " 'phonetic_features': 144,\n",
       " 'spiking': 145,\n",
       " 'lvq': 146,\n",
       " 'fasttext': 147,\n",
       " 'selection': 148,\n",
       " 'weighted': 149,\n",
       " 'linguistic': 150,\n",
       " 'structure': 151,\n",
       " 'networks': 152,\n",
       " 'credbank': 153,\n",
       " 'bayesian': 154,\n",
       " 'attentive': 155,\n",
       " 'trajectory': 156,\n",
       " 'classifier-free': 157,\n",
       " 'luggage': 158,\n",
       " 'binary': 159,\n",
       " 'diachronic': 160,\n",
       " '14.0': 161,\n",
       " 'transformations': 162,\n",
       " 'cluster': 163,\n",
       " 'aspect-terms': 164,\n",
       " 'strategies': 165,\n",
       " 'word': 166,\n",
       " 'labeling_effort': 167,\n",
       " 'clarification': 168,\n",
       " 'function': 169,\n",
       " 'assembly_language': 170,\n",
       " 'go': 171,\n",
       " 'prototypes': 172,\n",
       " 'filtering': 173,\n",
       " 'human_evaluation': 174,\n",
       " 'mapping': 175,\n",
       " 'curvature-based': 176,\n",
       " 'labeling': 177,\n",
       " 'danish': 178,\n",
       " 'textual': 179,\n",
       " 'consonants': 180,\n",
       " 'ensemble_learning': 181,\n",
       " 'intrinsic_evaluation': 182,\n",
       " 'peri–covid-19_pandemic': 183,\n",
       " 'mechanism': 184,\n",
       " 'si-gcn': 185,\n",
       " 'gradient': 186,\n",
       " 'ā': 187,\n",
       " 'cognition': 188,\n",
       " 'tsa': 189,\n",
       " 'flair': 190,\n",
       " 'ncrnlp': 191,\n",
       " 'covid': 192,\n",
       " 'inquiry': 193,\n",
       " 'uni-modal': 194,\n",
       " 'scale_invariant_feature_transform': 195,\n",
       " 'distance': 196,\n",
       " 'alignment': 197,\n",
       " 'softmax': 198,\n",
       " 'modelling': 199,\n",
       " 'sentence-bert': 200,\n",
       " 'complexity-based': 201,\n",
       " 'ground_truth': 202,\n",
       " 'deep_learning': 203,\n",
       " 'classifications': 204,\n",
       " 'pre-purchase': 205,\n",
       " 'al-maány': 206,\n",
       " 'scream': 207,\n",
       " 'dianping': 208,\n",
       " 'e-mail': 209,\n",
       " 'weights': 210,\n",
       " 'sound': 211,\n",
       " 'character': 212,\n",
       " '96.60': 213,\n",
       " 'textcnn_model': 214,\n",
       " 'grouped': 215,\n",
       " 'method': 216,\n",
       " 'textrank_algorithm': 217,\n",
       " 'relation': 218,\n",
       " 'robert': 219,\n",
       " 'fine_grained': 220,\n",
       " 'augmentation-agnostic': 221,\n",
       " 'temporal': 222,\n",
       " 'triage': 223,\n",
       " 'unsupported': 224,\n",
       " 'mining': 225,\n",
       " 'distribution': 226,\n",
       " 'inference': 227,\n",
       " '72.0': 228,\n",
       " 'framework': 229,\n",
       " 'interchangeable': 230,\n",
       " 'fine': 231,\n",
       " 'lectures': 232,\n",
       " 'theory': 233,\n",
       " 'talent': 234,\n",
       " 'manifestations': 235,\n",
       " 'hospital': 236,\n",
       " 'b-tree': 237,\n",
       " 'scheme': 238,\n",
       " 'expectation_maximization_algorithm': 239,\n",
       " 'bigram': 240,\n",
       " 'relative_improvement': 241,\n",
       " 'drop-in': 242,\n",
       " 'tracking': 243,\n",
       " 'patau_syndrome': 244,\n",
       " 'polite': 245,\n",
       " 'density': 246,\n",
       " 'alternate': 247,\n",
       " 'la': 248,\n",
       " 'emotional': 249,\n",
       " 'ascending': 250,\n",
       " 'softmax_classifier': 251,\n",
       " 'rewards': 252,\n",
       " '(': 253,\n",
       " 'loss': 254,\n",
       " 'variable': 255,\n",
       " 'naive_bayes_classifier': 256,\n",
       " 'module': 257,\n",
       " 'parameters': 258,\n",
       " 'subcontexts': 259,\n",
       " 'residual': 260,\n",
       " 'canny': 261,\n",
       " 'traditional_machine_learning_approaches': 262,\n",
       " 'anchor-based': 263,\n",
       " 'acoustic': 264,\n",
       " 'preferential': 265,\n",
       " 'layer': 266,\n",
       " 'speaker': 267,\n",
       " 'sentiment_scoring': 268,\n",
       " 'lstm-rnn': 269,\n",
       " 'classifier-based': 270,\n",
       " 'interlocutors': 271,\n",
       " 'error_detection': 272,\n",
       " '0.279': 273,\n",
       " 'global': 274,\n",
       " 'fuzzy_set': 275,\n",
       " 'recognition.in': 276,\n",
       " 'geometric': 277,\n",
       " 'quranic': 278,\n",
       " 'dependency': 279,\n",
       " 'alexnet-l': 280,\n",
       " 'functions': 281,\n",
       " 'tertiary-based': 282,\n",
       " 'bert-tfidf': 283,\n",
       " 'random': 284,\n",
       " 'information_assurance': 285,\n",
       " 'sociologically': 286,\n",
       " 'embedded': 287,\n",
       " 'cur': 288,\n",
       " 'columbia': 289,\n",
       " 'bounding': 290,\n",
       " 'bot': 291,\n",
       " 'revised': 292,\n",
       " 'adop': 293,\n",
       " 'generation': 294,\n",
       " 'dictionary': 295,\n",
       " 'matrix': 296,\n",
       " 'progression': 297,\n",
       " 'computer_mediated_communication': 298,\n",
       " 'local': 299,\n",
       " ',': 300,\n",
       " 'conditional': 301,\n",
       " 'vigorously': 302,\n",
       " 'dynamic': 303,\n",
       " 'approach': 304,\n",
       " 'knowledge-driven': 305,\n",
       " 'tool-is': 306,\n",
       " 'using': 307,\n",
       " 'vector_quantization': 308,\n",
       " 'bogoroditsky': 309,\n",
       " 'device': 310,\n",
       " 'advancement': 311,\n",
       " 'k-means': 312,\n",
       " 'appliance': 313,\n",
       " 'kchar/s': 314,\n",
       " 'alice': 315,\n",
       " 'information': 316,\n",
       " 'suits': 317,\n",
       " '775': 318,\n",
       " 'islanding': 319,\n",
       " 'minds': 320,\n",
       " 'layer.the': 321,\n",
       " 'anchor': 322,\n",
       " 'engage': 323,\n",
       " 'analyser': 324,\n",
       " 'activities/accidents': 325,\n",
       " 'issue_tracking': 326,\n",
       " 'booking': 327,\n",
       " 'knowledge': 328,\n",
       " 'producers': 329,\n",
       " 'vectors': 330,\n",
       " 'criterion': 331,\n",
       " 'reinforcement_learning': 332,\n",
       " 'pre-classification': 333,\n",
       " 'linguistic-anthropology': 334,\n",
       " 'extraction': 335,\n",
       " 'setting': 336,\n",
       " '380': 337,\n",
       " 'measures': 338,\n",
       " 'manifold': 339,\n",
       " 'sleeping': 340,\n",
       " 'multi-view': 341,\n",
       " 'ethnographic': 342,\n",
       " 'r-cnn': 343,\n",
       " 'error': 344,\n",
       " 'soft_label': 345,\n",
       " 'malware': 346,\n",
       " 'block': 347,\n",
       " 'grammar': 348,\n",
       " 'twitter.com': 349,\n",
       " 'principle': 350,\n",
       " 'node': 351,\n",
       " 'graphs': 352,\n",
       " 'labels': 353,\n",
       " 'r.': 354,\n",
       " 'text-projection': 355,\n",
       " 'prepandemic': 356,\n",
       " 'supervised': 357,\n",
       " 'nle-alone': 358,\n",
       " 'threshold': 359,\n",
       " 'attack': 360,\n",
       " 'distributions': 361,\n",
       " 'littérateur': 362,\n",
       " 'menu': 363,\n",
       " 'dimension-reduced': 364,\n",
       " 'teacher': 365,\n",
       " 'data_analysis': 366,\n",
       " 'patterns': 367,\n",
       " 'text': 368,\n",
       " \"'status\": 369,\n",
       " 'translation-based': 370,\n",
       " 'phoneme-based': 371,\n",
       " 'unsupervised_domain-adaptation': 372,\n",
       " 'machine_learning': 373,\n",
       " 'lsi-1dsom': 374,\n",
       " 'coarse-grained': 375,\n",
       " 'prediction': 376,\n",
       " 'scoring': 377,\n",
       " 'correlation': 378,\n",
       " 'disambiguated': 379,\n",
       " 'va.': 380,\n",
       " 'software_quality': 381,\n",
       " 'bert-bilstm-crf': 382,\n",
       " 'uncertainty': 383,\n",
       " 'sentiment': 384,\n",
       " 'traditional_machine_learning_algorithms': 385,\n",
       " 'condense': 386,\n",
       " 'sample': 387,\n",
       " 'schemes': 388,\n",
       " 'sliding_window': 389,\n",
       " 'could': 390,\n",
       " 'count': 391,\n",
       " 'electroencephalogram': 392,\n",
       " 'connections': 393,\n",
       " 'summarisation': 394,\n",
       " 'interactional': 395,\n",
       " 'acoustic-textual': 396,\n",
       " 'intact': 397,\n",
       " 'ure': 398,\n",
       " 'finer-grained': 399,\n",
       " 'enriched': 400,\n",
       " 'analysis': 401,\n",
       " 'category': 402,\n",
       " 'dimension': 403,\n",
       " 'classifier—a': 404,\n",
       " 'doctor–patient': 405,\n",
       " 'edit_distance': 406,\n",
       " '485': 407,\n",
       " 'without': 408,\n",
       " 'lvq3': 409,\n",
       " 'goodwin': 410,\n",
       " 'unstruc-': 411,\n",
       " 'adaptation': 412,\n",
       " 'processes': 413,\n",
       " 'semantic-interactive': 414,\n",
       " 'deep‐learning‐based': 415,\n",
       " 'wheel': 416,\n",
       " 'linear_regression': 417,\n",
       " 'top-2': 418,\n",
       " 'psycinfo': 419,\n",
       " 'across': 420,\n",
       " 'sensitive_words': 421,\n",
       " 'base': 422,\n",
       " 'rights': 423,\n",
       " 'maximum': 424,\n",
       " 'xml': 425,\n",
       " 'randomization': 426,\n",
       " 'adaptive': 427,\n",
       " 'text-guided': 428,\n",
       " 'data': 429,\n",
       " 'learned': 430,\n",
       " 'attesting': 431,\n",
       " 'jaccard': 432,\n",
       " 'automatically_identifies': 433,\n",
       " 'class-based': 434,\n",
       " 'geographic_information': 435,\n",
       " 'statistical_tests': 436,\n",
       " 'aes': 437,\n",
       " 'threatens': 438,\n",
       " 'naïve_bayes': 439,\n",
       " 'term-matching': 440,\n",
       " 'caption': 441,\n",
       " 'biomedical': 442,\n",
       " 'prompt': 443,\n",
       " 'defect_text': 444,\n",
       " 'visual': 445,\n",
       " 'measure': 446,\n",
       " 'image_analysis': 447,\n",
       " 'cognitive-behavioral': 448,\n",
       " 'encoding': 449,\n",
       " 'interviews': 450,\n",
       " 'hybrid': 451,\n",
       " 'asian_elephants': 452,\n",
       " 'ir_tasks': 453,\n",
       " 'computing': 454,\n",
       " 'frontness': 455,\n",
       " 'literary_texts': 456,\n",
       " 'latent_semantic_indexing': 457,\n",
       " 'technology': 458,\n",
       " 'document_indexing': 459,\n",
       " 'conspiracy_theories': 460,\n",
       " 'typological': 461,\n",
       " 'concepts': 462,\n",
       " 'linguistic_inquiry_and_word_count': 463,\n",
       " 'td-idf': 464,\n",
       " 'biattention': 465,\n",
       " 'k-bert': 466,\n",
       " 'alone': 467,\n",
       " 'morphological': 468,\n",
       " 'greedy': 469,\n",
       " 'communicable': 470,\n",
       " 'devil': 471,\n",
       " 'online_platforms': 472,\n",
       " 'text_fragments': 473,\n",
       " 'weight': 474,\n",
       " 'asstps': 475,\n",
       " 'location': 476,\n",
       " 'heuristic-based': 477,\n",
       " '9070': 478,\n",
       " 'rating': 479,\n",
       " 'diffusion': 480,\n",
       " 'space': 481,\n",
       " 'matched': 482,\n",
       " 'leaders': 483,\n",
       " 'supervised_learning': 484,\n",
       " 'leeuwen': 485,\n",
       " 'word_order': 486,\n",
       " 'multimedia': 487,\n",
       " 'classifier': 488,\n",
       " 'noisily': 489,\n",
       " 'supposition': 490,\n",
       " 'use/cover': 491,\n",
       " 'gan-based': 492,\n",
       " 'irrespective': 493,\n",
       " 'training': 494,\n",
       " 'loops': 495,\n",
       " 'ipad': 496,\n",
       " 'distribute': 497,\n",
       " 'consonant': 498,\n",
       " 'operation': 499,\n",
       " 'sociological': 500,\n",
       " 'text/non-text_classification': 501,\n",
       " 'relations': 502,\n",
       " '”': 503,\n",
       " 'tag': 504,\n",
       " 'light': 505,\n",
       " 'baudouin': 506,\n",
       " 'eliciting': 507,\n",
       " 'together': 508,\n",
       " 'she/he': 509,\n",
       " 'shapes': 510,\n",
       " 'formulation': 511,\n",
       " 'drugbank': 512,\n",
       " 'type-matching': 513,\n",
       " 'resort': 514,\n",
       " \"'chembl-like\": 515,\n",
       " 'semantic-embedding_vectors': 516,\n",
       " 'independent': 517,\n",
       " 'chinese/english': 518,\n",
       " 'front_end': 519,\n",
       " 'word-sense-based': 520,\n",
       " 'maem': 521,\n",
       " \"'non-dej-based\": 522,\n",
       " 'wörterbuch': 523,\n",
       " 'rule-based': 524,\n",
       " 'poorer': 525,\n",
       " ';': 526,\n",
       " 'axiomatic': 527,\n",
       " 'spatial': 528,\n",
       " 'compression': 529,\n",
       " 'attention': 530,\n",
       " 'qualified': 531,\n",
       " 'indo-aryan': 532,\n",
       " 'operators': 533,\n",
       " 'test_case': 534,\n",
       " 'examples': 535,\n",
       " 'reasoning': 536,\n",
       " 'decision': 537,\n",
       " '\\\\alpha': 538,\n",
       " 'prescriptive': 539,\n",
       " 'inaccuracies': 540,\n",
       " 'ahrs': 541,\n",
       " 'controversial': 542,\n",
       " 'polynomial': 543,\n",
       " 'hlspc': 544,\n",
       " 'may': 545,\n",
       " 'probability': 546,\n",
       " 'presentation': 547,\n",
       " 'classifiers': 548,\n",
       " 'standard': 549,\n",
       " 'association_rules': 550,\n",
       " 'gives': 551,\n",
       " 'image': 552,\n",
       " 'syntax-pragmatics': 553,\n",
       " 'medical': 554,\n",
       " 'easier-to-understand': 555,\n",
       " 'type': 556,\n",
       " 'samples': 557,\n",
       " 'me/cfs': 558,\n",
       " 'homology': 559,\n",
       " 'performs': 560,\n",
       " 'defaults': 561,\n",
       " 'properties': 562,\n",
       " 'topology-based': 563,\n",
       " 'oppose': 564,\n",
       " 'visual-semantic': 565,\n",
       " 'contrastive_learning': 566,\n",
       " 'group': 567,\n",
       " '.': 568,\n",
       " 'schizophrenic': 569,\n",
       " '73.39': 570,\n",
       " 'supervised_machine_learning': 571,\n",
       " 'relational_model': 572,\n",
       " 'regular_expression': 573,\n",
       " 'feature-based': 574,\n",
       " 'stochastic_gradient_descent': 575,\n",
       " 'back-propagation': 576,\n",
       " 'sorting': 577,\n",
       " 'arabie': 578,\n",
       " 'bilingual': 579,\n",
       " 'system': 580,\n",
       " 'index': 581,\n",
       " 'non-sub-sentences': 582,\n",
       " 'dynamical_systems': 583,\n",
       " 'intermediation': 584,\n",
       " '227': 585,\n",
       " 'sequential': 586,\n",
       " 'supports': 587,\n",
       " 'requirements': 588,\n",
       " 'sentence_structures': 589,\n",
       " 'positional': 590,\n",
       " 'underrepresentation': 591,\n",
       " 'multi-agent': 592,\n",
       " 'sacrificing': 593,\n",
       " 'nlp+css': 594,\n",
       " '95.5': 595,\n",
       " 'naturalistic': 596,\n",
       " 'pair': 597,\n",
       " 'first-level': 598,\n",
       " 'bags': 599,\n",
       " 'usage': 600,\n",
       " 'pirnas': 601,\n",
       " 'feature_maps': 602,\n",
       " 'geometry': 603,\n",
       " 'spoken_dialogue': 604,\n",
       " 'score': 605,\n",
       " 'character-based': 606,\n",
       " 'topic': 607,\n",
       " 'rtsl': 608,\n",
       " 'machine-learning': 609,\n",
       " 'procedure': 610,\n",
       " 'calculation': 611,\n",
       " 'tests': 612,\n",
       " 'active': 613,\n",
       " 'differing': 614,\n",
       " 'among': 615,\n",
       " 'computer-mediated_communication': 616,\n",
       " 'lexicon-based': 617,\n",
       " 'filter': 618,\n",
       " 'functional': 619,\n",
       " 'class': 620,\n",
       " 'indicators': 621,\n",
       " 'territories': 622,\n",
       " 'weighted_average': 623,\n",
       " 'image_editing': 624,\n",
       " 'cvae': 625,\n",
       " 'invariable': 626,\n",
       " 'archival': 627,\n",
       " 'developmental': 628,\n",
       " 'multi-question_classification': 629,\n",
       " 'llm-kd': 630,\n",
       " 'produced': 631,\n",
       " 'machine_learning-based': 632,\n",
       " 'related': 633,\n",
       " 'test': 634,\n",
       " 'reinterpret': 635,\n",
       " 'ac-cording': 636,\n",
       " 'la-gan': 637,\n",
       " '-': 638,\n",
       " 'multiple': 639,\n",
       " 'relationships': 640,\n",
       " 'attribute': 641,\n",
       " 'simultaneously': 642,\n",
       " '72.4': 643,\n",
       " 'estimator': 644,\n",
       " 'clue': 645,\n",
       " 'linguists': 646,\n",
       " 'ensemble': 647,\n",
       " 'som-based': 648,\n",
       " 'non-lexical_features': 649,\n",
       " '643': 650,\n",
       " 'insomnia': 651,\n",
       " 'purchase-states': 652,\n",
       " 'dative': 653,\n",
       " 'gan': 654,\n",
       " 'f1-micro': 655,\n",
       " 'human': 656,\n",
       " 'pft': 657,\n",
       " 'design': 658,\n",
       " 'statistically_significant': 659,\n",
       " 'class_label': 660,\n",
       " 'marginalization': 661,\n",
       " 'similarity': 662,\n",
       " 'variant': 663,\n",
       " 'sad‟': 664,\n",
       " 'post-synaptic': 665,\n",
       " 'guidelines': 666,\n",
       " 'transfer': 667,\n",
       " 'subgraph': 668,\n",
       " 'joint': 669,\n",
       " 'instances': 670,\n",
       " 'latent_topic': 671,\n",
       " 'individualism': 672,\n",
       " 'executable': 673,\n",
       " 'database': 674,\n",
       " 'multi-label': 675,\n",
       " 'kcnq2-': 676,\n",
       " 'level': 677,\n",
       " 'repair': 678,\n",
       " 'isometric-invariant': 679,\n",
       " 'complementary': 680,\n",
       " 'structures': 681,\n",
       " 'semantics': 682,\n",
       " 'macedonian': 683,\n",
       " 'word-emoticon': 684,\n",
       " 'buy': 685,\n",
       " 'interaction': 686,\n",
       " 'migration': 687,\n",
       " 'sequence_alignment': 688,\n",
       " 'decomposition': 689,\n",
       " 'variables': 690,\n",
       " 'theo': 691,\n",
       " 'internationally': 692,\n",
       " 'doctor/patient': 693,\n",
       " 'googlemybusiness': 694,\n",
       " 'penalties': 695,\n",
       " 'intent_classifier': 696,\n",
       " 'multiaspect': 697,\n",
       " 'x-vector': 698,\n",
       " 'excessively': 699,\n",
       " '811': 700,\n",
       " 'challenged': 701,\n",
       " 'frontiers': 702,\n",
       " 'aspect': 703,\n",
       " 'bioactivity': 704,\n",
       " 'text/non-text': 705,\n",
       " 'dbn': 706,\n",
       " 'plasticity': 707,\n",
       " 'combined': 708,\n",
       " 'distance_measures': 709,\n",
       " 'commas': 710,\n",
       " 'aspect-opinion': 711,\n",
       " 'critical_discourse_analysis': 712,\n",
       " 'perspective': 713,\n",
       " 'dependency-based': 714,\n",
       " 'agreed': 715,\n",
       " 'signals': 716,\n",
       " 'shared': 717,\n",
       " 'modulating': 718,\n",
       " 'discussions': 719,\n",
       " 'common': 720,\n",
       " 'traditional_machine_learning': 721,\n",
       " 'patent': 722,\n",
       " 'ethnicity': 723,\n",
       " 'crfs': 724,\n",
       " 'interviewer-bot': 725,\n",
       " 'sesam': 726,\n",
       " 'deep_neural_network': 727,\n",
       " 'within': 728,\n",
       " 'symptoms': 729,\n",
       " 'speech_signal': 730,\n",
       " 'images': 731,\n",
       " 'separates': 732,\n",
       " 'values': 733,\n",
       " 'processing': 734,\n",
       " 'late_fusion': 735,\n",
       " 'novelty_detection': 736,\n",
       " 'android': 737,\n",
       " 'estimation': 738,\n",
       " 'keyword': 739,\n",
       " 'statistics': 740,\n",
       " 'parameter_tuning': 741,\n",
       " 'opponents': 742,\n",
       " 'criteria': 743,\n",
       " 'dialogue_acts': 744,\n",
       " 'tuning': 745,\n",
       " 'vincent': 746,\n",
       " 'hinders': 747,\n",
       " 'mucic': 748,\n",
       " 'zahaba': 749,\n",
       " 'pipeline': 750,\n",
       " 'artificial_neural_network': 751,\n",
       " 'object': 752,\n",
       " 'integrated': 753,\n",
       " 'perceiver': 754,\n",
       " 'pure-mentions': 755,\n",
       " 'jest': 756,\n",
       " '0.837': 757,\n",
       " 'utilizing': 758,\n",
       " 'associated': 759,\n",
       " 'representative': 760,\n",
       " '0.635': 761,\n",
       " 'speech_act': 762,\n",
       " 'motion': 763,\n",
       " 'vader': 764,\n",
       " 'pairs': 765,\n",
       " 'codefest': 766,\n",
       " 'anti-vaccine': 767,\n",
       " '0.9648': 768,\n",
       " 'natural_images': 769,\n",
       " '5g': 770,\n",
       " 'value': 771,\n",
       " 'units': 772,\n",
       " 'conception': 773,\n",
       " 'predictions': 774,\n",
       " 'frequency_distribution': 775,\n",
       " 'station': 776,\n",
       " 'scale': 777,\n",
       " 'entail': 778,\n",
       " 'compile': 779,\n",
       " 'neuro-fuzzy': 780,\n",
       " 'construction': 781,\n",
       " 'ambiguity': 782,\n",
       " 'isomorphic': 783,\n",
       " 'multi-media': 784,\n",
       " 'parallel': 785,\n",
       " 'artificial': 786,\n",
       " 'open/closed': 787,\n",
       " '78.38': 788,\n",
       " 'formal': 789,\n",
       " 'grammatical': 790,\n",
       " 'user-related': 791,\n",
       " 'project-oriented': 792,\n",
       " 'point': 793,\n",
       " 'logic': 794,\n",
       " 'entity': 795,\n",
       " 'accidentology': 796,\n",
       " 'scaled': 797,\n",
       " 'discrete': 798,\n",
       " '1dsom': 799,\n",
       " 'bert-based': 800,\n",
       " 'sarcasm': 801,\n",
       " 'tools': 802,\n",
       " 'frequency': 803,\n",
       " 'word_frequency': 804,\n",
       " 'components': 805,\n",
       " 'attach': 806,\n",
       " 'levantine': 807,\n",
       " '0.75': 808,\n",
       " 'sudan': 809,\n",
       " 'link': 810,\n",
       " 'segment': 811,\n",
       " 'query': 812,\n",
       " 'completely': 813,\n",
       " 'scanned_documents': 814,\n",
       " 'reply': 815,\n",
       " 'adjacent': 816,\n",
       " 'grayscale': 817,\n",
       " 'word-based': 818,\n",
       " 'methodogical': 819,\n",
       " 'translation': 820,\n",
       " 'heli-ots': 821,\n",
       " 'blinking-leading': 822,\n",
       " 'text_corpora': 823,\n",
       " 'ordinal-imbalanced': 824,\n",
       " 'cnn': 825,\n",
       " 'static': 826,\n",
       " 'truth': 827,\n",
       " 'animal': 828,\n",
       " 'yunnan_province': 829,\n",
       " 'severity': 830,\n",
       " 'human-elephant': 831,\n",
       " 'labeled': 832,\n",
       " 'lft': 833,\n",
       " 'colab.re': 834,\n",
       " 'visualizing': 835,\n",
       " 'interpolated': 836,\n",
       " 'social_event': 837,\n",
       " 'message': 838,\n",
       " 'procedures': 839,\n",
       " 'n-gram': 840,\n",
       " 'slc13a5-': 841,\n",
       " 'head': 842,\n",
       " 'kohonen': 843,\n",
       " 'differentiation-indifferentiation': 844,\n",
       " 'confidence': 845,\n",
       " 'outputs': 846,\n",
       " 'adapted': 847,\n",
       " 'euclidean': 848,\n",
       " 'rather': 849,\n",
       " 'association_rule_mining': 850,\n",
       " '86.6': 851,\n",
       " 'separate': 852,\n",
       " 'facial': 853,\n",
       " 'masked_language_models': 854,\n",
       " 'evaluation': 855,\n",
       " 'genesis': 856,\n",
       " 'image-class-tags': 857,\n",
       " 'points': 858,\n",
       " 'seqgan-based': 859,\n",
       " 'longest_common': 860,\n",
       " 'bleu': 861,\n",
       " 'non-iterative': 862,\n",
       " 'content': 863,\n",
       " 'bias': 864,\n",
       " 'topic-word': 865,\n",
       " 'sentences': 866,\n",
       " 'intervention': 867,\n",
       " 'explicit': 868,\n",
       " 'centrality': 869,\n",
       " 'text_representations': 870,\n",
       " 'ctc-kws': 871,\n",
       " 'amendment': 872,\n",
       " 'distributed': 873,\n",
       " 'requirement': 874,\n",
       " 'transform': 875,\n",
       " 'hsv': 876,\n",
       " 'document': 877,\n",
       " 'aws-based': 878,\n",
       " 'classification_scheme': 879,\n",
       " 'scibert': 880,\n",
       " 'failure_modes': 881,\n",
       " '[': 882,\n",
       " 'contextually-relevant': 883,\n",
       " 'increases': 884,\n",
       " 'bands': 885,\n",
       " 'augmented': 886,\n",
       " 'showcase': 887,\n",
       " 'padi-web': 888,\n",
       " 'march': 889,\n",
       " 'resumed': 890,\n",
       " 'verbs': 891,\n",
       " 'links': 892,\n",
       " 'associate': 893,\n",
       " '63.5': 894,\n",
       " 'engineering': 895,\n",
       " 'micro_blogs': 896,\n",
       " '2d-grid': 897,\n",
       " 'certain/uncertain': 898,\n",
       " 'nets': 899,\n",
       " '19.8': 900,\n",
       " 'conversion': 901,\n",
       " 'vietnam': 902,\n",
       " 'states': 903,\n",
       " 'spectrogram': 904,\n",
       " 'objectively': 905,\n",
       " 'crypto-currency': 906,\n",
       " '232': 907,\n",
       " 'stf': 908,\n",
       " 'hadid': 909,\n",
       " 'sewese': 910,\n",
       " 'soundly': 911,\n",
       " 'translations': 912,\n",
       " 'classroom': 913,\n",
       " 'working': 914,\n",
       " 'sensor': 915,\n",
       " 'sift': 916,\n",
       " '1931-1958': 917,\n",
       " 'imagine': 918,\n",
       " 'words': 919,\n",
       " 'location_information': 920,\n",
       " 'similar': 921,\n",
       " 'context': 922,\n",
       " 'language-usage-patterns': 923,\n",
       " 'typing': 924,\n",
       " 'entity_mention': 925,\n",
       " 'till': 926,\n",
       " 'directly': 927,\n",
       " 'gender': 928,\n",
       " 'counseling': 929,\n",
       " 'tentatively': 930,\n",
       " 'amplifying': 931,\n",
       " 'comment': 932,\n",
       " 'several': 933,\n",
       " 'resistance': 934,\n",
       " 'via': 935,\n",
       " 'explanations': 936,\n",
       " 'customize': 937,\n",
       " 'recognition': 938,\n",
       " 'image_retrieval': 939,\n",
       " 'chillax': 940,\n",
       " 'content-based': 941,\n",
       " 'argument': 942,\n",
       " 'previously': 943,\n",
       " 'pico': 944,\n",
       " 'brazilian_portuguese': 945,\n",
       " 'dimensions': 946,\n",
       " 'self-organizing_map': 947,\n",
       " 'nli': 948,\n",
       " 'selections': 949,\n",
       " 'natural_language': 950,\n",
       " 'prey': 951,\n",
       " 'call': 952,\n",
       " 'long': 953,\n",
       " 'character_segmentation': 954,\n",
       " 'overlap': 955,\n",
       " 'expert': 956,\n",
       " 'corpus-based': 957,\n",
       " 'hmm': 958,\n",
       " 'plastics': 959,\n",
       " 'leave-one-subject-out': 960,\n",
       " 'depressed': 961,\n",
       " 'polite-rl': 962,\n",
       " 'lrn-wrn': 963,\n",
       " 'supplements': 964,\n",
       " 'works': 965,\n",
       " 'field-related': 966,\n",
       " 'aspect-category': 967,\n",
       " 'auxiliary': 968,\n",
       " 'capability': 969,\n",
       " 'sifting': 970,\n",
       " 'cues': 971,\n",
       " 'sustainable': 972,\n",
       " 'european': 973,\n",
       " 'topic_detection': 974,\n",
       " 'bullies': 975,\n",
       " 'pearson_correlation': 976,\n",
       " \"'better\": 977,\n",
       " 'continuous': 978,\n",
       " '2dsom': 979,\n",
       " 'english-based': 980,\n",
       " 'a.n': 981,\n",
       " 'dozen': 982,\n",
       " 'nodes': 983,\n",
       " 'anthropology': 984,\n",
       " 'encodings': 985,\n",
       " 'bigartm': 986,\n",
       " 'generated': 987,\n",
       " 'hesitant_fuzzy': 988,\n",
       " 'weakly-supervised': 989,\n",
       " 'subjective': 990,\n",
       " 'extrinsically': 991,\n",
       " 'world_view': 992,\n",
       " 'misleading_information': 993,\n",
       " 'improves': 994,\n",
       " 'age-matched': 995,\n",
       " 'parameter': 996,\n",
       " 'computational': 997,\n",
       " 'partition': 998,\n",
       " 'eli5': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity_embeddings(paper_reprs, class_reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78230924, 0.7538578 , 0.77479338, 0.80200045, 0.75536154])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8891159917311015,\n",
       " 0.9054942803449103,\n",
       " 0.8125112536457095,\n",
       " 0.8037140825601731,\n",
       " 0.7696589366397533]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cos_sim[c.papers[-1].id, c_id] for c_id, c in enumerate(taxo.root.children)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_d = {1:3, 4:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in temp_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,\n",
       " title : a comparison of classification methods for predicting deception in computer-mediated_communication ; abstract : the increased chance of deception in computer-mediated_communication and the potential risk of taking action based on deceptive information calls for automatic detection of deception . to achieve the ultimate_goal of automatic prediction of deception , we selected four common classification methods and empirically compared their performance in predicting deception . the deception and truth data were collected during two experimental studies . the results suggest that all of the four methods were promising for predicting deception with cues to deception . among them , neural_networks exhibited consistent performance and were robust across test settings . the comparisons also highlighted the importance of selecting important input variables and removing noise in an attempt to enhance the performance of classification methods . the selected cues offer both methodological and theoretical contributions to the body of deception and information systems research . © 2004 m.e . sharpe , inc .)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 3\n",
    "mapping[-1][id], taxo.collection[mapping[-1][id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mapped = []\n",
    "unmapped = []\n",
    "class_map = {i:[] for i in np.arange(len(taxo.root.children))}\n",
    "\n",
    "for p_id, l in enumerate(class_labels):\n",
    "    if len(l) <4:\n",
    "        all_mapped.append(p_id)\n",
    "        for c in l:\n",
    "            class_map[c].append(p_id)\n",
    "    else:\n",
    "        unmapped.append(p_id)\n",
    "\n",
    "class_map = {i:sorted(class_map[i], key=lambda x: -cos_sim[x, i]) for i in np.arange(len(taxo.root.children))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised_learning 445\n",
      "unsupervised_learning 463\n",
      "semi_supervised_learning 98\n",
      "deep_learning 104\n",
      "ensemble_methods 262\n"
     ]
    }
   ],
   "source": [
    "for c_id, c in enumerate(taxo.root.children):\n",
    "    print(c.label, len(class_map[c_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom_classes = np.argmax(np.diff(np.sort(cos_sim, axis=1), axis=1), axis=1) + 1\n",
    "\n",
    "# classes = np.argsort(cos_sim, axis=1)\n",
    "# class_labels = [classes[p_id][b:] for p_id, b in enumerate(bottom_classes)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[supervised_learning,\n",
       " unsupervised_learning,\n",
       " semi_supervised_learning,\n",
       " deep_learning,\n",
       " ensemble_methods]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0 2]\n",
      "20 [0 2]\n",
      "30 [0 2]\n",
      "32 [1 0 2]\n",
      "41 [1 0 2]\n",
      "49 [2]\n",
      "61 [3 0 2]\n",
      "63 [2]\n",
      "69 [2 0]\n",
      "74 [2]\n",
      "149 [1 0 2]\n",
      "152 [2]\n",
      "153 [0 1 2]\n",
      "155 [3 0 2]\n",
      "159 [0]\n",
      "215 [0]\n",
      "232 [1 0 2]\n",
      "235 [0 2]\n",
      "261 [1 0 2]\n",
      "291 [1 0 2]\n",
      "292 [1 2 0]\n",
      "293 [0 2]\n",
      "326 [1 0 2]\n",
      "341 [2]\n",
      "367 [1 0 2]\n",
      "380 [0 2]\n",
      "391 [2]\n",
      "404 [1 0 2]\n",
      "412 [0 2]\n",
      "413 [0]\n",
      "416 [0 1 2]\n",
      "419 [1 0 2]\n",
      "428 [0 1 2]\n",
      "430 [1 2 0]\n",
      "449 [1 0 2]\n",
      "451 [0 2]\n",
      "471 [0]\n",
      "475 [1 0 2]\n",
      "482 [1 0 2]\n",
      "500 [2]\n",
      "508 [0]\n",
      "510 [0 2]\n",
      "528 [2 0]\n",
      "536 [0]\n",
      "586 [2]\n",
      "597 [2]\n",
      "600 [2 0]\n",
      "616 [0 2]\n",
      "663 [0 1 2]\n",
      "670 [0 2]\n",
      "702 [0 2 1]\n",
      "703 [2]\n",
      "736 [0 2]\n",
      "758 [1 0 2]\n",
      "759 [0 2]\n",
      "766 [1 0 2]\n",
      "777 [0 2]\n",
      "794 [3 0 2]\n",
      "798 [1 0 2]\n",
      "803 [0 2]\n",
      "811 [2 0]\n",
      "849 [2 0]\n",
      "870 [2]\n",
      "928 [2 0]\n",
      "948 [0 1 2]\n",
      "950 [0 2]\n",
      "954 [1 0 2]\n",
      "957 [0 2]\n",
      "960 [0 1 2]\n",
      "967 [2 0]\n",
      "974 [2 0]\n",
      "978 [0 2]\n",
      "979 [0 1 2]\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for idx, l in enumerate(class_labels):\n",
    "    if len(l) < 4:\n",
    "        print(idx, l)\n",
    "        total += 1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41849622, 0.41452385, 0.4203428 , 0.39699356, 0.4142338 ],\n",
       "       [0.45134438, 0.44479938, 0.45326388, 0.44773577, 0.44168179],\n",
       "       [0.367142  , 0.36561152, 0.37268315, 0.34918534, 0.35883899],\n",
       "       ...,\n",
       "       [0.39107231, 0.38916193, 0.39409682, 0.37851251, 0.38478113],\n",
       "       [0.35289365, 0.35163603, 0.35420271, 0.32782443, 0.3481511 ],\n",
       "       [0.47117052, 0.46534621, 0.46529638, 0.44525649, 0.46789527]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 20, 11, 97, 16]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len([w for w in c.all_node_terms if w in static_emb]) for c in taxo.root.children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 33, 24, 105, 23]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(c.all_node_terms) for c in taxo.root.children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reprs = [average_with_harmonic_series(np.concatenate([static_emb[w].reshape((1,-1)) for w in c.all_node_terms if w in static_emb], axis=0)) for c in taxo.root.children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_terms = [w for w in taxo.collection[0].vocabulary if w in word2emb]\n",
    "ranked_tok = rank_by_significance(np.concatenate([word2emb[w].reshape((-1, 768)) for w in iv_terms], axis=0), class_reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{92: 0,\n",
       " 9: 1,\n",
       " 70: 2,\n",
       " 87: 3,\n",
       " 65: 4,\n",
       " 17: 5,\n",
       " 50: 6,\n",
       " 60: 7,\n",
       " 32: 8,\n",
       " 44: 9,\n",
       " 84: 10,\n",
       " 105: 11,\n",
       " 2: 12,\n",
       " 39: 13,\n",
       " 13: 14,\n",
       " 7: 15,\n",
       " 41: 16,\n",
       " 28: 17,\n",
       " 59: 18,\n",
       " 57: 19,\n",
       " 66: 20,\n",
       " 71: 21,\n",
       " 11: 22,\n",
       " 85: 23,\n",
       " 67: 24,\n",
       " 94: 25,\n",
       " 93: 26,\n",
       " 102: 27,\n",
       " 78: 28,\n",
       " 99: 29,\n",
       " 89: 30,\n",
       " 8: 31,\n",
       " 61: 32,\n",
       " 74: 33,\n",
       " 101: 34,\n",
       " 23: 35,\n",
       " 52: 36,\n",
       " 77: 37,\n",
       " 72: 38,\n",
       " 18: 39,\n",
       " 33: 40,\n",
       " 62: 41,\n",
       " 42: 42,\n",
       " 98: 43,\n",
       " 3: 44,\n",
       " 37: 45,\n",
       " 43: 46,\n",
       " 6: 47,\n",
       " 88: 48,\n",
       " 45: 49,\n",
       " 53: 50,\n",
       " 0: 51,\n",
       " 104: 52,\n",
       " 25: 53,\n",
       " 81: 54,\n",
       " 76: 55,\n",
       " 1: 56,\n",
       " 91: 57,\n",
       " 79: 58,\n",
       " 54: 59,\n",
       " 86: 60,\n",
       " 51: 61,\n",
       " 15: 62,\n",
       " 22: 63,\n",
       " 97: 64,\n",
       " 100: 65,\n",
       " 14: 66,\n",
       " 10: 67,\n",
       " 35: 68,\n",
       " 69: 69,\n",
       " 21: 70,\n",
       " 36: 71,\n",
       " 55: 72,\n",
       " 64: 73,\n",
       " 106: 74,\n",
       " 95: 75,\n",
       " 48: 76,\n",
       " 12: 77,\n",
       " 80: 78,\n",
       " 96: 79,\n",
       " 19: 80,\n",
       " 24: 81,\n",
       " 73: 82,\n",
       " 30: 83,\n",
       " 107: 84,\n",
       " 4: 85,\n",
       " 46: 86,\n",
       " 108: 87,\n",
       " 83: 88,\n",
       " 38: 89,\n",
       " 26: 90,\n",
       " 47: 91,\n",
       " 16: 92,\n",
       " 58: 93,\n",
       " 5: 94,\n",
       " 82: 95,\n",
       " 56: 96,\n",
       " 31: 97,\n",
       " 68: 98,\n",
       " 34: 99,\n",
       " 63: 100,\n",
       " 75: 101,\n",
       " 49: 102,\n",
       " 40: 103,\n",
       " 103: 104,\n",
       " 20: 105,\n",
       " 27: 106,\n",
       " 90: 107,\n",
       " 29: 108}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95788369, 0.93276707, 0.95489257, 0.94776222, 0.93760071]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_embeddings([static_emb[\"deep_learning\"].numpy()], class_reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 0; token: semi-supervised_learning\n",
      "rank: 1; token: multi-task_learning\n",
      "rank: 2; token: transfer_learning\n",
      "rank: 3; token: multi-class\n",
      "rank: 4; token: binary_classification\n",
      "rank: 5; token: supervised_learning\n",
      "rank: 6; token: prediction_accuracy\n",
      "rank: 7; token: hierarchies\n",
      "rank: 8; token: generalization\n",
      "rank: 9; token: outperforms\n",
      "rank: 10; token: classification_task\n",
      "rank: 11; token: nearest_neighbor\n",
      "rank: 12; token: mtl\n",
      "rank: 13; token: classifying\n",
      "rank: 14; token: training_examples\n",
      "rank: 15; token: datasets\n",
      "rank: 16; token: jointly\n",
      "rank: 17; token: task\n",
      "rank: 18; token: concept\n",
      "rank: 19; token: approach\n",
      "rank: 20; token: classification\n",
      "rank: 21; token: tasks\n",
      "rank: 22; token: abstract\n",
      "rank: 23; token: also\n",
      "rank: 24; token: problem\n",
      "rank: 25; token: methods\n",
      "rank: 26; token: using\n",
      "rank: 27; token: developed\n",
      "rank: 28; token: approaches\n",
      "rank: 29; token: namely\n",
      "rank: 30; token: especially\n",
      "rank: 31; token: independently\n",
      "rank: 32; token: classes\n",
      "rank: 33; token: work\n",
      "rank: 34; token: improvement\n",
      "rank: 35; token: paradigm\n",
      "rank: 36; token: performance\n",
      "rank: 37; token: models\n",
      "rank: 38; token: our\n",
      "rank: 39; token: hierarchical\n",
      "rank: 40; token: ,\n",
      "rank: 41; token: only\n",
      "rank: 42; token: and\n",
      "rank: 43; token: instead\n",
      "rank: 44; token: considerably\n",
      "rank: 45; token: each\n",
      "rank: 46; token: strength\n",
      "rank: 47; token: documents\n",
      "rank: 48; token: compare\n",
      "rank: 49; token: relationships\n",
      "rank: 50; token: several\n",
      "rank: 51; token: :\n",
      "rank: 52; token: two\n",
      "rank: 53; token: number\n",
      "rank: 54; token: traditional\n",
      "rank: 55; token: title\n",
      "rank: 56; token: develop\n",
      "rank: 57; token: we\n",
      "rank: 58; token: standard\n",
      "rank: 59; token: this\n",
      "rank: 60; token: use\n",
      "rank: 61; token: demonstrate\n",
      "rank: 62; token: )\n",
      "rank: 63; token: evaluate\n",
      "rank: 64; token: terms\n",
      "rank: 65; token: defining\n",
      "rank: 66; token: ;\n",
      "rank: 67; token: the\n",
      "rank: 68; token: show\n",
      "rank: 69; token: prediction\n",
      "rank: 70; token: is\n",
      "rank: 71; token: an\n",
      "rank: 72; token: when\n",
      "rank: 73; token: empirical\n",
      "rank: 74; token: solve\n",
      "rank: 75; token: there\n",
      "rank: 76; token: for\n",
      "rank: 77; token: single\n",
      "rank: 78; token: (\n",
      "rank: 79; token: results\n",
      "rank: 80; token: in\n",
      "rank: 81; token: ieee\n",
      "rank: 82; token: within\n",
      "rank: 83; token: different\n",
      "rank: 84; token: 2013\n",
      "rank: 85; token: ©\n",
      "rank: 86; token: a\n",
      "rank: 87; token: achieve\n",
      "rank: 88; token: few\n",
      "rank: 89; token: with\n",
      "rank: 90; token: established\n",
      "rank: 91; token: multiple\n",
      "rank: 92; token: are\n",
      "rank: 93; token: that\n",
      "rank: 94; token: learning\n",
      "rank: 95; token: wikipedia\n",
      "rank: 96; token: by\n",
      "rank: 97; token: .\n",
      "rank: 98; token: based\n",
      "rank: 99; token: between\n",
      "rank: 100; token: better\n",
      "rank: 101; token: across\n",
      "rank: 102; token: fewer\n",
      "rank: 103; token: per\n",
      "rank: 104; token: which\n",
      "rank: 105; token: of\n",
      "rank: 106; token: learned\n",
      "rank: 107; token: to\n",
      "rank: 108; token: against\n"
     ]
    }
   ],
   "source": [
    "for idx, rank in ranked_tok.items():\n",
    "    print(f\"rank: {rank}; token: {iv_terms[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 0; token: semi-supervised_learning\n",
      "rank: 1; token: multi-task_learning\n",
      "rank: 2; token: binary_classification\n",
      "rank: 3; token: transfer_learning\n",
      "rank: 4; token: multi-class\n",
      "rank: 5; token: supervised_learning\n",
      "rank: 6; token: prediction_accuracy\n",
      "rank: 7; token: hierarchies\n",
      "rank: 8; token: generalization\n",
      "rank: 9; token: outperforms\n",
      "rank: 10; token: nearest_neighbor\n",
      "rank: 11; token: classification_task\n",
      "rank: 12; token: classifying\n",
      "rank: 13; token: training_examples\n",
      "rank: 14; token: mtl\n",
      "rank: 15; token: datasets\n",
      "rank: 16; token: task\n",
      "rank: 17; token: jointly\n",
      "rank: 18; token: concept\n",
      "rank: 19; token: approach\n",
      "rank: 20; token: classification\n",
      "rank: 21; token: tasks\n",
      "rank: 22; token: abstract\n",
      "rank: 23; token: also\n",
      "rank: 24; token: problem\n",
      "rank: 25; token: our\n",
      "rank: 26; token: approaches\n",
      "rank: 27; token: especially\n",
      "rank: 28; token: independently\n",
      "rank: 29; token: developed\n",
      "rank: 30; token: methods\n",
      "rank: 31; token: using\n",
      "rank: 32; token: namely\n",
      "rank: 33; token: classes\n",
      "rank: 34; token: improvement\n",
      "rank: 35; token: models\n",
      "rank: 36; token: work\n",
      "rank: 37; token: instead\n",
      "rank: 38; token: each\n",
      "rank: 39; token: paradigm\n",
      "rank: 40; token: performance\n",
      "rank: 41; token: and\n",
      "rank: 42; token: ,\n",
      "rank: 43; token: strength\n",
      "rank: 44; token: documents\n",
      "rank: 45; token: only\n",
      "rank: 46; token: considerably\n",
      "rank: 47; token: hierarchical\n",
      "rank: 48; token: compare\n",
      "rank: 49; token: traditional\n",
      "rank: 50; token: we\n",
      "rank: 51; token: title\n",
      "rank: 52; token: number\n",
      "rank: 53; token: several\n",
      "rank: 54; token: relationships\n",
      "rank: 55; token: two\n",
      "rank: 56; token: :\n",
      "rank: 57; token: standard\n",
      "rank: 58; token: use\n",
      "rank: 59; token: develop\n",
      "rank: 60; token: evaluate\n",
      "rank: 61; token: this\n",
      "rank: 62; token: is\n",
      "rank: 63; token: prediction\n",
      "rank: 64; token: demonstrate\n",
      "rank: 65; token: show\n",
      "rank: 66; token: )\n",
      "rank: 67; token: ;\n",
      "rank: 68; token: when\n",
      "rank: 69; token: defining\n",
      "rank: 70; token: the\n",
      "rank: 71; token: there\n",
      "rank: 72; token: an\n",
      "rank: 73; token: solve\n",
      "rank: 74; token: ©\n",
      "rank: 75; token: empirical\n",
      "rank: 76; token: terms\n",
      "rank: 77; token: (\n",
      "rank: 78; token: with\n",
      "rank: 79; token: results\n",
      "rank: 80; token: in\n",
      "rank: 81; token: for\n",
      "rank: 82; token: different\n",
      "rank: 83; token: achieve\n",
      "rank: 84; token: 2013\n",
      "rank: 85; token: within\n",
      "rank: 86; token: single\n",
      "rank: 87; token: ieee\n",
      "rank: 88; token: established\n",
      "rank: 89; token: few\n",
      "rank: 90; token: are\n",
      "rank: 91; token: learning\n",
      "rank: 92; token: a\n",
      "rank: 93; token: that\n",
      "rank: 94; token: multiple\n",
      "rank: 95; token: between\n",
      "rank: 96; token: based\n",
      "rank: 97; token: better\n",
      "rank: 98; token: by\n",
      "rank: 99; token: .\n",
      "rank: 100; token: wikipedia\n",
      "rank: 101; token: across\n",
      "rank: 102; token: of\n",
      "rank: 103; token: per\n",
      "rank: 104; token: fewer\n",
      "rank: 105; token: which\n",
      "rank: 106; token: learned\n",
      "rank: 107; token: against\n",
      "rank: 108; token: to\n"
     ]
    }
   ],
   "source": [
    "for idx, rank in ranked_tok.items():\n",
    "    print(f\"rank: {rank}; token: {iv_terms[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[supervised_learning_methods,\n",
       " unsupervised_learning_methods,\n",
       " deep_learning_methods,\n",
       " ensemble_methods,\n",
       " transfer_learning_and_domain_adaptation]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class representations\n",
    "class_reprs = [c.emb for c in taxo.root.children]\n",
    "taxo.root.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78571259, 0.76266018, 0.8442609 , 0.87398888, 0.87627614]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_embeddings([sentence_model.encode(taxo.collection[7].title + \n",
    "                                                    \"[SEP]\" + \n",
    "                                                    taxo.collection[7].abstract)], \n",
    "                             class_reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[0.85956018, 0.90474514, 0.90334376, 0.88769259, 0.8412127 ]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [[] for i in taxo.root.children]\n",
    "unmapped = []\n",
    "\n",
    "for p in range(len(collection)):\n",
    "    class_freq = [0] * len(taxo.root.children)\n",
    "\n",
    "    for c_id, c in enumerate(taxo.root.children):\n",
    "        # how many total mentions of the node terms\n",
    "        class_freq[c_id] = np.sum([collection[p].vocabulary[ele] for ele in c.all_node_terms if ele in collection[p].vocabulary.keys()])\n",
    "    \n",
    "    nonzero_idx = np.nonzero(class_freq)[0]\n",
    "    if len(nonzero_idx) == 0:\n",
    "        unmapped.append(p)\n",
    "        continue\n",
    "\n",
    "    for i in nonzero_idx:\n",
    "        # score: class_i_mentions / log(total_len)\n",
    "        score = class_freq[i] / np.log(collection[p].length)\n",
    "        classes[i].append((score, p))\n",
    "\n",
    "classes = [sorted(c, reverse=True) for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unmapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bagging',\n",
       " 'boosting',\n",
       " 'stacking',\n",
       " 'voting',\n",
       " 'weighted_voting',\n",
       " 'random_forest',\n",
       " 'gradient_boosting',\n",
       " 'neural_network_ensemble',\n",
       " 'decision_tree_ensemble',\n",
       " 'support_vector_machine_ensemble',\n",
       " 'k_nearest_neighbors_ensemble',\n",
       " 'feature_bagging',\n",
       " 'feature_boosting',\n",
       " 'model_selection',\n",
       " 'hyperparameter_tuning',\n",
       " 'cross_validation',\n",
       " 'ensemble_methods',\n",
       " 'random_forests',\n",
       " 'base_learners',\n",
       " 'ensemble_learning',\n",
       " 'feature_combination',\n",
       " 'ensemble_techniques',\n",
       " 'cluster_based',\n",
       " 'rbf',\n",
       " 'mnb',\n",
       " 'dt',\n",
       " 'radial_basis_function',\n",
       " 'base_classifiers',\n",
       " 'gaussian_naive_bayes',\n",
       " 'multilayer_perceptron',\n",
       " 'c4.5',\n",
       " 'adaboost',\n",
       " 'attention_layer',\n",
       " 'feed-forward',\n",
       " 'thresholding',\n",
       " 'multinomial_logistic_regression',\n",
       " 'ensemble_classifier',\n",
       " 'memory-based',\n",
       " 'k-nearest_neighbor',\n",
       " 'nearest_neighbor',\n",
       " 'principal_component_analysis']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root.children[-1].all_node_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised_learning\n",
      "15 arabic_text_categorization via binary particle_swarm_optimization and support_vector_machines ; abstract : document_categorization concerns automatically assigning a category label to a text document , and has increasingly many applications , particularly in the domains of organizing , browsing and search in large_document_collections . it is typically achieved via machine_learning , where a model is built on the basis of a ( typically ) large collection of document features . feature_selection is critical in this process , since there are typically several thousand potential features ( distinct words or terms ) . here we explore binary particle_swarm_optimization ( bpso ) hybridized with either k-nearest-neighbour ( knn ) or a support_vector_machine ( svm ) , for feature_selection in arabic document_categorization tasks . comparison between feature_selection methods is done on the basis of using the selected features , in conjunction with each of svm , c4.5 and naive_bayes , to classify a holdout test set . using publicly available datasets , we show that the bpsosvm approach seems promising in this domain . we also analyse the sets of selected features and consider the differences between the types of feature that bpsoknn and bpsosvm tend to choose\n",
      "16 a novel approach for ontology-based dimensionality_reduction for web text document_classification ; abstract : dimensionality_reduction of feature_vector size plays a vital role in enhancing the text processing capabilities ; it aims in reducing the size of the feature_vector used in the mining tasks ( classification , clustering ... etc. ) . this paper proposes an efficient approach to be used in reducing the size of the feature_vector for web text document_classification process . this approach is based on using wordnet ontology , utilizing the benefit of its hierarchal structure , to eliminate words from the generated feature_vector that has no relation with any of wordnet lexical categories\n",
      "449 25th pacific-asia conference on knowledge discovery and data_mining , pakdd 2021 ; abstract : the proceedings contain 157 papers . the special focus in this conference is on knowledge discovery and data_mining . the topics include : self-supervised graph_representation learning with variational_inference ; manifold approximation and projection by maximizing graph information ; learning attention-based translational knowledge_graph embedding via nonlinear dynamic mapping ; multi-grained dependency_graph neural_network for chinese open_information_extraction ; human-understandable decision_making for visual_recognition ; lightcake : a lightweight framework for context-aware knowledge_graph embedding ; transferring domain_knowledge with an adviser in continuous tasks ; inferring hierarchical mixture structures : a bayesian nonparametric approach ; quality_control for hierarchical classification with incomplete annotations ; learning discriminative_features using multi-label dual_space ; universal representation for code ; autocluster : meta-learning based ensemble_method for automated unsupervised_clustering ; banditrank : learning to rank using contextual bandits ; a compressed and accelerated segnet for plant leaf disease segmentation : a differential_evolution based approach ; meta-context transformers for domain-specific response_generation ; a multi-task kernel learning algorithm for survival_analysis ; meta-data augmentation based search strategy through generative_adversarial_network for automl model_selection ; tree-capsule : tree-structured capsule_network for improving relation_extraction ; rule injection-based generative_adversarial imitation learning for knowledge_graph reasoning ; hierarchical self attention_based autoencoder for open-set human activity_recognition ; reinforced natural_language inference for distantly_supervised_relation classification ; self-supervised adaptive aggregator learning on graph ; sagcn : structure-aware graph convolution network for document-level relation_extraction ; addressing the class_imbalance problem in medical image_segmentation via accelerated tversky loss_function ; incorporating relational knowledge in explainable fake_news_detection\n",
      "373 estimating the generalization performance of polynomial svm classifier for text_categorization\n",
      "354 a hybrid documents classification based on svm and rough_sets\n",
      "280 automatic polarity identification on twitter using machine_learning ; abstract : this work presents a study of emotions to analyze the polarity of a set of data that was extracted from twitter , detailing each of the resources in the different forms that a language has , and to be able to observe feelings such as irony , sarcasm , and happiness , among others . this research can help us classify the polarity of each one of them deeply in the corpus that deals with this research work . experimental results conducted using different machine_learning methods are presented : support_vector_machines , naïve_bayes , logistic_regression , knn and random_forest , with which a classification system based on cross-validation was implemented . all experiments were performed in python . the results obtained are shown with two different corpus ; where the first set is made up of 10,653 tweets in total divided equally each with 3551 tweets with a positive , negative and neutral label\n",
      "184 margin maximization with feed-forward neural_networks : a comparative study with svm and adaboost\n",
      "236 classification of forensic autopsy reports through conceptual_graph-based document_representation model\n",
      "335 forestexter : an efficient random_forest algorithm for imbalanced text_categorization\n",
      "946 an efficient approach for ensemble of svm and ann for sentiment_classification\n",
      "\n",
      "\n",
      "unsupervised_learning\n",
      "111 deep graph neural_networks for text_classification task ; abstract : text_classification is to organizing documents into predetermined_categories , usually by machinery learn algorithms . it is a significant ways to organize and utilize the large amount of information that exists in unstructured_text format . text_classification is an important module in text processing , and its applications are also very extensive , such as garbage filtering , news classification , part-of-speech_tagging , and so on . with the continuous development of deep_learning in recent_years ! its applications are also very extensive , such as : garbage filtering , news classification , part-of-speech_tagging , and so on . but the text also has its own characteristics . according to the characteristics of the text , the general process of text_classification is : 1 . preprocessing ; 2 . text_representation and feature_selection ; 3 . construction of a classifier\n",
      "237 hybrid supervised clustering based ensemble scheme for text_classification\n",
      "144 a text feature_selection method using the improved mutual_information and information entropy\n",
      "159 a tensor space model-based deep_neural_network for text_classification ; abstract : most text_classification systems use machine_learning algorithms ; among these , naïve_bayes and support_vector_machine algorithms adapted to handle text data afford reasonable_performance . recently , given developments in deep_learning technology , several scholars have used deep_neural_networks ( recurrent and convolutional_neural_networks ) to improve text_classification . however , deep_learning-based text_classification has not greatly_improved performance compared to that of conventional algorithms . this is because a textual document is essentially expressed as a vector ( only ) , albeit with word dimensions , which compromises the inherent semantic information , even if the vector is ( appropriately ) transformed to add conceptual information . to solve this ‘ loss of term senses ’ problem , we develop a concept-driven deep_neural_network based upon our semantic tensor space model . the semantic tensor used for text_representation features a dependency between the term and the concept\n",
      "273 research convey on text_classification method based on deep_learning\n",
      "206 a component clustering_algorithm based on semantic similarity and optimization\n",
      "967 research progress of text_classification technology based on deep_learning\n",
      "734 a method of text_classification based on statistical technology and set_theory\n",
      "346 a comparative research of different granularities in korean text_classification ; abstract : text_classification is a process , which can make the specified documents group into several categories , predefined at the beginning through learning a series of rules or under the guidance of the goal function . this paper compared the subword-level , spacing-level of korean and the word-level , then analyzed the influence of the preprocessing of different granularities on the text_classification task of korean . after that , analyzed the results of classification linguistically . thus we can choose the proper granularity as the input to improve the classification effect . firstly , cut the corpus according to different granularities\n",
      "910 knowledge-based clustering scheme for collection management and retrieval of library books ; abstract : we propose a knowledge-based clustering scheme for grouping books in a library . such a grouping is achieved with the help of domain_knowledge in the form of the acm cr ( computing reviews ) category hierarchy . a new knowledge-based similarity_measure is defined and used in clustering books . the proposed scheme is useful in overcoming several problems associated with the existing book collection management and document_retrieval systems . more specifically , it can be used in : ( 1 ) helping the user select an appropriate collection of books in a library which contains the topics of interest ; ( 2 ) assigning a classification number to a new book ; ( 3 ) designing a more appropriate and uniform classification_scheme for books\n",
      "\n",
      "\n",
      "semi_supervised_learning\n",
      "122 meta_learning for few-shot joint intent_detection_and_slot-filling\n",
      "516 batch active_learning for text_classification and sentiment_analysis\n",
      "227 multi-domain active_learning for text_classification\n",
      "246 revisiting uncertainty-based query_strategies for active_learning with transformers\n",
      "773 cutting the error by half : investigation of very deep cnn and advanced training strategies for document_image_classification ; abstract : we present an exhaustive investigation of recent deep_learning architectures , algorithms , and strategies for the task of document_image_classification to finally reduce the error by more than half . existing_approaches , such as the deepdoc-classifier , apply standard convolutional_network architectures with transfer_learning from the object_recognition domain . the contribution of the paper is threefold : first , it investigates recently introduced very deep_neural_network architectures ( googlenet , vgg , resnet ) using transfer_learning ( from real images ) . second , it proposes transfer_learning from a huge set of document_images , i.e . 400\n",
      "661 protaugment : unsupervised diverse short-texts paraphrasing for intent_detection meta-learning\n",
      "338 exploiting the matching information in the support set for few shot event classification\n",
      "525 active_learning in automated text_classification : a case_study exploring bias in predicted model performance_metrics\n",
      "389 classifying syntactic errors in learner language\n",
      "777 enriching pre-trained_language_model with entity information for relation_classification\n",
      "\n",
      "\n",
      "deep_learning\n",
      "293 a multi-scale convolutional attention_based gru network for text_classification\n",
      "1 a novel model combining transformer and bi-lstm for news categorization ; abstract : news categorization ( nc ) , the aim of which is to identify distinct categories of news through analyzing the contents , has acquired substantial progress since deep_learning was introduced into the natural_language_processing ( nlp ) field . as a state-of-art model , transformer & # x2019\n",
      "776 an automatic method using hybrid neural_networks and attention_mechanism for software_bug triaging\n",
      "199 phrase2vec : phrase embedding based on parsing\n",
      "379 an integrated fuzzy neural_network with topic-aware auto-encoding for sentiment_analysis\n",
      "773 cutting the error by half : investigation of very deep cnn and advanced training strategies for document_image_classification ; abstract : we present an exhaustive investigation of recent deep_learning architectures , algorithms , and strategies for the task of document_image_classification to finally reduce the error by more than half . existing_approaches , such as the deepdoc-classifier , apply standard convolutional_network architectures with transfer_learning from the object_recognition domain . the contribution of the paper is threefold : first , it investigates recently introduced very deep_neural_network architectures ( googlenet , vgg , resnet ) using transfer_learning ( from real images ) . second , it proposes transfer_learning from a huge set of document_images , i.e . 400\n",
      "398 pseudo-labeling with transformers for improving question_answering systems\n",
      "449 25th pacific-asia conference on knowledge discovery and data_mining , pakdd 2021 ; abstract : the proceedings contain 157 papers . the special focus in this conference is on knowledge discovery and data_mining . the topics include : self-supervised graph_representation learning with variational_inference ; manifold approximation and projection by maximizing graph information ; learning attention-based translational knowledge_graph embedding via nonlinear dynamic mapping ; multi-grained dependency_graph neural_network for chinese open_information_extraction ; human-understandable decision_making for visual_recognition ; lightcake : a lightweight framework for context-aware knowledge_graph embedding ; transferring domain_knowledge with an adviser in continuous tasks ; inferring hierarchical mixture structures : a bayesian nonparametric approach ; quality_control for hierarchical classification with incomplete annotations ; learning discriminative_features using multi-label dual_space ; universal representation for code ; autocluster : meta-learning based ensemble_method for automated unsupervised_clustering ; banditrank : learning to rank using contextual bandits ; a compressed and accelerated segnet for plant leaf disease segmentation : a differential_evolution based approach ; meta-context transformers for domain-specific response_generation ; a multi-task kernel learning algorithm for survival_analysis ; meta-data augmentation based search strategy through generative_adversarial_network for automl model_selection ; tree-capsule : tree-structured capsule_network for improving relation_extraction ; rule injection-based generative_adversarial imitation learning for knowledge_graph reasoning ; hierarchical self attention_based autoencoder for open-set human activity_recognition ; reinforced natural_language inference for distantly_supervised_relation classification ; self-supervised adaptive aggregator learning on graph ; sagcn : structure-aware graph convolution network for document-level relation_extraction ; addressing the class_imbalance problem in medical image_segmentation via accelerated tversky loss_function ; incorporating relational knowledge in explainable fake_news_detection\n",
      "890 discriminative learning of generative_models : large_margin multinomial mixture_models for document_classification\n",
      "837 improving transformer-based end-to-end speech_recognition with connectionist temporal classification and language model integration\n",
      "\n",
      "\n",
      "ensemble_methods\n",
      "941 document_classification using symbolic classifiers ; abstract : in this paper , we present symbolic classifiers to classify text documents . we propose to use cluster_based symbolic_representation followed by symbolic feature_selection methods to classify text documents . in particular , we propose symbolic clustering approaches ; symbolic cluster_based without feature_selection ; symbolic cluster_based with feature_selection ( using similarity_measure )\n",
      "827 application of bagging_ensemble classifier based on genetic_algorithm in the text_classification of railway fault hazards\n",
      "47 comparative_analysis of binary classifiers on an array of scientific_publications\n",
      "141 predicting software defect severity_level using sentence_embedding and ensemble_learning\n",
      "237 hybrid supervised clustering based ensemble scheme for text_classification\n",
      "280 automatic polarity identification on twitter using machine_learning ; abstract : this work presents a study of emotions to analyze the polarity of a set of data that was extracted from twitter , detailing each of the resources in the different forms that a language has , and to be able to observe feelings such as irony , sarcasm , and happiness , among others . this research can help us classify the polarity of each one of them deeply in the corpus that deals with this research work . experimental results conducted using different machine_learning methods are presented : support_vector_machines , naïve_bayes , logistic_regression , knn and random_forest , with which a classification system based on cross-validation was implemented . all experiments were performed in python . the results obtained are shown with two different corpus ; where the first set is made up of 10,653 tweets in total divided equally each with 3551 tweets with a positive , negative and neutral label\n",
      "512 enhanced malay sentiment_analysis with an ensemble classification machine_learning approach\n",
      "59 an ensemble model for stance_detection in social_media texts\n",
      "817 an intelligent hybrid sentiment_analyzer for personal protective medical equipments based on word_embedding technique : the covid-19 era\n",
      "732 a scalable meta-classifier combining search and classification techniques for multi-level text_categorization\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(classes):\n",
    "    print(taxo.root.children[idx])\n",
    "    for p in i[:10]:\n",
    "        print(p[-1], collection[p[-1]].title)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/updated_qa_phrases.json\", \"w\", encoding='utf-8') as f:\n",
    "    json_out = {}\n",
    "    json_out[taxo.root.label] = {\"description\": taxo.root.desc, \"seeds\": taxo.root.seeds, \"terms\": taxo.root.all_node_terms}\n",
    "    for c in taxo.root.children:\n",
    "        json_out[c.label] = {\"description\": c.desc, \"seeds\": c.seeds, \"terms\": c.all_node_terms}\n",
    "    json.dump(json_out, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse",
   "language": "python",
   "name": "inverse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
