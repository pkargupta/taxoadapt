{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3,4\"\n",
    "from taxonomy import Taxonomy, Paper\n",
    "from utils import filter_phrases\n",
    "import subprocess\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.track = \"Text Classification\"\n",
    "        self.dim = \"Methodology\"\n",
    "        self.input_file = \"datasets/sample_1k.txt\"\n",
    "        self.iters = 4\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = []\n",
    "id = 0\n",
    "with open(args.input_file, \"r\") as f:\n",
    "    papers = f.read().strip().splitlines()\n",
    "    for p in papers:\n",
    "        title = re.findall(r'title\\s*:\\s*(.*) ; ', p, re.IGNORECASE)[0]\n",
    "        abstract = re.findall(r'abstract\\s*:\\s*(.*)', p, re.IGNORECASE)[0]\n",
    "        collection.append(Paper(id, title, abstract))\n",
    "        id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Taxonomy Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Types of Methodology Proposed in Text Classification Research Papers': {'description': None, 'seeds': None, 'terms': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering', 'self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn', 'convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting'], 'supervised_learning': {'description': 'Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.', 'seeds': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting'], 'terms': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting']}, 'unsupervised_learning': {'description': 'Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.', 'seeds': ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering'], 'terms': ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering']}, 'semi_supervised_learning': {'description': 'Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.', 'seeds': ['self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn'], 'terms': ['self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn']}, 'deep_learning': {'description': 'Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.', 'seeds': ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning', 'multi_task_learning'], 'terms': ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning', 'multi_task_learning']}, 'ensemble_methods': {'description': 'Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.', 'seeds': ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting', 'model_selection', 'hyperparameter_tuning', 'cross_validation'], 'terms': ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting', 'model_selection', 'hyperparameter_tuning', 'cross_validation']}}}\n"
     ]
    }
   ],
   "source": [
    "# input: track, dimension -> get base taxonomy (2 levels) -> Class Tree, Class Node (description, seed words)\n",
    "\n",
    "taxo = Taxonomy(args.track, args.dim)\n",
    "base_taxo = taxo.buildBaseTaxo(levels=1, num_terms=20)\n",
    "\n",
    "print(base_taxo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the input keywords file for seetopic -> get phrases -> filter using LLM\n",
    "dir_name = (args.track + \"_\" + args.dim).lower().replace(\" \", \"_\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}\"):\n",
    "    os.makedirs(f\"SeeTopic/{dir_name}\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}/{dir_name}.txt\"):\n",
    "    shutil.copyfile(args.input_file, f\"SeeTopic/{dir_name}/{dir_name}.txt\")\n",
    "\n",
    "## get first level of children\n",
    "children_with_terms = taxo.root.getChildren(terms=True)\n",
    "with open(f\"SeeTopic/{dir_name}/keywords_0.txt\", \"w\") as f:\n",
    "    for idx, c in enumerate(children_with_terms):\n",
    "        str_c = \",\".join(c[1])\n",
    "        f.write(f\"{idx}:{c[0]},{str_c}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Types of Methodology Proposed in Text Classification Research Papers\": {\"description\": null, \"seeds\": null, \"terms\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\", \"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\", \"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\"], \"supervised_learning\": {\"description\": \"Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.\", \"seeds\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\"], \"terms\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\"]}, \"unsupervised_learning\": {\"description\": \"Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.\", \"seeds\": [\"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\"], \"terms\": [\"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\"]}, \"semi_supervised_learning\": {\"description\": \"Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.\", \"seeds\": [\"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\"], \"terms\": [\"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\"]}, \"deep_learning\": {\"description\": \"Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.\", \"seeds\": [\"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"transfer_learning\", \"multi_task_learning\"], \"terms\": [\"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"transfer_learning\", \"multi_task_learning\"]}, \"ensemble_methods\": {\"description\": \"Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.\", \"seeds\": [\"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"random_forest\", \"gradient_boosting\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\", \"model_selection\", \"hyperparameter_tuning\", \"cross_validation\"], \"terms\": [\"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"random_forest\", \"gradient_boosting\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\", \"model_selection\", \"hyperparameter_tuning\", \"cross_validation\"]}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phrase Mining for Level 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Get PLM Embeddings===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5359/5359 [00:43<00:00, 123.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Iter 0: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_1/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\t\n",
      "transfer_learning\tactive_learning\tsupervised_learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\t\n",
      "bagging\tboosting\tstacking\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_1/res_cate.txt\n",
      "\u001b[32m===Iter 1: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 2: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 2: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_2/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tself_organizing_map\tterm_weighting\tsupervised_term_weighting\t\n",
      "transfer_learning\tactive_learning\tsupervised_learning\tmeta_learning\tmeta-learning\trepresentation_learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_2/res_cate.txt\n",
      "\u001b[32m===Iter 2: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 3: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 3: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_3/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tneural_networks\tfeature_selection\tfeature_engineering\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tterm_weighting\tself_organizing_map\tsupervised_term_weighting\tclustering_algorithm\tstring_vectors\tmutual_information\t\n",
      "transfer_learning\tactive_learning\tmeta-learning\tmeta_learning\tsupervised_learning\trepresentation_learning\tlabeling_cost\tintent_detection\tadaptively\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\tcross-layer\tmasked_language_model\tgated_recurrent_unit\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\trandom_forests\tfeature_combination\tbase_learners\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_3/res_cate.txt\n",
      "\u001b[32m===Iter 3: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 4: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 4: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_4/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tneural_networks\tfeature_selection\tfeature_engineering\tdata_augmentation\tcross_validation\tbagging\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tself_organizing_map\tterm_weighting\tstring_vectors\tmutual_information\tclustering_algorithm\tsupervised_term_weighting\tnumerical_vectors\tsimilarity_measure\tstring_vector\t\n",
      "transfer_learning\tactive_learning\tmeta-learning\tmeta_learning\tlabeling_cost\tintent_detection\tsupervised_learning\trepresentation_learning\tadaptively\ttraining_examples\tjoint_modeling\tlow_dimensional\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\tcross-layer\tgated_recurrent_unit\tmasked_language_model\tneural_architecture\tdependency_graph\tbi-directional_long_short-term_memory\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\trandom_forests\tfeature_combination\tbase_learners\tensemble_learning\tcluster_based\tensemble_techniques\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_4/res_cate.txt\n",
      "\u001b[32m===Iter 4: Ensemble===\u001b[m\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"./SeeTopic\")\n",
    "subprocess.check_call(['./seetopic.sh', dir_name, str(args.iters), \"bert_full_ft\"])\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "supervised_learning: [supervised_learning, naive_bayes, decision_trees, random_forest, support_vector_machines, logistic_regression, k_nearest_neighbors, neural_networks, feature_selection, feature_engineering, data_augmentation, cross_validation, bagging, boosting, stacking, voting, feature_space, knn, document_frequency, weighting, statistical_methods, classification_algorithms, feature_weights, feature_vector, feature_values, membership, category_labels, maximum_entropy, feature_reduction, regularization, determination]\n",
      "---\n",
      "---\n",
      "unsupervised_learning: [unsupervised_learning, latent_semantic_analysis, topic_modeling, string_vectors, similarity_measure, numerical_vectors, self_organizing_map, clustering_algorithm, term_weighting, mutual_information, string_vector, vector_space_model, feature_transformation, vectorization, gibbs_sampling, word_vectors, feature_vectors, lsi, word_vector, som, sentence_similarity, cluster_analysis, vsm, latent_dirichlet_allocation, lsa, text_classification, clustering, faceted_classification, term_frequency]\n",
      "---\n",
      "---\n",
      "semi_supervised_learning: [semi_supervised_learning, transfer_learning, active_learning, meta_learning, intent_detection, joint_modeling, representation_learning, supervised_learning, contrastive_learning, ensemble_models, classifier_ensemble, intent_classification, relation_classification, trigger_detection, weakly_supervised, distant_supervised, multi_labeled, n_way, phenotype, annotator, annotators, crowdsourcing, learner, autoencoder, significant_impact, deception_detection]\n",
      "---\n",
      "---\n",
      "deep_learning: ['deep_learning', 'convolutional_neural_networks','recurrent_neural_networks', 'attention_mechanism', 'transformers', 'word_embeddings', 'transfer_learning', 'gated_recurrent_unit','masked_language_model', 'neural_architecture', 'word_representations', 'attention_based','mca-gru', 'nle', 'gru','multi-grained', 'attention_network', 'neural_network', 'protaugment', 'deep_neural_network', 'generative_model','self-attention','seq2seq', 'lm','multi-classifier', 'context-aware', 'artificial_neural_network', 'hidden_layer', 'cmcn', 'tensor', 'bi-directional_lstm','sequence-based', 'pre-trained_language_model', 'pseudo', 'task-specific', 'elmo', 'gated', 'generative','model-agnostic']\n",
      "---\n",
      "---\n",
      "ensemble_methods: [ensemble_methods, bagging, boosting, stacking, voting, random_forest, random_forests, base_learners, ensemble_learning, cluster_based, feature_combination, ensemble_techniques, rule_induction, feature_mapping, radial_basis_function, linear_kernel, base_classifiers, capsule_networks, gaussian_naive_bayes, k-nearest_neighbour, mnb, multilayer_perceptron, mlp-based, c4.5, attention_layer, vote, thresholding, adaptive_neuro-fuzzy, multinomial_logistic_regression, ensemble_classifier, adaboost, k-nearest_neighbor, rf]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "with open(f\"./SeeTopic/{dir_name}/keywords_seetopic.txt\", \"r\") as f:\n",
    "    children_phrases = [i.strip().split(\":\")[1].split(\",\") for i in f.readlines()]\n",
    "    filtered_children_phrases = []\n",
    "    for c_id, c in enumerate(taxo.root.children):\n",
    "        # filter the child phrases\n",
    "        child_phrases = filter_phrases(c, f\"{c}: {children_phrases[c_id]}\\n\")\n",
    "        filtered_children_phrases.append(child_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_id, c in enumerate(taxo.root.children):\n",
    "    c.addTerms(filtered_children_phrases[c_id], addToParent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get initial, exact-matching pool of papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_bayes',\n",
       " 'decision_trees',\n",
       " 'random_forest',\n",
       " 'support_vector_machines',\n",
       " 'logistic_regression',\n",
       " 'k_nearest_neighbors',\n",
       " 'gradient_boosting',\n",
       " 'neural_networks',\n",
       " 'feature_selection',\n",
       " 'feature_engineering',\n",
       " 'data_augmentation',\n",
       " 'cross_validation',\n",
       " 'hyperparameter_tuning',\n",
       " 'model_selection',\n",
       " 'ensemble_methods',\n",
       " 'bagging',\n",
       " 'boosting',\n",
       " 'stacking',\n",
       " 'voting',\n",
       " 'weighted_voting',\n",
       " 'supervised_learning',\n",
       " 'feature_space',\n",
       " 'knn',\n",
       " 'document_frequency',\n",
       " 'weighting',\n",
       " 'statistical_methods',\n",
       " 'classification_algorithms',\n",
       " 'feature_weights',\n",
       " 'feature_vector',\n",
       " 'feature_values',\n",
       " 'membership',\n",
       " 'category_labels',\n",
       " 'maximum_entropy',\n",
       " 'feature_reduction',\n",
       " 'regularization',\n",
       " 'determination']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root.children[0].all_node_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'Paper' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m taxo\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m----> 2\u001b[0m     isRepr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mele\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mele\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_node_terms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isRepr:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(c\u001b[38;5;241m.\u001b[39mlabel)\n",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m taxo\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m----> 2\u001b[0m     isRepr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[43mele\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ele \u001b[38;5;129;01min\u001b[39;00m c\u001b[38;5;241m.\u001b[39mall_node_terms)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isRepr:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(c\u001b[38;5;241m.\u001b[39mlabel)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'Paper' is not iterable"
     ]
    }
   ],
   "source": [
    "for c in taxo.root.children:\n",
    "    isRepr = any(ele in collection[0] for ele in c.all_node_terms)\n",
    "    if isRepr:\n",
    "        print(c.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse",
   "language": "python",
   "name": "inverse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
