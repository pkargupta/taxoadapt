{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3,4,5,6\"\n",
    "from collections import Counter\n",
    "from taxonomy import Taxonomy, Paper\n",
    "from utils import filter_phrases\n",
    "import subprocess\n",
    "import shutil\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.track = \"Text Classification\"\n",
    "        self.dim = \"Methodology\"\n",
    "        self.input_file = \"datasets/sample_1k.txt\"\n",
    "        self.iters = 4\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = []\n",
    "# id = 0\n",
    "# with open(args.input_file, \"r\") as f:\n",
    "#     papers = f.read().strip().splitlines()\n",
    "#     for p in papers:\n",
    "#         title = re.findall(r'title\\s*:\\s*(.*) ; ', p, re.IGNORECASE)[0]\n",
    "#         abstract = re.findall(r'abstract\\s*:\\s*(.*)', p, re.IGNORECASE)[0]\n",
    "#         collection.append(Paper(id, title, abstract))\n",
    "#         id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Taxonomy Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Types of Methodology Proposed in Text Classification Research Papers': {'description': None, 'seeds': None, 'terms': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering', 'self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn', 'convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting'], 'supervised_learning': {'description': 'Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.', 'seeds': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting'], 'terms': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting']}, 'unsupervised_learning': {'description': 'Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.', 'seeds': ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering'], 'terms': ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering']}, 'semi_supervised_learning': {'description': 'Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.', 'seeds': ['self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn'], 'terms': ['self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn']}, 'deep_learning': {'description': 'Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.', 'seeds': ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning', 'multi_task_learning'], 'terms': ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning', 'multi_task_learning']}, 'ensemble_methods': {'description': 'Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.', 'seeds': ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting', 'model_selection', 'hyperparameter_tuning', 'cross_validation'], 'terms': ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting', 'model_selection', 'hyperparameter_tuning', 'cross_validation']}}}\n"
     ]
    }
   ],
   "source": [
    "# input: track, dimension -> get base taxonomy (2 levels) -> Class Tree, Class Node (description, seed words)\n",
    "\n",
    "taxo = Taxonomy(args.track, args.dim, args.input_file)\n",
    "base_taxo = taxo.buildBaseTaxo(levels=1, num_terms=20)\n",
    "\n",
    "print(base_taxo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the input keywords file for seetopic -> get phrases -> filter using LLM\n",
    "dir_name = (args.track + \"_\" + args.dim).lower().replace(\" \", \"_\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}\"):\n",
    "    os.makedirs(f\"SeeTopic/{dir_name}\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}/{dir_name}.txt\"):\n",
    "    shutil.copyfile(args.input_file, f\"SeeTopic/{dir_name}/{dir_name}.txt\")\n",
    "\n",
    "## get first level of children\n",
    "children_with_terms = taxo.root.getChildren(terms=True)\n",
    "with open(f\"SeeTopic/{dir_name}/keywords_0.txt\", \"w\") as f:\n",
    "    for idx, c in enumerate(children_with_terms):\n",
    "        str_c = \",\".join(c[1])\n",
    "        f.write(f\"{idx}:{c[0]},{str_c}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Types of Methodology Proposed in Text Classification Research Papers\": {\"description\": null, \"seeds\": null, \"terms\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\", \"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\", \"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\"], \"supervised_learning\": {\"description\": \"Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.\", \"seeds\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\"], \"terms\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\"]}, \"unsupervised_learning\": {\"description\": \"Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.\", \"seeds\": [\"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\"], \"terms\": [\"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\"]}, \"semi_supervised_learning\": {\"description\": \"Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.\", \"seeds\": [\"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\"], \"terms\": [\"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\"]}, \"deep_learning\": {\"description\": \"Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.\", \"seeds\": [\"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"transfer_learning\", \"multi_task_learning\"], \"terms\": [\"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"transfer_learning\", \"multi_task_learning\"]}, \"ensemble_methods\": {\"description\": \"Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.\", \"seeds\": [\"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"random_forest\", \"gradient_boosting\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\", \"model_selection\", \"hyperparameter_tuning\", \"cross_validation\"], \"terms\": [\"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"random_forest\", \"gradient_boosting\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\", \"model_selection\", \"hyperparameter_tuning\", \"cross_validation\"]}}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phrase Mining for Level 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Get PLM Embeddings===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5359/5359 [00:44<00:00, 121.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Iter 0: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_1/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\t\n",
      "transfer_learning\tactive_learning\tsupervised_learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\t\n",
      "bagging\tboosting\tstacking\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_1/res_cate.txt\n",
      "\u001b[32m===Iter 1: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 2: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 2: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_2/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tterm_weighting\tself_organizing_map\tclustering_algorithm\t\n",
      "transfer_learning\tactive_learning\tsupervised_learning\tmeta_learning\trepresentation_learning\tmeta-learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_2/res_cate.txt\n",
      "\u001b[32m===Iter 2: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 3: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 3: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_3/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tneural_networks\tfeature_selection\tfeature_engineering\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tself_organizing_map\tterm_weighting\tclustering_algorithm\tstring_vectors\tsupervised_term_weighting\tensemble_classifier\t\n",
      "transfer_learning\tactive_learning\tmeta-learning\tmeta_learning\tsupervised_learning\trepresentation_learning\tlabeling_cost\tintent_detection\ttraining_examples\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\tcross-layer\tneural_architecture\tgated_recurrent_unit\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\trandom_forests\tensemble_learning\tfeature_combination\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_3/res_cate.txt\n",
      "\u001b[32m===Iter 3: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 4: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 4: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_4/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tneural_networks\tfeature_selection\tfeature_engineering\tdata_augmentation\tcross_validation\tbagging\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tensemble_classifier\tself_organizing_map\tterm_weighting\tstring_vectors\tclustering_algorithm\tsupervised_term_weighting\tnumerical_vectors\tsimilarity_measure\tgenetic_algorithm\t\n",
      "transfer_learning\tactive_learning\tmeta-learning\tmeta_learning\tlabeling_cost\tintent_detection\trepresentation_learning\tsupervised_learning\ttraining_examples\tsubstantially\tdeep_learning\tlabeling_efforts\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\tcross-layer\tgated_recurrent_unit\tneural_architecture\tmasked_language_model\tdependency_graph\ttextual_representation\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\trandom_forests\tfeature_combination\tensemble_learning\trule_induction\tbase_learners\tcluster_based\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_4/res_cate.txt\n",
      "\u001b[32m===Iter 4: Ensemble===\u001b[m\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"./SeeTopic\")\n",
    "subprocess.check_call(['./seetopic.sh', dir_name, str(args.iters), \"bert_full_ft\"])\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./SeeTopic/{dir_name}/keywords_seetopic.txt\", \"r\") as f:\n",
    "    children_phrases = [i.strip().split(\":\")[1].split(\",\") for i in f.readlines()]\n",
    "    filtered_children_phrases = []\n",
    "    for c_id, c in enumerate(taxo.root.children):\n",
    "        # filter the child phrases\n",
    "        child_phrases = filter_phrases(c, f\"{c}: {children_phrases[c_id]}\\n\")\n",
    "        filtered_children_phrases.append(child_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_id, c in enumerate(taxo.root.children):\n",
    "    c.addTerms(filtered_children_phrases[c_id], addToParent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get initial, exact-matching pool of papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bagging',\n",
       " 'boosting',\n",
       " 'stacking',\n",
       " 'voting',\n",
       " 'weighted_voting',\n",
       " 'random_forest',\n",
       " 'gradient_boosting',\n",
       " 'neural_network_ensemble',\n",
       " 'decision_tree_ensemble',\n",
       " 'support_vector_machine_ensemble',\n",
       " 'k_nearest_neighbors_ensemble',\n",
       " 'feature_bagging',\n",
       " 'feature_boosting',\n",
       " 'model_selection',\n",
       " 'hyperparameter_tuning',\n",
       " 'cross_validation',\n",
       " 'ensemble_methods',\n",
       " 'base_learners',\n",
       " 'feature_combination',\n",
       " 'ensemble_learning',\n",
       " 'classifier_ensemble']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root.children[-1].all_node_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7,\n",
       "  title: automatic polarity identification on twitter using machine_learning; abstract: this work presents a study of emotions to analyze the polarity of a set of data that was extracted from twitter , detailing each of the resources in the different forms that a language has , and to be able to observe feelings such as irony , sarcasm , and happiness , among others . this research can help us classify the polarity of each one of them deeply in the corpus that deals with this research work . experimental results conducted using different machine_learning methods are presented : support_vector_machines , naïve_bayes , logistic_regression , knn and random_forest , with which a classification system based on cross-validation was implemented . all experiments were performed in python . the results obtained are shown with two different corpus ; where the first set is made up of 10,653 tweets in total divided equally each with 3551 tweets with a positive , negative and neutral label ; while the second set was handled with 10 % of all the tweets contained in the database mentioned in the article , where the first set shows a polarity precision of 74.9 % , having logistic_regression as the best classifier using the classification scenario known as cross_validation , while the second set shows an accuracy of 78.5 % , also having random_forest as the best classifier using cross_validation as the best classification scenario .),\n",
       " (7,\n",
       "  title: distributed text feature_selection based on bat algorithm optimization; abstract: the feature_selection effect directly_affects the classification_accuracy of the text . this paper introduces a new text feature_selection method based on bat optimization . this method uses the traditional feature_selection method to pre-select the original features , and then uses the bat group algorithm to optimize the pre-selected features in binary code form , and uses the classification_accuracy as the individual fitness . however , when the amount of text information is large , the execution time of the single machine is long . according to this shortcoming , combining the bat algorithm and the spark parallel_computing framework , the text feature_selection algorithm sbatfs is proposed . the algorithm combines the good search performance of the bat algorithm with the distributed and efficient calculation speed to realize the efficient solution of the text feature_selection optimization model . the results show that compared with the traditional feature_selection method , after sbatfs is used for feature optimization , the classification_accuracy is effectively improved .),\n",
       " (6,\n",
       "  title: comparative_analysis of binary classifiers on an array of scientific_publications; abstract: binary classifiers are studies on balanced text samples . the samplings are formed from scientific_publications in the field of computer science ( computer science ) . the first class contains articles on “ text data_mining ” ( the “ tdm ” class ) , the second one contains works on other topics of computer science ( the “ non-tdm ” class ) . all the main_stages of preliminary processing of text documents are considered , models of their presentation are analyzed . the problem of binary classification is formulated and the quality indicators used in the study are given . a method of sampling from the russian digital_library ( elibrary ) is proposed . the generated sampling consists of bibliographic descriptions of documents ( title , abstract and keywords ) . an exploratory_analysis was carried out and the sampling structure was studied . “ term clouds ” for two classes are constructed and analyzed , documents are visualized using the method of stochastic embedding of neighbors with t-distribution ( t-sne ) . based on the review and analysis of known classifiers , the following methods were selected for the study : the k-nearest_neighbor method , random_forest , gradient boosting , logistic_regression , and the support_vector method . profile methods based on the construction of a vector ( profile ) of the most informative terms determined by the frequency of occurrence of terms and classes are also used in the study . the parameters of the methods were configured using a five-fold_cross-validation . the best quality of classification in our sampling demonstrated the methods using the ensemble ( collective ) decision-making principle ( random_forest , gradient boosting ) , as well as the support_vector method . the best classifier , gradient boosting , had the proportion of correct answers ( accuracy ) about 0.98 , recall and precision about 0.99 . the other ( simpler ) methods used in the study also generally showed rather good quality of classification ( for the least accurate k-nearest_neighbor method accuracy , recall and precision were 0.90 , 0.81 , and 0.91 , respectively ) .),\n",
       " (6,\n",
       "  title: a cfs-based feature_weighting approach to naive_bayes text classifiers; abstract: recent work in supervised_learning has shown that naive_bayes text classifiers with strong assumptions of independence among features , such as multinomial_naive_bayes ( mnb ) , complement naive_bayes ( cnb ) and the one-versus-all-but-one model ( ova ) , have achieved remarkable classification performance . this fact raises the question of whether a naive_bayes text classifier with less restrictive assumptions can perform even better . responding to this question , we firstly evaluate the correlation-based feature_selection ( cfs ) approach in this paper and find that it performs even worse than the original versions . then , we propose a cfs-based feature_weighting approach to these naive_bayes text classifiers . we call our feature weighted versions fwmnb , fwcnb and fwova respectively . our proposed approach weakens the strong assumptions of independence among features by weighting the correlated features . the experimental results on a large suite of benchmark_datasets show that our feature weighted versions significantly_outperform the original versions in terms of classification_accuracy . ©_2014_springer_international_publishing_switzerland .),\n",
       " (6,\n",
       "  title: improved particle_swarm_optimization algorithm and its application in text feature_selection; abstract: text feature_selection is an importance step in text_classification and directly_affects the classification performance . classic_feature_selection methods mainly include document_frequency ( df ) , information gain ( ig ) , mutual_information ( mi ) , chi-square_test ( chi ) . theoretically , these methods are difficult to get improvement due to the deficiency of their mathematical_models . in order to further improve effect of feature_selection , many researches try to add intelligent optimization_algorithms into feature_selection method , such as improved ant_colony algorithm and genetic_algorithms , etc . compared to the ant_colony algorithm and genetic_algorithms , particle_swarm_optimization algorithm ( pso ) is simpler to implement and can find the optimal point quickly . thus , this paper attempt to improve the effect of text feature_selection through pso . by analyzing current achievements of improved pso and characteristic of classic_feature_selection methods , we have done many explorations in this paper . above all , we selected the common pso model , the two improved pso models based respectively on functional inertia weight and constant constriction factor to optimize feature_selection methods . afterwards , according to constant constriction factor , we constructed a new functional constriction factor and added it into traditional pso model . finally , we proposed two improved pso models based on both functional constriction factor and functional inertia weight , they are respectively the synchronously improved pso model and the asynchronously improved pso model . in our experiments , chi was selected as the basic feature_selection method . we improved chi through using the six pso models mentioned above . the experiment results and significance tests show that the asynchronously improved pso model is the best one among all models both in the effect of text_classification and in the stability of different dimensions .),\n",
       " (5,\n",
       "  title: arabic_text_categorization via binary particle_swarm_optimization and support_vector_machines; abstract: document_categorization concerns automatically assigning a category label to a text document , and has increasingly many applications , particularly in the domains of organizing , browsing and search in large_document_collections . it is typically achieved via machine_learning , where a model is built on the basis of a ( typically ) large collection of document features . feature_selection is critical in this process , since there are typically several thousand potential features ( distinct words or terms ) . here we explore binary particle_swarm_optimization ( bpso ) hybridized with either k-nearest-neighbour ( knn ) or a support_vector_machine ( svm ) , for feature_selection in arabic document_categorization tasks . comparison between feature_selection methods is done on the basis of using the selected features , in conjunction with each of svm , c4.5 and naive_bayes , to classify a holdout test set . using publicly available datasets , we show that the bpsosvm approach seems promising in this domain . we also analyse the sets of selected features and consider the differences between the types of feature that bpsoknn and bpsosvm tend to choose ; this leads to speculations concerning the appropriate feature_selection strategy , based on the relationship between the classes in the document_categorization task at hand .),\n",
       " (5,\n",
       "  title: forestexter : an efficient random_forest algorithm for imbalanced text_categorization; abstract: in this paper , we propose a new random_forest ( rf ) based ensemble_method , forestexter , to solve the imbalanced text_categorization problems . rf has shown great_success in many real-world applications . however , the problem of learning from text data with class_imbalance is a relatively new challenge that needs to be addressed . a rf algorithm tends to use a simple random_sampling of features in building their decision_trees . as a result , it selects many subspaces that contain few , if any , informative_features for the minority_class . furthermore , the gini measure for data splitting is considered to be skew sensitive and bias towards the majority_class . due to the inherent complex characteristics of imbalanced text datasets , learning rf from such data requires new approaches to overcome challenges related to feature_subspace selection and cut-point choice while performing node splitting . to this end , we propose a new tree induction method that selects splits , both feature_subspace selection and splitting criterion , for rf on imbalanced text data . the key idea is to stratify features into two groups and to generate effective term_weighting for the features . one group contains positive features for the minority_class and the other one contains the negative features for the majority_class . then , for feature_subspace selection , we effectively select features from each group based on the term_weights . the advantage of our approach is that each subspace contains adequate informative_features for both minority and majority_classes . one difference between our proposed tree induction method and the classical rf method is that our method uses support_vector_machines ( svm ) classifier to split the training_data into smaller and more balance subsets at each tree node , and then successively retrains the svm classifiers on the data partitions to refine the model while moving down the tree . in this way , we force the classifiers to learn from refined feature subspaces and data subsets to fit the imbalanced_data better . hence , the tree model becomes more robust for text_categorization task with imbalanced_dataset . experimental results on various benchmark imbalanced text datasets ( reuters-21578 , ohsumed , and imbalanced 20_newsgroup ) consistently demonstrate the effectiveness of our proposed forestexter method . the performance of our proposed approach is competitive against the standard random_forest and different variants of svm algorithms . © 2014 elsevier b.v. all rights_reserved .),\n",
       " (5,\n",
       "  title: applications of logistic_regression and naive_bayes in commodity sentiment_analysis; abstract: sentiment_analysis is popular research which helps people perceive the trend of public opinion commented in separate social_networking platforms . the aim of the paper is to investigate the real evaluation of goods when marketing tweets are eliminated and estimate the performance of two classifiers in sentiment_analysis . the initial process is data collection used crawler with user_comments on the series of ĝ€ ? for verification > xiaomi 11 ' in weibo . by collected data , pre-processing and features extracting can be instituted sequentially . we use term_frequency-inverse_document_frequency to evaluate the significant of words then filter out common words , and bag of word is used to convert the texts into numerical_features . finally , we use naive_bayes and logistic_regression for the classification of twitters reviews and get the results that logistic_regression classifier preforms better than naive_bayes_classifier . the analysis with former gives 3.572 % more accuracy , 6.632 % more precision and 2.81 % more recall .),\n",
       " (5,\n",
       "  title: study on feature_selection and weighting based on synonym merge in text_categorization; abstract: feature_selection and weighting is one of the key problem in text_categorization . the chief obstacles to feature_selection are noise and sparseness . this paper presents an approach of chinese text feature_selection and weighting based on semantic statistics . first , we use synonymous concepts to extract feature_values in text based on thesaurus which names tongyici cilin . then , we introduce a new weight function based on term_frequency and entropy , which adjusts the effect of the feature term in the classifier according to the feature term 's strength . experiments show that our method is much better than kinds of traditional feature_selection methods and it improve the performance of text_categorization systems © 2010 ieee .),\n",
       " (5,\n",
       "  title: opinion_mining using decision_tree based feature_selection through manhattan hierarchical cluster measure; abstract: opinion_mining plays a major role in text_mining applications in consumer attitude detection , brand and product positioning , customer_relationship_management , and market_research . these applications led to a new generation of companies and products meant for online market perception , reputation_management and online content monitoring . subjectivity and sentiment_analysis focus on private states automatic identification like beliefs , opinions , sentiments , evaluations , emotions and natural_language speculations . subjectivity classification labels data as either subjective or objective , whereas sentiment_classification adds additional granularity through further classification of subjective data as positive/negative or neutral . features are extracted from the data for classifying the sentiment . feature_selection has gained importance due to its contribution to save classification cost with regard to time and computation load . in this paper , the main focus is on feature_selection for opinion_mining using decision_tree based feature_selection . the proposed method is evaluated using imdb data set , and is compared with principal_component_analysis ( pca ) . the experimental results show that the proposed feature_selection method is promising . © 2005 - 2013 jatit_&_lls . all rights_reserved .),\n",
       " (5,\n",
       "  title: feature_selection methods in persian sentiment_analysis; abstract: with the enormous growth of digital content in internet , various types of online_reviews such as product and movie reviews present a wealth of subjective_information that can be very helpful for potential users . sentiment_analysis aims to use automated_tools to detect subjective_information from reviews . up to now as there are few researches conducted on feature_selection in sentiment_analysis , there are very rare works for persian sentiment_analysis . this paper considers the problem of sentiment_classification using different feature_selection methods for online_customer_reviews in persian_language . three of the challenges of persian text are using of a wide variety of declensional suffixes , different word spacing and many informal or colloquial words . in this paper we study these challenges by proposing a model for sentiment_classification of persian review documents . the proposed model is based on stemming and feature_selection and is employed naive_bayes algorithm for classification . we evaluate the performance of the model on a collection of cellphone reviews , where the results show the effectiveness of the proposed approaches . ©_2013_springer-verlag_berlin_heidelberg .),\n",
       " (4,\n",
       "  title: a text feature_selection method using the improved mutual_information and information entropy; abstract: mutual_information is a generally used evaluation function for feature_selection . but research showed that it might lead to the low classification_accuracy . to overcome the problem that the mutual_information is incline to selecting low-frequency words , this paper proposes a new feature evaluation function tfmiie for feature_selection , which combines the information entropy with the improved mutual_information . the improved mutual_information avoids to selecting the low-frequency unfamiliar words , and the entropy of feature favors to removing the feature words with unclear class properties . experimental results show that using tfmiie to select feature , and to repesent text and build classifiers can achieve better precision ratio and recall_ratio of text_classification with about 40 % increasing , which validated the proposed text feature_selection method using the improved mutual_information and the information entropy .),\n",
       " (4,\n",
       "  title: a new feature_selection algorithm based on category difference for text_categorization; abstract: the feature_selection is an important step which can reduce the dimensionality and improve the performance of the classifiers in text_categorization . many popular feature_selection methods do not consider the difference in the distribution of different categories on a feature . in this paper , we propose a new filter_based_feature_selection algorithm , namely fused distance feature_selection ( fdfs ) , which evaluates the significance of a feature by taking account of the difference in the distribution of different categories and selects more discriminative_features with the minimal number . the proposed algorithm is investigated both inside and outside perspectives on four benchmark document datasets , 20-newsgroups , webkb , csdmc2010 and ohsumed , using linear_support_vector machine ( lsvm ) and multinomial_naïve_bayes ( mnb ) classifiers . the experimental results indicate that our proposed method provides a competitive result , where its average ranking is 1.25 on lsvm and 1 on mnb .),\n",
       " (4,\n",
       "  title: an evaluation of statistical spam_filtering techniques; abstract: this paper evaluates five supervised_learning methods in the context of statistical spam_filtering . we study the impact of different feature pruning methods and feature_set sizes on each learner 's performance using cost-sensitive measures . it is observed that the significance of feature_selection varies greatly from classifier to classifier . in particular , we found support_vector_machine , adaboost , and maximum_entropy model are top performers in this evaluation , sharing similar characteristics : not sensitive to feature_selection strategy , easily scalable to very high feature_dimension , and good performances across different datasets . in contrast , naive_bayes , a commonly used classifier in spam_filtering , is found to be sensitive to feature_selection methods on small feature_set , and fails to function well in scenarios where false_positives are penalized heavily , the experiments also suggest that aggressive feature pruning should be avoided when building filters to be used in applications where legitimate mails are assigned a cost much higher than spams ( such as λ = 999 ) , so as to maintain a better-than-baseline performance . an interesting finding is the effect of mail headers on spam_filtering , which is often ignored in previous_studies . experiments show that classifiers using features from message header alone can achieve comparable or better performance than filters utilizing body features only . this implies that message headers can be reliable and powerfully discriminative feature sources for spam_filtering .),\n",
       " (4,\n",
       "  title: baseline english and maltese-english classification models for subjectivity detection , sentiment_analysis , emotion analysis , sarcasm detection , and irony detection; abstract: this paper presents baseline classification models for subjectivity detection , sentiment_analysis , emotion analysis , sarcasm detection , and irony detection . all models are trained on user-generated content gathered from newswires and social_networking services , in three different languages : english -a high-resourced language , maltese -a low-resourced language , and maltese-english -a code-switched language . traditional supervised algorithms namely , support_vector_machines , naïve_bayes , logistic_regression , decision_trees , and random_forest , are used to build a baseline for each classification_task , namely subjectivity , sentiment_polarity , emotion , sarcasm , and irony . baseline_models are established at a monolingual ( english ) level and at a code-switched level ( maltese-english ) . results obtained from all the classification models are presented .),\n",
       " (4,\n",
       "  title: immune based feature_selection for opinion_mining; abstract: opinions about a particular product , service or person are communicated effectively through online_media such as facebook , myspace and twitter . unfortunately only a few researchers had researched on the performance of opinion_mining using online messages that were written in malay languages . opinion_mining processing that uses natural_language_processing approach is difficult due to the high content of noisy_texts in online messages . on the other hand , opinion_mining that uses machine_learning approach requires a good feature_selection technique since the current filter typed feature_selection techniques require interference from the user to select the appropriate features . this study used a feature_selection technique based on artificial immune system to select the appropriated features for opinion_mining . experiments with 2000 online movie reviews illustrated that the technique has reduced 90 % of the features and improved opinion_mining accuracy up to 15 % with k_nearest_neighbor classifier and up to 6 % with naïve baiyes classifier .),\n",
       " (4,\n",
       "  title: classification of complaint categories in e-commerce : a case_study of pt bukalapak; abstract: bukalapak is one of the companies in indonesia that is engaged in e-commerce . it was recorded that in 2019 , bukalapak experienced a 230 % increase in user growth compared to the previous year . unfortunately , the growth in users is also followed by an increase in the number of complaints that occur in bukalapak . it was recorded that in 2020 , bukalapak complaints increased by 50 % at the end of 2020 when compared to the beginning of 2020 . the increase in complaints led to an increase in the average handle time for complaints which led to a decrease in user_satisfaction . 3 main issues that cause an increase in average handle time , namely an unstable system , complaint categorization is slow , and difficult to find solutions . this is certainly a concern for the management . in this study , the classification of complaints categories in bukalapak will be carried out . this study aims to find out what classification model is suitable to be used in determining the category of complaints in bukalapak . the classification model that will be used in this research is logistic_regression , k_nearest_neighbor , and support_vector_machine . while the data that will be used is data on complaints from january 2021 to december 2021 . from the results of the study , it was found that the logistic_regression classification model had the highest value among the other 2 models . the logistic_regression model managed to get an accuracy value of 83.9 % . the second position is occupied by the k_nearest_neighbors model with an accuracy of 77.6 % . last occupied by the svm model with an accuracy value of 40.5 % .),\n",
       " (4,\n",
       "  title: automatic recognition of native advertisements for the slovak_language; abstract: in recent_years the native advertisement is becoming more and more prevalent in online spaces . differentiating between genuine content and native advertisement using natural_language_processing is therefore also becoming a very interesting research topic . in this paper , we examine the possibilities of using deep textual_representation for the slovak_language to recognize the “ pr ( public_relations ) articles ” ( that serve as a native advertisement in this context ) from authentic news articles on popular slovak news websites . we show that the bert ( bidirectional_encoder_representations_from_transformers ) embeddings as a text_representation are sufficient for this task ( achieving accuracy over 80 % even with a statistical_model - logistic_regression ) and that the models generally perform better without prior lemmatization . we have scraped three slovak news websites ( for a total of 5455 news articles containing both paid-for content and a wide variety of genuine categories ) , and we have evaluated multiple binary classification methods ( logistic_regression , random_forest classifier and support_vector_machines ) trained on top of generated roberta sentence_embeddings . on our testing_set , we were able to achieve an accuracy of 85.13 % .),\n",
       " (4,\n",
       "  title: rls-mars to feature_selection for text_classification; abstract: feature_selection and feature_extraction will help to remove noisy features and reduce the dimensionality of data sets . in this paper a new rls-mars ( regularized least squares-multi angle regression and shrinkage ) feature_selection method for text_classification is proposed , while rls with lars can be viewed as an efficient approach for a modified rls formulation which uses both l2 regularizer and l1 regularizer . the present method is to find a series of direction in multidimensional space , leading the gradient vectors to change along those directions which would make the gradient matrix 's gradient descent during the procedure , and the feature in this direction can be easily selected . the experiments on reuters-21578 demonstrate the effectiveness of the new feature_selection method for text_classification in several classical algorithms : knn and svmlight .),\n",
       " (4,\n",
       "  title: enhanced malay sentiment_analysis with an ensemble classification machine_learning approach; abstract: sentiment_analysis is one of the challenging and important tasks that involves natural_language_processing , web_mining and machine_learning . this study aims to propose an enhanced ensemble of machine_learning classification methods for malay sentiment_analysis . three classification approaches ( naive_bayes , support_vector_machine and k-nearest_neighbour ) and five ensemble classification_algorithms ( bagging , stacking , voting , adaboost and metacost ) were experimented to achieve the best possible ensemble model for malay sentiment_classification . a wide range of ensemble experiments are conducted on a malay opinion corpus ( moc ) . this study demonstrates that ensemble approaches improve the performance of malay sentiment-based classification , however , the results depend on the classifier used and the ensemble algorithm as well as the number of classifiers in the ensemble approach . the experiments also show that the ensemble classification approaches achieve the best result with an f-measure of 85.81 % .),\n",
       " (4,\n",
       "  title: application of bagging_ensemble classifier based on genetic_algorithm in the text_classification of railway fault hazards; abstract: the safety accident hidden danger of on-site inspection by railway workers are stored in text_format , and this kind of data contains a lot of valuable_information related to railway safety , so it is urgent to classify and manage the data by classification model . in this paper , we analyze the characteristics of such data . firstly , we use tf-idf method to extract text features and convert them into vectors . then , decision_tree classifier is used to classify the data . in order to improve classification_accuracy , the bagging_ensemble classifier conduct a random sample training to text vector converted by tf-idf which decision as the base_classifier , produce bagging classification results , considering the bagging algorithm is a the number of base_classifiers results voting combination classification model which has a better classification performance , we use genetic_algorithm to calculate bagging classifier_combination optimization , better classification results are produced by the ensemble_classifier based on genetic_algorithm ( evolutionary ensemble_classifier ) . through the experimental analysis of text data on safety accident hidden danger of power supply catenary in a railway bureau , it is proved that the safety classification_accuracy , recall_rate and f-score value of the evolutionary ensemble_classifier model are significantly_improved .),\n",
       " (4,\n",
       "  title: arabic text_classification using principal_component_analysis with different supervised_classifiers; abstract: text_classification ( tc ) is the process of automatically labeling a text based on its topic . generally , tc suffers from data high dimensionality of the feature_space . in this paper , we use the principal_components_analysis ( pca ) as a feature_extraction method for reducing the space features size with an application to arabic texts . to evaluate the effect of this technique on the classification process , we use five popular classifiers : logistic_regression , k-nearest_neighbors , decision_trees , random_forest and support_vector_machines . the results show that the proposed method yields a substantial_improvement of classification_accuracy of most of the classifiers with reduction in execution time . to the best of our knowledge , it is the first time pca is used for arabic tc . in addition , the gain in training time is comprised between 5 and 800 . accuracy is improved in 60 % of the cases .),\n",
       " (4,\n",
       "  title: an extensive empirical_study of feature_selection metrics for text_classification; abstract: machine_learning for text_classification is the cornerstone of document_categorization , news filtering , document routing , and personalization . in text domains , effective feature_selection is essential to make the learning task efficient and more accurate . this paper presents an empirical comparison of twelve feature_selection methods ( e.g . information gain ) evaluated on a benchmark of 229 text_classification problem instances that were gathered from reuters , trec , ohsumed , etc . the results are analyzed from multiple goal perspectives-accuracy , f-measure , precision , and recall-since each is appropriate in different situations . the results reveal that a new feature_selection metric we call 'bi-normal separation ' ( bns ) , outperformed the others by a substantial margin in most situations . this margin widened in tasks with high class skew , which is rampant in text_classification problems and is particularly challenging for induction algorithms . a new evaluation methodology is offered that focuses on the needs of the data_mining practitioner faced with a single dataset who seeks to choose one ( or a pair of ) metrics that are most likely to yield the best performance . from this perspective , bns was the top single choice for all goals except precision , for which information gain yielded the best result most often . this analysis also revealed , for example , that information gain and chi-squared have correlated failures , and so they work poorly together . when choosing optimal pairs of metrics for each of the four performance goals , bns is consistently a member of the pair-e.g. , for greatest recall , the pair bns + f1-measure yielded the best performance on the greatest number of tasks by a considerable margin . © 2003 hewlett-packard .),\n",
       " (4,\n",
       "  title: document_classification using symbolic classifiers; abstract: in this paper , we present symbolic classifiers to classify text documents . we propose to use cluster_based symbolic_representation followed by symbolic feature_selection methods to classify text documents . in particular , we propose symbolic clustering approaches ; symbolic cluster_based without feature_selection ; symbolic cluster_based with feature_selection ( using similarity_measure ) ; symbolic cluster_based with feature_selection ( using dissimilarity measure ) and symbolic feature clustering approaches . the above mentioned representation methods are very powerful in reducing the dimensionality of feature_vectors for text_classification . to corroborate the efficacy of the proposed model , we conducted extensive_experimentation on various standard text datasets . the experimental results reveal that the symbolic feature clustering approach achieves better classification_accuracy over the existing cluster_based symbolic approaches .),\n",
       " (4,\n",
       "  title: improving the accuracy of naïve_bayes algorithm for hoax classification using particle_swarm_optimization; abstract: hoax news circulation is very widespread which occurs in the media information , both print and online_media . for some people hoax news can only appear on the online_media . but printed medias also often include hoax news in their published news . in the present era , it is very important providing information with relevant and additional facts otherwise it is categorized as hoax . therefore , hoax classification approach is needed . this paper focuses on improving the accuracy of hoax classification in textual_documents contents . naive_bayes algorithm is used to train dataset with the use of pso in the algorithm . experiment is conducted with the trained model over 600 documents . it shows that feature_selection with pso affects the classification results performed using naïve_bayes . accuracy increased from 91.17 % without using feature_selection , to 92.33 % when feature_selection is carried out using pso .),\n",
       " (4,\n",
       "  title: cached long short-term_memory neural_networks for document-level sentiment_classification; abstract: recently , neural_networks have achieved great_success on sentiment_classification due to their ability to alleviate feature_engineering . however , one of the remaining challenges is to model long texts in document-level sentiment_classification under a recurrent architecture because of the deficiency of the memory unit . to address this problem , we present a cached long short-term_memory neural_networks ( clstm ) to capture the overall semantic information in long texts . clstm introduces a cache mechanism , which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent_unit . the proposed clstm outperforms the state-of-the-art models on three publicly available document-level sentiment_analysis datasets .),\n",
       " (3,\n",
       "  title: comparison of machine_learning for sentiment_analysis in detecting anxiety based on social_media data; abstract: all groups of people felt the impact of the covid-19_pandemic . this situation triggers anxiety , which is bad for everyone . the government 's role is very influential in solving these problems with its work program . it also has many pros_and_cons that cause public anxiety . for that , it is necessary to detect anxiety to improve government programs that can increase public expectations . this study applies machine_learning to detecting anxiety based on social_media comments regarding government programs to deal with this pandemic . this concept will adopt a sentiment_analysis in detecting anxiety based on positive and negative comments from netizens . the machine_learning methods implemented include k-nn , bernoulli , decision_tree classifier , support_vector classifier , random_forest , and xg-boost . the data sample used is the result of crawling youtube comments . the data used amounted to 4862 comments consisting of negative and positive data with 3211 and 1651 . negative data identify anxiety , while positive data identifies hope ( not anxious ) . machine_learning is processed based on feature_extraction of count-vectorization and tf-idf . the results showed that the sentiment data amounted to 3889 and 973 in testing , and training with the greatest accuracy was the random_forest with feature_extraction of vectorization count and tf-idf of 84.99 % and 82.63 % , respectively . the best precision test is k-nn , while the best recall is xg-boost . thus , random_forest is the best accurate to detect someone 's anxiety based-on data from social_media .),\n",
       " (3,\n",
       "  title: a hybrid method for fake_news_detection using cosine_similarity scores; abstract: in this work , we propose a novel hybrid method for fake_news_detection . two approaches have been used to assess the authenticity of the news using web-scrapped data . in the first approach the data is the pre-processed using nlp techniques like extraction of raw_text , the removal of special-characters , white-spaces , and stop_words . this is followed by lemmatization which groups words with similar_meanings . after lemmatization we apply , term_frequency_-_inverse_document_frequency ( tf-idf ) vectorization to form a corpus which is further used to train the models . we propose the use of cosine_similarity score , obtained after performing topic_modelling along with the corpus to improve the classification_accuracies . the classifiers are knn , decision_tree , naive_bayes , logistic_regression , passive-aggressive_classifier , and svm to determine the news is reliable or unreliable . more focus has been given to improve the classification_accuracies of the passive_aggressive_classifier which is the most widely used classifier in fake_news_detection . in the second approach , we use ensemble_learning technique called as stacking along with cosine_similarity score to train another model which gives the result as reliable or unreliable . it is observed that the second approach shows good improvement in the accuracy of fake_news_detection .),\n",
       " (3,\n",
       "  title: predicting software defect severity_level using sentence_embedding and ensemble_learning; abstract: bug_tracking is one of the prominent activities during the maintenance phase of software_development . the severity of the bug acts as a key indicator of its criticality and impact towards planning evolution and maintenance of various types of software products . this indicator measures how negatively the bug may affect the system functionality . this helps in determining how quickly the development teams need to address the bug for successful execution of the software system . due to a large number of bugs reported every day , the developers find it really difficult to assign the severity_level to bugs accurately . assigning incorrect severity_level results in delaying the bug resolution process . thus automated systems were developed which will assign a severity_level using various machine_learning techniques . in this work , five different types of sentence_embedding techniques have been applied on bugs description to convert the description comments to an n-dimensional_vector . these computed vectors are used as an input of the software defect severity_level prediction models and ensemble_techniques like bagging , random_forest classifier , extra_trees classifier , adaboost and gradient boosting have been used to train these models . we have also considered different variants of the synthetic_minority_oversampling_technique ( smote ) to handle the class_imbalance problem as the considered datasets are not evenly_distributed . the experimental results on six projects highlight that the usage of sentence_embedding , ensemble_techniques , and different variants of smote techniques helps in improving the predictive ability of defect severity_level prediction models .),\n",
       " (3,\n",
       "  title: sarcasm detection in twitter using sentiment_analysis; abstract: designing efficient and robust algorithms for detection of sarcasm on twitter is the exciting challenge in opinion_mining field . sarcasm means the person speaks the contradictory of what the individual means , expressing gloomy feelings applying positive words . it helps the retailers to know the opinions of the customers . sarcasm is widely used in many social_networking and micro-blogging websites where people invade others which makes problematic for the individuals to say what it means . in the existing systems , logistic_regression technique is used to detect these sarcastic tweets , it has a drawback as it can not predict for continuous variables . in the proposed methodology sentiment_analysis , naive_bayes classification and adaboost algorithms are used to detect sarcasm on twitter . by using naive_bayes classification , the tweets are categorized into sarcastic and non-sarcastic . the adaboost_algorithm is used to make the weak statement to strong statements by iteratively considering the subset of training_data . sentiment_analysis is used to mine the opinions of customers to identify and extract information from the text . by using these two techniques , sarcastic statements can be easily classified and identified from twitter .),\n",
       " (3,\n",
       "  title: research on chinese text_classification based on word2vec; abstract: the set of features which the traditional feature_selection algorithm of chi-square selected is not complete . this causes the low performance for the final text_classification . therefore , this paper proposes a method . the method utilizes word2vec to generate word_vector to improve feature_selection algorithm of the chi_square . the algorithm applies the word_vector generated by word2vec to the process of the traditional feature_selection and uses these words to supplement the set of features as appropriate . ultimately , the set of features obtained by this method has better discriminatory power . because , the feature words with the better discriminatory power has the strong ability of distinguishing categories as its semantically_similar words . on this base , multiple experiments have been carried out in this paper . the experimental results show that the performance of text_classification can increase after extension of feature words .),\n",
       " (3,\n",
       "  title: margin maximization with feed-forward neural_networks : a comparative study with svm and adaboost; abstract: feed-forward neural_networks ( fnn ) and support_vector_machines ( svm ) are two machine_learning frameworks developed from very different starting points of view . in this work a new learning model for fnn is proposed such that , in the linearly_separable case , it tends to obtain the same solution as svm . the key idea of the model is a_weighting of the sum-of-squares error_function , which is inspired by the adaboost_algorithm . as in svm , the hardness of the margin can be controlled , so that this model can be also used for the non-linearly_separable case . in addition , it is not restricted to the use of kernel_functions , and it allows to deal with multiclass and multilabel problems as fnn usually do . finally , it is independent of the particular algorithm used to minimize the error_function . theoretic and experimental results on synthetic and real-world problems are shown to confirm these claims . several empirical comparisons among this new model , svm , and adaboost have been made in order to study the agreement between the predictions made by the respective classifiers . additionally , the results obtained show that similar performance does not imply similar predictions , suggesting that different models can be combined leading to better performance . © 2003 elsevier b.v. all rights_reserved .),\n",
       " (3,\n",
       "  title: feature_selection method based on simplified information gain of id3; abstract: at present , feature_selection is a core research topic in text_categorization . it firstly analyzed several classic_feature_selection methods and summarized their deficiencies , and combined word_frequency with document_frequency and presented an optimal document_frequency method . and then it analyzed shortcomings of information gain in id3 and introduced attribute dependence to improve information gain . next , according to the character of information gain , it simplified information gain to reduce the computing complexity by the convex_function . finally , it combined the simplified information gain with the optimal document_frequency method and proposed a comprehensive feature_selection method . the comprehensive method firstly uses the optimal document_frequency method to select features to reduce the sparsity of feature_spaces , and then uses to the simplified information gain to select features again , so can acquire the feature_subsets which are more representative . the experimental results show that the combined method is promising .),\n",
       " (3,\n",
       "  title: a novel statistical feature_selection approach for text_categorization; abstract: for text_categorization task , distinctive text features selection is important due to feature_space high dimensionality . it is important to decrease the feature_space dimension to decrease processing time and increase accuracy . in the current study , for text_categorization task , we introduce a novel statistical feature_selection approach . this approach measures the term_distribution in all collection documents , the term_distribution in a certain category and the term_distribution in a certain class relative to other classes . the proposed method results show its superiority over the traditional feature_selection methods .),\n",
       " (3,\n",
       "  title: hybrid supervised clustering based ensemble scheme for text_classification; abstract: purpose : the immense quantity of available unstructured_text documents serve as one of the largest source of information . text_classification can be an essential task for many purposes in information_retrieval , such as document_organization , text filtering and sentiment_analysis . ensemble_learning has been extensively_studied to construct efficient text_classification schemes with higher predictive performance and generalization_ability . the purpose of this paper is to provide diversity among the classification_algorithms of ensemble , which is a key_issue in the ensemble design . design/methodology/approach : an ensemble scheme based on hybrid supervised clustering is presented for text_classification . in the presented scheme , supervised hybrid clustering , which is based on cuckoo search_algorithm and k-means , is introduced to partition the data samples of each class into clusters so that training subsets with higher diversities can be provided . each classifier is trained on the diversified training subsets and the predictions of individual_classifiers are combined by the majority_voting rule . the predictive performance of the proposed classifier_ensemble is compared to conventional classification_algorithms ( such as naïve_bayes , logistic_regression , support_vector_machines and c4.5_algorithm ) and ensemble_learning methods ( such as adaboost , bagging and random_subspace ) using 11 text benchmarks . findings : the experimental results indicate that the presented classifier_ensemble outperforms the conventional classification_algorithms and ensemble_learning methods for text_classification . originality/value : the presented ensemble scheme is the first to use supervised clustering to obtain diverse ensemble for text_classification),\n",
       " (3,\n",
       "  title: misinformation analysis during covid-19_pandemic; abstract: online diffusion of misinformation has gained extreme attention in the research from past few years . moreover , during ongoing covid-19_pandemic , the proliferation of misinformation became more prominent . in this paper , a comparison of two feature_engineering techniques , namely term_frequency–inverse_document_frequency ( tf-idf ) and word_embeddings ( doc2vec ) , is done over different machine_learning classifiers . a web scraper is developed for fact-checking web_site , snopes.com , to extract labeled news related to covid-19 . although the size of dataset is less , the body content under headlines contains large amount of text . therefore , the model works well with both the feature_engineering techniques and machine_learning algorithms . apparently , we obtained best accuracy of 95.38 % with tf-idf on decision_tree and same accuracy of 90.77 % using doc2vec on support_vector_machine and logistic_regression machine_learning classifier .),\n",
       " (3,\n",
       "  title: chillax - at arabic hate_speech 2022 : a hybrid machine_learning and transformers based model to detect arabic offensive and hate_speech; abstract: hate_speech and offensive_language have become a crucial problem nowadays due to the extensive usage of social_media by people of different gender , nationality , religion and other types of characteristics , allowing anyone to share their thoughts and opinions . in this research paper , we proposed a hybrid model for the first and second tasks of osact2022 . this model used the arabic pre-trained bert language model marbert for feature_extraction of the arabic tweets in the dataset provided by the osact2022 shared_task , then fed the features to two classic machine_learning classifiers ( logistic_regression , random_forest ) . the best results achieved for the offensive tweet detection task were achieved by the logistic_regression model with accuracy , precision , recall , and f1-score of 80 % , 78 % , 78 % , and 78 % , respectively . the results for the hate_speech tweet detection task were 89 % , 72 % , 80 % , and 76 % .),\n",
       " (3,\n",
       "  title: hybrid feature_selection based on enhanced genetic_algorithm for text_categorization; abstract: this paper proposes hybrid feature_selection approaches based on the genetic_algorithm ( ga ) . this approach uses a hybrid search technique that combines the advantages of filter_feature_selection_methods with an enhanced ga ( ega ) in a wrapper_approach to handle the high dimensionality of the feature_space and improve categorization performance simultaneously . first , we propose ega by improving the crossover and mutation operators . the crossover operation is performed based on chromosome ( feature_subset ) partitioning with term and document_frequencies of chromosome entries ( features ) , while the mutation is performed based on the classifier performance of the original parents and feature_importance . thus , the crossover and mutation operations are performed based on useful information instead of using probability and random selection . second , we incorporate six well-known filter_feature_selection_methods with the ega to create hybrid feature_selection approaches . in the hybrid approach , the ega is applied to several feature_subsets of different sizes , which are ranked in decreasing order based on their importance , and dimension reduction is carried out . the ega operations are applied to the most important features that had the higher ranks . the effectiveness of the proposed approach is evaluated by using naïve_bayes and associative_classification on three different collections of arabic text datasets . the experimental results show the superiority of ega over ga , comparisons of ga with ega showed that the latter achieved better results in terms of dimensionality_reduction , time and categorization performance . furthermore , six proposed hybrid fs approaches consisting of a filter method and the ega are applied to various feature_subsets . the results showed that these hybrid approaches are more effective than single filter methods for dimensionality_reduction because they were able to produce a higher reduction rate without loss of categorization precision in most situations .),\n",
       " (3,\n",
       "  title: effect of incremental feature_enrichment on healthcare text_classification system : a machine_learning paradigm; abstract: background and objective : healthcare tweets are particularly challenging due to its sparse layout and its limited character size . compared to previous method based on “ bag of words ” ( bow ) model , this study uniquely identifies the enrichment protocol and learns how semantically different aspects of feature_selection such as bow ( feature f0 ) , term_frequency_inverse_document_frequency ( tf-idf , feature f1 ) , and latent_semantic_indexing ( lsi , feature f2 ) when applied sequentially with classifier improves the overall performance . methods : to study this enrichment concept , our ml model is tested on two kinds of diverse data sets : ( i ) d1 : disease data with conjunctivitis , diarrhea , stomach ache , cough and nausea related_tweets , and ( ii ) d2 : webkb4 dataset , while adapting three kind of classifiers ( a ) c1 : support_vector_machine with radial_basis_function ( svmr ) , ( b ) c2 : multi-layer_perceptron ( mlp ) and ( c ) c3 : random_forest ( rf ) . partition protocol ( k10 ) was adapted with different performance_metrics to evaluate machine_learning ( ml ) -system . results : using the combination of f1 , c1 , d1 , k10 , ml accuracy was : 94 % , while with f2 , c1 , d1 , k10 , ml accuracy was 97 % . using the incremental feature_enrichment from f0 to f2 , k10 protocol gave f1 improvement over f0 by 4.98 % on disease dataset , while f2 improvement over f0 was by 11.78 % on webkb4 dataset . we demonstrated the generalization over memorization process in our ml-design . the system was tested for stability and reliability . conclusions : we conclude that semantically different aspects of feature_selection , when adapted sequentially , leads to improvement in ml-accuracy for healthcare data sets . we validated the system by taking non-healthcare data sets .),\n",
       " (3,\n",
       "  title: hybrid optimization for feature_selection in opinion_mining; abstract: a sub-discipline of information_retrieval ( ir ) is opinion_mining and the lexicon of computers is not concerned of the subject of the doc-ument , but about the opinion expressed . it has caused a large impact in the arena of academics and industry as it has a wide area of re-search and the applications are widespread . feature_selection is a vital step in opinion_mining , as its individual feature decides the opin-ions expressed by the customers . feature_selection reduces the dimensionality of data by avoiding non-relevant features ; it can be con-sidered as a necessary and excellent process for data mining applications . in this study , feature_subset is optimized through particle_swarm_optimization ( pso ) algorithm , cuckoo search ( cs ) algorithm and hybridized pso-cs algorithm . classification is done through naïve_bayes and k-nearest_neighbours ( knn ) classifiers . feature_extraction has its basis on term_frequency-inverse document fre-quency ( tf-idf ) . the accuracy of classification precision is increased by the reduction in size of feature_subset and computational com-plexity .),\n",
       " (3,\n",
       "  title: a study on machine_learning applied to software_bug priority prediction; abstract: bugs are among the top problems faced by software developers . as the size and complexity of software projects increase so does the number of bugs and their complexity . bug priority prediction helps software developers focus their efforts on the most critical bugs that affect the core functionality of a software . by automating the process of priority prediction , it is possible to reduce the time spent analyzing new bug_reports . in this paper , we extract bug_reports from the bug_tracking software of six popular open-source projects hadoop , hbase , hdfs , mesos , spark , and mapreduce and apply five machine_learning classifiers multinomial_naive_bayes , decision_tree , logistic_regression , random_forest , adaboost to automatically predict bug priority using the title , description , and summary of the bug_report . we use tf-idf to extract useful features from the bug_reports and employ precision , recall , and f1-score for measuring the performance of the classifiers . a stratified 10-fold_cross-validation technique is used for model evaluation and the results are averaged over all 10 folds . we find that machine_learning applied to bug priority prediction provides excellent_results and can be used to significantly_reduce the time involved in the bug prioritization process . from our experiments , we observe that no single classifier consistently performs best on all priority levels and metrics across all datasets . however , trends from results show that multinomial_naive_bayes gives well balanced performance and is also fast to train and test . logistic_regression and adaboost also performed well and are potential alternatives .),\n",
       " (3,\n",
       "  title: a hybrid documents classification based on svm and rough_sets; abstract: standard machine_learning techniques like support_vector_machines ( svm ) and related large_margin methods have been successfully applied for text_classification . unfortunately , the high dimensionality of input feature_vectors impacts on the classification speed . the kernel parameters setting for svm in a training process impacts on the classification_accuracy . feature_selection is another factor that impacts classification_accuracy . the objective of this work is to reduce the dimension of feature_vectors , optimizing the parameters to improve the svm classification_accuracy and speed . in order to improve classification speed we spent rough_sets theory to reduce the feature_vector space . we present a genetic_algorithm approach for feature_selection and parameters optimization to improve classification_accuracy . experimental results indicate our method is more effective than traditional svm methods and other traditional_methods . © 2009 ieee .),\n",
       " (3,\n",
       "  title: integrating noun-based feature_ranking and selection methods with arabic text associative_classification approach; abstract: feature_ranking and selection ( fr & s ) is an important preprocessing_phase for text_classification , and it is in most cases produces small valuable sub-feature_space among the whole feature_space and reduces the classification errors . as the associative_classification ( ac ) approach is an efficient method and its training and testing depend on the way that features ranked and selected , the examining of feature_ranking methods is very significant . this paper presents an integration method of arabic noun extraction with four fr & s methods : term_frequency–inverse_document_frequency ( tf-idf ) , document_frequency , odd ratio , and class discriminating measure ( cdm ) . association_rule technology uses the result of the integrated feature_selection to construct an arabic text associative classifier . in this study , the majority_voting and ordered decision list prediction methods are used by ac to assign test document to its category . a set of experiments are conducted on collection of arabic text documents , and the experimental results show that our ac method works better with extracted nouns and feature_selection method than with feature_selection method individually . the ac based on cdm and tf-idf methods outperforms the other methods in terms of ac accuracy . as the results indicate , the proposed method produces satisfactory classification_accuracy and it has good selecting effect on the arabic text associative classifier .),\n",
       " (3,\n",
       "  title: linguistic feature-based praise or complaint classification from customer_reviews; abstract: online_reviews are very important in the customer ’ s decision-making process in selecting the appropriate products in the online_shopping portal . these reviews are then analyzed by business organizations to understand customer sentiment w.r.t . product/service . traditional sentiment_analysis techniques identify only positive , negative or neutral sentiment w.r.t . reviews and does not consider informativeness of reviews while analyzing sentiment . the extreme opinions like praise and complaint sentences are considered as a subset of positive and negative sentences and becomes difficult to find . praise sentences are more descriptive in nature . praises contain more nouns , adjectives , intensifiers as compared to plain positive sentences and complaint sentences contain more connectives and adverbs rather than the plain negative sentences . this paper proposes a linguistic feature-based approach for review sentences filtering and hybrid feature_selection method for classifying review sentence as praise or complaint . these praise and complaint sentences can be further analyzed by business organizations to identify the reasons for customer_satisfaction or dissatisfaction . it can also be used for creating automatic product description from online_reviews in terms of pro and con of the product/service . the performance of the four different supervised_machine_learning classifiers , namely random_forest , svc , kneighbors , mlp with hybrid feature_selection method is evaluated on three domains reviews using the parameters accuracy , precision , recall , and f1-score . the proposed method showed excellent_results as compared to the state of art classifiers .),\n",
       " (3,\n",
       "  title: sentiment_analysis of review datasets using naive_bayes and k-nn classifier; abstract: the advent of web_2.0 has led to an increase in the amount of sentimental content available in the web . such content is often found in social_media web_sites in the form of movie or product_reviews , user_comments , testimonials , messages in discussion_forums etc . timely discovery of the sentimental or opinionated web_content has a number of advantages , the most important of all being monetization . understanding of the sentiments of human masses towards different entities and products enables better services for contextual advertisements , recommendation_systems and analysis of market_trends . the focus of our project is sentiment focussed web_crawling framework to facilitate the quick discovery of sentimental contents of movie reviews and hotel reviews and analysis of the same . we use statistical_methods to capture elements of subjective style and the sentence polarity . the paper elaborately discusses two supervised_machine_learning_algorithms : k-nearest_neighbour ( k-nn ) and naive_bayes and compares their overall accuracy , precisions as well as recall values . it was seen that in case of movie reviews naive_bayes gave far better results than k-nn but for hotel reviews these algorithms gave lesser , almost same accuracies .),\n",
       " (3,\n",
       "  title: stemming versus light stemming as feature_selection techniques for arabic_text_categorization; abstract: this paper compares and contrasts two feature_selection techniques when applied to arabic corpus ; in particular ; stemming , and light stemming were employed . with stemming , words are reduced to their stems . with light stemming , words are reduced to their light stems . stemming is aggressive in the sense that it reduces words to their 3-letters roots . this affects the semantics as several words with different meanings might have the same root . light stemming , by comparison , removes frequently used prefixes and suffixes in arabic words . light stemming does n't produce the root and therefore does n't affect the semantics of words ; it maps several words , which have the same meaning to a common syntactical form . the effectiveness of above two feature_selection techniques was assessed in a text_categorization exercise for arabic corpus . this corpus consists of 15000 documents that fall into three categories . the k-nearest_neighbors ( knn ) classifier was used in this work . several experiments were carried out using two different representations of the same corpus ; the first version uses stem-vectors ; and the second uses light stem-vectors as representatives of documents . these two representations were assessed in terms of size , time and accuracy . the light stem representation was superior in terms of classifier accuracy when compared with stemming . ©2008 ieee .),\n",
       " (3,\n",
       "  title: a survey on sentiment_analysis_and_opinion_mining in greek social_media; abstract: as the amount of content that is created on social_media is constantly increasing , more and more opinions and sentiments are expressed by people in various subjects . in this respect , sentiment_analysis_and_opinion_mining techniques can be valuable for the automatic analysis of huge textual corpora ( comments , reviews , tweets etc. ) . despite the advances in text_mining algorithms , deep_learning techniques , and text_representation models , the results in such tasks are very good for only a few high-density languages ( e.g. , english ) that possess large training corpora and rich linguistic resources ; nevertheless , there is still room for improvement for the other lower-density languages as well . in this direction , the current work employs various language models for representing social_media texts and text classifiers in the greek_language , for detecting the polarity of opinions_expressed on social_media . the experimental results on a related dataset collected by the authors of the current work are promising , since various classifiers based on the language models ( naive_bayesian , random_forests , support_vector_machines , logistic_regression , deep feed-forward neural_networks ) outperform those of word or sentence-based embeddings ( word2vec , glove ) , achieving a classification_accuracy of more than 80 % . additionally , a new language model for greek social_media has also been trained on the aforementioned dataset , proving that language models based on domain_specific corpora can improve the performance of generic language models by a margin of 2 % . finally , the resulting models are made freely available to the research community .),\n",
       " (3,\n",
       "  title: a use case of patent classification using deep_learning with transfer_learning; abstract: purpose : patent classification is one of the areas in intellectual_property analytics ( ipa ) , and a growing use case since the number of patent applications has been increasing worldwide . we propose using machine_learning algorithms to classify portuguese patents and evaluate the performance of transfer_learning methodologies to solve this task . design/methodology/approach : we applied three different approaches in this paper . first , we used a dataset available by inpi to explore traditional_machine_learning_algorithms and ensemble_methods . after preprocessing data by applying tf-idf , fasttext and doc2vec , the models were evaluated by cross-validation in 5 folds . in a second approach , we used two different neural_networks architectures , a convolutional_neural_network ( cnn ) and a bi-directional_long_short-term_memory ( bilstm ) . finally , we used pre-trained bert , distilbert , and ulmfit models in the third approach . findings : berttimbau , a bert architecture model pre-trained on a large portuguese corpus , presented the best results for the task , even though with a performance of only 4 % superior to a linearsvc model using tf-idf feature_engineering . research limitations : the dataset was highly_imbalanced , as usual in patent applications , so the classes with the lowest samples were expected to present the worst performance . that result happened in some cases , especially in classes with less than 60 training_samples . practical_implications : patent classification is challenging because of the hierarchical classification system , the context overlap , and the underrepresentation of the classes . however , the final model presented an acceptable_performance given the size of the dataset and the task complexity . this model can support the decision and improve the time by proposing a category in the second level of icp , which is one of the critical phases of the grant patent process . originality/value : to our knowledge , the proposed models were never implemented for portuguese patent classification .),\n",
       " (3,\n",
       "  title: comparison of feature_selection metrics for classifying pornographic_web pages; abstract: this paper evaluates four features election metrics ( ptfidf , cc , or , gss ) in th eir application to inverse chi-square based classification of pornographic_web pages . for each metric , an effective feature_selection method is applied to select positive and negative features based on a threshold pair . for each metric , a sub-optimal threshold pair with top classification performance is obtained through systematic iteration of pair combinations . the experimental results indicate that the gss and cc feature_selection metrics produce best classification results . for gss and cc , we propose a heuristic to obtain a threshold pair with close to top performance more efficiently . the classification results for a predefined threshold pair , the iteratively computed sub-optimal threshold pair , and the heuristics generated threshold pair are then compared . experimental results illustrate that the classification performance of the heuristics generated threshold pair is comparable to that of iteratively computed sub-optimal threshold pair .),\n",
       " (3,\n",
       "  title: artificial bee colony optimization for feature_selection in opinion_mining; abstract: opinion_mining , a sub-discipline of information_retrieval and computational_linguistics concerns not with what a document is about , but with its expressed opinion . feature_selection is an important step in opinion_mining , as customers express product opinions separately according to individual features . earlier research on feature-based opinion_mining had many drawbacks like selecting a feature considering only grammatical_information or treating features with same meanings as different . however this led to a large corpus which subsequently affected the classification_accuracy . statistical techniques like correlation based feature ( cfs ) have been extensively used for feature_selection to reduce the corpus size . the selected features are sub optimal due to the non polynomial ( np ) hard nature of the technique used . in this work , we propose artificial bee colony ( abc ) algorithm for optimization of feature_subset . naïve_bayes , fuzzy unordered rule_induction algorithm ( furia ) and ripple down rule learner ( ridor ) classifiers are used for classification . the proposed method is compared with features extracted based on inverse_document_frequency ( idf ) . hence , this method is useful for reducing feature_subset size and computational_complexity thereby increasing the classification_accuracy . © 2005 - 2014 jatit_&_lls . all rights_reserved .),\n",
       " (3,\n",
       "  title: identifying student difficulty in a digital learning environment; abstract: this paper discusses the development of tutoralert , a natural_language_processing system similar to those used in sentiment_analysis , but applied to the data generated by students in a digital online learning environment in order to detect confused or frustrated students . a number of machine_learning algorithms were tested in the development process , including support_vector_machines ( svm ) , naive_bayes , and random_forest classifiers . as well , an array of natural_language preparation techniques were employed to determine the optimum preprocessing configuration to produce relevant results . we found that detecting potential student frustration or confusion was most successful using a sequential_minimal_optimization algorithm ( smo ) , along with the stanford part-of-speech tagger ( pos_tagger ) , the iterated version of the lovins stemmer , and a custom dictionary to help determine relevance probability . this model produced a promising initial f1 score of 0.79 and an accuracy of 0.83 . further , agreement values of 88 % were achieved during inter-rater_reliability testing between the classifier and human judges .),\n",
       " (3,\n",
       "  title: an automatic method using hybrid neural_networks and attention_mechanism for software_bug triaging; abstract: software defect repair ( also known as software_bug fixing ) is a necessary part of software_quality_assurance . in the collective-intelligence-based software_development environment on the internet , improving the efficiency and effectiveness of software_bug triaging can help raise bug_fixing rates and reduce maintenance costs . nowadays , automatic bug_triaging approaches based on machine_learning have become mainstream , but they also have some specific problems , such as hand-crafted features and an insufficient ability to represent texts . considering successful applications of deep_learning in the field of natural_language_processing , researchers have recently tried to introduce deep_learning into the field of automatic bug_triaging , to improve the performance of predicting the right bug fixer significantly . however , different types of neural_networks have their limitations . to address the problems mentioned above , in this study , we regard bug_triaging as a text_classification problem and propose an automatic bug_triaging approach based on hybrid neural_networks and an attention_mechanism , called atten-crnn . because atten-crnn combines the advantages of a convolutional_neural_network , a recurrent_neural_network , and an attention_mechanism , it can capture essential text features and sequence features of bug_reports more effectively and then provide more accurate fixer recommendation services for software_development and maintenance . an empirical_study was conducted on two popular large-scale open-source_software projects , namely eclipse and mozilla . the experimental results obtained from over 200 000 bug_reports indicate that atten-crnn achieves higher prediction_accuracy than convolutional_neural_networks and recurrent_neural_networks , regardless of the attention_mechanism .),\n",
       " (3,\n",
       "  title: an intelligent hybrid sentiment_analyzer for personal protective medical equipments based on word_embedding technique : the covid-19 era; abstract: due to the accelerated growth of symmetrical sentiment data across different platforms , experimenting with different sentiment_analysis ( sa ) techniques allows for better decision-making and strategic planning for different sectors . specifically , the emergence of covid-19 has enriched the data of people ’ s opinions and feelings about medical products . in this paper , we analyze people ’ s sentiments about the products of a well-known e-commerce website named alibaba.com . people ’ s sentiments are experimented with using a novel evolutionary approach by applying advanced pre-trained_word_embedding for word presentations and combining them with an evolutionary feature_selection mechanism to classify these opinions into different levels of ratings . the proposed approach is based on harmony search_algorithm and different classification techniques including random_forest , k-nearest_neighbor , adaboost , bagging , svm , and reptree to achieve competitive_results with the least possible features . the experiments are conducted on five different datasets including medical gloves , hand_sanitizer , medical oxygen , face masks , and a combination of all these datasets . the results show that the harmony search_algorithm successfully reduced the number of features by 94.25 % , 89.5 % , 89.25 % , 92.5 % , and 84.25 % for the medical glove , hand_sanitizer , medical oxygen , face masks , and whole datasets , respectively , while keeping a competitive_performance in terms of accuracy and root mean square error ( rmse ) for the classification techniques and decreasing the computational time required for classification .),\n",
       " (3,\n",
       "  title: classifying patient_portal messages using convolutional_neural_networks; abstract: objective patients communicate with healthcare providers via secure messaging in patient portals . as patient_portal adoption increases , growing messaging volumes may overwhelm providers . prior_research has demonstrated promise in automating classification of patient_portal messages into communication types to support message triage or answering . this paper examines if using semantic features and word context improves portal message classification . materials and methods portal_messages were classified into the following categories : informational , medical , social , and logistical . we constructed features from portal_messages including bag of words , bag of phrases , graph representations , and word_embeddings . we trained one-versus-all random_forest and logistic_regression classifiers , and convolutional_neural_network ( cnn ) with a softmax output . we evaluated each classifier 's performance using area under the curve ( auc ) . results representing the messages using bag of words , the random_forest detected informational , medical , social , and logistical communications in patient_portal messages with aucs : 0.803 , 0.884 , 0.828 , and 0.928 , respectively . graph representations of messages outperformed simpler features with aucs : 0.837 , 0.914 , 0.846 , 0.884 for informational , medical , social , and logistical communication , respectively . representing words with word2vec_embeddings , and mapping features using a cnn had the best performance with aucs : 0.908 for informational , 0.917 for medical , 0.935 for social , and 0.943 for logistical categories . discussion and conclusion word2vec and graph representations improved the accuracy of classifying portal_messages compared to features that lacked semantic information such as bag of words , and bag of phrases . furthermore , using word2vec along with a cnn model , which provide a higher_order representation , improved the classification of portal_messages .),\n",
       " (3,\n",
       "  title: a feature_selection approach based on term_distributions; abstract: feature_selection has a direct impact on text_categorization . most existing algorithms are based on document_level , and they haven ’ t considered the influence of term_frequency on text_categorization . based on these , we put forward a feature_selection approach , fsatd , based on term_distributions in the paper . in our proposed algorithm , three critical factors which are term_frequency , the inter-class distribution and the intra-class distribution of the terms are all considered synthetically . finally , experiments are made with the help of knn_classifier . and the corresponding results on 20newsgroup and sougoucs corpus show that fsatd algorithm achieves better performance than df and t-test algorithms .),\n",
       " (2,\n",
       "  title: research on the classification modeling for the natural_language texts with subjectivity characteristic; abstract: the methods of natural_language text_classification have the characteristic of diversification , and the text characteristics are the basis of the method effectiveness ; this paper takes the car service complaint data as an example to study the classification modeling for the texts with subjectivity characteristic . the effective handling of car service complaints is important for improving user_experience and maintaining brand reputation ; manual classification commonly has the disadvantages of experience dependence , prone to error , heavy workload and so on ; corresponding automatic classification modeling research is of great practical_significance . the core links of the research method in this study include word_segmentation , text_vectorization , feature_selection and dimensionality_reduction based on correlation , classification modeling based on progressive method and random_forest , and model reliability analysis ; the research results show that the car service complaint_texts could be effectively classified based on the method in this study , which could provide a reference for related further research and application .),\n",
       " (2,\n",
       "  title: aspect extraction and classification for sentiment_analysis in drug reviews; abstract: aspect-based sentiment_analysis ( absa ) of patients ’ opinions_expressed in drug reviews can extract valuable_information about specific aspects of a particular drug such as effectiveness , side effects and patient conditions . one of the most important and challenging tasks of absa is to extract the implicit and explicit aspects from a text , and to classify the extracted aspects into predetermined classes . supervised_learning algorithms possess high accuracy in extracting and classifying aspects ; however , they require annotated_datasets whose manual construction is time-consuming and costly . in this paper , first a new method was introduced for identifying expressions that indicate an aspect in user_reviews about drugs in english . then , distant_supervision was adopted to automate the construction of a training_set using sentences and phrases that are annotated as aspect classes in the drug domain . the results of the experiments showed that the proposed method is able to identify various aspects of the test set with 74.4 % f-measure , and outperforms the existing aspect extraction methods . also , training the random_forest classifier on the dataset that was constructed via distant_supervision obtained the f-measure of 73.96 % , and employing this dataset to fine-tune bert for aspect classification yielded better f-measure ( 78.05 % ) in comparison to an existing method in which the random_forest classifier trained on an accurate manually constructed dataset .),\n",
       " (2,\n",
       "  title: deep graph neural_networks for text_classification task; abstract: text_classification is to organizing documents into predetermined_categories , usually by machinery learn algorithms . it is a significant ways to organize and utilize the large amount of information that exists in unstructured_text format . text_classification is an important module in text processing , and its applications are also very extensive , such as garbage filtering , news classification , part-of-speech_tagging , and so on . with the continuous development of deep_learning in recent_years ! its applications are also very extensive , such as : garbage filtering , news classification , part-of-speech_tagging , and so on . but the text also has its own characteristics . according to the characteristics of the text , the general process of text_classification is : 1 . preprocessing ; 2 . text_representation and feature_selection ; 3 . construction of a classifier ; 4 . the task of text_classification refers to classifying texts into only single or many types in tc system . some researchers are beginning to apply deep_neural_networks to tasks such as the text_classification we mentioned above . although the research around the task has made great_progress , the review of this task is very scarce , and there is a lack of a comprehensive review of the development of the task in recent_years . therefore , we present a survey of research in text_classification to create taxonomies . finally , it is by giving vital effects , the direction of future_research , and those challenges that may counter in the research field .),\n",
       " (2,\n",
       "  title: a two-level deep_learning approach for emotion recognition in arabic news headlines; abstract: going online has created more opportunities for newspapers to present breaking_news in a timely manner . concentrating in spreading more bad news increases the feeling of danger and depression in the society . some authors believe on tendency of some media to be focused on sharing the bad events in life rather than the good ones because of the impact and the attraction over the audience is more significant . sentiment_analysis work has not been recognized , proposed , or documented on arabic news because of the challenges that arabic raises as a language including the different arabs ' dialects and its complex grammatical_structure . with the emerging flow of news that cause a global panic and anxiety worldwide , this study focuses on the serious need to identify what effective role could machine_learning classifiers have in the early detection process of the psychology impact on the readers by the daily news headlines . in this work , a dataset of news headlines were gathered from top popular online arabic news sites and were annotated to seven emotional_categories : anger , disgust , fear , happiness , neutral , sadness , and surprise . a convolutional_neural_network-based two-level approach was proposed for sentiment_classification . the performance of the proposed approach was compared to six machine_learning classifiers ( zeror , k-nearest_neighbor , decision_trees , naïve_bayes , random_forest and support_vector_machine ) and showed better accuracy , precision , and recall .),\n",
       " (2,\n",
       "  title: sentiment mapping : point pattern analysis of sentiment classified twitter data; abstract: detecting and monitoring collective public opinion via social_media platforms can provide real-time information to researchers and policymakers . human emotions , culture , and opinions can be tracked over time to understand where different sentiments manifest themselves geographically . expanding on existing methodology , the present study draws from sentiment_analysis and point pattern analysis to categorize and analyze sentiment toward natural_gas across the united_states as a means of applying these techniques together . three methods of machine_learning were used to classify collected_tweets into positive and negative categories : naïve_bayes , support_vector_machine , and logistic_regression . spatial clustering methods and spatial scan statistics were then applied to geocoded tweets to examine the distribution of sentiment about natural_gas . in this analysis , the logistic_regression and support_vector_machine methods outperformed naïve_bayes in classifying sentiment . the different methods produced not rather different classification results but also produced varying geographic results . the spatial analyses successfully indicated persistent patterns of negative and positive tweeting about natural_gas that correlate with expectations given the physical and cultural environment of various regions . further , the temporal variation of geographic hotspots of sentiment was readily apparent , suggesting that these approaches can reveal dynamic sentiment landscapes .),\n",
       " (2,\n",
       "  title: an improved knn_algorithm for text_classification; abstract: this paper analyzes the advantages and disadvantages of knn alogrithm and introduces an improved knn alogrithm ( wpsokn ) for text c1assification.it is based on particle_swarm_optimization which has the ability of random and directed global search within training document set.during the procedure for searching k_nearest_neighbors of the test sample , those document_vectors that are impossible to be the k closest vectors are kicked out quickly.besides it reduces the impact of individual particles from the overall.moreover , the interference factor is introduced to avoid premature to find the k_nearest_neighbors of test samples quickly.we conducted an extensive experimental study using real datasets , and the results show that the wpsoknn algorithm is more efficient than other knn_algorithm . © 2010 ieee .),\n",
       " (2,\n",
       "  title: a novel feature_selection and extraction technique for classification; abstract: pattern_recognition is a vast field which has seen significant advances over the years . as the datasets under consideration grow larger and more comprehensive , using efficient techniques to process them becomes increasingly important . we present a versatile technique for the purpose of feature_selection and extraction - class dependent features ( cdfs ) . cdfs identify the features innate to a class and extract them accordingly . the features thus extracted are relevant to the entire class and not just to the individual data item . this paper focuses on using cdfs to improve the accuracy of classification and at the same time control computational expense by tackling the curse of dimensionality . in order to demonstrate the generality of this technique , it is applied to two problem statements which have very little in common with each other - handwritten_digit_recognition and text_categorization . it is found that for both problem statements , the accuracy is comparable to state-of-the-art results and the speed of the operation is considerably greater . results are presented for reuters-21578 and web-kb datasets relating to text_categorization and the mnist and usps datasets for handwritten_digit_recognition .),\n",
       " (2,\n",
       "  title: effect of training_set size on svm and naïve_bayes for twitter sentiment_analysis; abstract: twitter sentiment_analysis has become an effective way in measuring public sentiment about a certain topic or product . thus , researchers have worked extensively in recent_years to build efficient models for sentiment_classification . in this paper , we will measure the effect of varying the training_set size on the classification_accuracy and f-score of svm and naive_bayes classifiers . we will expand our study even further by forming two ensembles : ensemble 1 and ensemble 2 . both ensembles include a single naive_bayes and svm classifier , but the ensembles differ in terms of the decision fusion technique utilized . ensemble 1 uses 'and-type ' fusion while ensemble 2 uses 'or-type ' fusion . in this paper , we measure the effect of training_set size on each ensemble configuration type by measuring their f-scores and classification_accuracies while varying the training_set size .),\n",
       " (2,\n",
       "  title: bert-based nlp techniques for classification and severity modeling in basic warranty data study; abstract: this paper is to explore data-driven models based on a newly_developed natural_language_processing ( nlp ) tool called bidirectional_encoder_representations_from_transformer ( bert ) to incorporate textural data information for group classification and loss amount prediction on truck 's basic warranty claims . in group classification modeling , multiple-class logistic_regression is compared with bert-based back-propagation neural_networks ( nn ) . in group loss severity modeling , direct nn regression is compared with bert-based nn regression prediction . furthermore , based on the results from a so-called optimal bin-width algorithm , the severity distribution is fitted in gamma and its parameters are then estimated using maximum_likelihood_estimation ( mle ) . the data experiments show that the bert framework for nlp improves the classification of the warranty claims as well as the loss severity prediction both in accuracy and stability .),\n",
       " (2,\n",
       "  title: a classical approach to handcrafted_feature extraction techniques for bangla handwritten_digit_recognition; abstract: bangla handwritten_digit_recognition is a significant step forward in the development of bangla ocr . however , intricate shape , structural likeness and distinctive composition style of bangla digits makes it relatively challenging to distinguish . thus , in this paper , we benchmarked four rigorous classifiers to recognize bangla handwritten_digit : k-nearest_neighbor ( knn ) , support_vector_machine ( svm ) , random_forest ( rf ) , and gradient-boosted decision_trees ( gbdt ) based on three handcrafted_feature extraction techniques : histogram of oriented gradients ( hog ) , local_binary_pattern ( lbp ) , and gabor filter on four publicly available bangla handwriting digits datasets : numtadb , cmartdb , ekush and bdrw . here , handcrafted_feature extraction methods are used to extract features from the dataset image , which are then utilized to train machine_learning classifiers to identify bangla handwritten_digits . we further fine-tuned the hyperparameters of the classification_algorithms in order to acquire the finest bangla handwritten_digits recognition performance from these algorithms , and among all the models we employed , the hog features combined with svm model ( hog+svm ) attained the best performance_metrics across all datasets . the recognition_accuracy of the hog+svm method on the numtadb , cmartdb , ekush and bdrw datasets reached 93.32 % , 98.08 % , 95.68 % and 89.68 % , respectively as well as we compared the model performance with recent state-of-art methods .),\n",
       " (2,\n",
       "  title: classifying cancer pathology reports with hierarchical self-attention networks; abstract: we introduce a deep_learning architecture , hierarchical self-attention networks ( hisans ) , designed for classifying pathology reports and show how its unique architecture leads to a new state-of-the-art in accuracy , faster_training , and clear interpretability . we evaluate performance on a corpus of 374,899 pathology reports obtained from the national_cancer_institute 's ( nci ) surveillance , epidemiology , and end results ( seer ) program . each pathology report is associated with five clinical classification_tasks – site , laterality , behavior , histology , and grade . we compare the performance of the hisan against other machine_learning and deep_learning approaches commonly used on medical text data – naive_bayes , logistic_regression , convolutional_neural_networks , and hierarchical_attention_networks ( the previous state-of-the-art ) . we show that hisans are superior to other machine_learning and deep_learning text classifiers in both accuracy and macro f-score across all five classification_tasks . compared to the previous state-of-the-art , hierarchical_attention_networks , hisans not only are an order of magnitude faster to train , but also achieve about 1 % better relative accuracy and 5 % better relative macro f-score .),\n",
       " (2,\n",
       "  title: new naive_bayes text_classification algorithm; abstract: according to the phenomena that the calculation of prior_probability in text_classification is time-consuming and has little effect on the classification result , and the accuracy loss of posterior_probability affects the accuracy of classification , the classical naive_bayes algorithm is improved and a new text_classification algorithm is proposed which restrains the effect of prior_probability and amplifies the effect of posterior_probability . in the new algorithm , the calculation of prior_probability is removed and an amplification factor is added to the calculation of posterior_probability . the experiments_prove that removing the calculation of prior_probability in text_classification can accelerate the classification speed and has little effect on the classification_accuracy , and adding an amplification factor in the calculation of posterior_probability can reduce the effect of error_propagation and improve the classification_accuracy .),\n",
       " (2,\n",
       "  title: olex : effective rule learning for text_categorization; abstract: this paper describes olex , a novel method for the automatic induction of rule-based text classifiers . olex supports a hypothesis language of the form `` if t1 or ⋯ or tn occurs in document d , and none of tn+1⋯tn + m occurs in d , then classify d under category c , where each ti is a conjunction of terms . the proposed method is simple and elegant . despite this , the results of a systematic experimentation performed on the reuters-21578 , the ohsumed , and the odp data collections show that olex provides classifiers that are accurate , compact , and comprehensible . a comparative_analysis conducted against some of the most well-known learning algorithms ( namely , naive_bayes , ripper , c4.5 , svm , and linear logistic_regression ) demonstrates that it is more than competitive in terms of both predictive_accuracy and efficiency . © 2006 ieee .),\n",
       " (2,\n",
       "  title: improved spam_email_filtering architecture using several feature_extraction techniques; abstract: research on spam_email_filtering is drawing experts from all over the world , as these junk email messages continue to affect people ’ s daily lives , whether consciously or unconsciously . the overwhelming use of irritating , destructive , and misleading emails appears to have damaged the values of email which prompted us to perform this research to construct a model for spam_filtering with faster_training time and enhanced accuracy . we have proposed two voting architectures built upon machine_learning models and ensemble_classifiers , respectively . in our work , we have also analyzed the performance of several individually applied classifiers and ensemble_techniques with various feature retrieval strategies . additionally , we have compared the training time of the proposed models with the deep lstm-cnn hybrid model . both of our suggested models have performed adequately , while the ml-based voting model ( type 1 ) produces the most accurate filtering ( 98 % ) taking bag of words for feature_extraction and can be trained above 200 times_faster than the lstm-cnn model .),\n",
       " (2,\n",
       "  title: adaptation of autoencoder for sparsity reduction from clinical_notes representation_learning; abstract: when dealing with clinical_text classification on a small dataset recent_studies have confirmed that a well-tuned multilayer_perceptron outperforms other generative classifiers , including deep_learning ones . to increase the performance of the neural_network classifier , feature_selection for the learning representation can effectively be used . however , most feature_selection methods only estimate the degree of linear dependency between variables and select the best features based on univariate statistical_tests . furthermore , the sparsity of the feature_space involved in the learning representation is ignored . goal : our aim is therefore to access an alternative approach to tackle the sparsity by compressing the clinical representation feature_space , where limited french clinical_notes can also be dealt with effectively . methods : this study proposed an autoencoder learning algorithm to take advantage of sparsity reduction in clinical note representation . the motivation was to determine how to compress sparse , high-dimensional data by reducing the dimension of the clinical note representation feature_space . the classification performance of the classifiers was then evaluated in the trained and compressed feature_space . results : the proposed approach provided overall performance_gains of up to 3 % for each evaluation . finally , the classifier achieved a 92 % accuracy , 91 % recall , 91 % precision , and 91 % f1-score in detecting the patient 's condition . furthermore , the compression working mechanism and the autoencoder prediction process were demonstrated by applying the theoretic information bottleneck framework .),\n",
       " (2,\n",
       "  title: classifying promotion images using optical_character_recognition and naïve_bayes classifier; abstract: promotion is one of the most effective ways to promote a business , and most people love promotions . usually these businesses announce their promo by uploading images to social medias such as instagram . however , most of the time these promo images are buried in the sea of other non-promotional images . it would be more practical if computers could be utilized to automatically look for images containing promotional offers . that is why this research is done to discuss about creating a system that is able to tell whether an image contains information about a promotional offer or not automatically without human intervention using optical_character_recognition ( ocr ) and naïve_bayes algorithm as the classifier . random_forest and k-nearest_neighbor are also used as a comparison to the naïve_bayes algorithm . in this experiment we use cross_validation method where we divide 158 images into five groups to train and test our model . the naïve_bayes model achieved 94,31 % accuracy , 94,33 % recall , 94,11 % precision , and 0.93 f1 score on average , which is the highest among these three algorithms . based on the results , we can conclude that optical_character_recognition ( ocr ) and naïve_bayes algorithm are quite suitable for this problem .),\n",
       " (2,\n",
       "  title: an enhanced artificial_neural_network based optical_character_recognition mechanism for business information_extraction and classification; abstract: -- automated text processing and information_extraction have gained attention of many researchers as it plays a vital role in the context of business information_processing and extraction . with the advent of various artificial_intelligence technologies using neural_networks which can certainly overcome this problem . this paper proposes a neural_networks and natural_language_processing based approach for hand_written and optical_character_recognition . the proposed methodology is based on a combination of optical_character_recognition ( ocr ) and a named_entity_recognition ( ner ) model for classification . the ocr produces text for a given image ( businesscard ) with is further classified by a well trained nlp-ner model to extract names and other details such as emails phone numbers , websites . furthermore , the obtained results indicate that the proposed method provides high efficiency of text_classification inspite of unstructured_text and lack of sentence formation in text extracted from business cards . the results obtained were further improved by the scikit-learn classifier and achieved 97.5 % accuracy on a significantly large dataset .),\n",
       " (2,\n",
       "  title: evidence weighted tree ensembles for text_classification; abstract: text documents are often mapped to vectors of binary values where 1 indicates the presence of a word and 0 indicates the absence . the vectors are then used to train predictive_models . in tree-based ensemble_models , predictions from some decision_trees may be made purely from absent words . this type of predictions should be trusted less as absent words can be interpreted in multiple ways . in this work , we propose to improve the comprehensibility and accuracy of ensemble_models by distinguishing word presence and absence . the presented method weights predictions based on word presence . experimental results on 35 real text datasets indicate that our method_outperforms state-of-the-art ensemble_methods on various text_classification tasks .),\n",
       " (2,\n",
       "  title: ensembles of classifiers for parallel categorization of large number of text documents expressing_opinions; abstract: opinions provided by people that used some services or purchased some goods are a rich source of knowledge . the opinion classification , applying mostly supervised_classifiers , is one of the essential tasks . computer ’ s technological capabilities are still a major obstacle , especially when processing huge volumes of data . this study proposes and evaluates experimentally a parallelism application to the classification of a very large number of contrary opinions_expressed as freely written text reviews . instead of training a single classifier on the entire data_set , an ensemble of classifiers is trained on disjunctive subsets of data and a group decision is used for the classification of unlabelled items . the main assessment criteria are computational_efficiency and error_rates , combined into a single measure to be able to compare ensembles of different sizes . support_vector_machines , artificial_neural_networks , and decision_trees , belonging to frequently used classification methods , were examined . the paper demonstrates the suggested method viability when the number of text reviews leads to computational_complexity , which is beyond the contemporary common pc ’ s capabilities . classification_accuracy and the values of other classification performance_measures ( precision , recall , f-measure ) did not decrease , which is a positive finding .),\n",
       " (2,\n",
       "  title: prominent feature_extraction for review analysis : an empirical_study; abstract: sentiment_analysis ( sa ) research has increased tremendously in recent_times . sa aims to determine the sentiment_orientation of a given text into positive or negative_polarity . motivation for sa research is the need for the industry to know the opinion of the users about their product from online portals , blogs , discussion boards and reviews and so on . efficient features need to be extracted for machine-learning algorithm for better sentiment_classification . in this paper , initially various features are extracted such as unigrams , bi-grams and dependency features from the text . in addition , new bi-tagged features are also extracted that conform to predefined part-of-speech patterns . furthermore , various composite features are created using these features . information gain ( ig ) and minimum_redundancy maximum relevancy ( mrmr ) feature_selection methods are used to eliminate the noisy and irrelevant_features from the feature_vector . finally , machine-learning algorithms are used for classifying the review document into positive or negative class . effects of different categories of features are investigated on four standard data-sets , namely , movie review and product ( book , dvd and electronics ) review data-sets . experimental results show that composite features created from prominent features of unigram and bi-tagged features perform better than other features for sentiment_classification . mrmr is a better feature_selection method as compared with ig for sentiment_classification . boolean multinomial_naïve_bayes ) algorithm performs better than support_vector_machine classifier for sa in terms of accuracy and execution time .),\n",
       " (2,\n",
       "  title: estimating the generalization performance of polynomial svm classifier for text_categorization; abstract: vc theory and structural risk minimization principle are key_concepts of statistical learning theory . developed from this theory , svm is widely investigated and used for text_categorization because of its high generalization performance . previous work showed that polynomial svm 's performance was irrevelant of the order and it was appropriate for high dimensional text_categorization problems without feature_selection . the research indicates over-fitting problems occur as the polynomial order increases . svm 's generalization performance decreases drastically if too many features are used , so feature_selection is necessary . based on the structural risk minimization principle , this fact is analyzed via estimating functional classes 's vc dimension . and the empirical results support the theoretical conclusions .),\n",
       " (2,\n",
       "  title: automated classification of social_network messages into smart_cities dimensions; abstract: a smart_city can be defined as a high-tech city with several public and private services capable to strategically solve ( or mitigate ) problems normally generated by rapid urbanization . different models of indicators have been developed to follow cities ’ evolution to become a smart_city . an example of such model is the standard 37120 from the international_organization_for_standardization ( iso ) that proposes a set of dimensions and indicators ( e.g . transportation , recreation , solid_waste ) for services and quality of life for sustainable cities and communities . it has been common to find official social_network profiles of organizations and governmental entities related to the services they provide or are responsible for ( water , waste , transportation , cultural events , etc . ) and that are used by citizens as a gateway to directly interact and communicate their complains and problems about those services . the present paper proposes to apply machine_learning algorithms over the urban data generated by social_networks in order to create classifiers to automatically categorize citizens messages according to the different cities services dimensions . for that , two distinct text datasets in portuguese were collected from two social_networks : twitter ( 1,950 tweets ) and colab.re ( 65,066 posts ) . the texts were mapped according to the different iso 37120 categories , preprocessed and mined through the use of 8 algorithms implemented in scikit-learn . initial results pointed out the feasibility of the proposal with models achieving average f1-measures around 55 % for f1-macro and 78 % for f1-micro when using linear vector classification , logistic_regression , decision_tree and complement naive_bayes . however , as the datasets were highly unbalanced , the performances of the models vary significantly for each iso category , with the best results occurring for wastewater , water & sanitation , energy and transportation . the classifiers generated here can be integrated on a number of different city services and systems such as : governmental support decision systems , customer complain systems , communities dashboards , police offices , transportation 's companies , cultural producers , environmental agencies , and recyclers ’ companies .),\n",
       " (2,\n",
       "  title: a feature based opinion_mining for product_reviews using naive_bayes and k-nearest_neighbor classifiers; abstract: the explosive_growth in the technology of web like social_media , directs more number of people to express their sentiments , feedbacks or opinions towards the service , events , individuals , topics , products or social_issues . since the data get bigger extensively in everyday_life , this makes so difficult for customers , manufactures as well as for end_users of online websites to derive a conclusion on the accessibility of big_data available as reviews . opinion_mining is referred to as a natural_language_processing technique that gives out the knowledge extraction of viewpoints or attitudes from the review texts . feature-based opinion_mining system aims to find the main aspects or features of a specified entity and the sentiment expressed on that entity by using natural_language_processing and ai techniques . most of the studies have been conducted on aspect_based_opinion_mining but not any of the particular works have justified to be adequate for assessing the critical or majorfactors . the major factors with respect to aspect_based_opinion_mining are implicit or implied aspects , explicit or direct aspects and multiple aspect based . the aspect_based_opinion_mining by considering all these critical factors helps in analyzing the aspect of a particular entity and its sentiment more accurately . in this paper , an explicit aspect_based_opinion_mining is carried out using naive_bayes and k-nearest neighborclassifiers to generate more accurate opinions for product_reviews . the end_user can easily get opinions based on the particular aspect of the entity and the proposed work proves that the sentiment_classification using naive_bayes_classifier provide with more accuracy than using k-nearest_neighbor classifier .),\n",
       " (2,\n",
       "  title: comparison of bert and xlnet accuracy with classical methods and algorithms in text_classification; abstract: the aim of this publication is to compare the accuracy of the bidirectional_encoder_representations_from_transformers ( bert ) and generalized autoregressive pretraining for language understanding ( xlnet ) models in text_classification with the accuracy of classical machine_learning methods and algorithms . analyzed : bidirectional_encoder_representations_from_transformers ( bert ) , generalized autoregressive pretraining for language understanding ( xlnet ) , bernoulli naive_bayes_classifier , gaussian_naive_bayes classifier , multinomial_naive_bayes classifier , support_vector_machines . the results show that when classifying 50,000 reviews in english , xlnet ranks with the highest_accuracy-96 % , which is nearly 8 % more than the best-performing classic classifier support_vector_machines .),\n",
       " (2,\n",
       "  title: statistical comparison of opinion_spam detectors in social_media with imbalanced_datasets; abstract: sentiment_analysis is a growing research area that analyzes people ’ s opinions towards a specific target using posts shared in social_media . however , spammers can inject false opinions to change sentiment-oriented decisions , e.g . low_quality products or policies can be promoted or advocated over others . therefore , identifying and removing spam posts in social_media is a crucial data cleaning operation for text_mining tasks including sentiment_analysis . an inherent problem related to spam_detection is the imbalanced-class problem . in this paper , we explore the impact of imbalance ratio on the performance of twitter spam_detection using multiple approaches of single and ensemble_classifiers . besides ensemble-based learning ( bagging and random_forest ) , we apply the smote oversampling technique to improve detection performance especially for classifiers sensitive to imbalanced_datasets .),\n",
       " (2,\n",
       "  title: a multiple-layer machine_learning architecture for improved accuracy in sentiment_analysis; abstract: twitter is an online micro-blogging platform through which one can explore the hidden valuable and delightful information about the current context at any point of time , which also serves as a data source to carry out sentiment_analysis . in this paper , the sentiments of large amount of tweets generated from twitter in the form of big_data have been analyzed using machine_learning algorithms . a multi-tier architecture for sentiment_classification is proposed in this paper , which includes modules such as tokenization , data cleaning , preprocessing , stemming , updated lexicon , stopwords and emoticon dictionaries , feature_selection and machine_learning classifier . unigram and bigrams have been used as feature_extractors together with χ2 ( chi-squared ) and singular_value_decomposition for dimensionality_reduction together with two model types ( binary and reg ) , with four types of scaling methods ( no scaling , standard , signed and unsigned ) and represented them in three different vector formats ( tf-idf , binary and int ) . accuracy is considered as the evaluation standard for random_forest and bagged trees classification methods . sentiments were analyzed through tokenization and having several stages of pre-processing and several combinations of feature_vectors and classification methods . through which it was possible to achieve an accuracy of 84.14 % . obtained results conclude that , the proposed scheme gives a better accuracy when compared with existing schemes in the literature .),\n",
       " (2,\n",
       "  title: a novel approach of sensitive data classification using convolution neural_network and logistic_regression; abstract: text_classification is a basic approach of text_mining and natural_language_processing . in previous use , classifiers use human interface features like frequency base and n-gram features which are not able to find non-linearity in features and increase overlapping in features which directly impacts the performance of classifiers . in this paper , proposed convolution based approach refines the traditional features in layered approach by activation_function . this process increase the effective pattern for learning which is learn by logistic_regression and optimized by boosting approach . in experiment , there is comparison of machine_learning approach which uses traditional features and deep_learning approach which refine the traditional approach for increasing non-linearity pattern . the results showed that proposed approach cnn-logistic_regression improves the accuracy significantly because of the improving pattern of features .),\n",
       " (2,\n",
       "  title: reducing misclassification due to overlapping classes in text_classification via stacking classifiers on different feature_subsets; abstract: correct classification of customer_support tickets or complaints can help companies to improve the quality of their services to the customers . one of the challenges in text_classification is when certain classes tend to share the same vocabulary . this can result in misclassification by the machine_learning algorithm used . the problem is worsened when the dataset is imbalanced . to address this issue , we propose a stacking algorithm based on combining different selected classifiers that operate on different feature_subsets ; depending on those features that tend to improve the recall and the precision of the overlapped classes . in our approach , first , we train different linear and non-linear_classifiers on the full feature_set . second , we use the chi2 test to determine the best feature_set for all our pre-trained classifiers that improve the f1-score for the overlapped class ( es ) . finally , we train a two-layered stacked model composed of the best base_learners obtained from the first step as layer-1 and combine it with a strong meta-learner for the second layer . the experimental results on a real-world dataset from a large it organization and a public consumer complaint database show an improvement in the overall accuracy as well as a reduction in the misclassification_rate for the overlapped classes .),\n",
       " (2,\n",
       "  title: performance evaluation of hybrid feature_selection technique for sentiment_classification based on food reviews; abstract: this paper presents an evaluation of the performance efficiency of sentiment_classification using a hybrid feature_selection technique . this technique is able to overcome the issue of lack in evaluating features importance by using a combination of tf-idf+svm-rfe ( term_frequency-inverse_document_frequency ( tf-idf ) and supports vector machine ( svm-rfe ) ) . feature_importance is measured and significant features are selected recursively based on the number of significant features known as k-top features . we tested this technique with a food reviews dataset from kaggle to classify a positive and negative review . finally , svm has been deployed as a classifier to evaluate the classification performance . the performance is observed based on the accuracy , precision , recall and f-measure . the highest_accuracy is 80 % , precision is 82 % , recall is 76 % and f-measure is 79 % . consequently , 24.5 % of the features to be classified in this technique have been reduced in obtaining these highest results . thus , the computational_resources are able to be utilized optimally from this reduction and the classification performance efficiency is able to be maintained .),\n",
       " (2,\n",
       "  title: an ensemble technique to detect fabricated news article using machine_learning and natural_language_processing techniques; abstract: fake_news or fabricated news , refers to false_information published under the guise of being authentic news , often to influence political views . fabricated news articles are a threat to people 's trust in the government and in effect , one of the biggest threats that modern-day democracies are facing . as the menace of fake_news is growing with each passing day , so is the research community getting more actively involved in curbing this issue . this paper reviews the current progress of the advancements done to solve the issue . the paper also presents various ensemble_techniques to perform the binary classification of news articles . additionally , the paper focuses on sources of articles to widen misclassification tolerance and make more accurate predictions . evaluation_metrics such as accuracy score , precision , recall , f-1 score have been used to make comparison among various models . the best performing model has been an ensemble of decision_tree , logistic_regression , bagging classifier used with a hard-voting_ensemble technique , which gives the accuracy of over 88 % .),\n",
       " (2,\n",
       "  title: a linguistic system for predicting sentiment in arabic tweets; abstract: the term sentiment_analysis is considered very important in our current era , especially with widespread of social_media , as it helps understand people 's feelings , behavior and opinions about a specific behavior or entity , individuals , organizations and any related topic . recently , with the development of machine_learning , there have been many studies concerned with analyzing feelings . still , most of these researches are concerned with the english_language more than other languages . this paper proposes a model for working with standard_arabic and some other arabic dialects such as levantine , egyptian , and gulf . working with the arabic_language poses several challenges due to the complex structure of the language , the large number of dialects used and the lack of associated resources . the data collected was divided into positive , negative , and neutral . several algorithms were used to predict sentiment in arabic texts such as naive_bayes classifiers ( nb ) , support_vector_machine ( svm ) , random_forest classifier , and bert model ( bidirectional_encoder_representations_from_transformers ) . the results obtained are very encouraging , especially with the bert model ( bidirectional_encoder_representations_from_transformers ) that gave very accurate results during the test , reaching more than 83 % .),\n",
       " (2,\n",
       "  title: a novel rs attribute_reduction technique for chinese classification in cqa; abstract: with the rapid development of the question and answer services which are based on community , like sina iask , baidu zhidao and yahoo ! answers , the community-based question_answering , cqa , service has become a new knowledge-sharing model , which has characteristic of interactivity , openness etc . increasing more people use the services which are provided on these sites to meet the information needs . in order to accurately understand the user ’ s query and provide useful information , it is necessary to deal with the questions in the community , and the question_classification in the cqa is the key component in this step . however , the difficulty of the question_classification is high-dimensional feature_vector , which usually uses the feature_selection as the primary method of dimensionality_reduction , and in this paper , a combined extract features method is presented to screen the question features for the first time to obtain a feature_subset . then the importance and dependence of the rough_set is used as the heuristic information for the feature_selection to further screen the useful features . experiments show that the algorithm is effective , and it not only make the question of feature dimensions reduced to some extent , but also improve the classification_accuracy of the question .),\n",
       " (2,\n",
       "  title: an incremental approach to classify healthcare urls using a novel ‘ web document_classification model ’; abstract: in the present research work , we proposed a novel web document_classification model ( wdcm ) to classify healthcare-related urls . this is required as the web is flooded with millions of urls and they may be duplicated or irrelevant most of the time . it is a challenging task to extract and organize web_documents specific to a certain domain . another challenge is to provide the updated and latest information to the users . in this research , the authors have proposed wdcm which tackles the above-mentioned challenges with the incremental_learning approach . authors have also used sentiment_analysis and document similarity_measures to classify urls in the healthcare domain . the analysis of the experimental results shows that with machine_learning classifiers , cosine_similarity measure with logistic_regression showed the highest_accuracy of 93.75 % . euclidean distance ( ed ) measure with logistic_regression showed minimum accuracy of 86.6 % . when implemented as an algorithm ed measure showed the highest_accuracy of 96.60 % for training and 95.83 % for the testing dataset in five iterations . jaccard distance_measure showed lower accuracy of 65.71 % for training and 85.33 for the testing dataset in five iterations . it is also observed that the maximum urls fetched are with the.com domain .),\n",
       " (2,\n",
       "  title: comparison of the accuracy and the execution time of classification_algorithms for bulgarian literary works; abstract: the purpose of this paper is to compare the accuracy and the execution time of machine_learning algorithms for classification of texts , written by bulgarian authors . the algorithms examined are : multinomial_naive_bayes classifier , support_vector_machines , random_forest and adaboost . the results show that the multinomial_naive_bayes classifier is the most accurate and fastest algorithm for classifying texts by two authors with an equal number of poems in bulgarian_language . the ensemble algorithm adaboost is the most accurate for unbalanced_data classification . the support_vector classification has the highest_accuracy . in a classification with an unbalanced set of data , the fastest algorithm is bernoulli naive_bayes_classifier .),\n",
       " (2,\n",
       "  title: feature_selection for text_classification using or+svm-rfe; abstract: feature_selection is the key_issue in text_classification because there are a large number of attributes . in this paper , we propose a new algorithm or+svm-rfe that integrates odds radio ( or ) with recursive feature_elimination based on svm ( svm-rfe ) . odds radio is first used to roughly and rapidly select a feature_subset . then svm-rfe is used to delicately select a smaller feature_subset . experiment results show the feature_subset selected by or+svm-rfe obtains a good classification performance with less features . ©2010 ieee .),\n",
       " (2,\n",
       "  title: automated_essay_scoring using multi-classifier fusion; abstract: the method of multi-classifier fusion was applied to essay scoring . in this paper , each essay was represented by vector_space_model ( vsm ) . after removing the stopwords , we extracted the features of contents and linguistics from the essays , and each vector was expressed by corresponding weight . three classical approaches including document_frequency ( df ) , information gain ( ig ) and chi-square statistic ( chi ) were used to select features by some predetermined thresholds . according to the experimental results , we classified the test essay to appropriate category using different classifiers , such as naive_bayes ( nb ) , k_nearest_neighbors ( knn ) and support_vector_machine ( svm ) . finally the ensemble_classifier was combined by those component classifiers . after training for multi-classifier fusion technique , the experiments on cet4 essays about same topic in chinese learner english corpus ( clec ) show that precision over 73 % was achieved . ©_2011_springer-verlag .),\n",
       " (2,\n",
       "  title: chi_square feature_extraction based svms arabic_text_categorization system; abstract: this paper aims to implement a support_vector_machines ( svms ) based text_classification system for arabic_language articles . this classifier uses chi_square method as a feature_selection method in the preprocessing_step of the text_classification system design procedure . comparing to other classification methods , our classification system shows a high classification effectiveness for arabic articles term of macroaveraged fl = 88.11 and microaveraged fl = 90.57 .),\n",
       " (2,\n",
       "  title: exploiting term relationship to boost text_classification; abstract: document_classification provides an effective way to handle the explosive online textual_data . however , in practical classification settings , we face the so-called feature_sparsity problem caused by a lack of training documents or the shortness of text to be classified . in this paper , we solve the sparsity problem by exploiting term relationships along with naive_bayes classifiers . the first method is to estimate term relationships based on the co-occurrence information of two terms in a certain context . the second method estimates the term relationships based on the distribution of terms over different hierarchical categories in a publicly available document taxonomy . thereafter , term relationship is used to augment naive_bayes classifiers . we test our methods on two open-domain data sets to demonstrate its advantages . the experimental results show that our method can significantly_improve the classification performance , especially when we do not have enough training_data or the texts are web_search queries . copyright 2009 acm .),\n",
       " (2,\n",
       "  title: sentiment_classification of customer_reviews about automobiles in roman urdu; abstract: text_mining is a broad field having sentiment mining as its important constituent in which we try to deduce the behavior of people towards a specific item , merchandise , politics , sports , social_media comments , review_sites etc . out of many issues in sentiment mining , analysis and classification , one major issue is that the reviews and comments can be in different languages like english , arabic , urdu etc . handling each language according to its rules is a difficult task . a lot of research work has been done in english_language for sentiment_analysis and classification but limited sentiment_analysis work is being carried out on other regional_languages like arabic , urdu and hindi . in this paper , waikato environment for knowledge analysis ( weka ) is used as a platform to execute different classification models for text_classification of roman urdu text . reviews dataset has been scrapped from different automobiles sites . these extracted roman urdu reviews , containing 1000 positive and 1000 negative reviews , are then saved in weka attribute-relation file_format ( arff ) as labeled_examples . training is done on 80 % of this data and rest of it is used for testing purpose which is done using different models and results are analyzed in each case . the results show that multinomial_naive_bayes outperformed bagging , deep_neural_network , decision_tree , random_forest , adaboost , k-nn and svm classifiers in terms of more accuracy , precision , recall and f-measure .),\n",
       " (2,\n",
       "  title: data-based prediction of sentiments using heterogeneous model ensembles; abstract: in this paper , we present an ensemble modeling approach for sentiment_analysis using machine_learning algorithms . the main_goal of sentiment_analysis is to develop estimators that are able to identify the sentiment_orientation ( positive , negative , or neutral ) of sentences found in any arbitrary source . the novel approach presented here relies on the analysis of the words found in sentences and the formation of large sets of heterogeneous models , i.e. , binary as well as multi-class classification models that are calculated by various different machine_learning methods ; these models shall represent the relationship between the presence of given words ( or combination of words ) and sentiments . all models trained during the learning phase are applied during the test phase and the final sentiment assessment is annotated with a confidence value that specifies , how reliable the models are regarding the presented decision . in the empirical part of this paper , we show results achieved using a german corpus of amazon recensions and a set of machine_learning methods ( decision_trees and adaptive_boosting , gaussian_processes , random_forests , k-nearest_neighbor classification , support_vector_machines and artificial_neural_networks with evolutionary feature and parameter_optimization , and genetic_programming ) . using a heterogeneous model ensemble_learning approach that combines multi-class classifiers as well as binary classifiers , the classification_accuracy can be increased significantly and the ratio of totally wrongly classified samples ( i.e. , those that are assigned to the completely opposite sentiment_orientation ) can be decreased significantly .),\n",
       " (2,\n",
       "  title: empirical_study of the model generalization for argument_mining in cross-domain and cross-topic settings; abstract: to date , the number of studies that address the generalization of argument models is still relatively small . in this study , we extend our stacking model from argument_identification to an argument_unit classification_task . using this model , and for each of the learned tasks , we address three real-world scenarios concerning the model robustness over multiple datasets , different domains and topics . consequently , we first compare single-datset learning ( sdl ) with multi-dataset learning ( mdl ) . second , we examine the model generalization over completely unseen dataset in our cross-domain experiments . third , we study the effect of sample and topic sizes on the model performance in our cross-topic experiments . we conclude that , in most cases , the ensemble_learning stacking approach is more stable over the generalization tests than a transfer_learning distilbert model . in addition , the argument_identification task seems to be easier to generalize across shifted domains than argument_unit classification . this work aims at filling the gap between computational argumentation and applied machine_learning with regard to the model generalization .),\n",
       " (2,\n",
       "  title: multi-class twitter sentiment_classification with emojis; abstract: purpose : recently , various twitter sentiment_analysis ( tsa ) techniques have been developed , but little has paid attention to the microblogging feature – emojis , and few works have been conducted on the multi-class sentiment_analysis of tweets . the purpose of this paper is to consider the popularity of emojis on twitter and investigate the feasibility of an emoji training heuristic for multi-class sentiment_classification of tweets . tweets from the “ 2016 orlando nightclub shooting ” were used as a source of study . besides , this study also aims to demonstrate how mapping can contribute to interpreting sentiments . design/methodology/approach : the authors presented a methodological framework to collect , pre-process , analyse and map public twitter postings related to the shooting . the authors designed and implemented an emoji training heuristic , which automatically prepares the training_data set , a feature needed in big_data research . the authors improved upon the previous framework by advancing the pre-processing techniques , enhancing feature_engineering and optimising the classification models . the authors constructed the sentiment model with a logistic_regression classifier and selected features . finally , the authors presented how to visualise citizen sentiments on maps dynamically using mapbox . findings : the sentiment model constructed with the automatically annotated training_sets using an emoji approach and selected features performs well in classifying tweets into five different sentiment classes , with a macro-averaged f-measure of 0.635 , a macro-averaged accuracy of 0.689 and the maem of 0.530 . compared to those experimental results in related_works , the results are satisfactory , indicating the model is effective and the proposed emoji training heuristic is useful and feasible in multi-class tsa . the maps authors created , provide a much easier-to-understand visual representation of the data , and make it more efficient to monitor citizen sentiments and distributions . originality/value : this work appears to be the first to conduct multi-class sentiment_classification on twitter with automatic annotation of training_sets using emojis . little attention has been paid to applying tsa to monitor the public ’ s attitudes towards terror attacks and country ’ s gun policies , the authors consider this work to be a pioneering work . besides , the authors have introduced a new data set of 2016 orlando shooting tweets , which will be made available for other researchers to mine the public ’ s political opinions about gun policies .),\n",
       " (2,\n",
       "  title: fake_news classification of social_media through sentiment_analysis; abstract: the impacts of the internet and the ability for information to flow in real-time to all corners of the globe has brought many benefits to society . however , this capability has downsides . information can be inexact , misleading or indeed downright and deliberately false . fake_news has now entered the common vernacular . in this work , we consider fake_news with specific regard to social_media . we hypothesise that fake_news typically deals with emotive topics that are deliberated targeted to cause a reaction and encourages the spread of information . as such , we explore sentiment_analysis of real and fake_news as reported in social_networks ( twitter ) . specifically , we develop an aws-based cloud platform utilising news contained in the untrustworthy resource fakenewsnet and a more trusted resource credbank . we train algorithms using naive_bayes , decision_tree and bi-lstm for sentiment_classification and feature_selection . we show how social_media sentiment can be used to improve the accuracy in identification of fake_news from real_news .),\n",
       " (2,\n",
       "  title: performance analysis of machine_learning classifiers on improved concept vector_space models; abstract: this paper provides a comprehensive performance analysis of parametric and non-parametric machine_learning classifiers including a deep feed-forward multi-layer_perceptron ( mlp ) network on two variants of improved concept vector_space ( icvs ) model . in the first variant , a_weighting scheme enhanced with the notion of concept importance is used to assess weight of ontology concepts . concept importance shows how important a concept is in an ontology and it is automatically computed by converting the ontology into a graph and then applying one of the markov based algorithms . in the second variant of icvs , concepts provided by the ontology and their semantically_related terms are used to construct concept vectors in order to represent the document into a semantic vector_space . we conducted various experiments using a variety of machine_learning classifiers for three different models of document_representation . the first model is a baseline concept vector_space ( cvs ) model that relies on an exact/partial match technique to represent a document into a vector_space . the second and third model is an icvs model that employs an enhanced concept weighting_scheme for assessing weights of concepts ( variant 1 ) , and the acquisition of terms that are semantically_related to concepts of the ontology for semantic document_representation ( variant 2 ) , respectively . additionally , a comparison between seven different classifiers is performed for all three models using precision , recall , and f1 score . results for multiple configurations of deep_learning architecture are obtained by varying the number of hidden_layers and nodes in each layer , and are compared to those obtained with conventional classifiers . the obtained results show that the classification performance is highly dependent upon the choice of a classifier , and that the random_forest , gradient boosting , and multilayer_perceptron are among the classifiers that performed rather well for all three models .),\n",
       " (2,\n",
       "  title: a systematic_review of text_classification research based on deep_learning models in arabic_language; abstract: classifying or categorizing texts is the process by which documents are classified into groups by subject , title , author , etc . this paper undertakes a systematic_review of the latest research in the field of the classification of arabic texts . several machine_learning techniques can be used for text_classification , but we have focused only on the recent trend of neural_network algorithms . in this paper , the concept of classifying texts and classification processes are reviewed . deep_learning techniques in classification and its type are discussed in this paper as well . neural_networks of various types , namely , rnn , cnn , ffnn , and lstm , are identified as the subject of study . through systematic_study , 12 research papers related to the field of the classification of arabic texts using neural_networks are obtained : for each paper the methodology for each type of neural_network and the accuracy ration for each type is determined . the evaluation_criteria used in the algorithms of different neural_network types and how they play a large role in the highly_accurate classification of arabic texts are discussed . our results provide some findings regarding how deep_learning models can be used to improve text_classification research in arabic_language .),\n",
       " (2,\n",
       "  title: a comparison of svm against pre-trained_language_models ( plms ) for text_classification tasks; abstract: the emergence of pre-trained_language_models ( plms ) has shown great_success in many natural_language_processing ( nlp ) tasks including text_classification . due to the minimal to no feature_engineering required when using these models , plms are becoming the de facto choice for any nlp task . however , for domain-specific corpora ( e.g. , financial , legal , and industrial ) , fine-tuning a pre-trained model for a specific task has shown to provide a performance_improvement . in this paper , we compare the performance of four different plms on three public domain-free datasets and a real-world dataset containing domain-specific words , against a simple svm linear_classifier with tfidf vectorized text . the experimental results on the four datasets show that using plms , even fine-tuned , do not provide significant gain over the linear_svm classifier . hence , we recommend that for text_classification tasks , traditional svm along with careful feature_engineering can pro-vide a cheaper and superior performance than plms .),\n",
       " (2,\n",
       "  title: categorization of medical documents using hybrid competitive neural_network with string_vector , a novel approach; abstract: text_categorization is one of the well studied problems in data_mining and information_retrieval . even if the research on text_categorization has been progressed very much , traditional_approaches to text_categorization require encoding documents into numerical_vectors which leads to the two main problems : huge dimensionality and sparse distribution in each numerical_vector . although many various feature_selection methods are developed to address the first problem , the reduced dimension remains still large . if the dimension is reduced excessively by a feature_selection method , robustness of text_categorization is degraded . the idea of this research as the solution to the problems is to encode medical documents into string_vectors and apply it to the novel competitive neural_network as a string_vector . the quantitative experiment_results_demonstrate that this method can significantly_improve the performance of medical document_classification . ©_2013_springer-verlag .),\n",
       " (2,\n",
       "  title: classifying arabic tweets based on credibility using content and user features; abstract: social_media services , such as facebook and twitter , have recently become a huge and continuous source of daily news . people all around the world rely heavily on news published via social_media to know more about current events and activities . as a result , many users have started to exploit social_media by broadcasting misleading news for financial and political purposes , which has an adverse impact on society . in this paper , we utilize machine_learning to identify fake_news from arabic tweets based on a supervised classification model . twitter content published in arabic is very noisy with a high level of uncertainty , where little work has been accomplished to process and extract important features for classification purposes . in this paper , we utilize content-and user-related features , and employ sentiment_analysis to generate new features for the detection of fake arabic news . sentiment_analysis led to improving the accuracy of the prediction process . among a number of machine_learning algorithms used to train the classification models , four algorithms are chosen , namely random_forest , decision_tree , adaboost , and logistic_regression . the experimental evaluation shows that our system can filter out fake_news with an accuracy of 76 % .),\n",
       " (2,\n",
       "  title: ssncse_nlp @ hasoc-dravidian-codemixfire2020 : offensive_language_identification on multilingual code_mixing text; abstract: the number of social_media users is increasing_rapidly . a myriad of people have started using native languages in roman alphabets . therefore , it has becomes a big concern to regulate the quality of the text content and messages that are being shared to the internet . in this paper we study the task of offensive message identification for tamil-english and malayalam-english code-mixed content . the char n-gram , tfidf and fine-tuned bert are compared in combination with machine_learning models such as mlp , random_forest and naive_bayes . this work explains the submissions made by ssncse_nlp in hasoc code-mix tasks for hate_speech and offensive_language_detection . we achieve f1 scores of 0.94 for task1-malayalam , 0.75 for task2-malayalam and 0.88 for task2-tamil on the test-set .),\n",
       " (2,\n",
       "  title: a comparison of approaches for imbalanced classification problems in the context of retrieving relevant_documents for an analysis; abstract: one of the first steps in many text-based social_science studies is to retrieve documents that are relevant for the analysis from large corpora of otherwise irrelevant_documents . the conventional approach in social_science to address this retrieval task is to apply a set of keywords and to consider those documents to be relevant that contain at least one of the keywords . but the application of incomplete keyword lists risks drawing biased inferences . more complex and costly methods such as query_expansion techniques , topic_model-based classification rules , and active as well as passive supervised_learning could have the potential to more accurately separate relevant from irrelevant_documents and thereby reduce the potential size of bias . yet , whether applying these more expensive approaches increases retrieval performance compared to keyword lists at all , and if so , by how much , is unclear as a comparison of these approaches is lacking . this study closes this gap by comparing these methods across three retrieval tasks associated with a data set of german tweets ( linder , 2017 ) , the social_bias inference corpus ( sbic ) ( sap et al. , 2020 ) , and the reuters-21578 corpus ( lewis , 1997 ) . results show that query_expansion techniques and topic_model-based classification rules in most studied settings tend to decrease rather than increase retrieval performance . active supervised_learning , however , if applied on a not too small set of labeled training_instances ( e.g . 1,000 documents ) , reaches a substantially higher retrieval performance than keyword lists .),\n",
       " (1,\n",
       "  title: a comparison of classification methods for predicting deception in computer-mediated_communication; abstract: the increased chance of deception in computer-mediated_communication and the potential risk of taking action based on deceptive information calls for automatic detection of deception . to achieve the ultimate_goal of automatic prediction of deception , we selected four common classification methods and empirically compared their performance in predicting deception . the deception and truth data were collected during two experimental studies . the results suggest that all of the four methods were promising for predicting deception with cues to deception . among them , neural_networks exhibited consistent performance and were robust across test settings . the comparisons also highlighted the importance of selecting important input variables and removing noise in an attempt to enhance the performance of classification methods . the selected cues offer both methodological and theoretical contributions to the body of deception and information systems research . © 2004 m.e . sharpe , inc .),\n",
       " (1,\n",
       "  title: native language identification of fluent and advanced non-native writers; abstract: native language identification ( nli ) aims at identifying the native languages of authors by analyzing their text samples written in a non-native language . most existing_studies investigate this task for educational applications such as second_language_acquisition and require the learner corpora . this article performs nli in a challenging context of the user-generated-content ( ugc ) where authors are fluent and advanced non-native speakers of a second language . existing nli studies with ugc ( i ) rely on the content-specific/social-network features and may not be generalizable to other domains and datasets , ( ii ) are unable to capture the variations of the language-usage-patterns within a text sample , and ( iii ) are not associated with any outlier handling mechanism . moreover , since there is a sizable number of people who have acquired non-english second languages due to the economic and immigration policies , there is a need to gauge the applicability of nli with ugc to other languages . unlike existing_solutions , we define a topic-independent feature_space , which makes our solution generalizable to other domains and datasets . based on our feature_space , we present a solution that mitigates the effect of outliers in the data and helps capture the variations of the language-usage-patterns within a text sample . specifically , we represent each text sample as a point set and identify the top-k stylistically similar text samples ( ssts ) from the corpus . we then apply the probabilistic k_nearest_neighbors ' classifier on the identified top-k ssts to predict the native languages of the authors . to conduct_experiments , we create three new corpora where each corpus is written in a different language , namely , english , french , and german . our experimental studies show that our solution outperforms competitive methods and reports more than 80 % accuracy across languages .),\n",
       " (1,\n",
       "  title: sentiment_analysis of twitter data; abstract: nowadays , people from all around the world use social_media sites to share information . twitter for example is a platform in which users send , read posts known as 'tweets ' and interact with different communities . users share their daily lives , post their opinions on everything such as brands and places . companies can benefit from this massive platform by collecting data related to opinions on them . the aim of this paper is to present a model that can perform sentiment_analysis of real data collected from twitter . data in twitter is highly unstructured which makes it difficult to analyze . however , our proposed model is different from prior work in this field because it combined the use of supervised and unsupervised machine_learning algorithms . the process of performing sentiment_analysis as follows : tweet extracted directly from twitter api , then cleaning and discovery of data performed . after that the data were fed into several models for the purpose of training . each tweet extracted classified based on its sentiment whether it is a positive , negative or neutral . data were collected on two subjects mcdonalds and kfc to show which restaurant has more popularity . different machine_learning algorithms were used . the result from these models were tested using various testing metrics like cross_validation and f-score . moreover , our model demonstrates strong performance on mining texts extracted directly from twitter .),\n",
       " (1,\n",
       "  title: sentiment_analysis on social_media data using intelligent techniques; abstract: social_media gives a simple method of communication technology for people to share their opinion , attraction and feeling . the aim of the paper is to extract various sentiment behaviour and will be used to make a strategic decision and also aids to categorize sentiment and affections of people as clear , contradictory or neutral . the data was preprocessed with the help of noise_removal for removing the noise . the research work applied various techniques . after the noise_removal , the popular classification methods were applied to extract the sentiment . the data were classified with the help of multi-layer_perceptron ( mlp ) , convolutional_neural_networks ( cnn ) . these two classification results were checked against the others classified such as support_vector_machine ( svm ) , random_forest , decision_tree , naïve_bayes , etc. , based on the sentiment_classification from twitter data and consumer affairs website . the proposed work found that multi-layer_perceptron and convolutional_neural_networks performs better than another machine_learning classifier .),\n",
       " (1,\n",
       "  title: similarity-based techniques for text document_classification; abstract: with large_scale text_classification labeling a large number of documents for training poses a considerable burden on human experts who need to read each document and assign it to appropriate categories . with this problem in mind , our goal was to develop a text_categorization system that uses fewer labeled_examples for training to achieve a given level of performance using a similarity-based learning algorithm and thresholding strategies . experimental results show that the proposed model is quite useful to build document_categorization systems . this has been designed for a small level implementation considering the size of the corpus being used . this can be enhanced for a larger data set and the efficiency can be proved against the performance of the presently available methods like svm , naive_bayes etc . this approach on the whole concentrates on categorizing small level documents and does the assigned task with completeness . © medwell journals , 2008 .),\n",
       " (1,\n",
       "  title: a two-stage biomedical event_trigger detection method based on hybrid neural_network and sentence_embeddings; abstract: biomedical_event_extraction is a challenging task in biomedical_text_mining , which plays an important role in improving biomedical_research and disease prevention . as the crucial and prerequisite step in event_extraction , biomedical trigger_detection has attracted much attention . previous_approaches usually depended on feature_engineering with unbalanced_data . in this paper , we propose a two-stage method based on hybrid neural_network for trigger_detection , which divides trigger_detection into recognition stage and classification stage . in the first stage , we build a bilstm based recognition model integrating attention_mechanism ( att-bilstm ) . in the second stage , the classification model based on passive-aggressive online_algorithm is constructed . furthermore , to enrich sentence-level features , we establish sentence_embeddings and add reading gate . on the multi-level event_extraction ( mlee ) corpus test dataset , our method_achieves an f-score of 80.26 % , which achieves the state-of-the-art systems .),\n",
       " (1,\n",
       "  title: extended category learning with spiking nets and spike timing dependent plasticity; abstract: neuroscience makes use of models of neurons , synapases , and learning rules that modify the efficiency of synapses in stimulating neurons . these models can be used to simulate spiking neural_networks , and the standard learning rule is based on the timing of the spikes of the pre and post-synaptic neurons . this paper describes the use of these models to categorise documents by translating this spike timing dependent plasticity into an unsupervised_learning rule by representing documents and categories in neurons and presenting them in specific fashion for learning and categorisation . the resulting system is comparable to other unsupervised machine_learning systems . this presentation mechanism is extended to combine input feature value pairs to resolve the exclusive or problem . it is further refined to approximate co-variance of features to an arbitrary degree of precision .),\n",
       " (1,\n",
       "  title: security-level classification for confidential documents by using adaptive_neuro-fuzzy inference systems; abstract: the security-level detection of a confidential document is a vital task for organizations to protect their confidential_information . diverse classification rules and techniques are being applied by human experts . increasing number of confidential_information in organizations is making difficult to classify all the documents carefully with human effort . the recommended frameworks in this study classify the internal documents of tubitak uekae ( national research institute of electronics and cryptology of turkey ) by using classification_algorithms naïve_bayes , support_vector_machines ( svms ) and adaptive_neuro-fuzzy inference systems ( anfiss ) . a hybrid approach involving support_vector classifiers and adaptive_neuro-fuzzy classifiers exposes the most successful accuracy_rates of expert system classification . this study also states preprocessing_tasks required for document_classification with natural_language_processing . to represent term-document relations , a recommended metric tf-idf was chosen to construct a weight_matrix . agglutinative nature of turkish documents is handled by turkish stemming algorithms . at the end of the article , some experimental results and success metrics are projected with accuracy_rates and receiver operating characteristic ( roc ) curves . © 2012 wiley publishing ltd .),\n",
       " (1,\n",
       "  title: genetic optimization of big_data sentiment_analysis; abstract: this paper deals with opinion_mining from unstructured textual_documents . the proposed method focuses on approach with minimum preliminary requirements about the knowledge of the analysed language and thus it can be deployed to any language . the proposed method builds on artificial_intelligence , which consists of support_vector_machines classifier , big_data analysis and genetic_algorithm optimization . to make the optimization feasible together with big_data approach we have proposed ga operators , which significantly accelerate conversion to the accurate solutions . in this work we outperformed the traditional_approaches ( which use language dependent text_preprocessing ) for text valence classification with the highest achieved accuracy 90.09 % . the data set for validation was czech texts .),\n",
       " (1,\n",
       "  title: on the use of emojis to train emotion classifiers; abstract: nowadays , the automatic detection of emotions is employed by many applications in different fields like security informatics , e-learning , humor detection , targeted_advertising , etc . many of these applications focus on social_media and treat this problem as a classification problem , which requires preparing training_data . the typical method for annotating the training_data by human experts is considered time consuming , labor intensive and sometimes prone to error . moreover , such an approach is not easily extensible to new domains/languages since such extensions require annotating new training_data . in this study , we propose a distant_supervised learning approach where the training sentences are automatically annotated based on the emojis they have . such training_data would be very cheap to produce compared with the manually_created training_data , thus , much larger training_data can be easily obtained . on the other hand , this training_data would naturally have lower quality as it may contain some errors in the annotation . nonetheless , we experimentally show that training classifiers on cheap , large and possibly erroneous data annotated using this approach leads to more accurate results compared with training the same classifiers on the more expensive , much smaller and error-free manually_annotated training_data . our experiments are conducted on an in-house dataset of emotional arabic tweets and the classifiers we consider are : support_vector_machine ( svm ) , multinomial_naive_bayes ( mnb ) and random_forest ( rf ) . in addition to experimenting with single classifiers , we also consider using an ensemble of classifiers . the results show that using an automatically annotated training_data ( that is only one order of magnitude larger than the manually_annotated one ) gives better results in almost all settings considered .),\n",
       " (1,\n",
       "  title: a big_data approach to sentiment_analysis using greedy feature_selection with cat swarm_optimization-based long short-term_memory neural_networks; abstract: sentiment_analysis is crucial in various systems such as opinion_mining and predicting . considerable research has been done to analyze sentiment using various machine_learning techniques . however , the high error_rates in these studies can reduce the entire system ’ s efficiency . we introduce a novel big_data and machine_learning technique for evaluating sentiment_analysis processes to overcome this problem . the data are collected from a huge volume of datasets , helpful in the effective analysis of systems . the noise in the data is eliminated using a preprocessing data_mining concept . from the cleaned sentiment data , effective features are selected using a greedy approach that selects optimal features processed by an optimal classifier called cat swarm_optimization-based long short-term_memory neural_network ( cso-lstmnn ) . the classifiers analyze sentiment-related features according to cat behavior , minimizing error_rate while examining features . this technique helps improve system efficiency , analyzed using experimental results of error_rate , precision , recall , and accuracy . the results obtained by implementing the greedy feature and cso-lstmnn algorithm and the particle_swarm_optimization ( pso ) algorithm are compared ; cso-lstmnn outperforms pso in terms of increasing accuracy and decreasing error_rate .),\n",
       " (1,\n",
       "  title: an ensemble model for stance_detection in social_media texts; abstract: the aim of this paper is to develop a model to classify the stance expressed in social_media texts . more specifically , the work presented focuses on tweets . in stance_detection ( sd ) tasks , the objective is to identify the stance of a person towards a target of interest . in this paper , a model for sd is established and its variations are evaluated using different classifiers . the single models differ based on the pre-processing and the combination of features . to reduce the dimensionality of the feature_space , analysis of variance ( anova ) test is used . then , two classifiers are employed as base_learners including random_forests ( rf ) and support_vector_machines ( svm ) . experimental analyses are conducted on semeval dataset that is used as a benchmark for sd . finally , the base_learners that resulted from different design alternatives , are combined into three ensemble_models . experimental results show the significance of the used features and the effectiveness of a manually built dictionary that is used in the pre-processing stage . moreover , the proposed ensembles outperform the state-of-the-art models in the overall test score , which suggests that ensemble_learning is the best tool for effective sd in tweets .),\n",
       " (1,\n",
       "  title: infosuggest : a system for automated information gathering : with a real-world case_study; abstract: departments of many organizations treat the world_wide_web as an important information source . they have a need to keep themselves up-to-date with current information in their domain . such information gathering is a time consuming process due to overload of available information and there are dedicated teams in many organizations for this task . in this paper , we present info suggest , a system for end-to-end information gathering from the web . info suggest improves efficiency of such focused information gathering process with the use of machine_learning . we employ a semi-supervised document_classification method called transductive support_vector_machines ( tsvms ) for learning user_preferences based on example articles provided by them . we also devise a strategy for unlabeled_data selection tsvm-meta that is applicable for an information gathering setting . in the paper , we discuss the system architecture and also present a case_study for information gathering for food_safety in an environmental_health department of a government_agency . we conduct_experiments and demonstrate that our system results in improving the efficiency by as much as 35 % by making it easier to find relevant content . © 2014 ieee .),\n",
       " (1,\n",
       "  title: deep bi-directional_long_short-term_memory neural_networks for sentiment_analysis of social data; abstract: sentiment_analysis ( sa ) has been attracting a lot of studies in the field of natural_language_processing and text_mining . recently , there are many algorithm ’ s enhancements in various sa applications are investigated and introduced . deep_convolutional_neural_networks ( dcnns ) have recently been shown to give the state-of-the-art performance on sentiment_classification of social data . although , these solutions effectively address issues of multi-levels features presentation but having some limitations of temporal modeling . in addition , the bidirectional_long_short-term_memory ( bltsm ) conventional models have encountered some limitations in presentation with multi-level features but can keep track of the temporal_information while enabling deep representations in the data . in this paper , we propose to use deep bi-directional_long_short-term_memory ( dblstm ) architecture with multi-levels feature presentation for sentiment_polarity classification ( spc ) on social data . by using dblstm , we can exploit more level features than bltsm and inherit temporal modeling in bltsm . moreover , the language of social data is very informal with misspellings and abbreviations . one word can be appeared in multiple formalities , which is a challenge in word-level models . we use character-level as input of dblstm neural_network ( called character dblstm - cdblstm ) for learning sentence_level presentation . the experimental results show that the performance of our model is competitive with state-of-the-art of spc on twitter ’ s data . our model achieves 85.86 % accuracy on stanford twitter sentiment corpus ( sts ) and 84.82 % accuracy on the subtasks b of semeval-2016 task 4 corpus .),\n",
       " (1,\n",
       "  title: enhancement of arabic text_classification using semantic relations of arabic wordnet; abstract: arabic text_classification methods have emerged as a natural result of the existence of a massive amount of varied textual_information ( written in arabic_language ) on the web . in most text_classification processes , feature_selection is crucial task since it highly affects the classification_accuracy . generally , two types of features could be used : statistical based features and semantic and concept features . the main interest of this paper is to specify the most effective semantic and concept features on arabic text_classification process . in this study , two novel features that use lexical , semantic and lexico-semantic relations of arabic wordnet ( awn ) ontology are suggested . the first feature_set is list of pertinent synsets ( lops ) , which is list of synsets that have a specific relation with the original terms . the second feature_set is list of pertinent words ( lopw ) , which is list of words that have a specific relation with the original terms . fifteen different relations ( defined in awn ontology ) are used with both proposed features . naïve_bayes classifier is used to perform the classification process . the experimental results , which are conducted on bbc_arabic dataset , show that using lops feature_set improves the accuracy of arabic text_classification compared with the well-known bag-of-word feature and the recent bag-of-concept ( synset ) features . also , it was found that lopw ( especially with related-to relation ) improves the classification_accuracy compared with lops , bagof- word and bag-of-concept .),\n",
       " (1,\n",
       "  title: generating hierarchical explanations on text_classification via feature interaction detection; abstract: generating explanations for neural_networks has become crucial for their applications in real-world with respect to reliability and trustworthiness . in natural_language_processing , existing_methods usually provide important features which are words or phrases selected from an input text as an explanation , but ignore the interactions between them . it poses challenges for humans to interpret an explanation and connect it to model prediction . in this work , we build hierarchical explanations by detecting feature interactions . such explanations visualize how words and phrases are combined at different levels of the hierarchy , which can help users understand the decision-making of black-box models . the proposed method is evaluated with three neural text classifiers ( lstm , cnn , and bert ) on two benchmark_datasets , via both automatic and human evaluations . experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans .),\n",
       " (1,\n",
       "  title: aspect_term_extraction and categorization for chinese mooc reviews; abstract: sentiment_analysis has become one of the most active topics in education research . so far , however , there has been little discussion about the recent application of sentiment_analysis for chinese mooc reviews . therefore , this paper sheds_light on some fine-grained sentiment_analysis technology to benefit the current students and education practitioners . firstly , we focus on extracting aspect terms associated with the course via dependency_parsing and sentiment word lexicons . secondly , we categorize the aspect terms with the naive_bayes . experimental results effectively demonstrate that the proposed approach and refine the granularity of sentiment categories in higher_education . this paper makes sentiment_analysis possible to increase students ' learning retention and improve teachers ' performance in online teaching .),\n",
       " (1,\n",
       "  title: a svm based speech to text converter for turkish_language; abstract: in proposed speech to text conversion , a support_vector_machines ( svm ) based turkish speech to text converter system has been developed . in the recognition system , mel_frequency_cepstral_coefficients ( mfcc ) has been applied to extract features of turkish speech and svm based classifier has been used to classify the phonemes . the morphological structure of turkish , a language based on phonemes , has been taken into consideration in the devoloped person-dependent voice_recognition system . unlike the multiclass classifiers which are used in the svm-mfcc based voice_recognition system , a new svm classifier system has been developed that uses fewer classes in layers , increasing the number of multiclass layers . a new text comparison algorithm is proposed , which also uses phoneme sequence to measure similarity in word_similarity measurement . along with these enhancements , as the training period becomes higher , performance of voice_recognition is improved and word recognition performance is increased . the performance of the proposed structure is compared with similar systems .),\n",
       " (1,\n",
       "  title: classification of summarized sensor data using sampling and clustering : a performance analysis; abstract: as humans and machines generate a tremendous amount of digital data in their daily life , we are in the era of big_data which poses unique_challenges of storing , processing and analyzing this voluminous data . sensors which continuously generate data are one important source of big_data and have innumerous applications in real_life scenario . as storing the entire data becomes expensive , summarization is the need of the hour . data summarization is a compact representation of the entire data which can reduce storage and processing requirements . in this work , we try to effectively summarize sensor data using simple but effective techniques such as sampling and clustering and analyze the performance of the summarized data in comparison to the complete dataset . popular classification techniques like knn , svm and naive_bayes are used to evaluate the efficiency of the summarization_techniques by training the classifiers using the summarized data and testing with the test data set . the performance of the summarized dataset and the complete dataset are compared . the experimental results show that summarized data set performs almost equally well as the complete data set .),\n",
       " (1,\n",
       "  title: simultaneous learning of trees and representations for extreme classification and density_estimation; abstract: we consider multi-class classification where the predictor has a hierarchical_structure that allows for a very large number of labels both at train and test time . the predictive_power of such models can heavily depend on the structure of the tree , and although past work showed how to learn the tree_structure , it expected that the feature_vectors remained static . we provide a novel algorithm to simultaneously perform representation_learning for the input data and learning of the hierarchi- cal predictor . our approach optimizes an objec- tive function which favors balanced and easily- separable multi-way node partitions . we theoret- ically analyze this objective , showing that it gives rise to a boosting style property and a bound on classification error . we next show how to extend the algorithm to conditional density_estimation . we empirically validate both variants of the al- gorithm on text_classification and language mod- eling , respectively , and show that they compare favorably to common baselines in terms of accu- racy and running time .),\n",
       " (1,\n",
       "  title: bag-of-words vs. graph vs. sequence in text_classification : questioning the necessity of text-graphs and the surprising strength of a wide mlp; abstract: graph neural_networks have triggered a resurgence of graph-based text_classification methods , defining today 's state of the art . we show that a wide multi-layer_perceptron ( mlp ) using a bag-of-words ( bow ) outperforms the recent graph-based models textgcn and hetegcn in an inductive text_classification setting and is comparable with hypergat . moreover , we fine-tune a sequence-based bert and a lightweight distilbert model , which both outperform all state-of-the-art models . these results question the importance of synthetic graphs used in modern text classifiers . in terms of efficiency , distilbert is still twice as large as our bow-based wide mlp , while graph-based models like textgcn require setting up an $ \\mathcal { o } ( n^2 ) $ graph , where $ n $ is the vocabulary plus corpus size . finally , since transformers need to compute $ \\mathcal { o } ( l^2 ) $ attention_weights with sequence length $ l $ , the mlp models show higher training and inference speeds on datasets with long sequences .),\n",
       " (1,\n",
       "  title: opinion-driven communities ' detection; abstract: purpose - the purpose of this paper is to address the challenge of opinion_mining in text documents to perform further analysis such as community detection and consistency control . more specifically , we aim to identify and extract opinions from natural_language documents and to represent them in a structured manner to identify communities of opinion holders based on their common opinions . another goal is to rapidly identify similar or contradictory opinions on a target issued by different holders . design/methodology/approach - for the opinion_extraction problem we opted for a supervised approach focusing on the feature_selection problem to improve our classification results . on the community detection problem , we rely on the infomap community detection algorithm and the multi-scale community detection framework used on a graph representation based on the available opinions and social data . findings - the classification performance in terms of precision and recall was significantly_improved by adding a set of `` meta-features '' based on grouping rules of certain part of speech ( pos ) instead of the actual words . concerning the evaluation of the community detection feature , we have used two quality metrics : the network modularity and the normalized mutual_information ( nmi ) . we evaluated seven one-target similarity_functions and ten multi-target aggregation functions and concluded that linear functions perform_poorly for data sets with multiple targets , while functions that calculate the average similarity have greater resilience to noise . originality/value - although our solution relies on existing_approaches , we managed to adapt and integrate them in an efficient_manner . based on the initial experimental results obtained , we managed to integrate original enhancements to improve the performance of the obtained results .),\n",
       " (1,\n",
       "  title: a memory based approach to word_sense_disambiguation in bengali using k-nn method; abstract: word_sense_disambiguation ( wsd ) is an important and challenging task in the area of natural_language_processing ( nlp ) where the task is to find the correct sense of an ambiguous_word given its context . there have been very few attempts on wsd in bengali or in indian languages . the k-nearest-neighbor ( k-nn ) algorithm is a very well known and popular method for text_classification . the k-nn algorithm determines the classification of a new sample from its k_nearest_neighbors . in this paper , we present how k-nn algorithm can be effectively applied to the task of wsd in bengali . the k-nn algorithm achieved an accuracy of over 71 % in a wsd task in bengali reported in this paper .),\n",
       " (1,\n",
       "  title: an ordinal multi-class classification method for readability_assessment of chinese documents; abstract: readability_assessment is worthwhile in recommending suitable documents for the readers . in this paper , we propose an ordinal multi-class classification with voting ( omcv ) method for estimating the reading levels of chinese documents . based on current achievements of natural_language_processing , we also design five groups of text features to explore the peculiarities of chinese . we collect the chinese primary_school language textbook dataset , and conduct_experiments to demonstrate the effectiveness of both the method and the features . experimental results show that our method has potential in improving the performance of the state-of-the-art classification and regression models , and the designed features are valuable in readability_assessment of chinese documents .),\n",
       " (1,\n",
       "  title: empirical_study to evaluate the performance of classification_algorithms on public datasets; abstract: in today ’ s world , a huge amount of data is stored in the form of electronic documents in the world_wide_web . text_classification algorithms have been widely used for classifying those text documents into a fixed number of predefined_classes . the applicable scopes and their performances of these algorithms are different . therefore , finding an appropriate algorithm for a dataset is becoming a significant emphasis for researchers to solve practical problems quickly . this paper puts_forward an experimental evaluation of five significant text_classification algorithms with each other and with tf and tf-idf feature_selection methods built using decision_tree ( c5.0 ) , support_vector_machine , k-nearest_neighbor , naïve_bayes , and neural_network on four public datasets , namely 20news-bydate , ohsumed-first-20000-docs , reuters 21578-apte-90 cat , and 20_newsgroup . the experimental results are examined from multiple perspectives and summarized to provide usefulness of different algorithms on different datasets .),\n",
       " (1,\n",
       "  title: solving stance_detection on tweets as multi-domain and multi-task text_classification; abstract: stance_detection on tweets aims at classifying the attitude of tweets towards given targets . existing work leverage attention-based models to learn target-aware stance representations . while those methods achieve substantial success , most of them usually train a model for each target separately despite the scarcity of annotated_data for each target . to alleviate limitation of annotated_data , some methods turn to external linguistic resources , additional sentiment annotations or target-aware data_augmentation techniques for better detection results . we argue that the sharedness of stance-related features across targets in the existing stance_detection dataset is not fully exploited . however , directly training on mixed examples of all targets may confuse the model in learning best features for each target . to this end , we borrow the idea from transfer_learning and multi-task_learning , and formulate stance_detection on tweets as a multi-domain multi-task classification problem . we apply the target adversarial_learning to capture stance-related features shared by all targets and target descriptors for learning stance-informative_features correlating to specific targets . experimental results on the benchmark semeval_2016 dataset demonstrate the effectiveness of our model , which outperforms bert model by over 2 % on macro_average_f1 and achieves superior performance than many recent methods utilizing external_resources . we further provide detailed analyses to illustrate the superiority of fully utilizing features shared by different targets .),\n",
       " (1,\n",
       "  title: software functional and non-function requirement classification using word-embedding; abstract: the classification of software requirements is an essential task in software_engineering . manual classification requires a large amount of efforts , time and cost . hence , automated techniques are required to classify software requirements . this work aims to develop requirement classification models based on extraction of relevant features from requirement documents and thereafter classifying requirement into functional and non-function requirements . in this paper , different word-_embedding techniques to extract numerical_features , feature_selection to remove irrelevant feature , smote to balance data , and six different classifiers for models training . the experiments have been conducted on promise software_engineering dataset . the experimental finding indicate that word2vec is best way to extracting numerical_features from requirement documents , rank-sum test is best way to find important features , and svm-r was found as the best classifier .),\n",
       " (1,\n",
       "  title: learning and knowledge-based sentiment_analysis in movie review key excerpts; abstract: we propose a data-driven approach based on back-off n-grams and support_vector_machines , which have recently become popular in the fields of sentiment and emotion recognition . in addition , we introduce a novel valence classifier based on linguistic analysis and the on-line knowledge sources conceptnet , general inquirer , and wordnet . as special benefit , this approach does not demand labeled_training_data . moreover , we show how such knowledge sources can be leveraged to reduce out-of-vocabulary events in learning-based processing . to profit from both of the two generally different concepts and independent knowledge sources , we employ information fusion techniques to combine their strengths , which ultimately leads to better overall performance . finally , we extend the data-driven classifier to solve a regression problem in order to obtain a more fine-grained resolution of valence . © 2011 springer .),\n",
       " (1,\n",
       "  title: sentiment_analysis on bengali text using lexicon based approach; abstract: in this modern_era , we daily involve in the internet strongly . we express our opinion about products , services , books , movies , songs , politics , sports , organizations , etc . through the internet in social_media , blogs , micro-blogging websites or any media . public opinion with bengali text in internet media is increasing very rapidly . due to a few works in bengali text sentiment_analysis , it has become an important issue of extracting opinions , emotions from bengali textual_data through sentiment_analysis ( sa ) for better knowledge extraction . sentiment_analysis ( sa ) is effectively used for classifying the opinion expressed in a text according to its polarity ( e.g. , positive , negative or neutral ) . this paper represents a lexicon dictionary-based approach for polarity_detection of bengali text data . we compared our proposed model with machine_learning classifiers such as decision_tree ( dt ) , naive_bayes ( nb ) and support_vector_machine ( svm ) classifiers and it works as a much better accurate model for bengali text polarity_detection .),\n",
       " (1,\n",
       "  title: performance comparison of machine_learning classifiers for fake_news_detection; abstract: information sharing on the web particularly via web-based networking media is increasing . ability to identify , evaluate and address such information is significantly important . fake information deliberately created is purposefully or unintentionally engendered over the internet . this is affecting a larger group of society who are blinded by technology . this paper illustrates model and methodology to detect fake_news from news article with the assistance of machine_learning and natural_language_processing . in this proposed work different feature_engineering methods like count vector , tf-idf and word_embedding are used to generate feature_vector . seven different machine_learning classification_algorithms are trained to classify news as fake or real and are compared considering accuracy , f1 score , recall , precision and best one is selected to build a model to classify news as fake or real .),\n",
       " (1,\n",
       "  title: document zone content classification for technical document_images using artificial_neural_networks and support_vector_machines; abstract: artificial_neural_networks ( ann ) are a classic pattern classifier and widely applicable to various problems and are relatively easy to use . three of the most popular anns are multilayer_perceptron ( mlp ) with backpropagation learning algorithm , self_organizing_map ( som ) and recurrent_neural_network ( rnn ) . support_vector_machines ( svm ) have gained great interest in the last few years in pattern_recognition . thus , this research compares the recognition performance of text and non-text images ( text , table , figure and graph ) from technical document_images based on the pixel intensity of various zones between bpnn , som , rnn and svm . symmetrical and non-symmetrical zoning algorithms were compared as input . 400 different datasets have been tested and the experiments indicate that svm classification is superior to the other three classifiers . the experiments also indicate that the combination of symmetrical and non-symmetrical zoning design is better than non-symmetrical or symmetrical zoning only . ©2009 ieee .),\n",
       " (1,\n",
       "  title: an improved naïve_bayes classifier method in public_opinion analysis; abstract: an improved naïve_bayes classifier is proposed . the method includes aspects of improvement : to get a reduced text feature word set by filtering the synonym , to iterate two different feature_selection methods , and to effectively_improve the representative feature_set . the experimental results show that this method can effectively_improve the performance of naïve_bayes classifier .),\n",
       " (1,\n",
       "  title: a novel approach to data augmentation for document_image_classification using deep convolutional generative_adversarial_networks; abstract: data augmentation is a procedure where new samples are generated from the training dataset by applying various techniques and algorithms to improve machine and deep_learning models ’ accuracy and generalization_ability . the recent advances in deep_learning and computer vision techniques have made scanned document_classification a painless and straightforward process . however , such approaches require a lot of labeled_data before training and validating the classifiers , which can be done by augmenting the existing dataset by different means . in this contribution , we explored using a system based on deep convolutional adversarial_networks ( dcgan ) to generate fake document_images using an existing scanned_documents dataset . moreover , we compared conventional data_augmentation techniques ( rotation , zoom , random cropping , etc . ) to dcgan-based augmentation in a document_classification context . the newly generated data could then be used along with the labeled_data to train a convolutional_neural_network ( cnn ) to classify scanned document_images . this experiment compared the performances of a system trained on different datasets ( original labeled_data , gan-augmented dataset , hybrid augmented dataset ) . its results revealed the effectiveness of the proposed approach .),\n",
       " (1,\n",
       "  title: identifying anatomical phrases in clinical_reports by shallow semantic parsing methods; abstract: natural_language_processing ( nlp ) is being applied for several information_extraction tasks in the biomedical domain . the unique nature of clinical information requires the need for developing an nlp system designed specifically for the clinical domain . we describe a method to identify semantically coherent phrases within clinical_reports . this is an important step towards full syntactic parsing within a clinical nlp system . we use this semantic phrase chunker to identify anatomical phrases within radiology reports related to the genitourinary domain . a discriminative classifier based on support_vector_machines was used to classify words into one of ave phrase classification categories . training of the classifier was performed using 1000 hand-tagged sentences from a corpus of genitourinary radiology reports . features used by the classifier include n-grams , syntactic tags and semantic labels . evaluation was conducted on a blind test set of 250 sentences from the same domain . the system achieved overall performance scores of 0.87 ( precision ) , 0.91 ( recall ) and 0.89 ( balanced f-score ) . anatomical phrase_extraction can be rapidly and accurately accomplished . © 2007 ieee .),\n",
       " (1,\n",
       "  title: a novel kernel for text_classification based on semantic and statistical information; abstract: in text_categorization , a document is usually represented by a vector_space_model which can accomplish the classification_task , but the model can not deal with chinese synonyms and polysemy phenomenon . this paper presents a novel approach which takes into account both the semantic and statistical information to improve the accuracy of text_classification . the proposed approach computes semantic information based on hownet and statistical information based on a kernel_function with class-based weighting . according to our experimental results , the proposed approach could achieve state-of-the-art or competitive_results as compared with traditional_approaches such as the k-nearest_neighbor ( knn ) , the naive_bayes and deep_learning models like convolutional_networks .),\n",
       " (1,\n",
       "  title: improving the accuracy of text_classification using the over sampling technique in the case of sinovac vaccine; abstract: the who has declared covid-19 ( coronavirus disease 2019 ) a global_health emergency . up to 19 november 2021 , the total positive cases in indonesia reached 4,252,705 , of which 4,100,837 recovered , and 143,714 died . therefore , vaccines have been developed to minimize covid-19 transmission . there are some kinds of vaccines developed by several companies such as sinovac , astrazeneca , pfizer , and moderna . the general public has a different opinion on sinovac vaccine on twitter , where some people promote it while others reject it . data used in this study were 1000 tweets about the sinovac vaccine . during the dataset collection unequal distribution often occurs , where the number of labels is more on one side . such a situation is called imbalance class . imbalance class in a dataset can reduce classification performance . to overcome the imbalance class , this study used the synthetic_minority_over-sampling_technique ( smote ) . the classification methods used were k-nearest_neighbors ( knn ) , support_vector_machine ( svm ) , and random_forest , and tf-idf was used to determine the weight of the words . the average rise of the accuracy value of the three algorithms after smote optimization was 14 % . the results of sentiment_analysis for the sinovac vaccine revealed a positive_sentiment of 81 % . thus , it can be concluded that the sinovac vaccine received a positive response from the public .),\n",
       " (1,\n",
       "  title: novel ogbee-based feature_selection and feature-level_fusion with mlp neural_network for social_media multimodal_sentiment_analysis; abstract: numerous public networks , namely instagram , youtube , facebook , twitter , etc. , share their own feelings and idea as videotapes , posts , and pictures . in future_research , adapting to such data and mining valuable_information from it will be an undeniably troublesome errand . this paper proposes a novel audio–video–textual-based multimodal_sentiment_analysis approach . the proposed approach investigates the sentiments that are collected from the web recordings that utilize audio , video , and textual modalities for further extraction . a feature-level_fusion technique is employed in fusing the extracted features from different modalities . therefore , the extracted features are optimally chosen by using a novel oppositional grass bee optimization ( ogbee ) algorithm to obtain the best optimal feature_set . here , 12 benchmark functions are developed to validate the numerical efficiency and the effectiveness of a novel ogbee algorithm for various aspects . moreover , our proposed approach utilizes multilayer_perceptron-based neural_network ( mlp-nn ) for sentiment_classification . the experimental analysis_reveals that the proposed approach provides better classification_accuracy of about 95.2 % with less computational time .),\n",
       " (1,\n",
       "  title: pseudo-labeling with transformers for improving question_answering systems; abstract: advances in neural_networks contributed to the fast development of natural_language_processing systems . as a result , question_answering systems have evolved and can classify and answer_questions in an intuitive yet communicative way . however , the lack of large_volumes of labeled_data prevents large-scale training and development of question_answering systems , confirming the need for further research . this paper aims to handle this real-world problem of lack of labeled datasets by applying a pseudo-labeling technique relying on a neural_network transformer model distilbert . in order to evaluate our contribution , we examined the performance of a text_classification transformer model that was fine-tuned on the data subject to prior pseudo-labeling . research has shown the usefulness of the applied pseudo-labeling technique on a neural_network text_classification transformer model distilbert . the results of our analysis indicated that the model with additional pseudo-labeled_data achieved the best results among other compared neural_network architectures . based on that result , question_answering systems may be directly improved by enriching their training steps with additional data acquired cost-effectively .),\n",
       " (1,\n",
       "  title: a novel text-mining approach for retrieving pharmacogenomics associations from the literature; abstract: text_mining in biomedical_literature is an emerging_field which has already been shown to have a variety of implementations in many research areas , including genetics , personalized_medicine , and pharmacogenomics . in this study , we describe a novel text-mining approach for the extraction of pharmacogenomics associations . the code that was used toward this end was implemented using r programming_language , either through custom scripts , where needed , or through utilizing functions from existing libraries . articles ( abstracts or full texts ) that correspond to a specified query were extracted from pubmed , while concept annotations were derived by pubtator central . terms that denote a mutation or a gene as well as chemical_compound terms corresponding to drug compounds were normalized and the sentences containing the aforementioned terms were filtered and preprocessed to create appropriate training_sets . finally , after training and adequate hyperparameter_tuning , four text classifiers were created and evaluated ( fasttext , linear_kernel svms , xgboost , lasso , and elastic-net regularized generalized linear models ) with regard to their performance in identifying pharmacogenomics associations . although further improvements are essential toward proper implementation of this text-mining approach in the clinical_practice , our study stands as a comprehensive , simplified , and up-to-date approach for the identification and assessment of research articles enriched in clinically_relevant pharmacogenomics relationships . furthermore , this work highlights a series of challenges concerning the effective application of text_mining in biomedical_literature , whose resolution could substantially contribute to the further development of this field .),\n",
       " (1,\n",
       "  title: cyberbullying detection through sentiment_analysis; abstract: in recent_years with the widespread of social_media platforms across the globe especially among young people , cyberbullying and aggression have become a serious and annoying problem that communities must deal with . such platforms provide various ways for bullies to attack and threaten others in their communities . various techniques and methodologies have been used or proposed to combat cyberbullying through early detection and alerts to discover and/or protect victims from such attacks . machine_learning ( ml ) techniques have been widely used to detect some language patterns that are exploited by bullies to attack their victims . also . sentiment_analysis ( sa ) of social_media content has become one of the growing areas of research in machine_learning . sa provides the ability to detect cyberbullying in real-time . sa provides the ability to detect cyberbullying in real-time . this paper proposes a sa model for identifying cyberbullying s in twitter social_media . support_vector_machines ( svm ) and naïve_bayes ( nb ) are used in this model as supervised_machine_learning classification tools . the results of the experiments conducted on this model showed encouraging outcomes when a higher n-grams language model is applied on such s in comparison with similar previous_research . also , the results showed that svm classifiers have better performance_measures than nb classifiers on such tweets .),\n",
       " (1,\n",
       "  title: context-aware misinformation_detection : a benchmark of deep_learning architectures using word_embeddings; abstract: new mass_media paradigms for information distribution have emerged with the digital age . with new digital-enabled mass_media , the communication process is centered around the user , while multimedia content is the new identity of news . thus , the media landscape has shifted from mass_media to personalized social_media . while this progress brings advantages , it also carries the risk of being detrimental to society through the emergence of misinformation ( false or inaccurate information ) and disinformation ( intentionally spreading misinformation ) in the form of fake_news . fake_news is a tool used to manipulate public opinion on particular topics , distort public perceptions , and generate social unrest while lacking the rigor of traditional journalism . driven by this current and real-world problem , in this paper , we train multiple deep_learning architectures for multi-class classification and compare their performance in detecting the veracity of the news articles . to achieve accurate models in detecting misinformation , we employ a large dataset containing 100 000 news articles labeled with ten classes ( one with real_news and the rest with different types of fake_news ) . we use two preprocessing_techniques , i.e. , one simple and another very aggressive , to clean the dataset . we also employ three word_embeddings that preserve the word context , i.e. , word2vec , fasttext , and glove , pre-trained and trained on our dataset to vectorize the preprocessed dataset . for the misinformation task , we train a logistic_regression as a baseline and compare its results with the performance of ten deep_learning architectures . we obtain the best results using a recurrent_convolutional_neural_network based architecture . the experimental results show that the models are highly dependable on text_preprocessing and the word_embedding employed .),\n",
       " (1,\n",
       "  title: lexicon-pointed hybrid n-gram features extraction model ( lenfem ) for sentence_level sentiment_analysis; abstract: sentiment_analysis of social_media textual posts can provide information and knowledge that is applicable in social settings , business_intelligence , evaluation of citizens ' opinions in governance , and in mood triggered devices in the internet of things . feature_extraction and selection is a key determinant of accuracy and computational_cost of machine_learning models for such analysis . most feature_extraction and selection techniques utilize bag of words , n-grams , and frequency-based algorithms especially term_frequency-inverse_document_frequency . however , these approaches do not consider relationships between words , they ignore words ' characteristics and they suffer high feature dimensionality . in this paper we propose and evaluate a feature_extraction and selection approach that utilizes a fixed hybrid n-gram window for feature_extraction and minimum_redundancy maximum relevance feature_selection algorithm for sentence_level sentiment_analysis . the approach improves the existing features extraction techniques , specifically the n-gram by generating a hybrid vector from words , part of speech ( pos ) tags , and word semantic orientation . the vector is extracted by using a static trigram window identified by a lexicon where a sentiment word appears in a sentence . a blend of the words , pos_tags , and the sentiment_orientations of the static trigram are used to build the feature_vector . the optimal features from the vector are then selected using minimum_redundancy maximum relevance ( mrmr ) algorithm . experiments were carried out using the public yelp dataset to compare the performance of the proposed model and existing feature_extraction models ( bow , normal n-grams and lexicon-based bag of words semantic orientations ) . using supervised_machine_learning classifiers the experimental results showed that the proposed model had the highest f-measure ( 88.64 % ) compared to the highest ( 83.55 % ) from baseline approaches . wilcoxon test carried out ascertained that the proposed approach performed significantly better than the baseline approaches . comparative performance analysis with other datasets further affirmed that the proposed approach is generalizable .),\n",
       " (1,\n",
       "  title: personality_disorders identification in written_texts; abstract: this article describes a method for dealing with the detection of possible personality_disorder without the necessary presence of specialists and using the patient ’ s self-essays . written text is analyzed by using nlp techniques and is categorized into one of the three main groups of personality_disorders we chosen— fear , procrastination and intolerance of uncertainty . we customized approach based on features extraction , sentiment_analysis and classification by well-known classifiers : naive_bayes , multi-class support_vector_machine , k-nearest_neighbors and decision_tree . the first experiential , based on real data consulted with specialists , have shown promising_results . greater or lesser personality_disorders are due to a stressful and time-titch way of life quite frequent nowadays . in the cases of restrictions or complications in the suffering individual ’ s life is early identification and problem_solving more than desirable . but some people consider visiting a specialist as a personal failure a and due a shame they do not solve the problem even if it suspects themselves . psychologists and psychiatrists on the other hand use several methods to detect personality_disorders today , either by observation during the interview , a questionnaire and a written self-essay .),\n",
       " (1,\n",
       "  title: senti-cs : building a lexical_resource for sentiment_analysis using subjective feature_selection and normalized chi-square-based feature_weight generation; abstract: sentiment_analysis involves the detection of sentiment content of text using natural_language_processing . natural_language_processing is a very challenging task due to syntactic ambiguities , named_entity_recognition , use of slangs , jargons , sarcasm , abbreviations and contextual sensitivity . sentiment_analysis can be performed using supervised as well as unsupervised approaches . as the amount of data grows , unsupervised approaches become vital as they cut down on the learning time and the requirements for availability of a labelled_dataset . sentiment_lexicons provide an easy application of unsupervised algorithms for text_classification . sentiwordnet is a lexical_resource widely employed by many researchers for sentiment_analysis and polarity_classification . however , the reported performance levels need improvement . the proposed research is focused on raising the performance of sentiwordnet3.0 by using it as a labelled corpus to build another sentiment_lexicon , named senti-cs . the part of speech information , usage based ranks and sentiment_scores are used to calculate chi-square-based feature_weight for each unique subjective term/part-of-speech pair extracted from sentiwordnet3.0 . this weight is then normalized in a range of −1 to +1 using min–max normalization . senti-cs based sentiment_analysis framework is presented and applied on a large dataset of 50000 movie reviews . these results are then compared with baseline sentiwordnet , mutual_information and information gain techniques . state of the art comparison is performed for the cornell movie review dataset . the analyses of results indicate that the proposed approach_outperforms state-of-the-art classifiers .),\n",
       " (1,\n",
       "  title: a novel heuristic text_classification algorithm based on support_vector_machines; abstract: support_vector_machines ( svm ) , one of the new techniques for text_classification , have been widely used in many application_areas . svm try to find an optimal hyperplane within the input_space so as to correctly classify the binary classification problem . we present a novel heuristic text_classification approach based on genetic_algorithm ( ga ) and svm . simulation results demonstrate that ga and svm are integrated effectively , and have good classification_accuracy . ©2010 ieee .),\n",
       " (1,\n",
       "  title: sentiment_analysis of mobile phone products reviews using classification_algorithms; abstract: sentiment_analysis or opinion_mining is a process of analyzing opinions and emotions to infer the tendencies and impressions shown on the analyzed data and classify them as positive or negative . sentiment_analysis is extremely important because it helps companies and institutions to measure the scope of their customer 's satisfaction with a specific product based on reviews in a very fast way . as the manual analysis of these reviews is very difficult because of the increase in the numbers of reviews on products day after day . this paper proposes a sentiment_analysis model to classify product_reviews as positive , neutral and negative . it applies five popular machine_learning classifiers namely : naive_bayes , support_vector_machine , decision_tree , k-nearest_neighbor and maximum_entropy with the aim to come up with the most efficient classifier . the dataset used consists of 82 , 815 reviews about mobile phone products , collected from kaggle website . in order to evaluate the five classifiers , we used recall , precision , f1-mesaure and accuracy to measure the performance of each algorithm . the results show that maximum_entropy and naïve_bayes outperforms the other classifiers in term of accuracy in all experiments . decision_tree algorithm gave the lowest results across all experiments in terms of accuracy .),\n",
       " (1,\n",
       "  title: 25th pacific-asia conference on knowledge discovery and data_mining , pakdd 2021; abstract: the proceedings contain 157 papers . the special focus in this conference is on knowledge discovery and data_mining . the topics include : self-supervised graph_representation learning with variational_inference ; manifold approximation and projection by maximizing graph information ; learning attention-based translational knowledge_graph embedding via nonlinear dynamic mapping ; multi-grained dependency_graph neural_network for chinese open_information_extraction ; human-understandable decision_making for visual_recognition ; lightcake : a lightweight framework for context-aware knowledge_graph embedding ; transferring domain_knowledge with an adviser in continuous tasks ; inferring hierarchical mixture structures : a bayesian nonparametric approach ; quality_control for hierarchical classification with incomplete annotations ; learning discriminative_features using multi-label dual_space ; universal representation for code ; autocluster : meta-learning based ensemble_method for automated unsupervised_clustering ; banditrank : learning to rank using contextual bandits ; a compressed and accelerated segnet for plant leaf disease segmentation : a differential_evolution based approach ; meta-context transformers for domain-specific response_generation ; a multi-task kernel learning algorithm for survival_analysis ; meta-data augmentation based search strategy through generative_adversarial_network for automl model_selection ; tree-capsule : tree-structured capsule_network for improving relation_extraction ; rule injection-based generative_adversarial imitation learning for knowledge_graph reasoning ; hierarchical self attention_based autoencoder for open-set human activity_recognition ; reinforced natural_language inference for distantly_supervised_relation classification ; self-supervised adaptive aggregator learning on graph ; sagcn : structure-aware graph convolution network for document-level relation_extraction ; addressing the class_imbalance problem in medical image_segmentation via accelerated tversky loss_function ; incorporating relational knowledge in explainable fake_news_detection ; incorporating syntactic information into relation representations for enhanced relation_extraction .),\n",
       " (1,\n",
       "  title: a survey on dimension reduction techniques in text_classification; abstract: dimension reduction is one of the key_points for text_classification . feature_selection and feature_extraction are the two common methods of dimension reduction . in this paper , we mainly discussed some dimension reduction techniques from two aspects including traditional_methods ( information gain , mutual_information , document_frequency , correlation_coefficient ) and new methods ( optimization mutual_information based on word_frequency , cdf ( concentration , dispersion and frequency ) , semantic relatedness ) . then analyzed the principle of these methods and illustrated their advantages as well as disadvantages .),\n",
       " (1,\n",
       "  title: hybrid approach for telugu handwritten_character_recognition using k-nn and svm classifiers; abstract: research in optical_character_recognition ( ocr ) had started more than five decades ago . the recognition_accuracy for printed_characters is above 90 % , whereas for handwritten_characters is very low and less than 60 % as reported in the literature . handwritten_character_recognition of indian languages is still at nascent stage of research and hence captivated our attention for further analysis . this paper describes the handwritten_character_recognition of telugu_language using two-stage classifiers . k-nearest_neighbor ( k-nn ) and support_vector_machines ( svm ) were used for classification in this work . the use of these two classifiers one after the other in two subsequent stages increases the recognition_accuracy of the system . various features extracted from the images are block pixel count , block based directions , histograms and boundaries for both the training and test images . in the first stage k-nn classifier was used and subsequently the wrongly recognized characters were tested with svm classifier . again the classifiers were interchanged in the first and second stages to check the improvement of accuracy . it was found that the recognition_accuracy increased to a great extent by cascading the two different classifiers . using these two classifiers the best recognition_accuracy obtained was 90.2 % .),\n",
       " (1,\n",
       "  title: arabic text_classification using master-slaves technique; abstract: text_classification ( tc ) is an essential field in both text_mining ( tm ) and natural_language_processing ( nlp ) . humans have a tendency to organize and categorize everything as they want to make things easier to understand . therefore , text_classification is an important step to achieve this goal . arabic text_classification ( atc ) is a difficult process because the arabic_language has complications and limitations resulting from the nature of its morphology . in this paper , a proposed approach called the master-slaves technique ( mst ) was used to improve arabic text_classification . it consists of two main phases : in the first phase , a new arabic corpus of 16757 text files was collected . these text files were classified into five categories manually . in the second phase , four different classifiers were implemented on the collected corpus . these classifiers are naïve_bayes ( nb ) , k-nearest_neighbour ( knn ) , multinomial_logistic_regression ( mlr ) and maximum weight ( mw ) . naïve_bayes classifier was implemented as master and the others as slaves . the results of these slave classifiers were used to change the probability of the naïve_bayes classifier ( master ) . the four classifiers used were implemented individually and the simple voting technique was implemented among them too on the collected corpus to check the effectiveness and efficiency of the proposed technique . all the tests were applied after the pre-processing of arabic text documents ( tokenization , stemming , and stop-word_removal ) and each document was represented as vector of weights . for the reliability of the results , 10-fold_cross-validation was used in this paper . the results showed that the master-slaves technique gives a good improvement in accuracy of text document_classification with accepted algorithm complexity compared to other techniques .),\n",
       " (1,\n",
       "  title: exploring data augmentation for classification of climate_change_denial : preliminary study; abstract: in order to address the growing need of monitoring climate-change_denial narratives in online_sources , nlp-based methods have the potential to automate this process . here , we report on preliminary_experiments of exploiting data_augmentation techniques for improving climate_change_denial classification . we focus on a selection of both known techniques , and augmentation transformations not reported elsewhere that replace certain type of named_entities with high probability of preserving labels . we also introduce a new benchmark_dataset consisting of text_snippets extracted from online_news labeled with fine-grained climate_change_denial types .),\n",
       " (1,\n",
       "  title: enhanced document_classification using noun verb ( nv ) terms extraction approach; abstract: the exponential_growth in digital documents and the constantly increasing online information have called for the necessity and lead to classify the documents . document_classification is increasingly vital and indispensable for modern applications . generally , documents comprise multiple terms of extraction . here , the main concentration of the most important words is on verbs and nouns , which signify the topics and events . however , nouns and verbs technique or simply called noun verb ( nv ) as an extraction method will greatly enhance the performance of document_classification . the aim and the implication of this research is to improve document_classification performance by using and utilizing nv extraction to detect the class of a document . three classifiers namely , k-nearest_neighbor ( knn ) , naive_bayes ( nb ) , and support_vector_machine ( svm ) are used to compare the results . nine benchmark_datasets were employed in testing the proposed document_classification . the anticipated classification was verified by evaluating its accuracy . the results exhibit that the verbs as extraction affect document_classification . this encouraged the research work to combine verbs with nouns as extraction . the nv method extraction outperformed other extraction methods ( e.g. , nouns , bag of word ( bow ) , and verbs ) .),\n",
       " (1,\n",
       "  title: semi-supervised text_classification algorithm based on a feature_mapping; abstract: there are many algorithms based on data distribution to effectively_solve the problem of semi-supervised text_categorization . however , they may perform badly when the labeled_data distribution is different from the unlabeled_data . to solve the problem , semi-supervised text_classification algorithm based on feature_mapping was proposed . first , three sets of features were selected respectively from labeled_data , unlabeled_data and test data by using different feature_selection methods , and their values were initialize . second , three feature_mapping functions were studied , and the weight of each feature was recalculated by them . finally , the em_algorithm was employ to classify the text data . experiments of standard data sets show that the proposed algorithm is effective .),\n",
       " (1,\n",
       "  title: firebert : hardening bert classifiers against adversarial_attack; abstract: we present firebert , a set of three proof-of-concept nlp classifiers hardened against textfooler-style word-perturbation by producing diverse alternatives to original samples . in one approach , we co-tune bert against the training_data and synthetic adversarial_samples . in a second approach , we generate the synthetic samples at evaluation time through substitution of words and perturbation of embedding_vectors . the diversified evaluation results are then combined by voting . a third approach replaces evaluation-time word_substitution with perturbation of embedding_vectors . we evaluate firebert for mnli and imdb movie review datasets , in the original and on adversarial_examples generated by textfooler . we also test whether textfooler is less successful in creating new adversarial_samples when manipulating firebert , compared to working on unhardened classifiers . we show that it is possible to improve the accuracy of bert-based models in the face of adversarial_attacks without significantly reducing the accuracy for regular benchmark samples . we present co-tuning with a synthetic_data generator as a highly effective method to protect against 95 % of pre-manufactured adversarial_samples while maintaining 98 % of original benchmark performance . we also demonstrate evaluation-time perturbation as a promising_direction for further research , restoring accuracy up to 75 % of benchmark performance for pre-made adversarials , and up to 65 % ( from a baseline of 75 % orig./12 % attack ) under active attack by textfooler .),\n",
       " (1,\n",
       "  title: albert over match-lstm network for intelligent questions classification in chinese; abstract: this paper introduces a series of experiments with an albert over match-lstm network on the top of pre-trained_word_vectors , for accurate classification of intelligent_question_answering and thus the guarantee of precise information service . to improve the performance of data classification , a short_text_classification method based on an albert and match-lstm model was proposed to overcome the limitations of the classification process , such as few vocabularies , sparse features , large amount of data , lots of noise and poor normalization . in the model , jieba word_segmentation tools and agricultural dictionary were selected to text segmentation , glove algorithm was then adopted to expand the text characteristic and weighted word_vector according to the text of key vector , bi-directional_gated_recurrent_unit was applied to catch the context feature information and multiconvolutional neural_networks were finally established to gain local multidimensional characteristics of text . batch_normalization , dropout , global average pooling and global max_pooling were utilized to solve overfitting problem . the results showed that the model could classify questions accurately , with a precision of 96.8 % . compared with other classification models , such as multi-svm model and cnn model , albert+match-lstm had obvious advantages in classification performance in intelligent agri-tech information service .),\n",
       " (1,\n",
       "  title: sentiment_score analysis for opinion_mining; abstract: sentiment_analysis has been widely used as a powerful tool in the era of predictive mining . however , combining sentiment_analysis with social_network analytics enhances the predictability power of the same . this research work attempts to provide the mining of the sentiments extracted from twitter social app for analysis of the current trending topic in india , i.e. , goods and services tax ( gst ) and its impact on different sectors of indian economy . this work is carried out to gain a bigger perspective of the current sentiment based on the live reactions and opinions of the people instead of smaller , restricted polls typically done by media corporations . a variety of classifiers are implemented to get the best possible accuracy on the dataset . a novel method is proposed to analyze the sentiment of the tweets and its impact on various sectors . further the sector trend is also analyzed through the stock_market analyses and the mapping between the two is made . furthermore , the accuracy of stated approach is compared with state of art classifiers like svm , naïve_bayes , and random_forest and the results demonstrate accuracy of stated approach outperformed all the other three techniques . also , a detailed analysis is presented in this manuscript regarding the effect of gst along with time series analysis followed by gender-wise analysis .),\n",
       " (1,\n",
       "  title: mooc video classification using natural_language_processing and machine_learning model; abstract: mooc opens up the doors for universal access to education remotely and serves as a constructive approach to acquire formal education informally by negating the traditional practices . in recent_years , the number of mooc video resources has increased exponentially . therefore , the need is a fully_automated system that would be proficient enough to store , analyze and manage such an immensity of videos while sustaining the quality in response . an automatic classification/prediction of videos is a challenging and complex aspect , although supervised_machine_learning can effectively achieve this task in an effective way . many applications use text_classification to categorize documents like , e.g . spam_filtering , email routing , sentiment_analysis , etc . in this study , we present a clever and adaptive technique for autonomous classification of mooc videos transcription using natural_language_processing and machine_learning model . our approach can predict the category of a targeted video ; the data_mining algorithms such as svm , random_forest , and naive_bayesian will be engaged to organize the mooc videos . experiments reveal that our approach outperformed other approaches in the field of transcription classification and supervised_learning .),\n",
       " (1,\n",
       "  title: classification of short texts based on nld-svm-rf model; abstract: [ objective ] this paper addresses the issue of data sparseness due to short texts , which also improves the performance of short texts classification . [ methods ] we proposed a multi-channel text model for the input of short text classifier by integrating the semantics , word_order features and topic features . then , we created the classification method named nld-svm-rf with the help of svm and random_forest algorithms . finally , we examined the new model with short text of complaints . [ results ] we compared the performance of our new model with the svm and rf single classifiers using doc2vec as the feature . when n =5 , the accuracy of the nld-svmrf method increased by 9 . 70 % and 6 . 25 % , respectively . [ limitations ] the experimental data size needs to be expanded . [ conclusions ] the nld-svm-rf model provides a practical solution for the business community to analyse short texts and improve decision-making .),\n",
       " (1,\n",
       "  title: sentiment_analysis of insomnia-related_tweets via a combination of transformers using dempster-shafer theory : pre– and peri–covid-19_pandemic retrospective study; abstract: background : the covid-19_pandemic has imposed additional stress on population health that may result in a change of sleeping behavior . objective : in this study , we hypothesized that using natural_language_processing to explore social_media would help with assessing the mental_health conditions of people experiencing insomnia after the outbreak of covid-19 . methods : we designed a retrospective study that used public social_media content from twitter . we categorized insomnia-related_tweets based on time , using the following two intervals : the prepandemic ( january 1 , 2019 , to january 1 , 2020 ) and peripandemic ( january 1 , 2020 , to january 1 , 2021 ) intervals . we performed a sentiment_analysis by using pretrained transformers in conjunction with dempster-shafer theory ( dst ) to classify the polarity of emotions as positive , negative , and neutral . we validated the proposed pipeline on 300 annotated_tweets . additionally , we performed a temporal analysis to examine the effect of time on twitter users ’ insomnia experiences , using logistic_regression . results : we extracted 305,321 tweets containing the word insomnia ( prepandemic tweets : n=139,561 ; peripandemic tweets : n=165,760 ) . the best combination of pretrained transformers ( combined via dst ) yielded 84 % accuracy . by using this pipeline , we found that the odds of posting negative tweets ( odds_ratio [ or ] 1.39 , 95 % ci 1.37-1.41 ; p < .001 ) were higher in the peripandemic interval compared to those in the prepandemic interval . the likelihood of posting negative tweets after midnight was 21 % higher than that before midnight ( or 1.21 , 95 % ci 1.19-1.23 ; p < .001 ) . in the prepandemic interval , while the odds of posting negative tweets were 2 % higher after midnight compared to those before midnight ( or 1.02 , 95 % ci 1.00-1.07 ; p=.008 ) , they were 43 % higher ( or 1.43 , 95 % ci 1.40-1.46 ; p < .001 ) in the peripandemic interval . conclusions : the proposed novel sentiment_analysis pipeline , which combines pretrained transformers via dst , is capable of classifying the emotions and sentiments of insomnia-related_tweets . twitter users shared more negative tweets about insomnia in the peripandemic interval than in the prepandemic interval . future_studies using a natural_language_processing framework could assess tweets about other types of psychological distress , habit changes , weight_gain resulting from inactivity , and the effect of viral_infection on sleep .),\n",
       " (1,\n",
       "  title: evaluation of service_quality for express industry through sentiment_analysis of online_reviews; abstract: the evaluation method based on questionnaire survey has limited respondents with low_quality , therefore , an evaluation method of express service_quality was put forward in this paper through sentiment_analysis of massive online_reviews by selecting online_reviews of express companies sf and st from dianping.com for experiment analysis . first , the servqual model of service_quality evaluation and the related theory of quality_evaluation of logistics service was applied combined with text analysis , to establish an index system of quality_evaluation of express service through sentiment_analysis . then online_reviews such as capturing , phrasing and marking were preprocessed , and the recall_ratio and precision ratio under different feature_selection algorithm and different classification algorithm were compared . 614 features were extracted and the useful text were identified by choose ig and svm as the best combination . furthermore , the polarity analysis and strength analysis of online_reviews were conducted based on semantic_similarity calculation of hownet and adverb classification method . finally , tf-idf with the weight of evaluation_index was applied to evaluate the express service_quality . compared with the rating scores from dianping.com , the experimental results indicate that the proposed method can effectively and better compare differences between sf and st in each express service_quality index . besides , the general evaluation scores are in alignment with dianping.com .),\n",
       " (1,\n",
       "  title: context_free frequently_asked_questions detection using machine_learning techniques; abstract: faqs are the lists of common questions and answers on particular topics . today one can find them in almost all web_sites on the internet and they can be a great tool to give information to the users . questions in faqs are usually identified by the site administrators on the basis of the questions that are asked by their users . while such questions can respond to required information about a service , topic , or particular subject , they can not easily be distinguished from non-faq questions . this paper describes machine_learning based parsing and question_classification for faqs . we demonstrate that questions for faqs can be distinguished from other types of questions . identification of specific features is the key to obtaining an accurate faq classifier . we propose a simple yet effective feature_set including bag of words , lexical , syntactical , and semantic features . to evaluate our proposed methods , we gathered a large data set of faqs in three different contexts , which were labeled by humans from real data . we showed that the svm and naive_bayes reach the accuracy of 80.3 % , which is an outstanding result for the early stage research on faq classification . experimental results show that the proposed approach can be a practical tool for question_answering systems . to evaluate the accuracy of our classifier we have conducted an evaluation process and built the questionnaire . therefore , we compared our classifier ranked questions with user rates and almost 81 % similarity of the question ratings gives some confidence .),\n",
       " (1,\n",
       "  title: leveraging multilingual transformers for hate_speech detection; abstract: detecting and classifying instances of hate in social_media text has been a problem of interest in natural_language_processing in the recent_years . our work leverages state of the art transformer language models to identify hate_speech in a multilingual setting . capturing the intent of a post or a comment on social_media involves careful evaluation of the language style , semantic content and additional pointers such as hashtags and emojis . in this paper , we look at the problem of identifying whether a twitter post is hateful and offensive or not . we further discriminate the detected toxic_content into one of the following three classes : ( a ) hate_speech ( hate ) , ( b ) offensive ( offn ) and ( c ) profane ( prfn ) . with a pre-trained multilingual transformer-based text encoder at the base , we are able to successfully identify and classify hate_speech from multiple_languages . on the provided testing corpora , we achieve macro_f1 scores of 90.29 , 81.87 and 75.40 for english , german and hindi respectively while performing hate_speech detection and of 60.70 , 53.28 and 49.74 during fine-grained classification . in our experiments , we show the efficacy of perspective api features for hate_speech classification and the effects of exploiting a multilingual training scheme . a feature_selection study is provided to illustrate impacts of specific features upon the architecture 's classification head .),\n",
       " (1,\n",
       "  title: named_entity_recognition : a genetic approach for vote based classifier_ensemble selection; abstract: in this paper , the search capability of genetic_algorithm ( ga ) is utilized to construct a vote based classifier_ensemble for named_entity_recognition ( ner ) . our underlying assumption is that the reliability of prediction of each classifier differs among the various named_entity ( ne ) classes . thus , it is necessary to find out the subset of classes for which any particular classifier is most suitable . we use ga to determine which classifier is suitable to vote for which ne classes . we use maximum_entropy ( me ) framework to build a number of classifiers depending upon the various feature_representations . one most important characteristic of these features is that these are language independent in nature and can be easily derived for almost all the languages with minimum effort . weighted_voting is used to combine the outputs of classifiers . the proposed technique is evaluated for a resource-constrained language like bengali . evaluation results for a resource-constrained language like bengali yield the recall , precision and f-measure values of 72.53 % , 83.54 % and 77.64 % , respectively . we also observe that the vote based classifier_ensemble identified by our proposed ga based method_outperforms all the individual_classifiers and performs reasonably better than the two different conventional baseline ensemble_techniques .),\n",
       " (1,\n",
       "  title: node slicing broad learning system for text_classification; abstract: text_classification is playing an increasingly important role in natural_language_processing ( nlp ) . most research adopts deep structure neural_networks to achieve text_classification tasks . however , deep structure networks often suffer from time-consuming trainning process and hardware dependence . in this paper , a flat network called broad learning system ( bls ) is employed to derive a novel learning method - node slicing broad learning system ( nsbls ) . firstly , one-to-one correspondence between the words and the feature node groups is established to obtain a feature layer with rich words , on the basic of which the enhancement layer is generated representing the global_information . then we activate some nodes in the feature node groups , compact them with the enhancement layer and use ridge_regression to obtain multiple outputs . finally , an intergration bls layer is used to correct and combine the multiple outputs to get the final output . the experiment_shows that nsbls has good performance on several datasets .),\n",
       " (1,\n",
       "  title: bi-lstm and ensemble based bilingual sentiment_analysis for a code-mixed hindi-english social_media text; abstract: india is a multilingual and multi-script country and a large part of its population speaks more than one language . it has been noted that such multilingual speakers switch between languages while communicating informally . the code-mixed_language is very common in informal communication and social_media , and extracting sentiments from these code-mixed sentences is a challenging task . in this work , we have worked on sentiment_classification for one of the most common code-mixed_language pairs in india i.e . hindi-english . the conventional sentiment_analysis techniques designed for a single language do n't provide satisfactory_results for such texts . we have proposed two approaches for better sentiment_classification . we have proposed an ensembling based approach which is based on hybridization of naive_bayes , svm , linear_regression , and sgd classifiers . we have also developed a bidirectional_lstm based novel approach . the approaches provide quite satisfactory_results for the code-mixed hindi-english text .),\n",
       " (1,\n",
       "  title: an enhanced data_mining model for text_classification; abstract: classification plays a vital role in many information_management and retrieval tasks . this paper studies classification of text document . text_classification is a supervised technique that uses labeled_training_data to learn the classification system and then automatically_classifies the remaining text using the learned system . in this paper , we propose a mining model consists of sentence-based concept analysis , document-based concept analysis , and corpus-based concept-analysis . then we analyze the term that contributes to the sentence semantics on the sentence , document , and corpus levels rather than the traditional analysis of the document only . after extracting feature_vector for each new document , feature_selection is performed . it is then followed by k-nearest_neighbour classification . the approach enhances the text_classification accuracy . © 2012 ieee .),\n",
       " (1,\n",
       "  title: on classification of abstracts obtained from medical journals; abstract: classification of medical documents was mostly carried out on english data sets and these studies were performed on hospital_records rather than academic texts . the main reasons behind this situation are the lack of publicly available data sets and the tasks being costly and time-consuming . as the first contribution of this study , two data sets including turkish and english counterparts of the same abstracts published in turkish medical journals were constructed . turkish is one of the widely used agglutinative languages worldwide and english is a good example of non-agglutinative languages . while english abstracts were obtained automatically from medline database with a computer program , turkish counterparts of these documents were collected manually from the internet . as the second contribution of this study , an extensive comparison on classification of abstracts obtained from turkish medical journals was made by using these two equivalent data sets . features were extracted from text documents with three different approaches : unigram , bigram and hybrid . hybrid approach includes a combination of unigram_and_bigram features . in the experiments , three different feature_selection methods and seven different classifiers were utilised . according to the results on both data sets , classification performance of the english abstracts outperformed the turkish counterparts . maximum accuracies were obtained from the combination of unigram_features , distinguishing_feature_selector ( dfs ) and multinomial_naïve_bayes ( mnb ) classifier for both data sets . unigram_features were generally more efficient than bigram and hybrid features . however , analysis of top-10 features indicated that nearly half of the features were translations of each other for turkish and english data sets .),\n",
       " (1,\n",
       "  title: prediction of user_ratings of dianping based on k-bert model; abstract: in the internet era , people usually consider the ratings and reviews of stores on review platforms when choosing travel locations . today , the mainstream rating scheme for total store scores is weighted by review scores , but this scoring system can be negatively affected by malicious scoring and uneven scoring . problems such as incompleteness and other issues will affect the authenticity of the store 's rating . to this end , this paper designs a k-bert dianping user rating prediction based on k-bert model to reflect real review ratings . compared with the traditional bert pre-training model , the k-bert model can solve knowledge-driven problems faster through knowledge_graph injection . in this paper , a dianping knowledge map is established . through the steps of text_preprocessing , text pre-training , and dianping dataset fine-tuning , it is found that the accuracy_rate of the k-bert model in the dianping rating classification is about 95 % . by comparing the model with bert , logistic_regression , it is found that the predicted effect of the k-bert model is significantly better than the above two models .),\n",
       " (1,\n",
       "  title: opinion_mining : is feature_engineering still relevant ?; abstract: this paper manifests the experimentation with sentiment_polarity detection over stanford 's imdb movie review dataset using a support_vector_machine classifier ( svm ) . our prime motivation was to find out the best possible combinations of classic features and preprocessing_techniques for the classification of positive and negative opinions . we also explored two variants of kernels with numerous parameter_settings for the classifier in the hope of getting the best svm model . our best model achieved an accuracy score of 85.45 % . the results indicate that a model with a non-linear radial_basis_function ( rbf ) kernel leads to the highest_accuracy . the features that contributed the most are stemmed word n-grams .),\n",
       " (1,\n",
       "  title: dmix : distance constrained interpolative mixup; abstract: interpolation-based regularisation methods have proven to be effective for various tasks and modalities . mixup is a data augmentation method that generates virtual training_samples from convex combinations of individual inputs and labels . we extend mixup and propose dmix , distance-constrained interpolative mixup for sentence_classification leveraging the hyperbolic_space . dmix achieves state-of-the-art results on sentence_classification over existing data_augmentation methods across datasets in four languages .),\n",
       " (1,\n",
       "  title: an approach to identify indic languages using text_classification and natural_language_processing; abstract: india is one of the most culturally and linguistically diverse nations in the world . india stands second in the world for the most languages spoken by its diverse population , who speak their own regional_languages for communication . english is offered as a second additional official_language in india . however , there is a communication gap in india because of how little english is used there . it 's nearly impossible for humans to bridge this breach by translating from one language into another . however , it is possible to translate languages by taking the help of a machine . as per the literature survey , it was observed that neural_machine_translation ( nmt ) is a cutting-edge strategy that significantly outperformed more conventional machine_translation methods for translating one language into another.the main objective of this proposed work is to achieve accurate identification of indic language texts and scripts and provide relevant names of the language after the detection process . the entire work is carried out in stages which includes , collection of the dataset from different sources , preprocessing with the help of data mining techniques , identifying the language of input and in future , approaches like rule_based , statistical and neural_networks will be used followed by post-processing and efficient tasks like machine_translations , named_entity_recognition , etc . will be carried out .),\n",
       " (1,\n",
       "  title: statistical bayesian learning for automatic arabic_text_categorization; abstract: approach : problem_statement : the rapid increasing of online arabic documents necessitated applying text_categorization techniques which are commonly used for english_language to categorize them automatically . the complex_morphology of arabic_language and its large_vocabulary size make using these techniques difficult and costly in time and effort . approach : we have investigated bayesian learning models in order to enhance arabic atc . three classifiers based on bayesian theorem had been implemented which are simple naïve_bayes ( nb ) , multi-variant bernoulli naïve_bayes ( mbnb ) and multinomial_naïve_bayes ( mnb ) models . trec-2002 light stemmer was applied for arabic stemming . for text_representation , bow and character-level 3 , 4 and 5 g had been used . in order to reduce the dimensionality of feature_space , we have used several feature_selection methods ; mutual_information ( mi ) , chi-square statistic ( chi ) , odds_ratio ( or ) and gss-coefficient ( gss ) . conclusion : mbnb classifier outperforms both of nb and mnb classifiers . bow_representation type leads to the best classification performance , nevertheless using character_level n-gram leads to satisfied results by bayesian learning for arabic atc . © 2011 science publications .),\n",
       " (1,\n",
       "  title: an exploration of semi-supervised text_classification; abstract: good performance in supervised text_classification is usually obtained with the use of large amounts of labeled_training_data . however , obtaining labeled_data is often expensive and time-consuming . to overcome these limitations , researchers have developed semi-supervised_learning ( ssl ) algorithms exploiting the use of unlabeled_data , which are generally easy and free to access . with ssl , unlabeled and labeled_data are combined to outperform supervised-learning algorithms . however , setting up ssl neural_networks for text_classification is cumbersome and frequently based on a trial and error process . we show that the hyperparameter configuration significantly impacts ssl performance , and the learning_rate is the most influential parameter . additionally , increasing model size also improves ssl performance , particularly when less pre-processing data are available . interestingly , as opposed to feed-forward models , recurrent models generally reach a performance threshold as pre-processing data size increases . this article expands the knowledge on hyperparameters and model size in relation to ssl application in text_classification . this work supports the use of ssl work in future nlp projects by optimizing model design and potentially lowering training time , particularly if time-restricted .),\n",
       " (1,\n",
       "  title: contrastive_learning with heterogeneous_graph attention networks on short_text_classification; abstract: graph neural_networks ( gnns ) have attracted extensive interest in text_classification tasks due to their expected superior performance in representation_learning . however , most existing_studies adopted the same semi-supervised_learning setting as the vanilla graph convolution network ( gcn ) , which requires a large amount of labelled_data during training and thus is less robust when dealing with large-scale graph data with fewer labels . additionally , graph structure information is normally captured by direct information aggregation via network schema and is highly dependent on correct adjacency information . therefore , any missing adjacency knowledge may hinder the performance . addressing these problems , this paper thus proposes a novel method to learn a graph structure , nc-hgat , by expanding a state-of-the-art self-supervised heterogeneous_graph neural_network model ( hgat ) with simple neighbour contrastive_learning . the new nc-hgat considers the graph structure information from heterogeneous graphs with multilayer_perceptrons ( mlps ) and delivers consistent results , despite the corrupted neighbouring connections . extensive_experiments have been implemented on four benchmark short-text datasets . the results demonstrate that our proposed model nc-hgat significantly_outperforms state-of-the-art methods on three datasets and achieves_competitive_performance on the remaining dataset .),\n",
       " (1,\n",
       "  title: irony and stereotype spreading author profiling on twitter using machine_learning : a bert-tfidf based approach; abstract: in this paper we introduce our system for the task of determining whether an author spreads irony and stereotype in english tweets or not , a part of pan 2022 ( irostereo ) task . for the irony spreading author classification_task , 600 authors each containing 200 tweets have been used . the uniqueness of the task is that it is not a classification between ironic and non ironic tweets , instead it is a classification of irony and non irony spreading authors . the task contains a subtask also that addresses stereotype stance_detection . for the previous years , several representation methods like character/word n-grams etc . have been used for tweet representations , but there was not a clear clue whether a combination of other representations would be helpful . to do this end , we introduce bert combined with tfidf representation to address this specific problem . later we used logistic_regression classifier for the classification_task . it was seen that the bert representation combined with tfidf showed very promising_results .),\n",
       " (1,\n",
       "  title: chinese text_classification for small_sample set; abstract: text_classification is one of the most important topics in the fields of internet information_management and natural_language_processing . machine_learning based text_classification methods are currently most popular ones with better performance than rule_based ones . but they always need lots of training_samples , which not only brings heavy work for previous manual classification , but also puts_forward a higher request for storage and computing resources during the computer post-processing . naïve_bayes algorithm is one of the most effective methods for text_classification with the same problem . only in the large training_sample set can it get a more accurate result . this paper mainly studies naïve_bayes classification algorithm for chinese text based on poisson_distribution model and feature_selection . the experimental results have shown that this method keeps high classification_accuracy even in a small_sample set . © 2011 the journal of china universities of posts and telecommunications .),\n",
       " (1,\n",
       "  title: multi-class emotions classification by sentic levels as features in sentiment_analysis; abstract: sentiment_analysis has become a critical research area in recent days and pervasive in real_life . considering the identification of emotions from textual_content , we propose the hourglass of emotions as the feature that comes from the intensity of affective dimensions and combination thereof . thus , based on a news dataset labeled with six primary emotions , we intend to solve the multi-class classification problem comparing decomposition methods - one against all and one against one - and several aggregation methods . as base_classifiers algorithms , we adopted support_vector_machine , naive_bayes , decision_tree and random_forests . anchored on the results , we found that it is feasible to use this new set of features . the combination of support_vector_machine and weng pairwise coupling method was the best one , producing an accuracy of 55.91 % .),\n",
       " (1,\n",
       "  title: augmentation-agnostic regularization for unsupervised contrastive_learning with its application to speaker verification; abstract: this paper presents a regularization method for unsupervised contrastive_learning and its application to speaker verification . the proposed method , called augmentation-agnostic regularization , enhances the training of speaker embeddings in an adversarial manner . our main idea is to use an augmentation seed classifier , which learns to classify the randomization seeds used in data_augmentation methods , and to train an embedding network with a regularization term to fool the classifier . this method prevents the characteristics of the augmentation procedure from remaining in the embed-dings , facilitating the extraction of speaker characteristics . in experiments , we demonstrate the effectiveness of the proposed regularization in two challenging data-deficient conditions , namely a small-sample training condition and a short-utterance testing condition , and show performance_improvements over the conventional augmented adversarial_training method . the unsupervised model trained with our method achieved compa-rable performance with the supervised x-vector baseline model .),\n",
       " (1,\n",
       "  title: a scalable meta-classifier combining search and classification techniques for multi-level text_categorization; abstract: nowadays , documents are increasingly associated with multi-level category hierarchies rather than a flat category scheme . as the volume and diversity of documents grow , so do the size and complexity of the corresponding category hierarchies . to be able to access such hierarchically classified documents in real-time , we need fast automatic methods to navigate these hierarchies . today 's data domains are also very different from each other , such as medicine and politics . these distinct domains can be handled by different classifiers . a document_representation system which incorporates the inherent category structure of the data should also add useful semantic content to the data vectors and thus lead to better separability of classes . in this paper , we present a scalable meta-classifier to tackle today 's problem of multi-level data classification in the presence of large_datasets . to speed up the classification process , we use a search-based method to detect the level-1 category of a test document . for this purpose , we use a category-hierarchy-based vector representation . we evaluate the meta-classifier by scaling to both longer_documents as well as to a larger category set and show it to be robust in both cases . we test the architecture of our meta-classifier using six different base_classifiers ( random_forest , c4.5 , multilayer_perceptron , naïve_bayes , bayesnet ( bn ) and part ) . we observe that even though there is a very small variation in the performance of different architectures , all of them perform much better than the corresponding single baseline_classifiers . we conclude that there is substantial potential in this meta-classifier architecture , rather than the classifiers themselves , which successfully improves classification performance .),\n",
       " (1,\n",
       "  title: constructing a natural_language inference dataset using generative neural_networks; abstract: natural_language inference is an important task for natural_language_understanding . it is concerned with classifying the logical relation between two sentences . in this paper , we propose several text generative neural_networks for generating text hypothesis , which allows construction of new natural_language inference datasets . to evaluate the models , we propose a new metric -- the accuracy of the classifier trained on the generated dataset . the accuracy obtained by our best generative_model is only 2.7 % lower than the accuracy of the classifier trained on the original , human crafted dataset . furthermore , the best generated dataset combined with the original dataset achieves the highest_accuracy . the best model learns a mapping embedding for each training example . by comparing various metrics we show that datasets that obtain higher rouge or meteor scores do not necessarily yield higher classification_accuracies . we also provide analysis of what are the characteristics of a good dataset including the distinguishability of the generated datasets from the original one .),\n",
       " (1,\n",
       "  title: on-line text_categorization algorithm based on information fusion : semantic svm; abstract: the aim of this paper is to make svms ( support_vector_machines ) more applicable to on-line text_categorization applications . as svms are of good generation ability even with small training_sets and text feature_vectors are clustery in the feature_space , an algorithm for text_categorization , namely , semantic support_vector_machine ( semantic svm ) , is proposed by substituting the original training text set with the semantic center set . this semantic center set is used as the training text and support_vector candidates . the steps to generate the semantic center set and the framework of the on-line learning algorithm of semantic svm are then presented , as well as the implementation of the on-line learning algorithm based on sequential_minimal_optimization . experimental results show that , compared with the standard svms , the proposed semantic svm and its algorithm can improve the on-line learning speed and the classifying speed by orders with a high classifying veracity .),\n",
       " (1,\n",
       "  title: machine_learning in building a collection of computer science course syllabi; abstract: syllabi are rich educational_resources . however , finding computer science syllabi on a generic search_engine does not work well . towards our goal of building a syllabus collection we have trained various decision_tree , naive-bayes , support_vector_machine and feed-forward_neural_network classifiers to recognize computer science syllabi from other web_pages . we have also trained our classifiers to distinguish between artificial_intelligence and software_engineering syllabi . our best classifiers are 95 % accurate at both the tasks . we present an analysis of the various feature_selection methods and classifiers we used hoping to help others developing their own collections . ©_2012_springer-verlag .),\n",
       " (1,\n",
       "  title: evaluating the quality of word_representation models for unstructured clinical_text based icu mortality prediction; abstract: in modern hospitals , the role of clinical_decision_support systems ( cdss ) in assisting care providers is well-established . most conventional cdss systems are built on the availability of patient data in the form of structured electronic_health_records . however , a significant percentage of patient data is still stored in the form of unstructured clinical_text notes , especially in developing_countries . these contain abundant patient-specific information , which has so far remained largely under-utilized in powering cdss applications . in this paper , we attempt to build one such cdss system for patient mortality prediction , using unstructured clinical_records . effectiveness of such prediction models largely depends on optimally capturing latent concept features , thus , word_representation quality is of utmost_importance . we experiment with three popular word_embedding models - word2vec , fasttext and glove for generating word_embeddings of unstructured nursing notes of patients from a standard , open dataset , mimic-iii . these word_representations are used as features to train machine_learning classifiers to build icu mortality prediction models , a critical cdss in icus of hospitals . experimental validation showed that a model built on word2vec skipgram based random_forest classifier was the most optimal word_embedding based mortality prediction model , that outperformed traditional severity scores like saps-ii , sofa , aps-iii and oasis , by a significant margin of 43-52 % .),\n",
       " (1,\n",
       "  title: neural_network model for video-based analysis of student ’ s emotions in e-learning; abstract: abstract : in this paper , we consider a problem of an automatic analysis of the emotional_state of students during online classes based on video surveillance data . this problem is actual in the field of e-learning . we propose a novel neural_network model for recognition of students ’ emotions based on video images of their faces and use it to construct an algorithm for classifying the individual and group emotions of students by video clips . at the first step , it performs detection of the faces and extracts their features followed by grouping the face of each student . to increase the accuracy , we propose to match students ’ names selected with the aid of the algorithms of the text recognition . at the second step , specially learned efficient neural_networks perform the extraction of emotional features of each selected person , their aggregation with the aid of statistical functions , and the subsequent classification . at the final_step , it is possible to visualize fragments of the video lesson with the most pronounced emotions of the student . our experiments with some datasets from emotiw ( emotion recognition in the wild ) show that the accuracy of the developed algorithms is comparable with their known analogous . however , when classifying emotions , the computational performance of these algorithms is higher .),\n",
       " (1,\n",
       "  title: improving sentiment_classification accuracy of financial_news using n-gram approach and feature_weighting methods; abstract: sentiment_classification of financial_news deals with the identification of positive and negative news so that they can be applied in decision_support system to perform stock trend predictions . this paper explores several types of feature_space as different datasets for sentiment_classification of the news article . experiments are conducted based on n-gram approach ( unigram , bigram and the combination of unigram_and_bigram ) used as feature_extraction with different feature_weighting methods , while , document_frequency ( df ) is used as feature_selection method . we performed experiments to measure the classification_accuracy of support_vector_machine ( svm ) with two kernel methods of linear and radial_basis_function ( rbf ) . results showed that an efficient feature_extraction increased classification_accuracy when it is used as a combination of unigram_and_bigram . moreover , we also found that df can be applied as a dimension reduction method to reduce the feature_space without loss of accuracy .),\n",
       " (1,\n",
       "  title: an automated approach for software_bug classification; abstract: open_source projects for example eclipse and firefox have open_source bug repositories . user reports bugs to these repositories . users of these repositories are usually non-technical and can not assign correct class to these bugs . triaging of bugs , to developer , to fix them is a tedious and time consuming task . developers are usually expert in particular areas . for example , few developers are expert in gui and others are in java functionality . assigning a particular bug to relevant developer could save time and would help to maintain the interest level of developers by assigning bugs according to their interest . however , assigning right bug to right developer is quite difficult for tri-ager without knowing the actual class , the bug belongs to . in this research , we have classified the bugs in different labels on the basis of summary of the bug . multinomial_naïve_bayes text classifier is used for classification purpose . for feature_selection , chi-square and tfidf algorithms were used . using naïve_bayes and chi-square , we get average of 83 % accuracy . © 2012 crown_copyright .),\n",
       " (1,\n",
       "  title: classifying case relations using syntactic , semantic and contextual features; abstract: this paper presents a classification of semantic roles using syntactic , semantic and contextual features . the aim of our work is to identify types of semantic roles involving events and their actors ; therefore , we fulfill a feature analysis in order to select the best feature_subset which improves the fulfillment of the task . in addition , we compare four classification_algorithms : support_vector_machine ( svm ) , k-nearest_neighbor ( k-nn ) , bayes_classifier and decision_tree classifier c4.5 . this comparison was made in order to analyze the performance of these algorithms with all features against relevant features for each semantic role category . in our experimentation , we obtain that feature_selection improved the performance of algorithms in our classification_task , since with relevant features we obtained the best performance of 84.6 % with decision_tree classifier c4.5 . the results for the labeling task can be used for knowledge_representation or ontology learning .),\n",
       " (1,\n",
       "  title: chinese text semantic representation for text_classification; abstract: text_representation based on word_frequency statistics is often unsatisfactory because it ignores the semantic relationships between words , and considers them as independent features . in this paper , a new chinese text semantic representation model is proposed by considering contextual semantic and background information on the words in the text . the method captures the semantic relationships between words using wikipedia as a knowledge_base . words with strong semantic relationships are combined into a word-package as indicated by a graph node , which is weighted with the sum of the number and frequency of the words it contains . the contextual relationship between words in different word-packages is stated by a directed edge , which is weighted with the maximum weight of its adjacent nodes . the model retains the contextual_information on each word with a large extent . meanwhile , the semantic meaning between words is strengthened . experimental results of chinese text_classification show that the proposed model can express the content of a text accurately and improve the performance of text_classification . compared to support_vector_machines , text semantic graph-based classification can improve the efficiency by 7.8 % , reduce the error_rate by 1/3 , and show more stability .),\n",
       " (1,\n",
       "  title: impact of word_embedding methods on software vulnerability severity prediction models; abstract: software vulnerability severity prediction has recently gained_popularity in software_engineering . to perform this task , a prediction model has to be developed . the model 's performance greatly depends on the feature_vectors used to train it . these feature_vectors are formed using textual_data present on software vulnerability and converting it to numerical form . there are various methods for feature_vector representation using word_embeddings . unlike_traditional methods such as tf-idf , word_embedding methods using deep_learning map the words into vectors , preserving the semantic relationships between words as well as doing automatic feature_selection , which holds great importance when dealing with textual_data . therefore , word_embedding methods using deep_learning showed promising_results over traditional tf-idf . hence , in this paper , we conducted a controlled experiment to examine the effect of various word_embedding methods for feature_vector representation on the performance of the prediction model . the models developed in this study used tf-idf , word2vec , and glove , coupled with svm , lstm , and bi-lstm classification_algorithms . the classification performance of different word_embedding methods is analyzed on five different vulnerability datasets of mozilla products . the experiments showed that the best performance is achieved by using the glove method with the bi-lstm algorithm giving an average auc value of 0.81 , which is an improvement of 12.5 % from the traditional method of feature_vector representation .),\n",
       " (1,\n",
       "  title: angular contour parameterization for signature identification; abstract: this present work presents a parameterization system based on angles from signature edge ( 2d-shape ) for off-line signature identification . we have used three different classifiers , the nearest_neighbor classifier ( k-nn ) , neural_networks ( nn ) and hidden_markov_models ( hmm ) . our off-line database has 800 writers with 24 samples per each writer ; in total , 19200 images have been used in our experiments . we have got a success_rate of 84.64 % , applying as classifier hidden_markov_model , and only used the information from this edge_detection method . ©_2009_springer-verlag_berlin_heidelberg .),\n",
       " (1,\n",
       "  title: optimal_feature_selection for learning-based algorithms for sentiment_classification; abstract: sentiment_classification is an important branch of cognitive computation—thus the further studies of properties of sentiment_analysis is important . sentiment_classification on text data has been an active topic for the last two decades and learning-based methods are very popular and widely used in various applications . for learning-based methods , a lot of enhanced technical strategies have been used to improve the performance of the methods . feature_selection is one of these strategies and it has been studied by many researchers . however , an existing unsolved difficult problem is the choice of a suitable number of features for obtaining the best sentiment_classification performance of the learning-based methods . therefore , we investigate the relationship between the number of features selected and the sentiment_classification performance of the learning-based methods . a new method for the selection of a suitable number of features is proposed in which the chi_square_feature_selection algorithm is employed and the features are selected using a preset score threshold . it is discovered that there is a relationship between the logarithm of the number of features selected and the sentiment_classification performance of the learning-based method , and it is also found that this relationship is independent of the learning-based method involved . the new findings in this research indicate that it is always possible for researchers to select the appropriate number of features for learning-based methods to obtain the best sentiment_classification performance . this can guide researchers to select the proper features for optimizing the performance of learning-based algorithms . ( a preliminary version of this paper received a best paper award at the international_conference on extreme_learning machines 2018 . )),\n",
       " (1,\n",
       "  title: fuzzy_rule_based_systems for gender classification from blog data; abstract: gender classification is a popular machine_learning task , which has been undertaken in various domains , e.g . business_intelligence , access_control and cyber_security . in the context of information granulation , gender related information can be divided into three types , namely , biological information , vision based information and social_network based information . in traditional_machine_learning , gender identification has been typically treated as a discriminative classification t ask , i.e . it is aimed at learning a classifier t hat d is criminates between male_and_female . i n this paper , we argue that it is not always appropriate to identify gender in the way of discriminative classification , especially when considering the case that both male_and_female people are of high diversity and thus individuals of different genders could have high similarity to each other in terms of their characteristics . in order to address the above issue , we propose the use of a fuzzy method for generative classification o f g ender . in particular , we focus on gender classification based on social_network information . we conduct an experiment study by using a blog data set , and compare the fuzzy method with c4.5 , naive_bayes and support_vector_machine in terms of classification performance . the results show that the fuzzy method_outperforms the other methods and is also capable of capturing the diversity of both male_and_female people and dealing with the fuzziness in terms of gender identification .),\n",
       " (1,\n",
       "  title: improving implicit_discourse_relation_classification by modeling inter-dependencies of discourse_units in a paragraph; abstract: we argue that semantic meanings of a sentence or clause can not be interpreted independently from the rest of a paragraph , or independently from all discourse_relations and the overall paragraph-level discourse_structure . with the goal of improving implicit_discourse_relation_classification , we introduce a paragraph-level neural_networks that model inter-dependencies between discourse_units as well as discourse_relation continuity and patterns , and predict a sequence of discourse_relations in a paragraph . experimental results show that our model outperforms the previous state-of-the-art systems on the benchmark_corpus of pdtb .),\n",
       " (1,\n",
       "  title: employing hierarchical bayesian networks in simple and complex emotion topic analysis; abstract: traditional emotion models , when tagging single emotions in documents , often ignore the fact that most documents convey complex human emotions . in this paper , we join emotion analysis with topic_models to find complex emotions in documents , as well as the intensity of the emotions , and study how the document emotions vary with topics . hierarchical bayesian networks are employed to generate the latent_topic variables and emotion variables . on average , our model on single emotion classification outperforms the traditional supervised_machine_learning models such as svm and naive_bayes . the other model on the complex emotion classification also achieves promising_results . we thoroughly analyze the impact of vocabulary quality and topic quantity to emotion and intensity prediction in our experiments . the distribution of topics such as friend and job are found to be sensitive to the documents ' emotions , which we call emotion topic variation in this paper . this reveals the deeper relationship between topics and emotions . © 2012 elsevier ltd. all rights_reserved .),\n",
       " (1,\n",
       "  title: machine_learning or lexicon based sentiment_analysis techniques on social_media posts; abstract: social_media provides an accessible and effective platform for individuals to offer thoughts and opinions across a wide range of interest areas . it also provides a great opportunity for researchers and businesses to understand and analyse a large_volume of online data for decision-making purposes . opinions on social_media platforms , such as twitter , can be very important for many industries due to the wide variety of topics and large_volume of data available . however , extracting and analysing this data can prove to be very challenging due to its diversity and complexity . recent methods of sentiment_analysis of social_media content rely on natural_language_processing techniques on a fundamental sentiment_lexicon , as well as machine_learning oriented techniques . in this work , we evaluate representatives of different sentiment_analysis methods , make recommendations and discuss advantages and disadvantages . specifically we look into : 1 ) variation of vader , a lexicon based method ; 2 ) a machine_learning neural_network based method ; and 3 ) a sentiment classifier using word_sense_disambiguation , maximum_entropy and naive_bayes classifiers . the results indicate that there is a significant correlation among all three sentiment_analysis methods , which demonstrates their ability to accurately determine the sentiment of social_media posts . additionally , the modified version of vader , a lexicon based method , is considered to be the most accurate and most appropriate method to use for the semantic analysis of social_media posts , based on its strong correlation and low computational time . obtained findings and recommendations can be valuable for researchers working on sentiment_analysis techniques for large data sets .),\n",
       " (1,\n",
       "  title: a new term_weighting_method by introducing class information for sentiment_classification of textual_data; abstract: with the popularity of text based communication tools such as blogs , plurk , twitter , and so on , customers can easily express their opinions , reviews or comments about purchased products/services . these personal opinions , especially negative comments , might have a significant influence on other consumers ' purchasing_decisions . therefore , how to detect users ' sentiment from textual_data to assist companies to carefully respond to customers ' comments has become a crucial task . recently , machine_learning methods have been considered as one of solutions in sentiment_classification . when applying machine_learning approaches to classify sentiment , term_frequency ( tf ) , term presence ( tp ) and term_frequency-inverse_document_frequency ( tf-idf ) usually have been employed to describe collected textual_data . however , these traditional term_weighting methods can not have positive influence on improving classification performance . therefore , this work proposes a new term_weighting_method called categorical difference weights ( cdw ) by introducing class information . besides , cdw will be integrated into support_vector_machines ( svm ) . finally , an actual case will be provided to illustrate the effectiveness of our proposed method . compared with traditional term_weighting methods , tf and tf-idf , experimental results indicated that the proposed cdw method indeed can improve the classification performance .),\n",
       " (1,\n",
       "  title: stochastic tokenization with a language model for neural text_classification; abstract: for unsegmented languages such as japanese and chinese , tokenization of a sentence has a significant_impact on the performance of text_classification . sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word ( or subword ) representations for neural_networks . however , segmentation is potentially ambiguous , and it is unclear whether the segmented tokens achieve the best performance for the target task . in this paper , we propose a method to simultaneously learn tokenization and text_classification to address these problems . our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously . to make the model robust against infrequent tokens , we sampled segmentation for each sentence stochastically during training , which resulted in improved performance of text_classification . we conducted experiments on sentiment_analysis as a text_classification task and show that our method_achieves better performance than previous methods .),\n",
       " (1,\n",
       "  title: election result prediction using twitter sentiment_analysis; abstract: the proliferation of social_media in the recent past has provided end_users a powerful platform to voice their opinions . businesses ( or similar entities ) need to identify the polarity of these opinions in order to understand user orientation and thereby make smarter decisions . one such application is in the field of politics , where political entities need to understand public opinion and thus determine their campaigning strategy . sentiment_analysis on social_media data has been seen by many as an effective tool to monitor user_preferences and inclination . popular text_classification algorithms like naive_bayes and svm are supervised_learning algorithms which require a training_data set to perform sentiment_analysis . the accuracy of these algorithms is contingent upon the quantity as well as the quality ( features and contextual relevance ) of the labeled_training_data . since most applications suffer from lack of training_data , they resort to cross_domain sentiment_analysis which misses out on features relevant to the target data . this , in turn , takes a toll on the overall accuracy of text_classification . in this paper , we propose a two stage framework which can be used to create a training_data from the mined twitter data without compromising on features and contextual relevance . finally , we propose a scalable machine_learning model to predict the election results using our two stage framework .),\n",
       " (1,\n",
       "  title: seeing both the forest and the trees : multi-head_attention for joint classification on different compositional levels; abstract: in natural_languages , words are used in association to construct sentences . it is not words in isolation , but the appropriate combination of hierarchical_structures that conveys the meaning of the whole sentence . neural_networks can capture expressive language features ; however , insights into the link between words and sentences are difficult to acquire automatically . in this work , we design a deep_neural_network architecture that explicitly wires lower and higher linguistic components ; we then evaluate its ability to perform the same task at different hierarchical_levels . settling on broad text_classification tasks , we show that our model , mhal , learns to simultaneously solve them at different levels of granularity by fluidly transferring_knowledge between hierarchies . using a multi-head_attention_mechanism to tie the representations between single words and full sentences , mhal systematically outperforms equivalent models that are not incentivized towards developing compositional representations . moreover , we demonstrate that , with the proposed architecture , the sentence information flows naturally to individual words , allowing the model to behave like a sequence labeller ( which is a lower , word-level task ) even without any word supervision , in a zero-shot fashion .),\n",
       " (1,\n",
       "  title: discriminative learning of generative_models : large_margin multinomial mixture_models for document_classification; abstract: in this paper , a novel discriminative learning method is proposed to estimate generative_models for multi-class pattern classification_tasks , where a discriminative objective_function is formulated with separation margins according to certain discriminative learning criterion , such as large_margin estimation ( lme ) . furthermore , the so-called approximation-maximization ( am ) method is proposed to optimize the discriminative objective_function w.r.t . parameters of generative_models . the am approach provides a good framework to deal with latent_variables in generative_models and it is flexible enough to discriminatively learn many rather complicated generative_models . in this paper , we are interested in a group of generative_models derived from multinomial distributions . under some minor relaxation conditions , it is shown that the am-based discriminative learning methods for these generative_models result in linear_programming ( lp ) problems that can be solved effectively and efficiently even for rather large-scale models . as a case_study , we have studied to learn multinomial mixture_models ( mmms ) for text document_classification based on the large_margin criterion . the proposed methods have been evaluated on a standard rcv1 text_corpus . experimental results show that large_margin mmms significantly_outperform the conventional mmms as well as pure discriminative models such as support_vector_machines ( svm ) , where over 25 % relative classification error_reduction is observed in three independent rcv1 test sets .),\n",
       " (1,\n",
       "  title: performance analysis of ad_hoc classifiers for categorization of uyghur texts; abstract: this paper starts from the characteristics and writing rules of uyghur , have established a relatively large text_corpus which include 20 categories , 300 documents for each category . and studied the knn , naive_bayes ( nb ) , and svm classification_algorithms more thoroughly , which have widely been used in domestic and foreign academic research fields , then classified the uyghur text by using these algorithms , and analyzed the performance of each algorithm separately . finally , some research directions on uyghur text_classification are also given in this paper .),\n",
       " (1,\n",
       "  title: naive_bayes_classifier and word2vec for sentiment_analysis on bahasa_indonesia cosmetic product_reviews; abstract: cosmetic_products are products that are widely sold on e-commerce . a product , including a cosmetic product can generate mixed sentiments in the form of customer_reviews . therefore , customer_reviews are one of the most important to be paid attention to . this is because from the customer_reviews , it can be known the level of customer_satisfaction about the product that has been purchased . sentiment_analysis is a solution that can be used to measure customer_satisfaction . sentiment_analysis is a text-based research field that is suitable to discuss the problem of customer_satisfaction about the product . the analysis used is based on several aspects of cosmetic_products , namely aroma , packaging , price , and product . in this study , the problem was solved by analyzing sentiment using the naive_bayes and word2vec methods . based on experiment results , naïve_bayes and word2vec could be used to classify the sentiment . the best model of this research produces an accuracy of 68.17 % with an accuracy of 56.36 % for product aspects , 70.96 % for price aspects , 68.79 % for packaging aspects , and 76.57 % for aroma aspects . this result is obtained when the translated data is used and validated using 2-fold_cross_validation . the parameters for word2vec are window = 7 and size = 300 .),\n",
       " (1,\n",
       "  title: a novel semantic smoothing kernel for text_classification with class-based weighting; abstract: in this study , we propose a novel methodology to build a semantic smoothing kernel to use with support_vector_machines ( svm ) for text_classification . the suggested approach is based on two key_concepts ; class-based term_weighting and changing the orthogonality of vector_space . a class-based term_weighting methodology is used for transformation of documents from the original space to the feature_space . this class-based weighting basically groups terms based on their importance for each class and consequently smooths the representation of documents . this is accomplished by changing the orthogonality of the vector_space_model ( vsm ) with introducing class-based dependencies between terms . as a result , on the extreme case , two documents can be seen as similar even if they do not share any terms but their terms are similarly weighted for a particular class . the resulting semantic kernel can directly make use of class information in extracting semantic information between terms , therefore it can be considered as a supervised kernel . for our experimental evaluation , we analyze the performance of the suggested kernel with a large number of experiments on benchmark textual datasets and present results with respect to varying experimental conditions . to the best of our knowledge , this is the first study to use class-based term_weighting in order to build a supervised semantic kernel for svm . we compare our results with kernels that are commonly used in svm such as linear_kernel , polynomial kernel , radial_basis_function ( rbf ) kernel and with several corpus-based semantic kernels . according to our experimental results the proposed method favorably improves classification_accuracy over linear_kernel and several corpus-based semantic kernels in terms of both accuracy and speed .),\n",
       " (1,\n",
       "  title: from accuracy to versatility : analysing text_classification models regarding transfer_learning; abstract: while the discriminative performance and classification benchmarks capture most of the attention , the vast amount of data required to train state-of-the-art models is often overlooked . inductive transfer_learning suggests shifting a model between different data domains and hence introduces versatility . in this work , we ( 1 ) thoroughly analyse the ability of text_classification models to adapt to transfer_learning tasks , whether they are specifically designed for it or not . we directly compare the transformer model with an attention-based bi-directional_lstm and naive logistic_regression as a baseline . ( 2 ) exploiting semantic embeddings , we quantify the domain_shift between different classes and predict the expected model transferability and performance . ( 3 ) drawing from our extensive analysis , we experimentally modify the vocabulary size , layer freezing and learning rates , pursing the goal to improve eligibility for transfer_learning . our study reveals that simplistic models may be advantageous in easy transfer_learning tasks due to faster_convergence . an inter-active , online colaboratory notebook that allows reproducing all results , is available here11https : //colab.research.google.com/drive/18cxphah1ym4t5dysuivjjl-b2ggiti4d .),\n",
       " (1,\n",
       "  title: a bi-lstm-rnn model for relation_classification using low-cost sequence features; abstract: relation_classification is associated with many potential applications in the artificial_intelligence area . recent approaches usually leverage neural_networks based on structure features such as syntactic or dependency features to solve this problem . however , high-cost structure features make such approaches inconvenient to be directly used . in addition , structure features are probably domain-dependent . therefore , this paper proposes a bi-directional_long-short-term-memory recurrent-neural-network ( bi-lstm-rnn ) model based on low-cost sequence features to address relation_classification . this model divides a sentence or text segment into five parts , namely two target entities and their three contexts . it learns the representations of entities and their contexts , and uses them to classify relations . we evaluate our model on two standard benchmark_datasets in different domains , namely semeval-2010 task 8 and bionlp-st 2016 task bb3 . in the former dataset , our model achieves comparable performance compared with other models using sequence features . in the latter dataset , our model obtains the third best results compared with other models in the official evaluation . moreover , we find that the context between two target entities plays the most important role in relation_classification . furthermore , statistic experiments show that the context between two target entities can be used as an approximate replacement of the shortest_dependency_path when dependency_parsing is not used .),\n",
       " (1,\n",
       "  title: sentiment_classification of web review using association_rules; abstract: sentiment_classification of web reviews or comments is an important and challenging task in web_mining and data_mining . this paper presents a novel approach using association_rules for sentiment_classification of web reviews . a new restraint measure ad-sup is used to extract discriminative frequent_term_sets and eliminate terms with no sentiment_orientation which contain close frequency in both positive and negative reviews . an optimal classification rule_set is then generated which abandons the redundant general rule with lower confidence than the specific one . in the class_label prediction procedure , we proposed a new metric voting_scheme to solve the problem when the covered rules are not adequately confident or not applicable . the final score of a test review depends on the overall contributions of four metrics . extensive_experiments on multiple domain datasets from web_site demonstrate that 50 % is the best min-conf to guarantee classification rules both abundant and persuasive and the voting strategy obtains improvements on other baselines of using confidence . another comparison to popular machine_learning algorithms such as svm , naïve_bayes and knn also indicates that the proposed method_outperforms these strong benchmarks . ©_2013_springer-verlag_berlin_heidelberg .),\n",
       " (1,\n",
       "  title: model-induced term-weighting_schemes for text_classification; abstract: the bag-of-words representation of text data is very popular for document_classification . in the recent literature , it has been shown that properly weighting the term feature_vector can improve the classification performance significantly beyond the original term-frequency based features . in this paper we demystify the success of the recent term-weighting strategies as well as provide possibly more reasonable modifications . we then propose novel term-weighting_schemes that can be induced from the well-known document probabilistic models such as the naive_bayes and the multinomial term model . interestingly , some of the intuition-based term-weighting_schemes coincide exactly with the proposed derivations . our term-weighting_schemes are tested on large-scale text_classification problems/datasets where we demonstrate improved prediction performance over existing_approaches .),\n",
       " (1,\n",
       "  title: an intelligent information agent for document title classification and filtering in document-intensive domains; abstract: effective decision_making is based on accurate and timely information . however , human decision_makers are often overwhelmed by the huge amount of electronic data these days . the main_contribution of this paper is the development of effective information agents which can autonomously classify and filter incoming electronic data on behalf of their human users . the proposed information agents are innovative because they can quickly classify electronic documents solely based on the short titles of these documents . moreover , supervised_learning is not required to train the classification models of these agents . document_classification is based on information inference conducted over a high dimensional semantic information space . what is more , a belief_revision mechanism continuously maintains a set of user preferred information categories and filter documents with respect to these categories . preliminary experimental results show that our document_classification and filtering mechanism outperforms the support_vector_machines ( svm ) model which is regarded as one of the best performing classifiers . © 2007 elsevier b.v. all rights_reserved .),\n",
       " (1,\n",
       "  title: emoji prediction from sentence; abstract: emojis are small pictures typically used in text_messages via social_media . the synthesis of the visual and textual quality of the same message creates a new way of conversation . despite being commonly used in social_media , from the prespective of natural_language_processing , the underlying semantics of emojis have gained limited attention . we explore the relationship between words and emojis in this project , researching the challenging task of predicting the emojis when conveyed by textual twitter posts . we experimented variant of word_embedding techniques , and train several models such as svc , linearsvc , random_forest classifier and decision_tree classifier .),\n",
       " (1,\n",
       "  title: probabilistic event categorization; abstract: this paper describes the automation of a new text_categorization task . the categories assigned in this task are more syntactically , semantically , and contextually complex than those typically assigned by fully automatic systems that process unseen_test data . our system for assigning these categories is a probabilistic_classifier , developed with a recent method for formulating a probabilistic model from a predefined set of potential features . this paper focuses on feature_selection . it presents a number of fully automatic features . it identifies and evaluates various approaches to organizing collocational properties into features , and presents the results of experiments covarying type of organization and type of property . we find that one organization is not best for all kinds of properties , so this is an experimental parameter worth investigating in nlp systems . in addition , the results suggest a way to take advantage of properties that are low_frequency but strongly indicative of a class . the problems of recognizing and organizing the various kinds of contextual_information required to perform a linguistically complex categorization task have rarely been systematically investigated in nlp .),\n",
       " (1,\n",
       "  title: neural_networks classifier for data selection in statistical_machine_translation; abstract: we address the data selection problem in statistical_machine_translation ( smt ) as a classification_task . the new data selection method is based on a neural_network classifier . we present a new method description and empirical results proving that our data selection method provides better translation quality , compared to a state-of-the-art method ( i.e. , cross_entropy ) . moreover , the empirical results reported are coherent across different language pairs .),\n",
       " (1,\n",
       "  title: sarcasm detection in arabic short text using deep_learning; abstract: recently , a growing interest among researchers has emerged in discovering ambiguous information in short sarcastic texts in arabic . nevertheless , sarcasm is a particularly challenging problem for sentiment_analysis algorithms due to its considerable impact on emotions . a short text evaluation can provide important information about a product or service . however , due to the currently small number of sarcastic datasets and their unbalanced nature , it is also important to preprocess data before classification , especially those with dialects . furthermore , to detect sarcasm , language models must be capable of capturing complicated relationships and ambiguous semantic meanings . in this paper , we attempt to detect sarcasm in arabic text using a large pre-trained_language_model ( bert ) . therefore , a new dataset for the `` isarcasmeval '' shared_task is examined in this paper . preprocessing of the dataset is performed first . moreover , the data is balanced by applying both random swap and random deletion , which are both data_augmentation techniques . following that , two transformer-based models , marbert and arabert , were used to analyze the data . experimental results reveal that the marbert model outperforms the arabert model in this dataset .),\n",
       " (1,\n",
       "  title: capsule_network on social_media text : an application to automatic detection of clickbaits; abstract: the advent of groundbreaking deep_learning techniques like capsule_network has changed the way of approaching a problem in data_science research . initially , capsule_networks were built and tested on image data and found to be of great use . their usage on textual_data is still very limited . in this paper , we try to investigate whether capsule_network can be used to address a research problem where the classification heavily depends on the textual_data . in various classification_task involving social_networks and online_sources , words and sentences across classes do not vary that much . but , the context and representation of those words play a significant role . one such problem is to correctly identify clickbaits . state of the art solutions either take into account various handcrafted_features from the data or use efficient text_classification techniques like lstm . our work is a stepping stone towards examining whether the need of network properties and feature_engineering can be omitted while using capsule_network . it relaxes the effort of manual feature_construction from the data and looks beyond the sequence to sequence modelling of an lstm based approach . our proposed approach of clickbait_detection using a capsule_network outperforms various existing_methods in terms of multiple performance metric .),\n",
       " (1,\n",
       "  title: graphics classification for enterprise knowledge_management; abstract: enterprise content repositories often consist of business documents comprising not only of traditional text data but also graphics ( org charts , graphs , architecture diagrams , etc . ) that get reused by people across the enterprise . despite this diversity of content , most of the research in enterprise_search has focused on improving document search . we describe a machine_learning approach for graphics classification that automatically_classifies graphics within enterprise documents into an enterprise graphics taxonomy and enables graphics search functionality to augment traditional document-centric enterprise_search . this allows legacy enterprise documents to be automatically converted into a reusable , tagged , graphics repository . our approach works by extracting reusable graphics from enterprise documents , performing feature_extraction to create textual , visual and structural_features that are subsequently used to classify these graphics . we provide experimental results on a real-world data set from accenture . the contributions of this work are automating the creation of a categorized graphics database for enterprise km systems , studying the utility of different feature_sets , and in demonstrating that existing classification and feature_selection methods are appropriate for this task . finally we describe several applications currently being deployed at accenture that are enabled by the categorized graphics repository . © 2010 ieee .),\n",
       " (1,\n",
       "  title: confiscation detection of criminal judgment using text_classification approach; abstract: as the system of confiscation becomes more and more perfect , grasping the distribution of the types of confiscations actually announced by the courts will enable you to understand changing of the trend . in addition to assisting legislators in formulating laws , it can also provide other people with an understanding of the actual operation of the confiscation system . in order to enable artificial_intelligence technology to automatically identify the distribution of confiscation , and consumes a lot of manpower and time costs of manual judgment . the purpose of this research is to establish an automated confiscation identification model that can quickly and accurately_identify the multiple label categories of confiscation , and provide the needs of all social circles for confiscation information , so as to facilitate subsequent law amendments or discretion . this research uses the first instance criminal cases as the main experimental data . according to the current laws , the confiscation is divided into three categories : contrabands , criminal tools and criminal proceeds , and perform multiple label identification . this research will use term_frequency_-_inverse_document_frequency ( tf-idf ) and word2vec algorithm as the feature_extraction algorithm , with random_forest classifier , and ckiplabbert pretrained_model for training and identification . the experimental results show that under the ckiplabbert pretrained_model , the best identification effect can be obtained when only use sentences with confiscated words mentioned in the judgment . when the task is case confiscation , the micro_f1 score can be as high as 96.2716 % , and when the task is defendant confiscation , the micro_f1 score is as high as 95.5478 % .),\n",
       " (1,\n",
       "  title: identifying training_sets for personalized article retrieval system; abstract: retrieving documents that are relevant to a particular researcher 's purpose is a big challenge , especially when searching through large database , such as pubmed . researchers who use traditional keyword-based document_retrieval systems often end up with a large collection of documents that are not directly relevant to their needs . what is needed is a personalized document_retrieval system that can select only relevant_articles for one 's specific research interests . obtaining an appropriate training_data set is essential in building and testing personalized article retrieval_systems . this study describes one approach to form such training_data set based on articles categorized by domain_experts under mesh major topics . text classifiers , learned using support_vector_machines , were used to test to what degree the training_set categories are differentiable . preliminary_results and analysis of the results are discussed . © 2011 authors .),\n",
       " (1,\n",
       "  title: kerminsvm for imbalanced_datasets with a case_study on arabic comics classification; abstract: many studies have been performed to classify large-sized text documents using different classifiers , ranging from simple distance classifiers such as k-nearest-neighbor ( knn ) to more advanced classifiers such as support_vector_machines . traditional_approaches fail when a short text is encountered due to sparsity resulting from a limited number of words . another common problem in text_classification is class_imbalance ( ci ) . ci occurs when one class of the data contains most of the samples while the other class contains only a few . standard classifiers , when applied to imbalanced_data , result in high accuracy for the majority_class and low accuracy for the minority one . we were motivated to propose a novel framework for classifying the content of arabic comics ; therefore , we propose kerminsvm , a kernel extension of our previously proposed minsvm coupled with a new dimensionality featuring a reduction scheme based on word root frequency ratios ( wrfr ) . kerminsvm was tested on multiple imbalanced benchmark_datasets , and the results were verified using three measures : accuracy , f-measure , and statistical_analysis . wrfr was applied to the manual construction of the arabic comic text dataset to detect strong content in children 's comic_books . test results revealed that our proposed framework outperformed most of the methods for imbalanced_datasets and short_text_classification .),\n",
       " (1,\n",
       "  title: classifying documents within multiple hierarchical datasets using multi-task_learning; abstract: multi-task_learning ( mtl ) is a supervised_learning paradigm in which the prediction models for several related_tasks are learned jointly to achieve better generalization performance . when there are only a few training_examples per task , mtl considerably outperforms the traditional single task learning ( stl ) in terms of prediction_accuracy . in this work we develop an mtl based approach for classifying documents that are archived within dual concept hierarchies , namely , dmoz and wikipedia . we solve the multi-class classification problem by defining oneversus-rest binary_classification tasks for each of the different classes across the two hierarchical datasets . instead of learning a linear_discriminant for each of the different tasks independently , we use a mtl approach with relationships between the different tasks across the datasets established using the non-parametric , lazy , nearest_neighbor approach . we also develop and evaluate a transfer_learning ( tl ) approach and compare the mtl ( and tl ) methods against the standard single task learning and semi-supervised_learning approaches . our empirical results demonstrate the strength of our developed methods that show an improvement especially when there are fewer number of training_examples per classification_task . © 2013 ieee .),\n",
       " (1,\n",
       "  title: enhancing aspect-based sentiment_analysis of arabic hotels ’ reviews using morphological , syntactic and semantic features; abstract: this research presents an enhanced approach for aspect-based sentiment_analysis ( absa ) of hotels ’ arabic reviews using supervised_machine_learning . the proposed approach employs a state-of-the-art research of training a set of classifiers with morphological , syntactic , and semantic features to address the research tasks namely : ( a ) t1 : aspect category identification , ( b ) t2 : opinion_target expression ( ote ) extraction , and ( c ) t3 : sentiment_polarity identification . employed classifiers include naïve_bayes , bayes networks , decision_tree , k-nearest_neighbor ( k-nn ) , and support-vector_machine ( svm ) .the approach was evaluated using a reference dataset based on semantic evaluation 2016 workshop ( semeval-2016 : task-5 ) . results show that the supervised_learning approach_outperforms related work evaluated using the same dataset . more precisely , evaluation results show that all classifiers in the proposed approach outperform the baseline approach , and the overall enhancement for the best performing classifier ( svm ) is around 53 % for t1 , around 59 % for t2 , and around 19 % in t3 .),\n",
       " (1,\n",
       "  title: aspect extraction and classification for sentiment_analysis in drug reviews; abstract: aspect-based sentiment_analysis ( absa ) of patients ’ opinions_expressed in drug reviews can extract valuable_information about specific aspects of a particular drug such as effectiveness , side effects and patient conditions . one of the most important and challenging tasks of absa is to extract the implicit and explicit aspects from a text , and to classify the extracted aspects into predetermined classes . supervised_learning algorithms possess high accuracy in extracting and classifying aspects ; however , they require annotated_datasets whose manual construction is time-consuming and costly . in this paper , first a new method was introduced for identifying expressions that indicate an aspect in user_reviews about drugs in english . then , distant_supervision was adopted to automate the construction of a training_set using sentences and phrases that are annotated as aspect classes in the drug domain . the results of the experiments showed that the proposed method is able to identify various aspects of the test set with 74.4 % f-measure , and outperforms the existing aspect extraction methods . also , training the random_forest classifier on the dataset that was constructed via distant_supervision obtained the f-measure of 73.96 % , and employing this dataset to fine-tune bert for aspect classification yielded better f-measure ( 78.05 % ) in comparison to an existing method in which the random_forest classifier trained on an accurate manually constructed dataset .),\n",
       " (1,\n",
       "  title: multi-part representation_learning for cross-domain web_content classification using neural_networks; abstract: owing to the tremendous_increase in the volume and variety of user_generated content , train-once-apply-forever models are insufficient for supervised_learning tasks . the need is to develop algorithms that can adapt across domains by leveraging labeled_data from source_domain ( s ) and efficiently perform the task in the unlabeled_target_domain . towards this , we present a novel two-stage neural_network learning algorithm for domain_adaptation which learns a multi-part hidden_layer where individual parts contribute differently to the tasks in source and target domains . the multiple parts of the representation ( i.e . hidden_layer ) are learned while being cognizant of what characteristics to transfer across domains and what to preserve within domains for enhanced performance . the first stage embroils around learning a two-part representation i.e . source specific and common representations in a manner such that the former do not detract the ability of the later to represent the target domain . in the second stage , the generalized common representation is further iteratively extended with discriminating target specific characteristics to adapt to the target domain . we empirically_demonstrate that the learned representations , in difierent arrangements , outperform existing domain_adaptation algorithms in the source classification as well as the cross-domain classification_tasks on the user_generated content from different domains on the web .),\n",
       " (1,\n",
       "  title: sentiment_analysis during the covid-19_pandemic as a tool for exploring psychosocial expression in cyberspace; abstract: this work presents the brazilian population ’ s sentiment_analysis during the covid-19_pandemic , using robert plutchik ’ s wheel of emotions classifications . the study ended by checking the application feasibility regarding the monitoring of social_media while exploring the psychosocial expression of cyberspace . after data preparation , there were assessed the best results and the supervised_learning algorithms . real data was the source for this work , with near 150,000 tweets , between march and october 2020 . a total of 4491 tweets were manually_labeled for training and test classification , in a proportion of 75 % being used for training and 25 % for testing . the results showed a precision of up to 0.78 , having a recall between 0.56 and 0.82 and f1-score ranging from 0.57 to 0.80 . considering the results of populational sentiments , varying minimally on tweets , given the presence of covid , while citing the brazilian_army , no causal relationship has been demonstrated between the general sentiments of the public in brazil and that related to the brazilian_army .),\n",
       " (1,\n",
       "  title: beyond supervised_learning of wrappers for extracting information from unseen web_sites; abstract: we investigate the problem of wrapper adaptation which aims at adapting a previously learned wrapper to an unseen target site . to achieve this goal , we make use of extraction rules previously discovered from a particular site to seek potential candidates of training_examples for the target site . we pose the problem of training example identification for the target site as a hybrid text_classification problem . the idea is to use a classification model to capture the characteristics of the attribute item of interests . based on the automatically annotated training_examples , a new wrapper for the unseen target web_site can then be discovered . we present encouraging experimental results on wrapper adaptation for some real-world web_sites . ©_springer-verlag 2003 .),\n",
       " (1,\n",
       "  title: multi-domain active_learning for text_classification; abstract: active_learning has been proven to be effective in reducing labeling_efforts for supervised_learning . however , existing active_learning work has mainly focused on training models for a single domain . in practical_applications , it is common to simultaneously train classifiers for multiple_domains . for example , some merchant web_sites ( like amazon.com ) may need a set of classifiers to predict the sentiment_polarity of product_reviews collected from various domains ( e.g. , electronics , books , shoes ) . though different domains have their own unique features , they may share some common latent features . if we apply active_learning on each domain separately , some data instances selected from different domains may contain duplicate knowledge due to the common features . therefore , how to choose the data from multiple_domains to label is crucial to further reducing the human labeling_efforts in multi-domain learning . in this paper , we propose a novel multi-domain active_learning framework to jointly select data instances from all domains with duplicate information considered . in our solution , a shared subspace is first learned to represent common latent features of different domains . by considering the common and the domain-specific features together , the model loss reduction induced by each data instance can be decomposed into a common part and a domain-specific part . in this way , the duplicate information across domains can be encoded into the common part of model loss reduction and taken into account when querying . we compare our method with the state-of-the-art active_learning approaches on several text_classification tasks : sentiment_classification , newsgroup classification and email_spam filtering . the experiment results show that our method reduces the human labeling_efforts by 33.2 % , 42.9 % and 68.7 % on the three tasks , respectively . © 2012 acm .),\n",
       " (1,\n",
       "  title: an evaluation of statistical spam_filtering techniques; abstract: this paper evaluates five supervised_learning methods in the context of statistical spam_filtering . we study the impact of different feature pruning methods and feature_set sizes on each learner 's performance using cost-sensitive measures . it is observed that the significance of feature_selection varies greatly from classifier to classifier . in particular , we found support_vector_machine , adaboost , and maximum_entropy model are top performers in this evaluation , sharing similar characteristics : not sensitive to feature_selection strategy , easily scalable to very high feature_dimension , and good performances across different datasets . in contrast , naive_bayes , a commonly used classifier in spam_filtering , is found to be sensitive to feature_selection methods on small feature_set , and fails to function well in scenarios where false_positives are penalized heavily , the experiments also suggest that aggressive feature pruning should be avoided when building filters to be used in applications where legitimate mails are assigned a cost much higher than spams ( such as λ = 999 ) , so as to maintain a better-than-baseline performance . an interesting finding is the effect of mail headers on spam_filtering , which is often ignored in previous_studies . experiments show that classifiers using features from message header alone can achieve comparable or better performance than filters utilizing body features only . this implies that message headers can be reliable and powerfully discriminative feature sources for spam_filtering .),\n",
       " (1,\n",
       "  title: analyzing the effectiveness and applicability of co-training; abstract: recently there has been significant interest in supervised_learning algorithms that combine labeled and unlabeled_data for text learning tasks . the co-training setting [ 1 ] applies to datasets that have a natural separation of their features into two disjoint_sets . w e demonstrate that when learning from labeled and unlabeled_data , algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not . when a natural split does not exist , co-training algorithms that manufacture a feature split may out-perform algorithms not using a split . these results help explain why co-training algorithms are both discriminative in nature and robust to the assumptions of their embedded classifiers .),\n",
       " (1,\n",
       "  title: natural_language inference prompts for zero-shot emotion classification in text across corpora; abstract: within textual emotion classification , the set of relevant labels depends on the domain and application scenario and might not be known at the time of model development . this conflicts with the classical paradigm of supervised_learning in which the labels need to be predefined . a solution to obtain a model with a flexible set of labels is to use the paradigm of zero-shot learning as a natural_language inference task , which in addition adds the advantage of not needing any labeled_training_data . this raises the question how to prompt a natural_language inference model for zero-shot learning emotion classification . options for prompt formulations include the emotion name anger alone or the statement `` this text expresses anger '' . with this paper , we analyze how sensitive a natural_language inference-based zero-shot-learning classifier is to such changes to the prompt under consideration of the corpus : how carefully does the prompt need to be selected ? we perform experiments on an established set of emotion datasets presenting different language registers according to different sources ( tweets , events , blogs ) with three natural_language inference models and show that indeed the choice of a particular prompt formulation needs to fit to the corpus . we show that this challenge can be tackled with combinations of multiple prompts . such ensemble is more robust across corpora than individual prompts and shows nearly the same performance as the individual best prompt for a particular corpus .),\n",
       " (1,\n",
       "  title: exploiting the matching information in the support set for few shot event classification; abstract: the existing event classification ( ec ) work primarily focuseson the traditional supervised_learning setting in which models are unableto extract event mentions of new/unseen event_types . few-shot learninghas not been investigated in this area although it enables ec models toextend their operation to unobserved event_types . to fill in this gap , inthis work , we investigate event classification under the few-shot learningsetting . we propose a novel training method for this problem that exten-sively exploit the support set during the training process of a few-shotlearning model . in particular , in addition to matching the query exam-ple with those in the support set for training , we seek to further matchthe examples within the support set themselves . this method providesmore training signals for the models and can be applied to every metric-learning-based few-shot learning methods . our extensive_experiments ontwo benchmark ec datasets show that the proposed method can improvethe best reported few-shot learning models by up to 10 % on accuracyfor event classification),\n",
       " (1,\n",
       "  title: solution for the epo codefest on green plastics : hierarchical multi-label_classification of patents relating to green plastics using deep_learning; abstract: this work aims at hierarchical multi-label patents classification for patents disclosing technologies related to green plastics . this is an emerging_field for which there is currently no classification_scheme , and hence , no labeled_data is available , making this task particularly challenging . we first propose a classification_scheme for this technology and a way to learn a machine_learning model to classify patents into the proposed classification_scheme . to achieve this , we come up with a strategy to automatically assign labels to patents in order to create a labeled training dataset that can be used to learn a classification model in a supervised_learning setting . using said training dataset , we come up with two classification models , a scibert neural_network ( sbnn ) model and a scibert hierarchical neural_network ( sbhnn ) model . both models use a bert model as a feature_extractor and on top of it , a neural_network as a classifier . we carry out extensive_experiments and report commonly evaluation_metrics for this challenging classification problem . the experiment results verify the validity of our approach and show that our model sets a very strong benchmark for this problem . we also interpret our models by visualizing the word importance given by the trained model , which indicates the model is capable to extract high-level semantic information of input documents . finally , we highlight how our solution fulfills the evaluation_criteria for the epo codefest and we also outline possible directions for future work . our code has been made available at https : //github.com/epo/cf22-green-hands),\n",
       " (1,\n",
       "  title: a cfs-based feature_weighting approach to naive_bayes text classifiers; abstract: recent work in supervised_learning has shown that naive_bayes text classifiers with strong assumptions of independence among features , such as multinomial_naive_bayes ( mnb ) , complement naive_bayes ( cnb ) and the one-versus-all-but-one model ( ova ) , have achieved remarkable classification performance . this fact raises the question of whether a naive_bayes text classifier with less restrictive assumptions can perform even better . responding to this question , we firstly evaluate the correlation-based feature_selection ( cfs ) approach in this paper and find that it performs even worse than the original versions . then , we propose a cfs-based feature_weighting approach to these naive_bayes text classifiers . we call our feature weighted versions fwmnb , fwcnb and fwova respectively . our proposed approach weakens the strong assumptions of independence among features by weighting the correlated features . the experimental results on a large suite of benchmark_datasets show that our feature weighted versions significantly_outperform the original versions in terms of classification_accuracy . ©_2014_springer_international_publishing_switzerland .),\n",
       " (1,\n",
       "  title: degree of belonging as an input for automatic text_classification : a syntactic approach; abstract: grouping documents into categories is one of the solutions adopted to streamline the information_retrieval process , which is increasingly relevant due to the large amount of information available today . the manual localization of documents of a specific theme , available in digital repositories , involves reading the title , abstract and keywords , in addition to further detailed evaluation in order to identify whether the publication belongs to the desired thematic axis . considering the number of publications in a digital repository , manually locating all the desired texts on a given topic can be laborious and time-consuming . this research proposes an architecture for automatic classification of texts that is based on syntactic questions , that is , it undertakes a comparison of n-grams , which are combinations of n-pairs of words that are identified throughout the text . an exploratory applied research was carried out , which applied a type of supervised_learning , fundamentally based on the document_representation model called bag-of-words ( bow ) . the paper ’ s macro objective was to classify texts in general , according to pre-defined_categories , by generating and comparing degrees of belonging between texts , as one of the key criteria . the results of these comparisons , using n-gram = 3 , demonstrate that in the use of classifications by n-grams , the greater the number of grams , and with the removal of the stop_words , we obtain a reduced degree of belonging , demonstrating greater rigor in identifying the match during the classification . in order to have greater confidence in the results , a larger training corpus is necessary to expand the number of words that characterize the pre-defined_categories , to be used in the classification of the texts .),\n",
       " (1,\n",
       "  title: chinese relation_extraction based on deep_belief nets; abstract: relation_extraction is a fundamental task in information_extraction , which is to identify the semantic relationships between two entities in the text . in this paper , deep_belief nets ( dbn ) , which is a classifier of a combination of several unsupervised_learning networks , named rbm ( restricted_boltzmann machine ) and a supervised_learning network named bp ( back-propagation ) , is presented to detect and classify the relationships among chinese name entities . the rbm layers maintain as much information as possible when feature_vectors are transferred to next layer . the bp layer is trained to classify the features generated by the last rbm layer . the experiments are conducted on the automatic content extraction 2004 dataset . this paper proves that a character-based feature is more suitable for chinese relation_extraction than a word-based feature . in addition , the paper also performs a set of experiments to assess the chinese relation_extraction on different assumptions of an entity categorization feature . these experiments showed the comparison among models with correct entity_types and imperfect entity_type classified by dbn and without entity_type . the results show that dbn is a successful approach in the high-dimensional-feature-space information_extraction task . it outperforms state-of-the-art learning models such as svm and back-propagation networks . © 2012 iscas .),\n",
       " (1,\n",
       "  title: batch active_learning for text_classification and sentiment_analysis; abstract: supervised_learning of classifiers for text_classification and sentiment_analysis relies on the availability of labels that may be either difficult or expensive to obtain . a standard procedure is to add labels to the training dataset sequentially by querying an annotator until the model reaches a satisfactory_performance . active_learning is a process that optimizes unlabeled_data records selection for which the knowledge of the label would bring the highest discriminability of the dataset . batch active_learning is a generalization of a single instance active_learning by selecting a batch of documents for labeling . this task is much more demanding because plenty of different factors come into consideration ( i. e. batch_size , batch evaluation , etc. ) . in this paper , we provide a large_scale study by decomposing the existing algorithms into building_blocks and systematically comparing meaningful combinations of these blocks with a subsequent evaluation on different text datasets . while each block is known ( warm start weights initialization , dropout mc , entropy sampling , etc . ) , many of their combinations like bayesian strategies with agglomerative_clustering are first proposed in our paper with excellent_performance . particularly , our extension of the warm start method to batch active_learning is among the top performing strategies on all datasets . we studied the effect of this proposal comparing the outcomes of varying distinct factors of an active_learning algorithm . some of these factors include initialization of the algorithm , uncertainty representation , acquisition function , and batch selection_strategy . further , various combinations of these are tested on selected nlp problems with documents encoded using roberta embeddings . datasets cover context integrity ( gibberish wackerow ) , fake_news_detection ( kaggle fake_news_detection ) , categorization of short texts by emotional context ( twitter sentiment140 ) , and sentiment_classification ( amazon reviews ) . ultimately , we show that each of the active_learning factors has advantages for certain datasets or experimental settings .),\n",
       " (1,\n",
       "  title: mooc video classification using natural_language_processing and machine_learning model; abstract: mooc opens up the doors for universal access to education remotely and serves as a constructive approach to acquire formal education informally by negating the traditional practices . in recent_years , the number of mooc video resources has increased exponentially . therefore , the need is a fully_automated system that would be proficient enough to store , analyze and manage such an immensity of videos while sustaining the quality in response . an automatic classification/prediction of videos is a challenging and complex aspect , although supervised_machine_learning can effectively achieve this task in an effective way . many applications use text_classification to categorize documents like , e.g . spam_filtering , email routing , sentiment_analysis , etc . in this study , we present a clever and adaptive technique for autonomous classification of mooc videos transcription using natural_language_processing and machine_learning model . our approach can predict the category of a targeted video ; the data_mining algorithms such as svm , random_forest , and naive_bayesian will be engaged to organize the mooc videos . experiments reveal that our approach outperformed other approaches in the field of transcription classification and supervised_learning .),\n",
       " (1,\n",
       "  title: an optimization based feature_extraction and machine_learning techniques for named_entity identification; abstract: the processing of unstructured and structured documents involves the recognition of specific entity classes in the named_entity_recognition ( ner ) and the categorization of these entities into certain predefined_classes . biomedical instances such as rnas , dnas , disorders , viruses , proteins , genes and chemical components are identified using biomedical_named_entity_recognition ( bner ) . the techniques used to retrieve those other ebontities have a major role to play in this bner . supervised_machine_learning ( sml ) approaches are used in various bner techniques.the primary benefit of supervised_learning is the ability to gather data or generate data output from prior experiences . if your training_set lacks the examples you wish to include in a class , the decision_boundary of your model may be overstretched.the boundary_condition is employed when a particle goes past the region where a boundary constraint is no longer valid.in these approaches , in order to enhance the recognition process 's effectiveness , these features are used . a set of distinguishing and discriminating characteristics are used for identifying features , which is having ability for indicating entity occurrence.bio curators annotates only limited number of articles also consumes more processing time . in this work , propose an enhanced system for curatable-biomedical named_entities recognition ( ecbner ) and feature_extraction approaches for bio-medical named_entity_recognition using aimproved particle_swarm_optimization ( ipso ) . classification of curatable named-entities is useful in facilitating biocuration with a straightforward technique for accelerating workflow of proposed biocuration . curatable and non-curatable are classified using a support_vector_machine ( svm ) in this work.the process of gathering and organizing knowledge , facts , and information in the realm of life sciences is known as “ biocuration. ” in ml , combination of classifiers provides productive exploration guidance and it is a successful strategy of it . an independent classifier 's exhibition in characterization can be improved utilizing this . consequence of different classifiers mix is accumulated to defeat singular classifiers conceivable nearby soft spot for delivering exceptionally strong acknowledgment . quality/disease ner is handled under conditional_random_field ( crf ) and all activity terms are gathered and prepared in a simultaneous way to extricate precise biomedical named substances . at long last , this overall structure to learn portrayal by joining general and area explicit highlights is proposed and assessed , demonstrating exact outcomes contrasted with existing systems .),\n",
       " (1,\n",
       "  title: ontology-based modelling of related work sections in research articles : using crfs for developing semantic data based information_retrieval systems; abstract: research articles are an important form of scientific communication . however , currently there are hardly any systems which exploit the content of research articles for information_retrieval . the paper describes our work carried out in developing ontology-based information_retrieval system using information extracted about sentences in research articles . we present results of a supervised_learning mechanism using conditional_random_fields for context identification and sentence_classification of sentences in the related work section of research articles . the labelling of sentences is carried out based on a classification framework , which we propose for classifying sentences in these sections . we proceed to develop a sentence context ontology for modelling the classified data obtained through crfs . we also show how the ontology is further used for creating rdf data . finally , we describe the user_interface developed using sewese tags and sparql for querying the developed rdf data . © 2010 acm .),\n",
       " (1,\n",
       "  title: offensive_language classification of code-mixed tamil with keras; abstract: this paper presents the method adopted for completing task 1 of dravidian-codemix-hasoc ( hate_speech_and_offensive content identification in english and indo-european_languages ) shared_task proposed by the forum of information_retrieval evaluation in 2021 , for offensive_language_detection . for detecting offensive_language , a custom model architecture using convolutional_neural_networks was created using keras for supervised_learning , and trained on a dataset of youtube comments , written in code-mixed tamil in both roman and tamil scripts . the 5 layer neural_network was built only using keras , and required simple tokenized data , padded to an appropriate length . recurrent_neural_networks and transfer_learning were not used , and an f-score of 0.835 was achieved with the created cnn model .),\n",
       " (1,\n",
       "  title: election result prediction using twitter sentiment_analysis; abstract: the proliferation of social_media in the recent past has provided end_users a powerful platform to voice their opinions . businesses ( or similar entities ) need to identify the polarity of these opinions in order to understand user orientation and thereby make smarter decisions . one such application is in the field of politics , where political entities need to understand public opinion and thus determine their campaigning strategy . sentiment_analysis on social_media data has been seen by many as an effective tool to monitor user_preferences and inclination . popular text_classification algorithms like naive_bayes and svm are supervised_learning algorithms which require a training_data set to perform sentiment_analysis . the accuracy of these algorithms is contingent upon the quantity as well as the quality ( features and contextual relevance ) of the labeled_training_data . since most applications suffer from lack of training_data , they resort to cross_domain sentiment_analysis which misses out on features relevant to the target data . this , in turn , takes a toll on the overall accuracy of text_classification . in this paper , we propose a two stage framework which can be used to create a training_data from the mined twitter data without compromising on features and contextual relevance . finally , we propose a scalable machine_learning model to predict the election results using our two stage framework .),\n",
       " (1,\n",
       "  title: optical_character_recognition for printed tamil text using unicode; abstract: optical_character_recognition ( ocr ) refers to the process of converting printed tamil text documents into software translated unicode tamil text . the printed_documents available in the form of books , papers , magazines , etc . are scanned using standard scanners which produce an image of the scanned document . as part of the preprocessing_phase the image file is checked for skewing . if the image is skewed , it is corrected by a simple rotation technique in the appropriate direction . then the image is passed through a noise elimination phase and is binarized . the preprocessed image is segmented using an algorithm which decomposes the scanned text into paragraphs using special space detection technique and then the paragraphs into lines using vertical histograms , and lines into words using horizontal histograms , and words into character_image glyphs using horizontal histograms . each image glyph is comprised of 32 × 32 pixels . thus a database of character_image glyphs is created out of the segmentation phase . then all the image glyphs are considered for recognition using unicode mapping . each image glyph is passed through various routines which extract the features of the glyph . the various features that are considered for classification are the character height , character width , the number of horizontal lines ( long and short ) , the number of vertical lines ( long and short ) , the horizontally oriented curves , the vertically oriented curves , the number of circles , number of slope lines , image centroid and special dots . the glyphs are now set ready for classification based on these features . the extracted features are passed to a support_vector_machine ( svm ) where the characters are classified by supervised_learning algorithm . these classes are mapped onto unicode for recognition . then the text is reconstructed using unicode fonts .),\n",
       " (1,\n",
       "  title: an intelligent information agent for document title classification and filtering in document-intensive domains; abstract: effective decision_making is based on accurate and timely information . however , human decision_makers are often overwhelmed by the huge amount of electronic data these days . the main_contribution of this paper is the development of effective information agents which can autonomously classify and filter incoming electronic data on behalf of their human users . the proposed information agents are innovative because they can quickly classify electronic documents solely based on the short titles of these documents . moreover , supervised_learning is not required to train the classification models of these agents . document_classification is based on information inference conducted over a high dimensional semantic information space . what is more , a belief_revision mechanism continuously maintains a set of user preferred information categories and filter documents with respect to these categories . preliminary experimental results show that our document_classification and filtering mechanism outperforms the support_vector_machines ( svm ) model which is regarded as one of the best performing classifiers . © 2007 elsevier b.v. all rights_reserved .)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(taxo.root.children[0].papers, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [[] for i in taxo.root.children]\n",
    "unmapped = []\n",
    "\n",
    "for p in range(len(collection)):\n",
    "    class_freq = [0] * len(taxo.root.children)\n",
    "\n",
    "    for c_id, c in enumerate(taxo.root.children):\n",
    "        # how many total mentions of the node terms\n",
    "        class_freq[c_id] = np.sum([collection[p].vocabulary[ele] for ele in c.all_node_terms if ele in collection[p].vocabulary.keys()])\n",
    "    \n",
    "    nonzero_idx = np.nonzero(class_freq)[0]\n",
    "    if len(nonzero_idx) == 0:\n",
    "        unmapped.append(p)\n",
    "        continue\n",
    "\n",
    "    for i in nonzero_idx:\n",
    "        # score: class_i_mentions / log(total_len)\n",
    "        score = class_freq[i] / np.log(collection[p].length)\n",
    "        classes[i].append((score, p))\n",
    "\n",
    "classes = [sorted(c, reverse=True) for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unmapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bagging',\n",
       " 'boosting',\n",
       " 'stacking',\n",
       " 'voting',\n",
       " 'weighted_voting',\n",
       " 'random_forest',\n",
       " 'gradient_boosting',\n",
       " 'neural_network_ensemble',\n",
       " 'decision_tree_ensemble',\n",
       " 'support_vector_machine_ensemble',\n",
       " 'k_nearest_neighbors_ensemble',\n",
       " 'feature_bagging',\n",
       " 'feature_boosting',\n",
       " 'model_selection',\n",
       " 'hyperparameter_tuning',\n",
       " 'cross_validation',\n",
       " 'ensemble_methods',\n",
       " 'random_forests',\n",
       " 'base_learners',\n",
       " 'ensemble_learning',\n",
       " 'feature_combination',\n",
       " 'ensemble_techniques',\n",
       " 'cluster_based',\n",
       " 'rbf',\n",
       " 'mnb',\n",
       " 'dt',\n",
       " 'radial_basis_function',\n",
       " 'base_classifiers',\n",
       " 'gaussian_naive_bayes',\n",
       " 'multilayer_perceptron',\n",
       " 'c4.5',\n",
       " 'adaboost',\n",
       " 'attention_layer',\n",
       " 'feed-forward',\n",
       " 'thresholding',\n",
       " 'multinomial_logistic_regression',\n",
       " 'ensemble_classifier',\n",
       " 'memory-based',\n",
       " 'k-nearest_neighbor',\n",
       " 'nearest_neighbor',\n",
       " 'principal_component_analysis']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root.children[-1].all_node_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised_learning\n",
      "15 arabic_text_categorization via binary particle_swarm_optimization and support_vector_machines ; abstract : document_categorization concerns automatically assigning a category label to a text document , and has increasingly many applications , particularly in the domains of organizing , browsing and search in large_document_collections . it is typically achieved via machine_learning , where a model is built on the basis of a ( typically ) large collection of document features . feature_selection is critical in this process , since there are typically several thousand potential features ( distinct words or terms ) . here we explore binary particle_swarm_optimization ( bpso ) hybridized with either k-nearest-neighbour ( knn ) or a support_vector_machine ( svm ) , for feature_selection in arabic document_categorization tasks . comparison between feature_selection methods is done on the basis of using the selected features , in conjunction with each of svm , c4.5 and naive_bayes , to classify a holdout test set . using publicly available datasets , we show that the bpsosvm approach seems promising in this domain . we also analyse the sets of selected features and consider the differences between the types of feature that bpsoknn and bpsosvm tend to choose\n",
      "16 a novel approach for ontology-based dimensionality_reduction for web text document_classification ; abstract : dimensionality_reduction of feature_vector size plays a vital role in enhancing the text processing capabilities ; it aims in reducing the size of the feature_vector used in the mining tasks ( classification , clustering ... etc. ) . this paper proposes an efficient approach to be used in reducing the size of the feature_vector for web text document_classification process . this approach is based on using wordnet ontology , utilizing the benefit of its hierarchal structure , to eliminate words from the generated feature_vector that has no relation with any of wordnet lexical categories\n",
      "449 25th pacific-asia conference on knowledge discovery and data_mining , pakdd 2021 ; abstract : the proceedings contain 157 papers . the special focus in this conference is on knowledge discovery and data_mining . the topics include : self-supervised graph_representation learning with variational_inference ; manifold approximation and projection by maximizing graph information ; learning attention-based translational knowledge_graph embedding via nonlinear dynamic mapping ; multi-grained dependency_graph neural_network for chinese open_information_extraction ; human-understandable decision_making for visual_recognition ; lightcake : a lightweight framework for context-aware knowledge_graph embedding ; transferring domain_knowledge with an adviser in continuous tasks ; inferring hierarchical mixture structures : a bayesian nonparametric approach ; quality_control for hierarchical classification with incomplete annotations ; learning discriminative_features using multi-label dual_space ; universal representation for code ; autocluster : meta-learning based ensemble_method for automated unsupervised_clustering ; banditrank : learning to rank using contextual bandits ; a compressed and accelerated segnet for plant leaf disease segmentation : a differential_evolution based approach ; meta-context transformers for domain-specific response_generation ; a multi-task kernel learning algorithm for survival_analysis ; meta-data augmentation based search strategy through generative_adversarial_network for automl model_selection ; tree-capsule : tree-structured capsule_network for improving relation_extraction ; rule injection-based generative_adversarial imitation learning for knowledge_graph reasoning ; hierarchical self attention_based autoencoder for open-set human activity_recognition ; reinforced natural_language inference for distantly_supervised_relation classification ; self-supervised adaptive aggregator learning on graph ; sagcn : structure-aware graph convolution network for document-level relation_extraction ; addressing the class_imbalance problem in medical image_segmentation via accelerated tversky loss_function ; incorporating relational knowledge in explainable fake_news_detection\n",
      "373 estimating the generalization performance of polynomial svm classifier for text_categorization\n",
      "354 a hybrid documents classification based on svm and rough_sets\n",
      "280 automatic polarity identification on twitter using machine_learning ; abstract : this work presents a study of emotions to analyze the polarity of a set of data that was extracted from twitter , detailing each of the resources in the different forms that a language has , and to be able to observe feelings such as irony , sarcasm , and happiness , among others . this research can help us classify the polarity of each one of them deeply in the corpus that deals with this research work . experimental results conducted using different machine_learning methods are presented : support_vector_machines , naïve_bayes , logistic_regression , knn and random_forest , with which a classification system based on cross-validation was implemented . all experiments were performed in python . the results obtained are shown with two different corpus ; where the first set is made up of 10,653 tweets in total divided equally each with 3551 tweets with a positive , negative and neutral label\n",
      "184 margin maximization with feed-forward neural_networks : a comparative study with svm and adaboost\n",
      "236 classification of forensic autopsy reports through conceptual_graph-based document_representation model\n",
      "335 forestexter : an efficient random_forest algorithm for imbalanced text_categorization\n",
      "946 an efficient approach for ensemble of svm and ann for sentiment_classification\n",
      "\n",
      "\n",
      "unsupervised_learning\n",
      "111 deep graph neural_networks for text_classification task ; abstract : text_classification is to organizing documents into predetermined_categories , usually by machinery learn algorithms . it is a significant ways to organize and utilize the large amount of information that exists in unstructured_text format . text_classification is an important module in text processing , and its applications are also very extensive , such as garbage filtering , news classification , part-of-speech_tagging , and so on . with the continuous development of deep_learning in recent_years ! its applications are also very extensive , such as : garbage filtering , news classification , part-of-speech_tagging , and so on . but the text also has its own characteristics . according to the characteristics of the text , the general process of text_classification is : 1 . preprocessing ; 2 . text_representation and feature_selection ; 3 . construction of a classifier\n",
      "237 hybrid supervised clustering based ensemble scheme for text_classification\n",
      "144 a text feature_selection method using the improved mutual_information and information entropy\n",
      "159 a tensor space model-based deep_neural_network for text_classification ; abstract : most text_classification systems use machine_learning algorithms ; among these , naïve_bayes and support_vector_machine algorithms adapted to handle text data afford reasonable_performance . recently , given developments in deep_learning technology , several scholars have used deep_neural_networks ( recurrent and convolutional_neural_networks ) to improve text_classification . however , deep_learning-based text_classification has not greatly_improved performance compared to that of conventional algorithms . this is because a textual document is essentially expressed as a vector ( only ) , albeit with word dimensions , which compromises the inherent semantic information , even if the vector is ( appropriately ) transformed to add conceptual information . to solve this ‘ loss of term senses ’ problem , we develop a concept-driven deep_neural_network based upon our semantic tensor space model . the semantic tensor used for text_representation features a dependency between the term and the concept\n",
      "273 research convey on text_classification method based on deep_learning\n",
      "206 a component clustering_algorithm based on semantic similarity and optimization\n",
      "967 research progress of text_classification technology based on deep_learning\n",
      "734 a method of text_classification based on statistical technology and set_theory\n",
      "346 a comparative research of different granularities in korean text_classification ; abstract : text_classification is a process , which can make the specified documents group into several categories , predefined at the beginning through learning a series of rules or under the guidance of the goal function . this paper compared the subword-level , spacing-level of korean and the word-level , then analyzed the influence of the preprocessing of different granularities on the text_classification task of korean . after that , analyzed the results of classification linguistically . thus we can choose the proper granularity as the input to improve the classification effect . firstly , cut the corpus according to different granularities\n",
      "910 knowledge-based clustering scheme for collection management and retrieval of library books ; abstract : we propose a knowledge-based clustering scheme for grouping books in a library . such a grouping is achieved with the help of domain_knowledge in the form of the acm cr ( computing reviews ) category hierarchy . a new knowledge-based similarity_measure is defined and used in clustering books . the proposed scheme is useful in overcoming several problems associated with the existing book collection management and document_retrieval systems . more specifically , it can be used in : ( 1 ) helping the user select an appropriate collection of books in a library which contains the topics of interest ; ( 2 ) assigning a classification number to a new book ; ( 3 ) designing a more appropriate and uniform classification_scheme for books\n",
      "\n",
      "\n",
      "semi_supervised_learning\n",
      "122 meta_learning for few-shot joint intent_detection_and_slot-filling\n",
      "516 batch active_learning for text_classification and sentiment_analysis\n",
      "227 multi-domain active_learning for text_classification\n",
      "246 revisiting uncertainty-based query_strategies for active_learning with transformers\n",
      "773 cutting the error by half : investigation of very deep cnn and advanced training strategies for document_image_classification ; abstract : we present an exhaustive investigation of recent deep_learning architectures , algorithms , and strategies for the task of document_image_classification to finally reduce the error by more than half . existing_approaches , such as the deepdoc-classifier , apply standard convolutional_network architectures with transfer_learning from the object_recognition domain . the contribution of the paper is threefold : first , it investigates recently introduced very deep_neural_network architectures ( googlenet , vgg , resnet ) using transfer_learning ( from real images ) . second , it proposes transfer_learning from a huge set of document_images , i.e . 400\n",
      "661 protaugment : unsupervised diverse short-texts paraphrasing for intent_detection meta-learning\n",
      "338 exploiting the matching information in the support set for few shot event classification\n",
      "525 active_learning in automated text_classification : a case_study exploring bias in predicted model performance_metrics\n",
      "389 classifying syntactic errors in learner language\n",
      "777 enriching pre-trained_language_model with entity information for relation_classification\n",
      "\n",
      "\n",
      "deep_learning\n",
      "293 a multi-scale convolutional attention_based gru network for text_classification\n",
      "1 a novel model combining transformer and bi-lstm for news categorization ; abstract : news categorization ( nc ) , the aim of which is to identify distinct categories of news through analyzing the contents , has acquired substantial progress since deep_learning was introduced into the natural_language_processing ( nlp ) field . as a state-of-art model , transformer & # x2019\n",
      "776 an automatic method using hybrid neural_networks and attention_mechanism for software_bug triaging\n",
      "199 phrase2vec : phrase embedding based on parsing\n",
      "379 an integrated fuzzy neural_network with topic-aware auto-encoding for sentiment_analysis\n",
      "773 cutting the error by half : investigation of very deep cnn and advanced training strategies for document_image_classification ; abstract : we present an exhaustive investigation of recent deep_learning architectures , algorithms , and strategies for the task of document_image_classification to finally reduce the error by more than half . existing_approaches , such as the deepdoc-classifier , apply standard convolutional_network architectures with transfer_learning from the object_recognition domain . the contribution of the paper is threefold : first , it investigates recently introduced very deep_neural_network architectures ( googlenet , vgg , resnet ) using transfer_learning ( from real images ) . second , it proposes transfer_learning from a huge set of document_images , i.e . 400\n",
      "398 pseudo-labeling with transformers for improving question_answering systems\n",
      "449 25th pacific-asia conference on knowledge discovery and data_mining , pakdd 2021 ; abstract : the proceedings contain 157 papers . the special focus in this conference is on knowledge discovery and data_mining . the topics include : self-supervised graph_representation learning with variational_inference ; manifold approximation and projection by maximizing graph information ; learning attention-based translational knowledge_graph embedding via nonlinear dynamic mapping ; multi-grained dependency_graph neural_network for chinese open_information_extraction ; human-understandable decision_making for visual_recognition ; lightcake : a lightweight framework for context-aware knowledge_graph embedding ; transferring domain_knowledge with an adviser in continuous tasks ; inferring hierarchical mixture structures : a bayesian nonparametric approach ; quality_control for hierarchical classification with incomplete annotations ; learning discriminative_features using multi-label dual_space ; universal representation for code ; autocluster : meta-learning based ensemble_method for automated unsupervised_clustering ; banditrank : learning to rank using contextual bandits ; a compressed and accelerated segnet for plant leaf disease segmentation : a differential_evolution based approach ; meta-context transformers for domain-specific response_generation ; a multi-task kernel learning algorithm for survival_analysis ; meta-data augmentation based search strategy through generative_adversarial_network for automl model_selection ; tree-capsule : tree-structured capsule_network for improving relation_extraction ; rule injection-based generative_adversarial imitation learning for knowledge_graph reasoning ; hierarchical self attention_based autoencoder for open-set human activity_recognition ; reinforced natural_language inference for distantly_supervised_relation classification ; self-supervised adaptive aggregator learning on graph ; sagcn : structure-aware graph convolution network for document-level relation_extraction ; addressing the class_imbalance problem in medical image_segmentation via accelerated tversky loss_function ; incorporating relational knowledge in explainable fake_news_detection\n",
      "890 discriminative learning of generative_models : large_margin multinomial mixture_models for document_classification\n",
      "837 improving transformer-based end-to-end speech_recognition with connectionist temporal classification and language model integration\n",
      "\n",
      "\n",
      "ensemble_methods\n",
      "941 document_classification using symbolic classifiers ; abstract : in this paper , we present symbolic classifiers to classify text documents . we propose to use cluster_based symbolic_representation followed by symbolic feature_selection methods to classify text documents . in particular , we propose symbolic clustering approaches ; symbolic cluster_based without feature_selection ; symbolic cluster_based with feature_selection ( using similarity_measure )\n",
      "827 application of bagging_ensemble classifier based on genetic_algorithm in the text_classification of railway fault hazards\n",
      "47 comparative_analysis of binary classifiers on an array of scientific_publications\n",
      "141 predicting software defect severity_level using sentence_embedding and ensemble_learning\n",
      "237 hybrid supervised clustering based ensemble scheme for text_classification\n",
      "280 automatic polarity identification on twitter using machine_learning ; abstract : this work presents a study of emotions to analyze the polarity of a set of data that was extracted from twitter , detailing each of the resources in the different forms that a language has , and to be able to observe feelings such as irony , sarcasm , and happiness , among others . this research can help us classify the polarity of each one of them deeply in the corpus that deals with this research work . experimental results conducted using different machine_learning methods are presented : support_vector_machines , naïve_bayes , logistic_regression , knn and random_forest , with which a classification system based on cross-validation was implemented . all experiments were performed in python . the results obtained are shown with two different corpus ; where the first set is made up of 10,653 tweets in total divided equally each with 3551 tweets with a positive , negative and neutral label\n",
      "512 enhanced malay sentiment_analysis with an ensemble classification machine_learning approach\n",
      "59 an ensemble model for stance_detection in social_media texts\n",
      "817 an intelligent hybrid sentiment_analyzer for personal protective medical equipments based on word_embedding technique : the covid-19 era\n",
      "732 a scalable meta-classifier combining search and classification techniques for multi-level text_categorization\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(classes):\n",
    "    print(taxo.root.children[idx])\n",
    "    for p in i[:10]:\n",
    "        print(p[-1], collection[p[-1]].title)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse",
   "language": "python",
   "name": "inverse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
