{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3,4\"\n",
    "from taxonomy import Taxonomy, Paper\n",
    "from utils import filter_phrases\n",
    "import subprocess\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.track = \"Text Classification\"\n",
    "        self.dim = \"Methodology\"\n",
    "        self.input_file = \"datasets/sample_1k.txt\"\n",
    "        self.iters = 4\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = []\n",
    "id = 0\n",
    "with open(args.input_file, \"r\") as f:\n",
    "    papers = f.read().strip().splitlines()\n",
    "    for p in papers:\n",
    "        title = re.findall(r'title\\s*:\\s*(.*) ; ', p, re.IGNORECASE)\n",
    "        abstract = re.findall(r'abstract\\s*:\\s*(.*)', p, re.IGNORECASE)\n",
    "        collection.append(Paper(id, title, abstract))\n",
    "        id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Taxonomy Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Types of Methodology Proposed in Text Classification Research Papers': {'description': None, 'seeds': None, 'terms': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering', 'self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn', 'convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting'], 'supervised_learning': {'description': 'Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.', 'seeds': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting'], 'terms': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting']}, 'unsupervised_learning': {'description': 'Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.', 'seeds': ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering'], 'terms': ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering']}, 'semi_supervised_learning': {'description': 'Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.', 'seeds': ['self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn'], 'terms': ['self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn']}, 'deep_learning': {'description': 'Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.', 'seeds': ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning', 'multi_task_learning'], 'terms': ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning', 'multi_task_learning']}, 'ensemble_methods': {'description': 'Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.', 'seeds': ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting', 'model_selection', 'hyperparameter_tuning', 'cross_validation'], 'terms': ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting', 'model_selection', 'hyperparameter_tuning', 'cross_validation']}}}\n"
     ]
    }
   ],
   "source": [
    "# input: track, dimension -> get base taxonomy (2 levels) -> Class Tree, Class Node (description, seed words)\n",
    "\n",
    "taxo = Taxonomy(args.track, args.dim)\n",
    "base_taxo = taxo.buildBaseTaxo(levels=1, num_terms=20)\n",
    "\n",
    "print(base_taxo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the input keywords file for seetopic -> get phrases -> filter using LLM\n",
    "dir_name = (args.track + \"_\" + args.dim).lower().replace(\" \", \"_\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}\"):\n",
    "    os.makedirs(f\"SeeTopic/{dir_name}\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}/{dir_name}.txt\"):\n",
    "    shutil.copyfile(args.input_file, f\"SeeTopic/{dir_name}/{dir_name}.txt\")\n",
    "\n",
    "## get first level of children\n",
    "children_with_terms = taxo.root.getChildren(terms=True)\n",
    "with open(f\"SeeTopic/{dir_name}/keywords_0.txt\", \"w\") as f:\n",
    "    for idx, c in enumerate(children_with_terms):\n",
    "        str_c = \",\".join(c[1])\n",
    "        f.write(f\"{idx}:{c[0]},{str_c}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Types of Methodology Proposed in Text Classification Research Papers\": {\"description\": null, \"seeds\": null, \"terms\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\", \"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\", \"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\"], \"supervised_learning\": {\"description\": \"Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.\", \"seeds\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\"], \"terms\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\"]}, \"unsupervised_learning\": {\"description\": \"Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.\", \"seeds\": [\"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\"], \"terms\": [\"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\"]}, \"semi_supervised_learning\": {\"description\": \"Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.\", \"seeds\": [\"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\"], \"terms\": [\"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\"]}, \"deep_learning\": {\"description\": \"Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.\", \"seeds\": [\"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"transfer_learning\", \"multi_task_learning\"], \"terms\": [\"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"transfer_learning\", \"multi_task_learning\"]}, \"ensemble_methods\": {\"description\": \"Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.\", \"seeds\": [\"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"random_forest\", \"gradient_boosting\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\", \"model_selection\", \"hyperparameter_tuning\", \"cross_validation\"], \"terms\": [\"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"random_forest\", \"gradient_boosting\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\", \"model_selection\", \"hyperparameter_tuning\", \"cross_validation\"]}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phrase Mining for Level 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Get PLM Embeddings===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5359/5359 [00:43<00:00, 123.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Iter 0: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_1/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\t\n",
      "transfer_learning\tactive_learning\tsupervised_learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\t\n",
      "bagging\tboosting\tstacking\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_1/res_cate.txt\n",
      "\u001b[32m===Iter 1: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 2: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 2: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_2/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tself_organizing_map\tterm_weighting\tsupervised_term_weighting\t\n",
      "transfer_learning\tactive_learning\tsupervised_learning\tmeta_learning\tmeta-learning\trepresentation_learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_2/res_cate.txt\n",
      "\u001b[32m===Iter 2: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 3: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 3: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_3/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tneural_networks\tfeature_selection\tfeature_engineering\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tterm_weighting\tself_organizing_map\tsupervised_term_weighting\tclustering_algorithm\tstring_vectors\tmutual_information\t\n",
      "transfer_learning\tactive_learning\tmeta-learning\tmeta_learning\tsupervised_learning\trepresentation_learning\tlabeling_cost\tintent_detection\tadaptively\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\tcross-layer\tmasked_language_model\tgated_recurrent_unit\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\trandom_forests\tfeature_combination\tbase_learners\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_3/res_cate.txt\n",
      "\u001b[32m===Iter 3: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 4: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 4: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_4/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tneural_networks\tfeature_selection\tfeature_engineering\tdata_augmentation\tcross_validation\tbagging\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tself_organizing_map\tterm_weighting\tstring_vectors\tmutual_information\tclustering_algorithm\tsupervised_term_weighting\tnumerical_vectors\tsimilarity_measure\tstring_vector\t\n",
      "transfer_learning\tactive_learning\tmeta-learning\tmeta_learning\tlabeling_cost\tintent_detection\tsupervised_learning\trepresentation_learning\tadaptively\ttraining_examples\tjoint_modeling\tlow_dimensional\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\tcross-layer\tgated_recurrent_unit\tmasked_language_model\tneural_architecture\tdependency_graph\tbi-directional_long_short-term_memory\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\trandom_forests\tfeature_combination\tbase_learners\tensemble_learning\tcluster_based\tensemble_techniques\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_4/res_cate.txt\n",
      "\u001b[32m===Iter 4: Ensemble===\u001b[m\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"./SeeTopic\")\n",
    "subprocess.check_call(['./seetopic.sh', dir_name, str(args.iters), \"bert_full_ft\"])\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "supervised_learning: [supervised_learning, naive_bayes, decision_trees, random_forest, support_vector_machines, logistic_regression, k_nearest_neighbors, neural_networks, feature_selection, feature_engineering, data_augmentation, cross_validation, bagging, boosting, stacking, voting, feature_space, knn, document_frequency, weighting, statistical_methods, classification_algorithms, feature_weights, feature_vector, feature_values, membership, category_labels, maximum_entropy, feature_reduction, regularization, determination]\n",
      "---\n",
      "---\n",
      "unsupervised_learning: [unsupervised_learning, latent_semantic_analysis, topic_modeling, string_vectors, similarity_measure, numerical_vectors, self_organizing_map, clustering_algorithm, term_weighting, mutual_information, string_vector, vector_space_model, feature_transformation, vectorization, gibbs_sampling, word_vectors, feature_vectors, lsi, word_vector, som, sentence_similarity, cluster_analysis, vsm, latent_dirichlet_allocation, lsa, text_classification, clustering, faceted_classification, term_frequency]\n",
      "---\n",
      "---\n",
      "semi_supervised_learning: [semi_supervised_learning, transfer_learning, active_learning, meta_learning, intent_detection, joint_modeling, representation_learning, supervised_learning, contrastive_learning, ensemble_models, classifier_ensemble, intent_classification, relation_classification, trigger_detection, weakly_supervised, distant_supervised, multi_labeled, n_way, phenotype, annotator, annotators, crowdsourcing, learner, autoencoder, significant_impact, deception_detection]\n",
      "---\n",
      "---\n",
      "deep_learning: ['deep_learning', 'convolutional_neural_networks','recurrent_neural_networks', 'attention_mechanism', 'transformers', 'word_embeddings', 'transfer_learning', 'gated_recurrent_unit','masked_language_model', 'neural_architecture', 'word_representations', 'attention_based','mca-gru', 'nle', 'gru','multi-grained', 'attention_network', 'neural_network', 'protaugment', 'deep_neural_network', 'generative_model','self-attention','seq2seq', 'lm','multi-classifier', 'context-aware', 'artificial_neural_network', 'hidden_layer', 'cmcn', 'tensor', 'bi-directional_lstm','sequence-based', 'pre-trained_language_model', 'pseudo', 'task-specific', 'elmo', 'gated', 'generative','model-agnostic']\n",
      "---\n",
      "---\n",
      "ensemble_methods: [ensemble_methods, bagging, boosting, stacking, voting, random_forest, random_forests, base_learners, ensemble_learning, cluster_based, feature_combination, ensemble_techniques, rule_induction, feature_mapping, radial_basis_function, linear_kernel, base_classifiers, capsule_networks, gaussian_naive_bayes, k-nearest_neighbour, mnb, multilayer_perceptron, mlp-based, c4.5, attention_layer, vote, thresholding, adaptive_neuro-fuzzy, multinomial_logistic_regression, ensemble_classifier, adaboost, k-nearest_neighbor, rf]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "with open(f\"./SeeTopic/{dir_name}/keywords_seetopic.txt\", \"r\") as f:\n",
    "    children_phrases = [i.strip().split(\":\")[1].split(\",\") for i in f.readlines()]\n",
    "    filtered_children_phrases = []\n",
    "    for c_id, c in enumerate(taxo.root.children):\n",
    "        # filter the child phrases\n",
    "        child_phrases = filter_phrases(c, f\"{c}: {children_phrases[c_id]}\\n\")\n",
    "        filtered_children_phrases.append(child_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_id, c in enumerate(taxo.root.children):\n",
    "    c.addTerms(filtered_children_phrases[c_id], addToParent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised_learning: ['naive_bayes', 'logistic_regression', 'decision_trees', 'random_forest', 'support_vector_machines', 'k_nearest_neighbors', 'neural_networks', 'gradient_boosting', 'feature_extraction', 'text_features', 'supervised_learning', 'feature_transformation', 'rule_based', 'shot', 'sentence_classification', 'parameterization', 'capsule_network', 'commentary', 'feature_representations', 'opportunities', 'numerical_vectors', 'approaches', 'feature_vectors', '3a', 'phenotype', 'biomedical_abstracts', 'aspect-level_sentiment_classification', 'algerian', 'auto-tagging', 'open_information_extraction', 'maximum_entropy', 'pathways', 'string_vectors', 'tackling', 'bug_triaging', 'network-based', 'statistical_methods', 'online_handwritten', 'feature_vector', 'multi-lingual', 'dialog_act', 'machine_learning', 'multi-modal', 'gene_expression', 'cross-lingual', 'text_mining', 'document-level', 'decision_making', 'classification_algorithms', 'feature_space', 'social_event', 'conceptum', 'subsections', 'drifting', 'sparseness', 'bivalency', 'synonymy_and_polysemy', 'target-dependent', 'higher-order', 'specialized', 'small_sample', 'optimization_problem', 'gap', 'feature_weights', 'writing', 'feature_values', 'higher_education', 'error_propagation', 'begin', 'participation', 'segmenting', 'fitting', 'similarity_functions', 'ega', 'feature_weight', 'n-way', 'deals', 'methodological', 'logical', 'data_mining', 'assertion', 'match-lstm', 'multi-classification', 'relating', 'empirical_study', 'regularization', 'informed', 'overcoming', 'projection_method', 'handwritten_text', 'previous_works', 'information_management', 'sift', 'covering', 'feature_selection', 'feature_reduction', 'albert', 'ordinal', 'overfitting', 'industrial', 'cancer', 'experimentation', 'bag-of-concepts']\n",
      "unsupervised_learning: ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'topic_modeling', 'non_negative_matrix_factorization', 'word_embeddings', 'doc2vec', 'glove', 'word2vec', 'latent_semantic_analysis', 'unsupervised_learning', 'self_organizing_map', 'ensemble_classifier', 'vectorization', 'latent_semantic_indexing', 'classifier_ensemble', 'word_vector', 'classic_feature_selection', 'minimum_redundancy', 'chi', 'chi_square', 'supervised_topic_model', 'short_term_memory', 'genetic_algorithm', 'word_representation', 'petclu', 'tokenization', 'cso-lstmnn', 'document_representation', 'bat', 'text_classification', 'petc', 'word_vectors', 'generative_model', 'swarm', 'bee', 'text_generation', 'abc', 'document_clustering', 'pre-trained_word_embedding', 'associative_classification', 'topic_model', 'statistic', 'topic_modelling', 'svm-rfe', 'supervised_term_weighting', 'inception', 'vote', 'document-clustering', 'ga', 'word_segmentation', 'chi-square', 'weight_matrix', 'document_classification', 'long-term', 'keyword_extraction', 'nmffe', 'topic_models', 'regularized', 'hownet', 'latent_variable', 'dl-per', 'weighted_average', 'domain_generalization', 'accuracy_improvement', 'term_frequency_inverse_document_frequency', 'semantic_role_labeling', 'sliding_window', 'icvs', 'clustering_algorithm', 'som', 'term_weighting', 'lsi', 'particle_swarm_optimization', 'classifier', 'principal_component_analysis', 'search_algorithm', 'simulation', 'multi-classifier', 'or+svm-rfe', 'document_image_classification', 'constriction', 'vector_space_model', 'ig', 'optimizer', 'particle', 'thereafter', 'conceptual_graph-based', 'sentiment_analysis', '1d-som', 'textrank_algorithm', 'generative_models', 'augmented', 'parsing', 'sentiment_lexicon', 'kernels', 'short_text_classification', 'dui', 'meta-classifier', 'essa-acnn', 'sentiment_classification', 'pso', 'image_classification', 'files', 'generalization']\n",
      "semi_supervised_learning: ['self_training', 'co_training', 'generative_adversarial_networks', 'transfer_learning', 'multi_task_learning', 'active_learning', 'reinforcement_learning', 'curriculum_learning', 'learning_from_noisy_labels', 'learning_from_imbalanced_data', 'semi_supervised_learning', 'meta-learning', 'meta_learning', 'intent_detection', 'labeling_cost', 'contrastive_learning', 'feature_representation', 'representation_learning', 'pseudo-labeling', 'stance_classification', 'channeling', 'latency', 'binary_classification', 'assertion_classification', 'neural_architecture', 'multi-domain', 'supervised_learning', 'leopard', 'statistical_machine_translation', 'limits', 'deep_learning', 'labeling_efforts', 'task-specific', 'prompt_tuning', 'mismatch', 'transferability', 'dmix', 'ensemble_models', 'predictor', 'few-shot', 'stance_detection', 'paraphrasing', 'protaugment', 'prevents', 'semi-supervised_learning', 'german_language', 'self-supervised', 'text_representation', 'progressively', 'intent_classification', 'term-weighting_schemes', 'relation_classification', 'textual_representation', 'distant_supervised', 'self-supervised_learning', 'docscan', 'generalize', 'smt', 'nc-hgat', 'adaptability', 'cross_language', 'digitalization', 'expert_knowledge', 'self-pretraining', 'green', 'great_success', 'hyperbolic_space', 'joint_modeling', 'ee', 'training_examples', 'adaptively', 'plastics', 'suggests', 'domain_adaptation', 'lightweight', 'opens', 'learner', 'couple', 'setup', 'decoding', 'scene_text_recognition', 'https', 'mind', '17', 'inductive', 'mesh', 'bottlenecks', 'tackled', 'incrementally', 'resource-poor', 'projecting', 'administrative', 'splitting', 'weakly_supervised', 'adversarial_training', 'personalization', 'uncertainty-based', 'nl-lda', 'pacs', 'predictive', 'multi-task', 'autoencoder', 'multi-task_learning', 'extensive_evaluation', 'dynamically', 'hmlc', 'observations', 'class_labels', 'paradigms']\n",
      "deep_learning: ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'transformers', 'attention_mechanism', 'word_attention', 'character_level_models', 'subword_level_models', 'language_models', 'deep_learning', 'trigger_detection', 'bigru', 'dependency_graph', 'attention_layer', 'gated_recurrent_unit', 'attention_network', 'pattern_matching', 'masked_language_model', 'attention_based', 'cross-layer', 'mlcnn', 'feature-enhanced', 'fuse', 'multi-grained', 'sentence_representation', 'gru', 'divides', 'neural_network', 'gated', 'mca-gru', 'bi-directional_long_short-term_memory', 'multi-turn', 'hidden_layer', 'cross-modal', 'bidirectional_long_short-term_memory', 'word_representations', 'dual-channel', 'cmcn', 'hidden_markov_models', 'dblstm', 'biomedical_named_entity_recognition', 'self-attention', 'tensor', 'multimodal_sentiment_classification', 'stacked', 'clstm', 'convolutional', 'interactive_attention', 'bidirectional_lstm', 'low-cost', 'deep_belief_network', 'context-aware', 'heterogeneous_dependency', 'string_matching', 'short-term_memory', 'keras', 'gibbs_sampling', 'sentence_similarity', 'low-resource_languages', 'word_sequence', 'phrase2vec', 'speech_emotion_recognition', 'trains', 'sentence-level', 'nle', 'rigorous', 'pre-trained_language_model', 'bi-directional_lstm', 'skip-gram', 'significant_progress', 'anomaly_detection', 'nen', 'msc', 'bidirectional', 'temporal_relations', 'dnns', 'resolving', 'ds-caps', 'gaussian', 'hisans', 'semantic-interactive', 'similarity_measure', 'bilstm', 'latent_topic', 'mutual_information', 'visual-semantic', 'lstm', 'lm', 'pos_tagging', 'speech-to-intent_classification', 'dependencies', 'automatically_constructed', 'subtitle', 'rbm', 'voice_recognition', 'knowledge_base', 'motifclass', 'context-level', 'word_level', 'flair', 'mltc', 'word_frequency', 'primary_focus', 'tagset', 'knowledge_graph', 'relation_extraction']\n",
      "ensemble_methods: ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'feature_bagging', 'ensemble_methods', 'random_forests', 'base_learners', 'feature_combination', 'ensemble_learning', 'cluster_based', 'rule_induction', 'morphological_analysis', 'feature_mapping', 'ensemble_techniques', 'me', 'rbf', 'gaussian_naive_bayes', 'multilayer_perceptron', 'k-nearest_neighbour', 'dt', 'fuzzy_rule', 'mnb', 'bernoulli', 'capsule_networks', 'c4.5', 'distance_measure', 'squares', 'base_classifiers', 'joint_representation', 'complement', 'f2', 'pooling', 'radial_basis_function', 'chunking', 'linear_kernel', 'mfom', 'partitioning', 'mlp-based', 'c4.5_algorithm', 'f-scores', 'thresholding', 'non-symmetrical', 'catastrophic_forgetting', 'f0', 'response_generation', 'multinomial_naive_bayes', 'optimally', 'mrmr', 'emotional_states', 'forestexter', 'c1', 'training_samples', 'k10', 'zoning', 'unigram_and_bigram', 'k-nearest_neighbor', 'adaboost', 'df', 'em_algorithm', 'white-box', 'bottleneck', 'artificial_neural_networks', 'classification', 'spark', 'numerical_features', 'cat', 'intention_classification', 'memory-based', 'multinomial_logistic_regression', 'document_vectors', 'cross_validation', 'gss', '±', 'hog', 'deception_detection', 'nb', 'rf', 'odds', 'query_expansion', 'feed-forward', 'c5.0', 'effectively_improve', 'ac', 'orthogonality', 'unbalanced_data', 'ann', 'term_weighting_schemes', 'nearest_neighbor', 'generalizes', 'sgd', 'assigning', 'handcrafted_feature', 'cfs', 'k-nearest_neighbors', 'individually', 'multi-layer_perceptron', 'adaptive_neuro-fuzzy', 'term_frequency-inverse_document_frequency', 'automatically', 'handwritten_character_recognition', 'classifying']\n"
     ]
    }
   ],
   "source": [
    "phrase_terms = \"\"\n",
    "for c in taxo.root.children:\n",
    "    phrase_terms += f\"{c}: {c.all_node_terms}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse",
   "language": "python",
   "name": "inverse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
