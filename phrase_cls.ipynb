{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3,4\"\n",
    "from collections import Counter\n",
    "from taxonomy import Taxonomy, Paper\n",
    "from utils import filter_phrases, cosine_similarity_embeddings\n",
    "import subprocess\n",
    "import shutil\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.track = \"Text Classification\"\n",
    "        self.dim = \"Methodology\"\n",
    "        self.input_file = \"datasets/sample_1k.txt\"\n",
    "        self.iters = 4\n",
    "        self.model = \"bert_full_ft\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Taxonomy Construction & Reading in Papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pk36/inverse_knowledge_search/inverse/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Types of Methodology Proposed in Text Classification Research Papers': {'description': None, 'seeds': None, 'terms': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering', 'self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn', 'convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting'], 'supervised_learning': {'description': 'Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.', 'seeds': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting'], 'terms': ['naive_bayes', 'decision_trees', 'random_forest', 'support_vector_machines', 'logistic_regression', 'k_nearest_neighbors', 'gradient_boosting', 'neural_networks', 'feature_selection', 'feature_engineering', 'data_augmentation', 'cross_validation', 'hyperparameter_tuning', 'model_selection', 'ensemble_methods', 'bagging', 'boosting', 'stacking', 'voting', 'weighted_voting']}, 'unsupervised_learning': {'description': 'Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.', 'seeds': ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering'], 'terms': ['kmeans', 'hierarchical_clustering', 'density_based_clustering', 'dbscan', 'apriori', 'association_rule_learning', 'frequent_itemset_mining', 'decision_trees_for_clustering', 'self_organizing_maps', 'competitive_learning', 'non_negative_matrix_factorization', 'latent_semantic_analysis', 'topic_modeling', 'non_negative_factorization', 'matrix_factorization', 'collaborative_filtering', 'content_based_filtering']}, 'semi_supervised_learning': {'description': 'Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.', 'seeds': ['self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn'], 'terms': ['self_training', 'co_training', 'generative_adversarial_networks', 'semi_supervised_neural_networks', 'graph_based_methods', 'label_propagation', 'transductive_learning', 'inductive_learning', 'transfer_learning', 'multi_task_learning', 'few_shot_learning', 'active_learning', 'reinforcement_learning_for_semi_supervised_learning', 'curriculum_learning', 'learning_to_learn']}, 'deep_learning': {'description': 'Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.', 'seeds': ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning', 'multi_task_learning'], 'terms': ['convolutional_neural_networks', 'recurrent_neural_networks', 'long_short_term_memory', 'gated_recurrent_units', 'bidirectional_recurrent_units', 'attention_mechanism', 'transformers', 'word_embeddings', 'character_level_models', 'subword_level_models', 'language_models', 'pre_trained_word_embeddings', 'fine_tuning', 'transfer_learning', 'multi_task_learning']}, 'ensemble_methods': {'description': 'Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.', 'seeds': ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting', 'model_selection', 'hyperparameter_tuning', 'cross_validation'], 'terms': ['bagging', 'boosting', 'stacking', 'voting', 'weighted_voting', 'random_forest', 'gradient_boosting', 'neural_network_ensemble', 'decision_tree_ensemble', 'support_vector_machine_ensemble', 'k_nearest_neighbors_ensemble', 'feature_bagging', 'feature_boosting', 'model_selection', 'hyperparameter_tuning', 'cross_validation']}}}\n"
     ]
    }
   ],
   "source": [
    "# input: track, dimension -> get base taxonomy (2 levels) -> Class Tree, Class Node (description, seed words)\n",
    "\n",
    "taxo = Taxonomy(args.track, args.dim, args.input_file)\n",
    "base_taxo = taxo.buildBaseTaxo(levels=1, num_terms=20)\n",
    "\n",
    "print(base_taxo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the input keywords file for seetopic -> get phrases -> filter using LLM\n",
    "dir_name = (args.track + \"_\" + args.dim).lower().replace(\" \", \"_\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}\"):\n",
    "    os.makedirs(f\"SeeTopic/{dir_name}\")\n",
    "\n",
    "if not os.path.exists(f\"SeeTopic/{dir_name}/{dir_name}.txt\"):\n",
    "    shutil.copyfile(args.input_file, f\"SeeTopic/{dir_name}/{dir_name}.txt\")\n",
    "\n",
    "## get first level of children\n",
    "children_with_terms = taxo.root.getChildren(terms=True)\n",
    "with open(f\"SeeTopic/{dir_name}/keywords_0.txt\", \"w\") as f:\n",
    "    for idx, c in enumerate(children_with_terms):\n",
    "        str_c = \",\".join(c[1])\n",
    "        f.write(f\"{idx}:{c[0]},{str_c}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Types of Methodology Proposed in Text Classification Research Papers\": {\"description\": null, \"seeds\": null, \"terms\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\", \"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\", \"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\"], \"supervised_learning\": {\"description\": \"Approaches in text classification where the model is trained on labeled data, with the goal of predicting the correct label for a given text sample.\", \"seeds\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\"], \"terms\": [\"naive_bayes\", \"decision_trees\", \"random_forest\", \"support_vector_machines\", \"logistic_regression\", \"k_nearest_neighbors\", \"gradient_boosting\", \"neural_networks\", \"feature_selection\", \"feature_engineering\", \"data_augmentation\", \"cross_validation\", \"hyperparameter_tuning\", \"model_selection\", \"ensemble_methods\", \"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\"]}, \"unsupervised_learning\": {\"description\": \"Techniques in text classification where the model is trained on unlabeled data, with the goal of discovering patterns and relationships in the text.\", \"seeds\": [\"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\"], \"terms\": [\"kmeans\", \"hierarchical_clustering\", \"density_based_clustering\", \"dbscan\", \"apriori\", \"association_rule_learning\", \"frequent_itemset_mining\", \"decision_trees_for_clustering\", \"self_organizing_maps\", \"competitive_learning\", \"non_negative_matrix_factorization\", \"latent_semantic_analysis\", \"topic_modeling\", \"non_negative_factorization\", \"matrix_factorization\", \"collaborative_filtering\", \"content_based_filtering\"]}, \"semi_supervised_learning\": {\"description\": \"Approaches in text classification that combine both labeled and unlabeled data, leveraging the strengths of both supervised and unsupervised learning.\", \"seeds\": [\"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\"], \"terms\": [\"self_training\", \"co_training\", \"generative_adversarial_networks\", \"semi_supervised_neural_networks\", \"graph_based_methods\", \"label_propagation\", \"transductive_learning\", \"inductive_learning\", \"transfer_learning\", \"multi_task_learning\", \"few_shot_learning\", \"active_learning\", \"reinforcement_learning_for_semi_supervised_learning\", \"curriculum_learning\", \"learning_to_learn\"]}, \"deep_learning\": {\"description\": \"Techniques in text classification that utilize deep neural networks, often with convolutional and recurrent layers, to learn complex patterns in text data.\", \"seeds\": [\"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"transfer_learning\", \"multi_task_learning\"], \"terms\": [\"convolutional_neural_networks\", \"recurrent_neural_networks\", \"long_short_term_memory\", \"gated_recurrent_units\", \"bidirectional_recurrent_units\", \"attention_mechanism\", \"transformers\", \"word_embeddings\", \"character_level_models\", \"subword_level_models\", \"language_models\", \"pre_trained_word_embeddings\", \"fine_tuning\", \"transfer_learning\", \"multi_task_learning\"]}, \"ensemble_methods\": {\"description\": \"Strategies in text classification that combine the predictions of multiple models, often to improve the overall accuracy and robustness of the system.\", \"seeds\": [\"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"random_forest\", \"gradient_boosting\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\", \"model_selection\", \"hyperparameter_tuning\", \"cross_validation\"], \"terms\": [\"bagging\", \"boosting\", \"stacking\", \"voting\", \"weighted_voting\", \"random_forest\", \"gradient_boosting\", \"neural_network_ensemble\", \"decision_tree_ensemble\", \"support_vector_machine_ensemble\", \"k_nearest_neighbors_ensemble\", \"feature_bagging\", \"feature_boosting\", \"model_selection\", \"hyperparameter_tuning\", \"cross_validation\"]}}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phrase Mining for Level 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Get PLM Embeddings===\u001b[m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/pk36/Comparative-Summarization/bert_full_ft/checkpoint-8346/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5359/5359 [00:45<00:00, 118.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m===Iter 0: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 1: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_1/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\t\n",
      "transfer_learning\tactive_learning\tsupervised_learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\t\n",
      "bagging\tboosting\tstacking\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_1/res_cate.txt\n",
      "\u001b[32m===Iter 1: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 2: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 2: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_2/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\t\n",
      "latent_semantic_analysis\ttopic_modeling\tclassic_feature_selection\tself_organizing_map\tterm_weighting\tensemble_classifier\t\n",
      "transfer_learning\tactive_learning\tsupervised_learning\tlabeling_cost\tmeta-learning\tmeta_learning\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_2/res_cate.txt\n",
      "\u001b[32m===Iter 2: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 3: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 3: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_3/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tneural_networks\tfeature_selection\tfeature_engineering\t\n",
      "latent_semantic_analysis\ttopic_modeling\tensemble_classifier\tclassic_feature_selection\tterm_weighting\tself_organizing_map\tstring_vectors\tclustering_algorithm\tgenetic_algorithm\t\n",
      "transfer_learning\tactive_learning\tlabeling_cost\tmeta-learning\tsupervised_learning\tmeta_learning\ttraining_examples\tstance_classification\tunrestricted\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\tcross-layer\tneural_architecture\tgated_recurrent_unit\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\trandom_forests\tensemble_learning\tfeature_combination\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_3/res_cate.txt\n",
      "\u001b[32m===Iter 3: Ensemble===\u001b[m\n",
      "\u001b[32m===Iter 4: PLM Module===\u001b[m\n",
      "\u001b[32m===Iter 4: Local Module===\u001b[m\n",
      "make: 'cate' is up to date.\n",
      "Starting training using file ../text_classification_methodology/text_classification_methodology.txt\n",
      "Reading topics from file text_classification_methodology_4/keywords.txt\n",
      "Vocab size: 5312\n",
      "Words in train file: 186462\n",
      "Read 5 topics\n",
      "naive_bayes\tdecision_trees\trandom_forest\tsupport_vector_machines\tlogistic_regression\tk_nearest_neighbors\tneural_networks\tfeature_selection\tfeature_engineering\tdata_augmentation\tcross_validation\tbagging\t\n",
      "latent_semantic_analysis\ttopic_modeling\tensemble_classifier\tclassic_feature_selection\tself_organizing_map\tgenetic_algorithm\tterm_weighting\tstring_vectors\tclustering_algorithm\tsimilarity_measure\tnumerical_vectors\tsparseness\t\n",
      "transfer_learning\tactive_learning\tlabeling_cost\tmeta-learning\tmeta_learning\ttraining_examples\tsupervised_learning\tunrestricted\tstance_classification\totherwise\tdeep_learning\treadings\t\n",
      "convolutional_neural_networks\trecurrent_neural_networks\tattention_mechanism\ttransformers\tword_embeddings\ttransfer_learning\tcross-layer\tgated_recurrent_unit\tneural_architecture\tmasked_language_model\tdeep_neural_networks\tdependency_graph\t\n",
      "bagging\tboosting\tstacking\tvoting\trandom_forest\tcross_validation\trandom_forests\tfeature_combination\tensemble_learning\trule_induction\tbase_learners\tcluster_based\t\n",
      "Pre-training for 2 epochs, in total 2 + 10 = 12 epochs\n",
      "Topic mining results written to file text_classification_methodology_4/res_cate.txt\n",
      "\u001b[32m===Iter 4: Ensemble===\u001b[m\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"./SeeTopic\")\n",
    "subprocess.check_call(['./seetopic.sh', dir_name, str(args.iters), args.model])\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./SeeTopic/{dir_name}/keywords_seetopic.txt\", \"r\") as f:\n",
    "    children_phrases = [i.strip().split(\":\")[1].split(\",\") for i in f.readlines()]\n",
    "    filtered_children_phrases = []\n",
    "    for c_id, c in enumerate(taxo.root.children):\n",
    "        # filter the child phrases\n",
    "        child_phrases = filter_phrases(c, f\"{c}: {children_phrases[c_id]}\\n\")\n",
    "        filtered_children_phrases.append(child_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_id, c in enumerate(taxo.root.children):\n",
    "    c.addTerms(filtered_children_phrases[c_id], addToParent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get initial, exact-matching pool of papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Types of Methodology Proposed in Text Classification Research Papers"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparative_analysis of binary classifiers on an array of scientific_publications\n",
      "hybrid supervised clustering based ensemble scheme for text_classification\n",
      "automatic polarity identification on twitter using machine_learning\n",
      "application of bagging_ensemble classifier based on genetic_algorithm in the text_classification of railway fault hazards\n",
      "comparison of machine_learning for sentiment_analysis in detecting anxiety based on social_media data\n",
      "predicting software defect severity_level using sentence_embedding and ensemble_learning\n",
      "forestexter : an efficient random_forest algorithm for imbalanced text_categorization\n",
      "enhanced malay sentiment_analysis with an ensemble classification machine_learning approach\n",
      "sentiment_analysis of chinese_micro-blog using semantic sentiment space model\n",
      "an ensemble model for stance_detection in social_media texts\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(taxo.root.children[4].papers, key=lambda x: x[0], reverse=True)[:10]:\n",
    "    print(i[1].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node-Oriented Sentence Representations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2emb = {}\n",
    "with open(f'./SeeTopic/{dir_name}/embedding_{args.model}.txt') as fin:\n",
    "\tfor line in fin:\n",
    "\t\tdata = line.strip().split()\n",
    "\t\tif len(data) != 769:\n",
    "\t\t\tcontinue\n",
    "\t\tword = data[0]\n",
    "\t\temb = np.array([float(x) for x in data[1:]])\n",
    "\t\temb = emb / np.linalg.norm(emb)\n",
    "\t\tword2emb[word] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kmeans',\n",
       " 'hierarchical_clustering',\n",
       " 'density_based_clustering',\n",
       " 'dbscan',\n",
       " 'apriori',\n",
       " 'association_rule_learning',\n",
       " 'frequent_itemset_mining',\n",
       " 'decision_trees_for_clustering',\n",
       " 'self_organizing_maps',\n",
       " 'competitive_learning',\n",
       " 'non_negative_matrix_factorization',\n",
       " 'latent_semantic_analysis',\n",
       " 'topic_modeling',\n",
       " 'non_negative_factorization',\n",
       " 'matrix_factorization',\n",
       " 'collaborative_filtering',\n",
       " 'content_based_filtering',\n",
       " 'unsupervised_learning',\n",
       " 'string_vectors',\n",
       " 'similarity_measure',\n",
       " 'numerical_vectors',\n",
       " 'self_organizing_map',\n",
       " 'clustering_algorithm',\n",
       " 'mutual_information',\n",
       " 'vector_space_model',\n",
       " 'text_representation',\n",
       " 'word_vectors',\n",
       " 'term_frequency',\n",
       " 'feature_vectors',\n",
       " 'word_vector',\n",
       " 'som',\n",
       " 'text_generation',\n",
       " 'vector_space',\n",
       " 'relation_extraction',\n",
       " 'clustering',\n",
       " 'vsm']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class representations\n",
    "class_reprs = []\n",
    "taxo.root.children[1].all_node_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66561028]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_embeddings([word2emb[\"kmeans\"]], \n",
    "                             [word2emb[\"hierarchical_clustering\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [[] for i in taxo.root.children]\n",
    "unmapped = []\n",
    "\n",
    "for p in range(len(collection)):\n",
    "    class_freq = [0] * len(taxo.root.children)\n",
    "\n",
    "    for c_id, c in enumerate(taxo.root.children):\n",
    "        # how many total mentions of the node terms\n",
    "        class_freq[c_id] = np.sum([collection[p].vocabulary[ele] for ele in c.all_node_terms if ele in collection[p].vocabulary.keys()])\n",
    "    \n",
    "    nonzero_idx = np.nonzero(class_freq)[0]\n",
    "    if len(nonzero_idx) == 0:\n",
    "        unmapped.append(p)\n",
    "        continue\n",
    "\n",
    "    for i in nonzero_idx:\n",
    "        # score: class_i_mentions / log(total_len)\n",
    "        score = class_freq[i] / np.log(collection[p].length)\n",
    "        classes[i].append((score, p))\n",
    "\n",
    "classes = [sorted(c, reverse=True) for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unmapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bagging',\n",
       " 'boosting',\n",
       " 'stacking',\n",
       " 'voting',\n",
       " 'weighted_voting',\n",
       " 'random_forest',\n",
       " 'gradient_boosting',\n",
       " 'neural_network_ensemble',\n",
       " 'decision_tree_ensemble',\n",
       " 'support_vector_machine_ensemble',\n",
       " 'k_nearest_neighbors_ensemble',\n",
       " 'feature_bagging',\n",
       " 'feature_boosting',\n",
       " 'model_selection',\n",
       " 'hyperparameter_tuning',\n",
       " 'cross_validation',\n",
       " 'ensemble_methods',\n",
       " 'random_forests',\n",
       " 'base_learners',\n",
       " 'ensemble_learning',\n",
       " 'feature_combination',\n",
       " 'ensemble_techniques',\n",
       " 'cluster_based',\n",
       " 'rbf',\n",
       " 'mnb',\n",
       " 'dt',\n",
       " 'radial_basis_function',\n",
       " 'base_classifiers',\n",
       " 'gaussian_naive_bayes',\n",
       " 'multilayer_perceptron',\n",
       " 'c4.5',\n",
       " 'adaboost',\n",
       " 'attention_layer',\n",
       " 'feed-forward',\n",
       " 'thresholding',\n",
       " 'multinomial_logistic_regression',\n",
       " 'ensemble_classifier',\n",
       " 'memory-based',\n",
       " 'k-nearest_neighbor',\n",
       " 'nearest_neighbor',\n",
       " 'principal_component_analysis']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxo.root.children[-1].all_node_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised_learning\n",
      "15 arabic_text_categorization via binary particle_swarm_optimization and support_vector_machines ; abstract : document_categorization concerns automatically assigning a category label to a text document , and has increasingly many applications , particularly in the domains of organizing , browsing and search in large_document_collections . it is typically achieved via machine_learning , where a model is built on the basis of a ( typically ) large collection of document features . feature_selection is critical in this process , since there are typically several thousand potential features ( distinct words or terms ) . here we explore binary particle_swarm_optimization ( bpso ) hybridized with either k-nearest-neighbour ( knn ) or a support_vector_machine ( svm ) , for feature_selection in arabic document_categorization tasks . comparison between feature_selection methods is done on the basis of using the selected features , in conjunction with each of svm , c4.5 and naive_bayes , to classify a holdout test set . using publicly available datasets , we show that the bpsosvm approach seems promising in this domain . we also analyse the sets of selected features and consider the differences between the types of feature that bpsoknn and bpsosvm tend to choose\n",
      "16 a novel approach for ontology-based dimensionality_reduction for web text document_classification ; abstract : dimensionality_reduction of feature_vector size plays a vital role in enhancing the text processing capabilities ; it aims in reducing the size of the feature_vector used in the mining tasks ( classification , clustering ... etc. ) . this paper proposes an efficient approach to be used in reducing the size of the feature_vector for web text document_classification process . this approach is based on using wordnet ontology , utilizing the benefit of its hierarchal structure , to eliminate words from the generated feature_vector that has no relation with any of wordnet lexical categories\n",
      "449 25th pacific-asia conference on knowledge discovery and data_mining , pakdd 2021 ; abstract : the proceedings contain 157 papers . the special focus in this conference is on knowledge discovery and data_mining . the topics include : self-supervised graph_representation learning with variational_inference ; manifold approximation and projection by maximizing graph information ; learning attention-based translational knowledge_graph embedding via nonlinear dynamic mapping ; multi-grained dependency_graph neural_network for chinese open_information_extraction ; human-understandable decision_making for visual_recognition ; lightcake : a lightweight framework for context-aware knowledge_graph embedding ; transferring domain_knowledge with an adviser in continuous tasks ; inferring hierarchical mixture structures : a bayesian nonparametric approach ; quality_control for hierarchical classification with incomplete annotations ; learning discriminative_features using multi-label dual_space ; universal representation for code ; autocluster : meta-learning based ensemble_method for automated unsupervised_clustering ; banditrank : learning to rank using contextual bandits ; a compressed and accelerated segnet for plant leaf disease segmentation : a differential_evolution based approach ; meta-context transformers for domain-specific response_generation ; a multi-task kernel learning algorithm for survival_analysis ; meta-data augmentation based search strategy through generative_adversarial_network for automl model_selection ; tree-capsule : tree-structured capsule_network for improving relation_extraction ; rule injection-based generative_adversarial imitation learning for knowledge_graph reasoning ; hierarchical self attention_based autoencoder for open-set human activity_recognition ; reinforced natural_language inference for distantly_supervised_relation classification ; self-supervised adaptive aggregator learning on graph ; sagcn : structure-aware graph convolution network for document-level relation_extraction ; addressing the class_imbalance problem in medical image_segmentation via accelerated tversky loss_function ; incorporating relational knowledge in explainable fake_news_detection\n",
      "373 estimating the generalization performance of polynomial svm classifier for text_categorization\n",
      "354 a hybrid documents classification based on svm and rough_sets\n",
      "280 automatic polarity identification on twitter using machine_learning ; abstract : this work presents a study of emotions to analyze the polarity of a set of data that was extracted from twitter , detailing each of the resources in the different forms that a language has , and to be able to observe feelings such as irony , sarcasm , and happiness , among others . this research can help us classify the polarity of each one of them deeply in the corpus that deals with this research work . experimental results conducted using different machine_learning methods are presented : support_vector_machines , naïve_bayes , logistic_regression , knn and random_forest , with which a classification system based on cross-validation was implemented . all experiments were performed in python . the results obtained are shown with two different corpus ; where the first set is made up of 10,653 tweets in total divided equally each with 3551 tweets with a positive , negative and neutral label\n",
      "184 margin maximization with feed-forward neural_networks : a comparative study with svm and adaboost\n",
      "236 classification of forensic autopsy reports through conceptual_graph-based document_representation model\n",
      "335 forestexter : an efficient random_forest algorithm for imbalanced text_categorization\n",
      "946 an efficient approach for ensemble of svm and ann for sentiment_classification\n",
      "\n",
      "\n",
      "unsupervised_learning\n",
      "111 deep graph neural_networks for text_classification task ; abstract : text_classification is to organizing documents into predetermined_categories , usually by machinery learn algorithms . it is a significant ways to organize and utilize the large amount of information that exists in unstructured_text format . text_classification is an important module in text processing , and its applications are also very extensive , such as garbage filtering , news classification , part-of-speech_tagging , and so on . with the continuous development of deep_learning in recent_years ! its applications are also very extensive , such as : garbage filtering , news classification , part-of-speech_tagging , and so on . but the text also has its own characteristics . according to the characteristics of the text , the general process of text_classification is : 1 . preprocessing ; 2 . text_representation and feature_selection ; 3 . construction of a classifier\n",
      "237 hybrid supervised clustering based ensemble scheme for text_classification\n",
      "144 a text feature_selection method using the improved mutual_information and information entropy\n",
      "159 a tensor space model-based deep_neural_network for text_classification ; abstract : most text_classification systems use machine_learning algorithms ; among these , naïve_bayes and support_vector_machine algorithms adapted to handle text data afford reasonable_performance . recently , given developments in deep_learning technology , several scholars have used deep_neural_networks ( recurrent and convolutional_neural_networks ) to improve text_classification . however , deep_learning-based text_classification has not greatly_improved performance compared to that of conventional algorithms . this is because a textual document is essentially expressed as a vector ( only ) , albeit with word dimensions , which compromises the inherent semantic information , even if the vector is ( appropriately ) transformed to add conceptual information . to solve this ‘ loss of term senses ’ problem , we develop a concept-driven deep_neural_network based upon our semantic tensor space model . the semantic tensor used for text_representation features a dependency between the term and the concept\n",
      "273 research convey on text_classification method based on deep_learning\n",
      "206 a component clustering_algorithm based on semantic similarity and optimization\n",
      "967 research progress of text_classification technology based on deep_learning\n",
      "734 a method of text_classification based on statistical technology and set_theory\n",
      "346 a comparative research of different granularities in korean text_classification ; abstract : text_classification is a process , which can make the specified documents group into several categories , predefined at the beginning through learning a series of rules or under the guidance of the goal function . this paper compared the subword-level , spacing-level of korean and the word-level , then analyzed the influence of the preprocessing of different granularities on the text_classification task of korean . after that , analyzed the results of classification linguistically . thus we can choose the proper granularity as the input to improve the classification effect . firstly , cut the corpus according to different granularities\n",
      "910 knowledge-based clustering scheme for collection management and retrieval of library books ; abstract : we propose a knowledge-based clustering scheme for grouping books in a library . such a grouping is achieved with the help of domain_knowledge in the form of the acm cr ( computing reviews ) category hierarchy . a new knowledge-based similarity_measure is defined and used in clustering books . the proposed scheme is useful in overcoming several problems associated with the existing book collection management and document_retrieval systems . more specifically , it can be used in : ( 1 ) helping the user select an appropriate collection of books in a library which contains the topics of interest ; ( 2 ) assigning a classification number to a new book ; ( 3 ) designing a more appropriate and uniform classification_scheme for books\n",
      "\n",
      "\n",
      "semi_supervised_learning\n",
      "122 meta_learning for few-shot joint intent_detection_and_slot-filling\n",
      "516 batch active_learning for text_classification and sentiment_analysis\n",
      "227 multi-domain active_learning for text_classification\n",
      "246 revisiting uncertainty-based query_strategies for active_learning with transformers\n",
      "773 cutting the error by half : investigation of very deep cnn and advanced training strategies for document_image_classification ; abstract : we present an exhaustive investigation of recent deep_learning architectures , algorithms , and strategies for the task of document_image_classification to finally reduce the error by more than half . existing_approaches , such as the deepdoc-classifier , apply standard convolutional_network architectures with transfer_learning from the object_recognition domain . the contribution of the paper is threefold : first , it investigates recently introduced very deep_neural_network architectures ( googlenet , vgg , resnet ) using transfer_learning ( from real images ) . second , it proposes transfer_learning from a huge set of document_images , i.e . 400\n",
      "661 protaugment : unsupervised diverse short-texts paraphrasing for intent_detection meta-learning\n",
      "338 exploiting the matching information in the support set for few shot event classification\n",
      "525 active_learning in automated text_classification : a case_study exploring bias in predicted model performance_metrics\n",
      "389 classifying syntactic errors in learner language\n",
      "777 enriching pre-trained_language_model with entity information for relation_classification\n",
      "\n",
      "\n",
      "deep_learning\n",
      "293 a multi-scale convolutional attention_based gru network for text_classification\n",
      "1 a novel model combining transformer and bi-lstm for news categorization ; abstract : news categorization ( nc ) , the aim of which is to identify distinct categories of news through analyzing the contents , has acquired substantial progress since deep_learning was introduced into the natural_language_processing ( nlp ) field . as a state-of-art model , transformer & # x2019\n",
      "776 an automatic method using hybrid neural_networks and attention_mechanism for software_bug triaging\n",
      "199 phrase2vec : phrase embedding based on parsing\n",
      "379 an integrated fuzzy neural_network with topic-aware auto-encoding for sentiment_analysis\n",
      "773 cutting the error by half : investigation of very deep cnn and advanced training strategies for document_image_classification ; abstract : we present an exhaustive investigation of recent deep_learning architectures , algorithms , and strategies for the task of document_image_classification to finally reduce the error by more than half . existing_approaches , such as the deepdoc-classifier , apply standard convolutional_network architectures with transfer_learning from the object_recognition domain . the contribution of the paper is threefold : first , it investigates recently introduced very deep_neural_network architectures ( googlenet , vgg , resnet ) using transfer_learning ( from real images ) . second , it proposes transfer_learning from a huge set of document_images , i.e . 400\n",
      "398 pseudo-labeling with transformers for improving question_answering systems\n",
      "449 25th pacific-asia conference on knowledge discovery and data_mining , pakdd 2021 ; abstract : the proceedings contain 157 papers . the special focus in this conference is on knowledge discovery and data_mining . the topics include : self-supervised graph_representation learning with variational_inference ; manifold approximation and projection by maximizing graph information ; learning attention-based translational knowledge_graph embedding via nonlinear dynamic mapping ; multi-grained dependency_graph neural_network for chinese open_information_extraction ; human-understandable decision_making for visual_recognition ; lightcake : a lightweight framework for context-aware knowledge_graph embedding ; transferring domain_knowledge with an adviser in continuous tasks ; inferring hierarchical mixture structures : a bayesian nonparametric approach ; quality_control for hierarchical classification with incomplete annotations ; learning discriminative_features using multi-label dual_space ; universal representation for code ; autocluster : meta-learning based ensemble_method for automated unsupervised_clustering ; banditrank : learning to rank using contextual bandits ; a compressed and accelerated segnet for plant leaf disease segmentation : a differential_evolution based approach ; meta-context transformers for domain-specific response_generation ; a multi-task kernel learning algorithm for survival_analysis ; meta-data augmentation based search strategy through generative_adversarial_network for automl model_selection ; tree-capsule : tree-structured capsule_network for improving relation_extraction ; rule injection-based generative_adversarial imitation learning for knowledge_graph reasoning ; hierarchical self attention_based autoencoder for open-set human activity_recognition ; reinforced natural_language inference for distantly_supervised_relation classification ; self-supervised adaptive aggregator learning on graph ; sagcn : structure-aware graph convolution network for document-level relation_extraction ; addressing the class_imbalance problem in medical image_segmentation via accelerated tversky loss_function ; incorporating relational knowledge in explainable fake_news_detection\n",
      "890 discriminative learning of generative_models : large_margin multinomial mixture_models for document_classification\n",
      "837 improving transformer-based end-to-end speech_recognition with connectionist temporal classification and language model integration\n",
      "\n",
      "\n",
      "ensemble_methods\n",
      "941 document_classification using symbolic classifiers ; abstract : in this paper , we present symbolic classifiers to classify text documents . we propose to use cluster_based symbolic_representation followed by symbolic feature_selection methods to classify text documents . in particular , we propose symbolic clustering approaches ; symbolic cluster_based without feature_selection ; symbolic cluster_based with feature_selection ( using similarity_measure )\n",
      "827 application of bagging_ensemble classifier based on genetic_algorithm in the text_classification of railway fault hazards\n",
      "47 comparative_analysis of binary classifiers on an array of scientific_publications\n",
      "141 predicting software defect severity_level using sentence_embedding and ensemble_learning\n",
      "237 hybrid supervised clustering based ensemble scheme for text_classification\n",
      "280 automatic polarity identification on twitter using machine_learning ; abstract : this work presents a study of emotions to analyze the polarity of a set of data that was extracted from twitter , detailing each of the resources in the different forms that a language has , and to be able to observe feelings such as irony , sarcasm , and happiness , among others . this research can help us classify the polarity of each one of them deeply in the corpus that deals with this research work . experimental results conducted using different machine_learning methods are presented : support_vector_machines , naïve_bayes , logistic_regression , knn and random_forest , with which a classification system based on cross-validation was implemented . all experiments were performed in python . the results obtained are shown with two different corpus ; where the first set is made up of 10,653 tweets in total divided equally each with 3551 tweets with a positive , negative and neutral label\n",
      "512 enhanced malay sentiment_analysis with an ensemble classification machine_learning approach\n",
      "59 an ensemble model for stance_detection in social_media texts\n",
      "817 an intelligent hybrid sentiment_analyzer for personal protective medical equipments based on word_embedding technique : the covid-19 era\n",
      "732 a scalable meta-classifier combining search and classification techniques for multi-level text_categorization\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(classes):\n",
    "    print(taxo.root.children[idx])\n",
    "    for p in i[:10]:\n",
    "        print(p[-1], collection[p[-1]].title)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse",
   "language": "python",
   "name": "inverse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
